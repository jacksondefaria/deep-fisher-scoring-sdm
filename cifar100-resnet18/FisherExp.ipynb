{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e0659f",
   "metadata": {
    "papermill": {
     "duration": 0.007764,
     "end_time": "2023-01-17T01:43:30.726202",
     "exception": false,
     "start_time": "2023-01-17T01:43:30.718438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fisher Exponential\n",
    "\n",
    "Now that we made several advances in the FisherFIFO algorithm, maybe we can lend some of these improvements to the FisherExponential as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e076443",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:30.741052Z",
     "iopub.status.busy": "2023-01-17T01:43:30.740347Z",
     "iopub.status.idle": "2023-01-17T01:43:32.016693Z",
     "shell.execute_reply": "2023-01-17T01:43:32.015751Z"
    },
    "papermill": {
     "duration": 1.286529,
     "end_time": "2023-01-17T01:43:32.019257",
     "exception": false,
     "start_time": "2023-01-17T01:43:30.732728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import gp_minimize\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d6e994e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:32.034780Z",
     "iopub.status.busy": "2023-01-17T01:43:32.033384Z",
     "iopub.status.idle": "2023-01-17T01:43:32.038939Z",
     "shell.execute_reply": "2023-01-17T01:43:32.038096Z"
    },
    "papermill": {
     "duration": 0.015063,
     "end_time": "2023-01-17T01:43:32.041042",
     "exception": false,
     "start_time": "2023-01-17T01:43:32.025979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d268eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:32.055000Z",
     "iopub.status.busy": "2023-01-17T01:43:32.054725Z",
     "iopub.status.idle": "2023-01-17T01:43:33.970661Z",
     "shell.execute_reply": "2023-01-17T01:43:33.969749Z"
    },
    "papermill": {
     "duration": 1.92578,
     "end_time": "2023-01-17T01:43:33.973144",
     "exception": false,
     "start_time": "2023-01-17T01:43:32.047364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0a9014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:33.994420Z",
     "iopub.status.busy": "2023-01-17T01:43:33.994015Z",
     "iopub.status.idle": "2023-01-17T01:43:34.000090Z",
     "shell.execute_reply": "2023-01-17T01:43:33.999201Z"
    },
    "papermill": {
     "duration": 0.01886,
     "end_time": "2023-01-17T01:43:34.002115",
     "exception": false,
     "start_time": "2023-01-17T01:43:33.983255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_device():    \n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        device = torch.device('cuda')\n",
    "        print( torch.cuda.get_device_name(device) )\n",
    "        print( torch.cuda.get_device_properties(device) )\n",
    "\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(device)\n",
    "        \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74af1149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:34.024157Z",
     "iopub.status.busy": "2023-01-17T01:43:34.023850Z",
     "iopub.status.idle": "2023-01-17T01:43:45.637130Z",
     "shell.execute_reply": "2023-01-17T01:43:45.636015Z"
    },
    "papermill": {
     "duration": 11.627827,
     "end_time": "2023-01-17T01:43:45.639613",
     "exception": false,
     "start_time": "2023-01-17T01:43:34.011786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ff962a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:45.655735Z",
     "iopub.status.busy": "2023-01-17T01:43:45.654758Z",
     "iopub.status.idle": "2023-01-17T01:43:45.716287Z",
     "shell.execute_reply": "2023-01-17T01:43:45.714811Z"
    },
    "papermill": {
     "duration": 0.071589,
     "end_time": "2023-01-17T01:43:45.718307",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.646718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla P100-PCIE-16GB\n",
      "_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n"
     ]
    }
   ],
   "source": [
    "class cfg:\n",
    "    # n_features = 28 * 28\n",
    "    img_size = (32, 32)\n",
    "    img_channels = 3\n",
    "    n_classes = 100  ## we have 100 classes in CIFAR100\n",
    "    \n",
    "    # device = torch.device('cpu')\n",
    "    device = get_device()\n",
    "    \n",
    "    max_loss = 20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df7ca7",
   "metadata": {
    "papermill": {
     "duration": 0.007045,
     "end_time": "2023-01-17T01:43:45.732738",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.725693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "105274d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:45.747199Z",
     "iopub.status.busy": "2023-01-17T01:43:45.746910Z",
     "iopub.status.idle": "2023-01-17T01:43:45.753173Z",
     "shell.execute_reply": "2023-01-17T01:43:45.752156Z"
    },
    "papermill": {
     "duration": 0.01571,
     "end_time": "2023-01-17T01:43:45.755097",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.739387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset_mnist(batch_size):\n",
    "    print(f'generating MNIST data with {cfg.n_classes} classes')\n",
    "    \n",
    "    transf_ = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize(size=[14, 14]),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transf_)\n",
    "    mnist_test  = datasets.MNIST(root='./data', train=False, download=True, transform=transf_)\n",
    "    \n",
    "    mnist_train_dataloader = DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True)\n",
    "    mnist_test_dataloader  = DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return mnist_train_dataloader, mnist_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820312fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:45.770361Z",
     "iopub.status.busy": "2023-01-17T01:43:45.769529Z",
     "iopub.status.idle": "2023-01-17T01:43:45.776059Z",
     "shell.execute_reply": "2023-01-17T01:43:45.775226Z"
    },
    "papermill": {
     "duration": 0.016256,
     "end_time": "2023-01-17T01:43:45.778025",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.761769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset_cifar10(batch_size):\n",
    "    print(f'generating CIFAR10 data with {cfg.n_classes} classes')\n",
    "    \n",
    "    transf_ = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize(size=[14, 14]),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transf_)\n",
    "    cifar10_test  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transf_)\n",
    "    \n",
    "    cifar10_train_dataloader = DataLoader(dataset=cifar10_train, batch_size=batch_size, shuffle=True)\n",
    "    cifar10_test_dataloader  = DataLoader(dataset=cifar10_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return cifar10_train_dataloader, cifar10_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fed4262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:45.792907Z",
     "iopub.status.busy": "2023-01-17T01:43:45.792056Z",
     "iopub.status.idle": "2023-01-17T01:43:45.798342Z",
     "shell.execute_reply": "2023-01-17T01:43:45.797529Z"
    },
    "papermill": {
     "duration": 0.015656,
     "end_time": "2023-01-17T01:43:45.800283",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.784627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset_cifar100(batch_size):\n",
    "    print(f'generating CIFAR100 data with {cfg.n_classes} classes')\n",
    "    \n",
    "    transf_ = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    cifar100_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transf_)\n",
    "    cifar100_test  = datasets.CIFAR100(root='./data', train=False, download=True, transform=transf_)\n",
    "    \n",
    "    cifar100_train_dataloader = DataLoader(dataset=cifar100_train, batch_size=batch_size, shuffle=True)\n",
    "    cifar100_test_dataloader  = DataLoader(dataset=cifar100_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return cifar100_train_dataloader, cifar100_test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbaf501",
   "metadata": {
    "papermill": {
     "duration": 0.006464,
     "end_time": "2023-01-17T01:43:45.813387",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.806923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## declaring network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12073c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:45.828012Z",
     "iopub.status.busy": "2023-01-17T01:43:45.827476Z",
     "iopub.status.idle": "2023-01-17T01:43:45.833436Z",
     "shell.execute_reply": "2023-01-17T01:43:45.832466Z"
    },
    "papermill": {
     "duration": 0.015259,
     "end_time": "2023-01-17T01:43:45.835322",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.820063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_default_network(c=16, device=cfg.device):\n",
    "    net = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=cfg.n_features, out_features=c),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=c, out_features=c),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=c, out_features=c),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=c, out_features=cfg.n_classes)\n",
    "    )\n",
    "    \n",
    "    torchsummary.summary(net, input_size=[[cfg.n_features]], device='cpu')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9268fac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:45.850092Z",
     "iopub.status.busy": "2023-01-17T01:43:45.849302Z",
     "iopub.status.idle": "2023-01-17T01:43:45.858636Z",
     "shell.execute_reply": "2023-01-17T01:43:45.857816Z"
    },
    "papermill": {
     "duration": 0.018714,
     "end_time": "2023-01-17T01:43:45.860582",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.841868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cnn_network(in_channels=cfg.img_channels, c=16, p_drop=0.1, device=cfg.device):\n",
    "    \n",
    "    img_flat_size = (4 * c * (cfg.img_size[0] // 8) * (cfg.img_size[1] // 8) )\n",
    "    print(img_flat_size)\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=c, kernel_size=5, stride=2, padding=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(in_channels=c, out_channels=(2 * c), kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout2d(p=p_drop),\n",
    "        \n",
    "        nn.Conv2d(in_channels=(2 * c), out_channels=(4 * c), kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout2d(p=p_drop),\n",
    "        \n",
    "        nn.Flatten(),\n",
    "        \n",
    "        nn.Linear(in_features=img_flat_size, out_features=(8 * c) ),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=p_drop),\n",
    "        \n",
    "        nn.Linear(in_features=(8 * c), out_features=(4 * c) ),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=p_drop),\n",
    "        \n",
    "        nn.Linear(in_features=(4 * c), out_features=cfg.n_classes)\n",
    "    )\n",
    "    \n",
    "    torchsummary.summary(net, input_size=[[cfg.img_channels, *cfg.img_size]], device='cpu')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3970ca16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:45.875589Z",
     "iopub.status.busy": "2023-01-17T01:43:45.874814Z",
     "iopub.status.idle": "2023-01-17T01:43:45.883447Z",
     "shell.execute_reply": "2023-01-17T01:43:45.882634Z"
    },
    "papermill": {
     "duration": 0.01797,
     "end_time": "2023-01-17T01:43:45.885379",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.867409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cnn_network_v2(in_channels=cfg.img_channels, p_drop=0.1, device=cfg.device):\n",
    "    \n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=96, kernel_size=5, padding=2),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(in_channels=96, out_channels=80, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout2d(p=p_drop),\n",
    "        \n",
    "        nn.Conv2d(in_channels=80, out_channels=96, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p=p_drop),\n",
    "        \n",
    "        nn.Conv2d(in_channels=96, out_channels=64, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p=p_drop),\n",
    "        \n",
    "        nn.Flatten(),\n",
    "        \n",
    "        # nn.Linear(in_features=4096, out_features=256 ),\n",
    "        nn.Linear(in_features=(cfg.img_size[0] // 4) * (cfg.img_size[1] // 4) * 64, out_features=256 ),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=p_drop),\n",
    "        \n",
    "        nn.Linear(in_features=256, out_features=cfg.n_classes)\n",
    "    )\n",
    "    \n",
    "    torchsummary.summary(net, input_size=[[cfg.img_channels, *cfg.img_size]], device='cpu')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "191d928d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:45.899879Z",
     "iopub.status.busy": "2023-01-17T01:43:45.899330Z",
     "iopub.status.idle": "2023-01-17T01:43:45.904647Z",
     "shell.execute_reply": "2023-01-17T01:43:45.903811Z"
    },
    "papermill": {
     "duration": 0.014567,
     "end_time": "2023-01-17T01:43:45.906560",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.891993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_resnet18(device=cfg.device):\n",
    "    \n",
    "    net = torchvision.models.resnet18(num_classes=cfg.n_classes)\n",
    "    torchsummary.summary(net, input_size=[[cfg.img_channels, *cfg.img_size]], device='cpu')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1877127",
   "metadata": {
    "papermill": {
     "duration": 0.006518,
     "end_time": "2023-01-17T01:43:45.919645",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.913127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# object for calculation of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91921d9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:45.934331Z",
     "iopub.status.busy": "2023-01-17T01:43:45.933561Z",
     "iopub.status.idle": "2023-01-17T01:43:45.942631Z",
     "shell.execute_reply": "2023-01-17T01:43:45.941840Z"
    },
    "papermill": {
     "duration": 0.018404,
     "end_time": "2023-01-17T01:43:45.944590",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.926186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, value_round=None, time_round=None):\n",
    "        self.metrics_dict = {}\n",
    "        self.set_initial_time()\n",
    "        self.val_round = value_round\n",
    "        self.time_round = time_round\n",
    "        \n",
    "    def set_initial_time(self):\n",
    "        self.init_time = time.time()\n",
    "        \n",
    "    def get_time(self):\n",
    "        return time.time() - self.init_time\n",
    "    \n",
    "    def add(self, key, value, step=None):\n",
    "        \n",
    "        if step is None:\n",
    "            step = np.nan\n",
    "        \n",
    "        if key not in self.metrics_dict:\n",
    "            self.metrics_dict[key] = []\n",
    "        \n",
    "        t = self.get_time()\n",
    "        if self.time_round is not None:\n",
    "            t = round(t, ndigits=self.time_round)\n",
    "        \n",
    "        if self.val_round is not None:\n",
    "            value = round(value, ndigits=self.val_round)\n",
    "        \n",
    "        self.metrics_dict[key].append( (value, step, t) )\n",
    "    \n",
    "    def add_(self, dict_, step=None):\n",
    "        for key, value in dict_.items():\n",
    "            self.add(key, value, step)\n",
    "    \n",
    "    def get(self, key, get_step=False, get_time=False):\n",
    "        y, x, t = zip(*self.metrics_dict[key])\n",
    "        y, x, t = list(y), list(x), list(t)\n",
    "        \n",
    "        return x, y, t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f76b8d",
   "metadata": {
    "papermill": {
     "duration": 0.006445,
     "end_time": "2023-01-17T01:43:45.957725",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.951280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fisher Information calculation objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a100ab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:45.972644Z",
     "iopub.status.busy": "2023-01-17T01:43:45.972379Z",
     "iopub.status.idle": "2023-01-17T01:43:45.996640Z",
     "shell.execute_reply": "2023-01-17T01:43:45.995716Z"
    },
    "papermill": {
     "duration": 0.034136,
     "end_time": "2023-01-17T01:43:45.998487",
     "exception": false,
     "start_time": "2023-01-17T01:43:45.964351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FisherExp():\n",
    "    def __init__(self,\n",
    "                 named_params,\n",
    "                 beta,\n",
    "                 partition_size,\n",
    "                 block_updates):\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.partition_size = partition_size\n",
    "        self.block_updates = block_updates\n",
    "        \n",
    "        self.max_inv_norm = 10 * math.sqrt(partition_size)\n",
    "        \n",
    "        named_params = list(named_params)\n",
    "        \n",
    "        self.partition_fisher_list = []\n",
    "        total_partitions, total_block_upd = 0, 0\n",
    "        for pi, (n, p) in enumerate( named_params ):\n",
    "            part_fisher = PartitionerFisher(param = p,\n",
    "                                            name = n,\n",
    "                                            partition_size = partition_size,\n",
    "                                            block_updates = block_updates)\n",
    "            \n",
    "            self.partition_fisher_list.append( (p, part_fisher, total_partitions, total_block_upd) )\n",
    "            \n",
    "            total_partitions += part_fisher.num_part\n",
    "            total_block_upd += part_fisher.block_updates\n",
    "            \n",
    "        self.num_part = total_partitions\n",
    "        self.total_block_updates = total_block_upd\n",
    "        \n",
    "        print(f'total partitions: {self.num_part} - effective block updates: {self.total_block_updates}')\n",
    "        \n",
    "        ## pre-alocate the memory for the tensor that stores the selected gradients (changes every iteration)\n",
    "        self.g = torch.zeros(size=[self.total_block_updates, partition_size, 1], dtype=torch.float, device=cfg.device)\n",
    "        \n",
    "        ## pre-alocate the memory for the tensor that stores the inverses\n",
    "        self.fisher_inv = torch.zeros(size=[self.num_part, partition_size, partition_size], dtype=torch.float, device=cfg.device)        \n",
    "    \n",
    "        print('initializing inverses...')\n",
    "        ## now we initialize the inverse for all partitions. In the case of the FisherExponential, we can just\n",
    "        ## use the identity matrix\n",
    "        i = 0\n",
    "        for _, part_fisher, _, _ in self.partition_fisher_list:\n",
    "            for _, start, end in part_fisher.ind_fisher_list:\n",
    "\n",
    "                if i == 0 or ( (i + 1) % 10000 ) == 0 or i == (self.num_part - 1):\n",
    "                    print(f'partition {i+1}/{self.num_part}')\n",
    "\n",
    "                n = end - start\n",
    "                fisher_inv = torch.eye(n = n, dtype=torch.float, device=cfg.device)\n",
    "\n",
    "                self.fisher_inv[i, :n, :n] = fisher_inv\n",
    "\n",
    "                i += 1\n",
    "                \n",
    "\n",
    "    def get_idx_lists(self):\n",
    "        run_enc_list = []\n",
    "        default_idx_list = []\n",
    "        for p, part_fisher, num_part, block_upd in self.partition_fisher_list:\n",
    "            init_block, end_block, g_init_idx, g_end_idx = part_fisher.get_random_blocks()\n",
    "\n",
    "            run_enc_list.append( (num_part + init_block, num_part + end_block, g_init_idx, g_end_idx) )\n",
    "            default_idx_list.append( np.arange(start=num_part + init_block, stop=num_part + end_block + 1) )    \n",
    "            \n",
    "        return run_enc_list, np.concatenate(default_idx_list)\n",
    "    \n",
    "    \n",
    "    def read_gradients(self, idx):\n",
    "        self_g_start = 0\n",
    "        for i, (_, _, g_start, g_end) in enumerate(idx):\n",
    "            n_grad = g_end - g_start\n",
    "            # self_g_end = min( self_g_start + n_grad, torch.numel(self.g) )\n",
    "            self_g_end = self_g_start + n_grad\n",
    "            \n",
    "            p, _, _, _ = self.partition_fisher_list[i]\n",
    "            \n",
    "            self.g.view(-1)[self_g_start:self_g_end] = p.grad.view(-1)[g_start:g_end]\n",
    "            \n",
    "            if (n_grad % self.partition_size) > 0:\n",
    "                extra_zeros = self.partition_size - (n_grad % self.partition_size)\n",
    "                self.g.view(-1)[self_g_end:(self_g_end + extra_zeros)] = 0.0\n",
    "            else:\n",
    "                extra_zeros = 0\n",
    "\n",
    "            self_g_start = self_g_end + extra_zeros\n",
    "            \n",
    "\n",
    "    def write_gradients(self, idx):\n",
    "        self_g_start = 0\n",
    "        for i, (_, _, g_start, g_end) in enumerate(idx):\n",
    "            n_grad = g_end - g_start\n",
    "            self_g_end = self_g_start + n_grad\n",
    "            \n",
    "            p, _, _, _ = self.partition_fisher_list[i]\n",
    "            p.grad.view(-1)[g_start:g_end] = self.g.view(-1)[self_g_start:self_g_end]\n",
    "\n",
    "            if (n_grad % self.partition_size) > 0:\n",
    "                extra_zeros = self.partition_size - (n_grad % self.partition_size)\n",
    "            else:\n",
    "                extra_zeros = 0\n",
    "            \n",
    "            self_g_start = self_g_end + extra_zeros\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        ## selects the blocks to be updated\n",
    "        run_enc_idx, default_idx = self.get_idx_lists()\n",
    "        \n",
    "        ## read the selected blocks gradients and stores them in self.g\n",
    "        self.read_gradients(run_enc_idx)\n",
    "        \n",
    "        ## set apart the inverses for the selected blocks\n",
    "        inv = self.fisher_inv[default_idx, ...]\n",
    "\n",
    "        ## update the inverses and modify current gradients        \n",
    "        _, new_inv = self.upd_inverse(g = math.sqrt(1 - self.beta) * self.g, inverse = inv / self.beta)\n",
    "        \n",
    "        ## calculate inverses Frob. norm, and clip the matrices\n",
    "        new_inv_norm = torch.sqrt( torch.sum(new_inv**2, dim=[1, 2], keepdim=True) )\n",
    "        norm_coefs = torch.where( new_inv_norm > self.max_inv_norm, self.max_inv_norm / new_inv_norm, torch.ones_like(new_inv_norm) )\n",
    "        new_inv = new_inv * norm_coefs\n",
    "\n",
    "        ## get the modified gradient using \"de facto\" the new inverses and the gradients\n",
    "        # self.g = self.modify_grad(self.g, inv)\n",
    "        self.g = self.modify_grad(self.g, new_inv)\n",
    "        \n",
    "        ## return the inverses and buffers to the main tensor\n",
    "        self.fisher_inv[default_idx, ...] = new_inv\n",
    "        # print(new_inv.shape)\n",
    "        \n",
    "        ## return the modified gradients to the parameters\n",
    "        self.write_gradients(run_enc_idx)\n",
    "\n",
    "\n",
    "    def upd_inverse(self, g, inverse, type_='sum'):\n",
    "        ## update the inverse based on the woodbury inversion\n",
    "        f_inv_g = torch.bmm(inverse, g)\n",
    "\n",
    "        if type_ == 'sum':\n",
    "            d = 1 + torch.sum(g * f_inv_g, dim=[1, 2], keepdim=True)\n",
    "            inverse[:] = inverse - (f_inv_g * torch.transpose(f_inv_g, 1, 2) / d)\n",
    "\n",
    "        elif type_ == 'sub':\n",
    "            d = 1 - torch.sum(g * f_inv_g, dim=[1, 2], keepdim=True)\n",
    "            inverse[:] = inverse + (f_inv_g * torch.transpose(f_inv_g, 1, 2) / d)\n",
    "\n",
    "        else:\n",
    "            ## incorrect type\n",
    "            print('incorrect rank-1 update type: ' + type_)\n",
    "        \n",
    "        return f_inv_g, inverse\n",
    "\n",
    "\n",
    "    def modify_grad(self, g, inverse):\n",
    "        return torch.bmm(inverse, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc52f0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:46.013084Z",
     "iopub.status.busy": "2023-01-17T01:43:46.012635Z",
     "iopub.status.idle": "2023-01-17T01:43:46.022275Z",
     "shell.execute_reply": "2023-01-17T01:43:46.021429Z"
    },
    "papermill": {
     "duration": 0.019178,
     "end_time": "2023-01-17T01:43:46.024210",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.005032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PartitionerFisher():\n",
    "    def __init__(self,\n",
    "                 param,\n",
    "                 name,\n",
    "                 partition_size,\n",
    "                 block_updates):\n",
    "        \n",
    "        self.param = param\n",
    "        self.name = name \n",
    "        \n",
    "        if partition_size is None:\n",
    "            self.partition_size = param.numel()\n",
    "        else:\n",
    "            self.partition_size = partition_size\n",
    "        \n",
    "        ## calculates the number of partitions required. It is calculated using the param size and\n",
    "        ## our partition maximum size. The gradient (the same size as param) is going to be partitioned in\n",
    "        ## equal pieces (except possibly the last one) to be processed individually by our \"IndividualFisherXXX\"\n",
    "        self.param_size = param.numel()\n",
    "        self.num_part = math.ceil(self.param_size / self.partition_size)\n",
    "        \n",
    "        ## the number of blocks (partitions) to update at each iteration. This can be < num_part to make\n",
    "        ## the algorithm more efficient. (we dont update every partition at every iteration)\n",
    "        if block_updates is None:\n",
    "            self.block_updates = self.num_part\n",
    "        else:\n",
    "            self.block_updates = min(block_updates, self.num_part)\n",
    "        \n",
    "        print(f'FisherPartitioner: param: {self.param_size} - partition: {self.partition_size} - nº part: {self.num_part} - block updates: {self.block_updates}')\n",
    "                \n",
    "        ## the list stores the indexes used to partition the gradient\n",
    "        self.ind_fisher_list = []\n",
    "        for i in range(self.num_part):\n",
    "            start = i * self.partition_size\n",
    "            end = min(start + self.partition_size, self.param_size)\n",
    "            \n",
    "            self.ind_fisher_list.append( (i, start, end) )\n",
    "        \n",
    "    \n",
    "    def get_random_blocks(self, num_part=None, block_upd=None):\n",
    "        \n",
    "        if num_part is None:\n",
    "            num_part = self.num_part\n",
    "        \n",
    "        if block_upd is None:\n",
    "            block_upd = self.block_updates\n",
    "        \n",
    "        ## choose the initial block randomly\n",
    "        init_block = np.random.choice(num_part - block_upd + 1)\n",
    "        \n",
    "        ## the final block will be necessarily `block_upd` blocks further. This means we select\n",
    "        ## a contiguous sequence of blocks. This is going to be used for performance reasons\n",
    "        end_block = init_block + block_upd - 1\n",
    "        \n",
    "        ## therefore, the starting and ending index to be used to fetch the gradient positions for the\n",
    "        ## blocks will be the starting index for the first block and the ending positions for the last block\n",
    "        _, g_init_idx, _ = self.ind_fisher_list[init_block]\n",
    "        _, _, g_end_idx = self.ind_fisher_list[end_block]\n",
    "        \n",
    "        return init_block, end_block, g_init_idx, g_end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f0788",
   "metadata": {
    "papermill": {
     "duration": 0.00644,
     "end_time": "2023-01-17T01:43:46.037283",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.030843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642641b4",
   "metadata": {
    "papermill": {
     "duration": 0.006454,
     "end_time": "2023-01-17T01:43:46.050309",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.043855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# utils function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bce1f1c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:46.064974Z",
     "iopub.status.busy": "2023-01-17T01:43:46.064239Z",
     "iopub.status.idle": "2023-01-17T01:43:46.068798Z",
     "shell.execute_reply": "2023-01-17T01:43:46.067880Z"
    },
    "papermill": {
     "duration": 0.013762,
     "end_time": "2023-01-17T01:43:46.070670",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.056908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_score_tns(y_true, y_pred):\n",
    "    return torch.mean( (y_true == y_pred).to(dtype=torch.float) ).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "173cacee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:46.085427Z",
     "iopub.status.busy": "2023-01-17T01:43:46.084638Z",
     "iopub.status.idle": "2023-01-17T01:43:46.090573Z",
     "shell.execute_reply": "2023-01-17T01:43:46.089765Z"
    },
    "papermill": {
     "duration": 0.015176,
     "end_time": "2023-01-17T01:43:46.092470",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.077294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_iteration(x, y, net, optim, loss, fisher=None):\n",
    "    net.train()\n",
    "    net.zero_grad()\n",
    "    \n",
    "    y_pred = net(x)\n",
    "    l = loss(y_pred, y)\n",
    "    \n",
    "    l.backward()\n",
    "    \n",
    "    if fisher is not None:\n",
    "        fisher.step()\n",
    "    \n",
    "    optim.step()\n",
    "    \n",
    "    return l.item(), accuracy_score_tns( y.view(-1), y_pred.argmax(dim=1).view(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "190f7530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:46.107259Z",
     "iopub.status.busy": "2023-01-17T01:43:46.106416Z",
     "iopub.status.idle": "2023-01-17T01:43:46.113640Z",
     "shell.execute_reply": "2023-01-17T01:43:46.112836Z"
    },
    "papermill": {
     "duration": 0.016539,
     "end_time": "2023-01-17T01:43:46.115580",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.099041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, loss):\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        loss_list = []\n",
    "        y_pred_list = []\n",
    "        y_label_list = []\n",
    "        for x, y in dataloader:\n",
    "            \n",
    "            x = x.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "\n",
    "            y_pred = net(x)\n",
    "            l = loss(y_pred, y)\n",
    "\n",
    "            loss_list.append( l.cpu().item() )\n",
    "            y_pred_list.append( y_pred.argmax(dim=1).view(-1) )\n",
    "            y_label_list.append( y.view(-1) )\n",
    "\n",
    "        y_pred_list = torch.cat(y_pred_list).view(-1)\n",
    "        y_label_list = torch.cat(y_label_list).view(-1)\n",
    "\n",
    "    return np.mean(loss_list), accuracy_score_tns(y_label_list, y_pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b7d41",
   "metadata": {
    "papermill": {
     "duration": 0.006415,
     "end_time": "2023-01-17T01:43:46.128574",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.122159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "053d57ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:46.143130Z",
     "iopub.status.busy": "2023-01-17T01:43:46.142811Z",
     "iopub.status.idle": "2023-01-17T01:43:46.157789Z",
     "shell.execute_reply": "2023-01-17T01:43:46.156915Z"
    },
    "papermill": {
     "duration": 0.024821,
     "end_time": "2023-01-17T01:43:46.160003",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.135182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_network_fisher_optimization(batch_size = 32,\n",
    "                                      lr = 1e-3,\n",
    "                                      momentum = 0.9,\n",
    "                                      epochs = 30,\n",
    "                                      beta = 0.9,\n",
    "                                      partition_size = 256,\n",
    "                                      block_updates = 4,\n",
    "                                      net_params = {'c':16, 'p':0.1},\n",
    "                                      apply_fisher = True,\n",
    "                                      # gpu_memory_check = 20,\n",
    "                                      time_limit_secs = 600,\n",
    "                                      interval_print = 100):\n",
    "\n",
    "    ## declare (instantiate) the dataset\n",
    "    # train_dataloader, test_dataloader = generate_dataset_cifar10(batch_size = batch_size)\n",
    "    # train_dataloader, test_dataloader = generate_dataset_mnist(batch_size = batch_size)\n",
    "    train_dataloader, test_dataloader = generate_dataset_cifar100(batch_size = batch_size)\n",
    "\n",
    "    ## instantiate the network\n",
    "    # net = get_cnn_network_v2(p_drop = net_params['p']).to(device=cfg.device)\n",
    "    net = get_resnet18().to(device=cfg.device)\n",
    "    \n",
    "    if apply_fisher:\n",
    "        fisher_obj = FisherExp(named_params = net.named_parameters(),\n",
    "                                beta = beta,\n",
    "                                partition_size = partition_size,\n",
    "                                block_updates = block_updates)\n",
    "    else:\n",
    "        fisher_obj = None\n",
    "\n",
    "    ## create loss object: we multiply by our constant to stabilize norms\n",
    "    # cross_entropy = nn.CrossEntropyLoss(reduction='mean') # standard version\n",
    "    cross_entropy_standard = nn.CrossEntropyLoss(reduction='mean')\n",
    "    cross_entropy = lambda y_pred, y: math.sqrt(batch_size) * cross_entropy_standard(y_pred, y)\n",
    "    \n",
    "    ## create optimize objects\n",
    "    optim = torch.optim.SGD(params=net.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    default_metrics = Metrics(value_round=3, time_round=2)\n",
    "\n",
    "    ini_time = time.time()\n",
    "\n",
    "    step = 0\n",
    "    training_finished = False\n",
    "    for epc in range(1, epochs + 1):\n",
    "        \n",
    "        if training_finished:\n",
    "            break\n",
    "        \n",
    "        print(f'starting epoch: {epc}/{epochs}')\n",
    "\n",
    "        for nbt, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "            if training_finished:\n",
    "                break\n",
    "\n",
    "            x = x.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "\n",
    "            train_loss, train_acc = train_iteration(x, y, net, optim, cross_entropy, fisher_obj)\n",
    "            default_metrics.add_({'train-loss': train_loss, 'train-acc': train_acc}, step=step)\n",
    "            \n",
    "            ## check time limit\n",
    "            t = int(time.time() - ini_time)\n",
    "            if t > time_limit_secs:\n",
    "                print('time is up! finishing training')\n",
    "                training_finished = True\n",
    "\n",
    "            if ( (nbt + 1) % interval_print ) == 0 or (nbt + 1) == len(train_dataloader) or training_finished:\n",
    "                avg_train_loss = np.mean( default_metrics.get('train-loss')[1][-interval_print:] )\n",
    "                avg_train_acc = np.mean( default_metrics.get('train-acc')[1][-interval_print:] )\n",
    "                \n",
    "                test_loss, test_acc = evaluate(net, test_dataloader, cross_entropy)\n",
    "                default_metrics.add_({'test-loss': test_loss, 'test-acc': test_acc}, step=step)\n",
    "\n",
    "                m, s = t // 60, t % 60\n",
    "\n",
    "                print(f'batch: {nbt + 1}/{len(train_dataloader)}', end='')\n",
    "                print(f' - train loss: {avg_train_loss:.4f} - test loss: {test_loss:.4f}', end='')\n",
    "                print(f' - train acc: {avg_train_acc:.4f} - test acc: {test_acc:.4f}', end='')\n",
    "                print(f' - {m}m {s}s')\n",
    "                \n",
    "            step += 1\n",
    "\n",
    "        ## check for GPU memory consumption\n",
    "        if torch.cuda.is_available():\n",
    "            mem_alloc_gb = torch.cuda.memory_allocated(cfg.device) / 1024**3\n",
    "            mem_res_gb = torch.cuda.memory_reserved(cfg.device) / 1024**3\n",
    "            max_mem_alloc_gb = torch.cuda.max_memory_allocated(cfg.device) / 1024**3\n",
    "            max_mem_res_gb = torch.cuda.max_memory_reserved(cfg.device) / 1024**3\n",
    "\n",
    "            print(f'GPU memory used: {mem_alloc_gb:.2f} GB - max: {max_mem_alloc_gb:.2f} GB - memory reserved: {mem_res_gb:.2f} GB - max: {max_mem_res_gb:.2f} GB')\n",
    "\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "    return default_metrics, fisher_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e576f9c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:46.174303Z",
     "iopub.status.busy": "2023-01-17T01:43:46.174046Z",
     "iopub.status.idle": "2023-01-17T01:43:46.177746Z",
     "shell.execute_reply": "2023-01-17T01:43:46.176847Z"
    },
    "papermill": {
     "duration": 0.013179,
     "end_time": "2023-01-17T01:43:46.179726",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.166547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# net = torchvision.models.resnet18()\n",
    "# torchsummary.summary(net, input_size=[[3, 64, 64]], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b7644c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:46.193804Z",
     "iopub.status.busy": "2023-01-17T01:43:46.193546Z",
     "iopub.status.idle": "2023-01-17T01:43:46.199597Z",
     "shell.execute_reply": "2023-01-17T01:43:46.198731Z"
    },
    "papermill": {
     "duration": 0.015202,
     "end_time": "2023-01-17T01:43:46.201439",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.186237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_step_saved = None\n",
    "\n",
    "def results_list_to_json(results_list, out_dir='/kaggle/working', step=0):\n",
    "    global last_step_saved\n",
    "\n",
    "    json_results = []\n",
    "\n",
    "    for metrics, bt, ps, bu in results_list:\n",
    "        json_results.append({\n",
    "            'beta': bt,\n",
    "            'partition-size': ps,\n",
    "            'blocks-updates': bu,\n",
    "            'metrics': metrics.metrics_dict\n",
    "        })\n",
    "\n",
    "    with open( os.path.join(out_dir, f'results_step_{step}.json'), 'w' ) as fp:\n",
    "        json.dump(json_results, fp)\n",
    "    \n",
    "    if last_step_saved is not None:\n",
    "        old_file = os.path.join(out_dir, f'results_step_{last_step_saved}.json')\n",
    "        if os.path.exists(old_file):\n",
    "            os.remove(old_file)\n",
    "    \n",
    "    last_step_saved = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c795cf30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:46.215898Z",
     "iopub.status.busy": "2023-01-17T01:43:46.215376Z",
     "iopub.status.idle": "2023-01-17T01:43:46.219667Z",
     "shell.execute_reply": "2023-01-17T01:43:46.218765Z"
    },
    "papermill": {
     "duration": 0.013559,
     "end_time": "2023-01-17T01:43:46.221576",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.208017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_min_test_loss(metrics):\n",
    "    _, test_loss, _ = metrics.get('test-loss')\n",
    "    return min(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9944c",
   "metadata": {
    "papermill": {
     "duration": 0.006463,
     "end_time": "2023-01-17T01:43:46.234620",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.228157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## running FisherExp in CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d6f5332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T01:43:46.248973Z",
     "iopub.status.busy": "2023-01-17T01:43:46.248461Z",
     "iopub.status.idle": "2023-01-17T05:11:18.223926Z",
     "shell.execute_reply": "2023-01-17T05:11:18.222832Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 12451.985468,
     "end_time": "2023-01-17T05:11:18.226604",
     "exception": false,
     "start_time": "2023-01-17T01:43:46.241136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing - beta: 0.9 - part size: 8 - block upd: 64 - combination nº: 1\n",
      "generating CIFAR100 data with 100 classes\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0dac7adb53480087b3dd8bda40580d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169001437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 8 - nº part: 1176 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 73728 - partition: 8 - nº part: 9216 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 8192 - partition: 8 - nº part: 1024 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 294912 - partition: 8 - nº part: 36864 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 32768 - partition: 8 - nº part: 4096 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 1179648 - partition: 8 - nº part: 147456 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 131072 - partition: 8 - nº part: 16384 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 51200 - partition: 8 - nº part: 6400 - block updates: 64\n",
      "FisherPartitioner: param: 100 - partition: 8 - nº part: 13 - block updates: 13\n",
      "total partitions: 1403477 - effective block updates: 2557\n",
      "initializing inverses...\n",
      "partition 1/1403477\n",
      "partition 10000/1403477\n",
      "partition 20000/1403477\n",
      "partition 30000/1403477\n",
      "partition 40000/1403477\n",
      "partition 50000/1403477\n",
      "partition 60000/1403477\n",
      "partition 70000/1403477\n",
      "partition 80000/1403477\n",
      "partition 90000/1403477\n",
      "partition 100000/1403477\n",
      "partition 110000/1403477\n",
      "partition 120000/1403477\n",
      "partition 130000/1403477\n",
      "partition 140000/1403477\n",
      "partition 150000/1403477\n",
      "partition 160000/1403477\n",
      "partition 170000/1403477\n",
      "partition 180000/1403477\n",
      "partition 190000/1403477\n",
      "partition 200000/1403477\n",
      "partition 210000/1403477\n",
      "partition 220000/1403477\n",
      "partition 230000/1403477\n",
      "partition 240000/1403477\n",
      "partition 250000/1403477\n",
      "partition 260000/1403477\n",
      "partition 270000/1403477\n",
      "partition 280000/1403477\n",
      "partition 290000/1403477\n",
      "partition 300000/1403477\n",
      "partition 310000/1403477\n",
      "partition 320000/1403477\n",
      "partition 330000/1403477\n",
      "partition 340000/1403477\n",
      "partition 350000/1403477\n",
      "partition 360000/1403477\n",
      "partition 370000/1403477\n",
      "partition 380000/1403477\n",
      "partition 390000/1403477\n",
      "partition 400000/1403477\n",
      "partition 410000/1403477\n",
      "partition 420000/1403477\n",
      "partition 430000/1403477\n",
      "partition 440000/1403477\n",
      "partition 450000/1403477\n",
      "partition 460000/1403477\n",
      "partition 470000/1403477\n",
      "partition 480000/1403477\n",
      "partition 490000/1403477\n",
      "partition 500000/1403477\n",
      "partition 510000/1403477\n",
      "partition 520000/1403477\n",
      "partition 530000/1403477\n",
      "partition 540000/1403477\n",
      "partition 550000/1403477\n",
      "partition 560000/1403477\n",
      "partition 570000/1403477\n",
      "partition 580000/1403477\n",
      "partition 590000/1403477\n",
      "partition 600000/1403477\n",
      "partition 610000/1403477\n",
      "partition 620000/1403477\n",
      "partition 630000/1403477\n",
      "partition 640000/1403477\n",
      "partition 650000/1403477\n",
      "partition 660000/1403477\n",
      "partition 670000/1403477\n",
      "partition 680000/1403477\n",
      "partition 690000/1403477\n",
      "partition 700000/1403477\n",
      "partition 710000/1403477\n",
      "partition 720000/1403477\n",
      "partition 730000/1403477\n",
      "partition 740000/1403477\n",
      "partition 750000/1403477\n",
      "partition 760000/1403477\n",
      "partition 770000/1403477\n",
      "partition 780000/1403477\n",
      "partition 790000/1403477\n",
      "partition 800000/1403477\n",
      "partition 810000/1403477\n",
      "partition 820000/1403477\n",
      "partition 830000/1403477\n",
      "partition 840000/1403477\n",
      "partition 850000/1403477\n",
      "partition 860000/1403477\n",
      "partition 870000/1403477\n",
      "partition 880000/1403477\n",
      "partition 890000/1403477\n",
      "partition 900000/1403477\n",
      "partition 910000/1403477\n",
      "partition 920000/1403477\n",
      "partition 930000/1403477\n",
      "partition 940000/1403477\n",
      "partition 950000/1403477\n",
      "partition 960000/1403477\n",
      "partition 970000/1403477\n",
      "partition 980000/1403477\n",
      "partition 990000/1403477\n",
      "partition 1000000/1403477\n",
      "partition 1010000/1403477\n",
      "partition 1020000/1403477\n",
      "partition 1030000/1403477\n",
      "partition 1040000/1403477\n",
      "partition 1050000/1403477\n",
      "partition 1060000/1403477\n",
      "partition 1070000/1403477\n",
      "partition 1080000/1403477\n",
      "partition 1090000/1403477\n",
      "partition 1100000/1403477\n",
      "partition 1110000/1403477\n",
      "partition 1120000/1403477\n",
      "partition 1130000/1403477\n",
      "partition 1140000/1403477\n",
      "partition 1150000/1403477\n",
      "partition 1160000/1403477\n",
      "partition 1170000/1403477\n",
      "partition 1180000/1403477\n",
      "partition 1190000/1403477\n",
      "partition 1200000/1403477\n",
      "partition 1210000/1403477\n",
      "partition 1220000/1403477\n",
      "partition 1230000/1403477\n",
      "partition 1240000/1403477\n",
      "partition 1250000/1403477\n",
      "partition 1260000/1403477\n",
      "partition 1270000/1403477\n",
      "partition 1280000/1403477\n",
      "partition 1290000/1403477\n",
      "partition 1300000/1403477\n",
      "partition 1310000/1403477\n",
      "partition 1320000/1403477\n",
      "partition 1330000/1403477\n",
      "partition 1340000/1403477\n",
      "partition 1350000/1403477\n",
      "partition 1360000/1403477\n",
      "partition 1370000/1403477\n",
      "partition 1380000/1403477\n",
      "partition 1390000/1403477\n",
      "partition 1400000/1403477\n",
      "partition 1403477/1403477\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1840 - test loss: 24.8300 - train acc: 0.0380 - test acc: 0.0564 - 0m 8s\n",
      "batch: 200/1563 - train loss: 23.7681 - test loss: 23.4579 - train acc: 0.0696 - test acc: 0.0809 - 0m 12s\n",
      "batch: 300/1563 - train loss: 22.9395 - test loss: 22.3244 - train acc: 0.0720 - test acc: 0.1011 - 0m 17s\n",
      "batch: 400/1563 - train loss: 22.2468 - test loss: 21.3701 - train acc: 0.0993 - test acc: 0.1275 - 0m 22s\n",
      "batch: 500/1563 - train loss: 21.8011 - test loss: 20.9801 - train acc: 0.1071 - test acc: 0.1337 - 0m 26s\n",
      "batch: 600/1563 - train loss: 21.1726 - test loss: 21.3121 - train acc: 0.1347 - test acc: 0.1309 - 0m 31s\n",
      "batch: 700/1563 - train loss: 20.8371 - test loss: 21.0706 - train acc: 0.1272 - test acc: 0.1337 - 0m 36s\n",
      "batch: 800/1563 - train loss: 20.3001 - test loss: 19.9442 - train acc: 0.1553 - test acc: 0.1593 - 0m 40s\n",
      "batch: 900/1563 - train loss: 19.8221 - test loss: 19.1227 - train acc: 0.1662 - test acc: 0.1896 - 0m 45s\n",
      "batch: 1000/1563 - train loss: 19.4263 - test loss: 19.7990 - train acc: 0.1781 - test acc: 0.1564 - 0m 50s\n",
      "batch: 1100/1563 - train loss: 19.6949 - test loss: 19.2756 - train acc: 0.1660 - test acc: 0.1789 - 0m 55s\n",
      "batch: 1200/1563 - train loss: 19.2380 - test loss: 18.7083 - train acc: 0.1947 - test acc: 0.1980 - 0m 59s\n",
      "batch: 1300/1563 - train loss: 18.8332 - test loss: 18.7548 - train acc: 0.1984 - test acc: 0.1984 - 1m 4s\n",
      "batch: 1400/1563 - train loss: 18.8818 - test loss: 18.3432 - train acc: 0.1904 - test acc: 0.2140 - 1m 9s\n",
      "batch: 1500/1563 - train loss: 18.6494 - test loss: 19.2801 - train acc: 0.1966 - test acc: 0.1680 - 1m 13s\n",
      "batch: 1563/1563 - train loss: 18.2651 - test loss: 18.1196 - train acc: 0.2119 - test acc: 0.2143 - 1m 18s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.6835 - test loss: 18.7750 - train acc: 0.2303 - test acc: 0.1970 - 1m 22s\n",
      "batch: 200/1563 - train loss: 17.6371 - test loss: 18.4153 - train acc: 0.2366 - test acc: 0.2220 - 1m 27s\n",
      "batch: 300/1563 - train loss: 17.4392 - test loss: 17.8930 - train acc: 0.2303 - test acc: 0.2260 - 1m 31s\n",
      "batch: 400/1563 - train loss: 17.3971 - test loss: 17.5573 - train acc: 0.2419 - test acc: 0.2432 - 1m 36s\n",
      "batch: 500/1563 - train loss: 17.2870 - test loss: 17.3628 - train acc: 0.2434 - test acc: 0.2428 - 1m 41s\n",
      "batch: 600/1563 - train loss: 17.2738 - test loss: 17.1302 - train acc: 0.2506 - test acc: 0.2529 - 1m 45s\n",
      "batch: 700/1563 - train loss: 16.7972 - test loss: 17.5900 - train acc: 0.2588 - test acc: 0.2462 - 1m 50s\n",
      "batch: 800/1563 - train loss: 16.7477 - test loss: 17.8634 - train acc: 0.2606 - test acc: 0.2363 - 1m 55s\n",
      "batch: 900/1563 - train loss: 16.5443 - test loss: 16.5899 - train acc: 0.2762 - test acc: 0.2695 - 2m 0s\n",
      "batch: 1000/1563 - train loss: 17.0021 - test loss: 16.7176 - train acc: 0.2569 - test acc: 0.2644 - 2m 4s\n",
      "batch: 1100/1563 - train loss: 16.3164 - test loss: 16.5779 - train acc: 0.2762 - test acc: 0.2641 - 2m 9s\n",
      "batch: 1200/1563 - train loss: 16.7156 - test loss: 17.3077 - train acc: 0.2650 - test acc: 0.2517 - 2m 14s\n",
      "batch: 1300/1563 - train loss: 16.4138 - test loss: 16.5303 - train acc: 0.2797 - test acc: 0.2697 - 2m 19s\n",
      "batch: 1400/1563 - train loss: 16.2815 - test loss: 16.2849 - train acc: 0.2865 - test acc: 0.2876 - 2m 23s\n",
      "batch: 1500/1563 - train loss: 16.0776 - test loss: 15.8632 - train acc: 0.2950 - test acc: 0.2997 - 2m 28s\n",
      "batch: 1563/1563 - train loss: 15.8173 - test loss: 15.7883 - train acc: 0.3002 - test acc: 0.2999 - 2m 32s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.9724 - test loss: 16.7253 - train acc: 0.3219 - test acc: 0.2674 - 2m 36s\n",
      "batch: 200/1563 - train loss: 15.1448 - test loss: 15.9882 - train acc: 0.3168 - test acc: 0.2942 - 2m 41s\n",
      "batch: 300/1563 - train loss: 14.9432 - test loss: 16.7123 - train acc: 0.3172 - test acc: 0.2842 - 2m 46s\n",
      "batch: 400/1563 - train loss: 15.0489 - test loss: 15.2048 - train acc: 0.3146 - test acc: 0.3257 - 2m 51s\n",
      "batch: 500/1563 - train loss: 14.7065 - test loss: 15.4562 - train acc: 0.3362 - test acc: 0.3088 - 2m 56s\n",
      "batch: 600/1563 - train loss: 15.2081 - test loss: 15.3632 - train acc: 0.3181 - test acc: 0.3187 - 3m 0s\n",
      "batch: 700/1563 - train loss: 14.8749 - test loss: 15.2766 - train acc: 0.3365 - test acc: 0.3212 - 3m 5s\n",
      "batch: 800/1563 - train loss: 14.6598 - test loss: 15.3753 - train acc: 0.3406 - test acc: 0.3182 - 3m 10s\n",
      "batch: 900/1563 - train loss: 14.7670 - test loss: 15.3989 - train acc: 0.3394 - test acc: 0.3141 - 3m 14s\n",
      "batch: 1000/1563 - train loss: 14.6034 - test loss: 15.5821 - train acc: 0.3428 - test acc: 0.3117 - 3m 19s\n",
      "batch: 1100/1563 - train loss: 14.3696 - test loss: 16.2055 - train acc: 0.3447 - test acc: 0.2876 - 3m 24s\n",
      "batch: 1200/1563 - train loss: 14.5867 - test loss: 15.1341 - train acc: 0.3484 - test acc: 0.3304 - 3m 29s\n",
      "batch: 1300/1563 - train loss: 14.6357 - test loss: 14.7102 - train acc: 0.3338 - test acc: 0.3401 - 3m 34s\n",
      "batch: 1400/1563 - train loss: 14.3213 - test loss: 15.2242 - train acc: 0.3553 - test acc: 0.3240 - 3m 38s\n",
      "batch: 1500/1563 - train loss: 14.5629 - test loss: 16.0810 - train acc: 0.3525 - test acc: 0.2941 - 3m 43s\n",
      "batch: 1563/1563 - train loss: 14.4460 - test loss: 16.3654 - train acc: 0.3387 - test acc: 0.2849 - 3m 47s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.7628 - test loss: 15.4955 - train acc: 0.4009 - test acc: 0.3181 - 3m 51s\n",
      "batch: 200/1563 - train loss: 13.1951 - test loss: 15.1889 - train acc: 0.3885 - test acc: 0.3363 - 3m 56s\n",
      "batch: 300/1563 - train loss: 13.0124 - test loss: 14.3417 - train acc: 0.3900 - test acc: 0.3542 - 4m 1s\n",
      "batch: 400/1563 - train loss: 13.0040 - test loss: 16.2937 - train acc: 0.3926 - test acc: 0.2927 - 4m 6s\n",
      "batch: 500/1563 - train loss: 13.2314 - test loss: 14.5386 - train acc: 0.3928 - test acc: 0.3505 - 4m 11s\n",
      "batch: 600/1563 - train loss: 13.0920 - test loss: 14.4092 - train acc: 0.3919 - test acc: 0.3545 - 4m 15s\n",
      "batch: 700/1563 - train loss: 13.2470 - test loss: 14.5975 - train acc: 0.3922 - test acc: 0.3488 - 4m 20s\n",
      "batch: 800/1563 - train loss: 13.2349 - test loss: 14.6760 - train acc: 0.3885 - test acc: 0.3476 - 4m 25s\n",
      "batch: 900/1563 - train loss: 13.1737 - test loss: 14.7052 - train acc: 0.3943 - test acc: 0.3482 - 4m 30s\n",
      "batch: 1000/1563 - train loss: 13.2128 - test loss: 14.3239 - train acc: 0.3892 - test acc: 0.3624 - 4m 34s\n",
      "batch: 1100/1563 - train loss: 13.4869 - test loss: 15.1561 - train acc: 0.3794 - test acc: 0.3356 - 4m 39s\n",
      "batch: 1200/1563 - train loss: 13.1107 - test loss: 13.9671 - train acc: 0.3935 - test acc: 0.3638 - 4m 44s\n",
      "batch: 1300/1563 - train loss: 13.2206 - test loss: 13.9910 - train acc: 0.3887 - test acc: 0.3733 - 4m 48s\n",
      "batch: 1400/1563 - train loss: 13.3152 - test loss: 14.9153 - train acc: 0.3953 - test acc: 0.3383 - 4m 53s\n",
      "batch: 1500/1563 - train loss: 12.9576 - test loss: 14.6229 - train acc: 0.4084 - test acc: 0.3484 - 4m 58s\n",
      "batch: 1563/1563 - train loss: 12.9481 - test loss: 14.0701 - train acc: 0.4088 - test acc: 0.3704 - 5m 2s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.4495 - test loss: 14.0522 - train acc: 0.4603 - test acc: 0.3671 - 5m 7s\n",
      "batch: 200/1563 - train loss: 11.4162 - test loss: 13.6477 - train acc: 0.4569 - test acc: 0.3842 - 5m 11s\n",
      "batch: 300/1563 - train loss: 11.5856 - test loss: 13.7504 - train acc: 0.4493 - test acc: 0.3860 - 5m 16s\n",
      "batch: 400/1563 - train loss: 11.4844 - test loss: 14.7058 - train acc: 0.4481 - test acc: 0.3548 - 5m 21s\n",
      "batch: 500/1563 - train loss: 11.7594 - test loss: 14.5380 - train acc: 0.4428 - test acc: 0.3603 - 5m 25s\n",
      "batch: 600/1563 - train loss: 12.0744 - test loss: 14.1981 - train acc: 0.4322 - test acc: 0.3733 - 5m 30s\n",
      "batch: 700/1563 - train loss: 11.9816 - test loss: 14.3744 - train acc: 0.4328 - test acc: 0.3662 - 5m 35s\n",
      "batch: 800/1563 - train loss: 11.6914 - test loss: 15.3743 - train acc: 0.4519 - test acc: 0.3360 - 5m 40s\n",
      "batch: 900/1563 - train loss: 11.9178 - test loss: 13.8108 - train acc: 0.4404 - test acc: 0.3755 - 5m 44s\n",
      "batch: 1000/1563 - train loss: 11.7203 - test loss: 13.9975 - train acc: 0.4503 - test acc: 0.3733 - 5m 49s\n",
      "batch: 1100/1563 - train loss: 11.7893 - test loss: 15.3216 - train acc: 0.4472 - test acc: 0.3489 - 5m 54s\n",
      "batch: 1200/1563 - train loss: 11.8083 - test loss: 14.0692 - train acc: 0.4497 - test acc: 0.3677 - 5m 58s\n",
      "batch: 1300/1563 - train loss: 12.2814 - test loss: 14.1331 - train acc: 0.4366 - test acc: 0.3789 - 6m 3s\n",
      "batch: 1400/1563 - train loss: 12.0200 - test loss: 13.8035 - train acc: 0.4313 - test acc: 0.3781 - 6m 8s\n",
      "batch: 1500/1563 - train loss: 12.2609 - test loss: 13.4127 - train acc: 0.4166 - test acc: 0.3916 - 6m 13s\n",
      "batch: 1563/1563 - train loss: 11.8382 - test loss: 14.9565 - train acc: 0.4356 - test acc: 0.3516 - 6m 16s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.2048 - test loss: 13.4779 - train acc: 0.4985 - test acc: 0.3940 - 6m 21s\n",
      "batch: 200/1563 - train loss: 10.2733 - test loss: 13.4455 - train acc: 0.5087 - test acc: 0.4027 - 6m 26s\n",
      "batch: 300/1563 - train loss: 10.3004 - test loss: 13.5392 - train acc: 0.5088 - test acc: 0.3974 - 6m 31s\n",
      "batch: 400/1563 - train loss: 10.4066 - test loss: 13.9773 - train acc: 0.4982 - test acc: 0.3805 - 6m 36s\n",
      "batch: 500/1563 - train loss: 10.4469 - test loss: 14.0989 - train acc: 0.4975 - test acc: 0.3790 - 6m 40s\n",
      "batch: 600/1563 - train loss: 10.4154 - test loss: 14.3665 - train acc: 0.5066 - test acc: 0.3707 - 6m 45s\n",
      "batch: 700/1563 - train loss: 10.9131 - test loss: 14.0103 - train acc: 0.4812 - test acc: 0.3737 - 6m 49s\n",
      "batch: 800/1563 - train loss: 10.6191 - test loss: 13.6834 - train acc: 0.4987 - test acc: 0.3929 - 6m 54s\n",
      "batch: 900/1563 - train loss: 10.3085 - test loss: 13.7228 - train acc: 0.5000 - test acc: 0.3969 - 6m 59s\n",
      "batch: 1000/1563 - train loss: 10.7497 - test loss: 14.1030 - train acc: 0.4878 - test acc: 0.3832 - 7m 4s\n",
      "batch: 1100/1563 - train loss: 10.8589 - test loss: 13.7303 - train acc: 0.4794 - test acc: 0.3952 - 7m 9s\n",
      "batch: 1200/1563 - train loss: 11.0805 - test loss: 13.5998 - train acc: 0.4715 - test acc: 0.3941 - 7m 13s\n",
      "batch: 1300/1563 - train loss: 10.8041 - test loss: 13.5022 - train acc: 0.4788 - test acc: 0.3968 - 7m 18s\n",
      "batch: 1400/1563 - train loss: 10.8903 - test loss: 13.1526 - train acc: 0.4757 - test acc: 0.4071 - 7m 23s\n",
      "batch: 1500/1563 - train loss: 10.7036 - test loss: 14.5926 - train acc: 0.4875 - test acc: 0.3737 - 7m 27s\n",
      "batch: 1563/1563 - train loss: 10.7845 - test loss: 13.9525 - train acc: 0.4894 - test acc: 0.3852 - 7m 31s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.7210 - test loss: 13.9763 - train acc: 0.5769 - test acc: 0.3995 - 7m 36s\n",
      "batch: 200/1563 - train loss: 8.8445 - test loss: 13.7997 - train acc: 0.5703 - test acc: 0.3984 - 7m 41s\n",
      "batch: 300/1563 - train loss: 8.6742 - test loss: 14.3950 - train acc: 0.5747 - test acc: 0.3865 - 7m 46s\n",
      "batch: 400/1563 - train loss: 9.3069 - test loss: 13.6858 - train acc: 0.5375 - test acc: 0.4045 - 7m 51s\n",
      "batch: 500/1563 - train loss: 9.2863 - test loss: 13.6229 - train acc: 0.5515 - test acc: 0.4022 - 7m 55s\n",
      "batch: 600/1563 - train loss: 9.3567 - test loss: 14.0540 - train acc: 0.5459 - test acc: 0.3944 - 8m 0s\n",
      "batch: 700/1563 - train loss: 9.6198 - test loss: 14.7981 - train acc: 0.5331 - test acc: 0.3701 - 8m 4s\n",
      "batch: 800/1563 - train loss: 9.5858 - test loss: 13.7815 - train acc: 0.5303 - test acc: 0.3946 - 8m 10s\n",
      "batch: 900/1563 - train loss: 9.8691 - test loss: 13.9783 - train acc: 0.5184 - test acc: 0.3951 - 8m 14s\n",
      "batch: 1000/1563 - train loss: 9.7104 - test loss: 13.8224 - train acc: 0.5347 - test acc: 0.3967 - 8m 19s\n",
      "batch: 1100/1563 - train loss: 9.9559 - test loss: 14.3102 - train acc: 0.5134 - test acc: 0.3929 - 8m 24s\n",
      "batch: 1200/1563 - train loss: 9.7682 - test loss: 15.7564 - train acc: 0.5116 - test acc: 0.3598 - 8m 29s\n",
      "batch: 1300/1563 - train loss: 10.0112 - test loss: 13.2338 - train acc: 0.5162 - test acc: 0.4071 - 8m 33s\n",
      "batch: 1400/1563 - train loss: 10.0076 - test loss: 13.7968 - train acc: 0.5215 - test acc: 0.4004 - 8m 38s\n",
      "batch: 1500/1563 - train loss: 9.8129 - test loss: 14.1097 - train acc: 0.5213 - test acc: 0.3873 - 8m 43s\n",
      "batch: 1563/1563 - train loss: 10.0073 - test loss: 13.5904 - train acc: 0.5163 - test acc: 0.3972 - 8m 47s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.4790 - test loss: 13.5585 - train acc: 0.6344 - test acc: 0.4129 - 8m 52s\n",
      "batch: 200/1563 - train loss: 7.6424 - test loss: 13.7278 - train acc: 0.6197 - test acc: 0.4169 - 8m 57s\n",
      "batch: 300/1563 - train loss: 8.0942 - test loss: 14.8637 - train acc: 0.6041 - test acc: 0.3832 - 9m 1s\n",
      "batch: 400/1563 - train loss: 8.1673 - test loss: 14.1917 - train acc: 0.5928 - test acc: 0.3958 - 9m 6s\n",
      "batch: 500/1563 - train loss: 8.2750 - test loss: 14.5679 - train acc: 0.5803 - test acc: 0.3971 - 9m 11s\n",
      "batch: 600/1563 - train loss: 8.3554 - test loss: 14.6513 - train acc: 0.5896 - test acc: 0.3936 - 9m 16s\n",
      "batch: 700/1563 - train loss: 8.4168 - test loss: 14.4830 - train acc: 0.5790 - test acc: 0.3907 - 9m 21s\n",
      "batch: 800/1563 - train loss: 8.8665 - test loss: 13.6817 - train acc: 0.5563 - test acc: 0.4150 - 9m 25s\n",
      "batch: 900/1563 - train loss: 8.8443 - test loss: 15.7247 - train acc: 0.5666 - test acc: 0.3655 - 9m 30s\n",
      "batch: 1000/1563 - train loss: 8.5589 - test loss: 13.7038 - train acc: 0.5759 - test acc: 0.4132 - 9m 35s\n",
      "batch: 1100/1563 - train loss: 8.5601 - test loss: 14.3937 - train acc: 0.5653 - test acc: 0.3897 - 9m 40s\n",
      "batch: 1200/1563 - train loss: 8.9508 - test loss: 14.0457 - train acc: 0.5615 - test acc: 0.4035 - 9m 44s\n",
      "batch: 1300/1563 - train loss: 9.0383 - test loss: 14.0015 - train acc: 0.5528 - test acc: 0.4034 - 9m 49s\n",
      "batch: 1400/1563 - train loss: 9.2850 - test loss: 14.2517 - train acc: 0.5387 - test acc: 0.3964 - 9m 54s\n",
      "batch: 1500/1563 - train loss: 9.1675 - test loss: 13.5856 - train acc: 0.5416 - test acc: 0.4155 - 9m 59s\n",
      "batch: 1563/1563 - train loss: 8.7961 - test loss: 13.9020 - train acc: 0.5563 - test acc: 0.4027 - 10m 3s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4660 - test loss: 14.4694 - train acc: 0.6766 - test acc: 0.3976 - 10m 7s\n",
      "batch: 200/1563 - train loss: 6.5777 - test loss: 14.2845 - train acc: 0.6681 - test acc: 0.4048 - 10m 12s\n",
      "batch: 300/1563 - train loss: 6.5750 - test loss: 14.8906 - train acc: 0.6672 - test acc: 0.3959 - 10m 17s\n",
      "batch: 400/1563 - train loss: 7.1963 - test loss: 14.2535 - train acc: 0.6275 - test acc: 0.4074 - 10m 22s\n",
      "batch: 500/1563 - train loss: 7.4148 - test loss: 14.2539 - train acc: 0.6259 - test acc: 0.4037 - 10m 27s\n",
      "batch: 600/1563 - train loss: 7.5020 - test loss: 14.0258 - train acc: 0.6138 - test acc: 0.4156 - 10m 31s\n",
      "batch: 700/1563 - train loss: 7.3628 - test loss: 14.3389 - train acc: 0.6178 - test acc: 0.4085 - 10m 36s\n",
      "batch: 800/1563 - train loss: 7.8272 - test loss: 14.4295 - train acc: 0.6038 - test acc: 0.4084 - 10m 41s\n",
      "batch: 900/1563 - train loss: 7.8389 - test loss: 15.4796 - train acc: 0.6031 - test acc: 0.3761 - 10m 46s\n",
      "batch: 1000/1563 - train loss: 7.7894 - test loss: 14.8638 - train acc: 0.5988 - test acc: 0.3927 - 10m 51s\n",
      "batch: 1100/1563 - train loss: 7.9870 - test loss: 14.6777 - train acc: 0.5950 - test acc: 0.3995 - 10m 55s\n",
      "batch: 1200/1563 - train loss: 7.8733 - test loss: 15.0374 - train acc: 0.5987 - test acc: 0.3968 - 11m 0s\n",
      "batch: 1300/1563 - train loss: 8.1457 - test loss: 13.9179 - train acc: 0.5918 - test acc: 0.4197 - 11m 5s\n",
      "batch: 1400/1563 - train loss: 8.2353 - test loss: 13.8677 - train acc: 0.5722 - test acc: 0.4177 - 11m 10s\n",
      "batch: 1500/1563 - train loss: 8.1400 - test loss: 14.0243 - train acc: 0.6022 - test acc: 0.4087 - 11m 14s\n",
      "batch: 1563/1563 - train loss: 8.3407 - test loss: 14.1339 - train acc: 0.5816 - test acc: 0.4088 - 11m 19s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.6103 - test loss: 14.3126 - train acc: 0.7162 - test acc: 0.4163 - 11m 23s\n",
      "batch: 200/1563 - train loss: 5.4994 - test loss: 14.3498 - train acc: 0.7187 - test acc: 0.4222 - 11m 28s\n",
      "batch: 300/1563 - train loss: 5.9520 - test loss: 14.7736 - train acc: 0.6879 - test acc: 0.4068 - 11m 33s\n",
      "batch: 400/1563 - train loss: 6.1859 - test loss: 14.8771 - train acc: 0.6713 - test acc: 0.4068 - 11m 37s\n",
      "batch: 500/1563 - train loss: 6.3722 - test loss: 14.9841 - train acc: 0.6706 - test acc: 0.3976 - 11m 42s\n",
      "batch: 600/1563 - train loss: 6.5092 - test loss: 15.7655 - train acc: 0.6669 - test acc: 0.3943 - 11m 47s\n",
      "batch: 700/1563 - train loss: 6.4154 - test loss: 14.6598 - train acc: 0.6672 - test acc: 0.4159 - 11m 52s\n",
      "batch: 800/1563 - train loss: 6.6393 - test loss: 15.3202 - train acc: 0.6588 - test acc: 0.3982 - 11m 57s\n",
      "batch: 900/1563 - train loss: 6.7971 - test loss: 15.4050 - train acc: 0.6482 - test acc: 0.4059 - 12m 1s\n",
      "batch: 1000/1563 - train loss: 7.1032 - test loss: 15.0130 - train acc: 0.6400 - test acc: 0.4072 - 12m 6s\n",
      "batch: 1100/1563 - train loss: 6.8371 - test loss: 14.7876 - train acc: 0.6431 - test acc: 0.4153 - 12m 11s\n",
      "batch: 1200/1563 - train loss: 7.0088 - test loss: 14.5355 - train acc: 0.6434 - test acc: 0.4226 - 12m 15s\n",
      "batch: 1300/1563 - train loss: 7.2296 - test loss: 15.4800 - train acc: 0.6269 - test acc: 0.3984 - 12m 20s\n",
      "batch: 1400/1563 - train loss: 7.4027 - test loss: 14.8714 - train acc: 0.6188 - test acc: 0.4112 - 12m 25s\n",
      "batch: 1500/1563 - train loss: 7.3185 - test loss: 14.5047 - train acc: 0.6203 - test acc: 0.4214 - 12m 30s\n",
      "batch: 1563/1563 - train loss: 7.3899 - test loss: 14.9397 - train acc: 0.6163 - test acc: 0.4016 - 12m 34s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.8510 - test loss: 14.6976 - train acc: 0.7481 - test acc: 0.4188 - 12m 39s\n",
      "batch: 200/1563 - train loss: 4.7454 - test loss: 15.0869 - train acc: 0.7506 - test acc: 0.4088 - 12m 43s\n",
      "batch: 300/1563 - train loss: 4.9239 - test loss: 15.0786 - train acc: 0.7394 - test acc: 0.4178 - 12m 48s\n",
      "batch: 400/1563 - train loss: 5.0126 - test loss: 16.1669 - train acc: 0.7368 - test acc: 0.3982 - 12m 53s\n",
      "batch: 500/1563 - train loss: 5.3560 - test loss: 15.7731 - train acc: 0.7182 - test acc: 0.4039 - 12m 58s\n",
      "batch: 600/1563 - train loss: 5.4063 - test loss: 15.4152 - train acc: 0.7107 - test acc: 0.4075 - 13m 3s\n",
      "batch: 700/1563 - train loss: 5.5537 - test loss: 15.3357 - train acc: 0.7069 - test acc: 0.4159 - 13m 7s\n",
      "batch: 800/1563 - train loss: 5.8966 - test loss: 15.2189 - train acc: 0.6869 - test acc: 0.4205 - 13m 12s\n",
      "batch: 900/1563 - train loss: 5.6627 - test loss: 15.4329 - train acc: 0.7023 - test acc: 0.4166 - 13m 17s\n",
      "batch: 1000/1563 - train loss: 6.0056 - test loss: 15.5418 - train acc: 0.6725 - test acc: 0.4032 - 13m 22s\n",
      "batch: 1100/1563 - train loss: 6.2577 - test loss: 15.2234 - train acc: 0.6725 - test acc: 0.4077 - 13m 26s\n",
      "batch: 1200/1563 - train loss: 6.4780 - test loss: 15.9538 - train acc: 0.6566 - test acc: 0.4056 - 13m 31s\n",
      "batch: 1300/1563 - train loss: 6.5354 - test loss: 15.7737 - train acc: 0.6634 - test acc: 0.4020 - 13m 36s\n",
      "batch: 1400/1563 - train loss: 6.3222 - test loss: 15.1945 - train acc: 0.6663 - test acc: 0.4154 - 13m 41s\n",
      "batch: 1500/1563 - train loss: 6.5005 - test loss: 16.2757 - train acc: 0.6613 - test acc: 0.3803 - 13m 45s\n",
      "batch: 1563/1563 - train loss: 6.5806 - test loss: 15.4595 - train acc: 0.6660 - test acc: 0.4012 - 13m 49s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.0609 - test loss: 15.7505 - train acc: 0.7862 - test acc: 0.4055 - 13m 54s\n",
      "batch: 200/1563 - train loss: 3.9421 - test loss: 15.7285 - train acc: 0.7837 - test acc: 0.4125 - 13m 59s\n",
      "batch: 300/1563 - train loss: 3.9391 - test loss: 16.0811 - train acc: 0.7809 - test acc: 0.4102 - 14m 4s\n",
      "batch: 400/1563 - train loss: 4.2269 - test loss: 16.8811 - train acc: 0.7625 - test acc: 0.3945 - 14m 9s\n",
      "batch: 500/1563 - train loss: 4.5836 - test loss: 17.2450 - train acc: 0.7571 - test acc: 0.3903 - 14m 13s\n",
      "batch: 600/1563 - train loss: 4.6214 - test loss: 16.8129 - train acc: 0.7478 - test acc: 0.3982 - 14m 18s\n",
      "batch: 700/1563 - train loss: 4.9406 - test loss: 16.2870 - train acc: 0.7382 - test acc: 0.4059 - 14m 24s\n",
      "batch: 800/1563 - train loss: 5.1578 - test loss: 16.4435 - train acc: 0.7263 - test acc: 0.4036 - 14m 28s\n",
      "batch: 900/1563 - train loss: 5.2660 - test loss: 16.1920 - train acc: 0.7222 - test acc: 0.4141 - 14m 33s\n",
      "batch: 1000/1563 - train loss: 5.3909 - test loss: 16.8836 - train acc: 0.7165 - test acc: 0.3952 - 14m 38s\n",
      "batch: 1100/1563 - train loss: 5.2497 - test loss: 16.6141 - train acc: 0.7197 - test acc: 0.3945 - 14m 43s\n",
      "batch: 1200/1563 - train loss: 5.1188 - test loss: 16.3937 - train acc: 0.7288 - test acc: 0.4084 - 14m 47s\n",
      "batch: 1300/1563 - train loss: 5.6635 - test loss: 16.2726 - train acc: 0.7075 - test acc: 0.4068 - 14m 52s\n",
      "batch: 1400/1563 - train loss: 5.8844 - test loss: 16.3877 - train acc: 0.6857 - test acc: 0.3947 - 14m 57s\n",
      "batch: 1500/1563 - train loss: 5.7793 - test loss: 15.7838 - train acc: 0.6937 - test acc: 0.4073 - 15m 2s\n",
      "batch: 1563/1563 - train loss: 6.0747 - test loss: 16.0892 - train acc: 0.6860 - test acc: 0.4042 - 15m 6s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.4418 - test loss: 15.6689 - train acc: 0.8156 - test acc: 0.4237 - 15m 11s\n",
      "batch: 200/1563 - train loss: 3.2115 - test loss: 16.4842 - train acc: 0.8296 - test acc: 0.4145 - 15m 15s\n",
      "batch: 300/1563 - train loss: 3.4408 - test loss: 16.3516 - train acc: 0.8184 - test acc: 0.4122 - 15m 20s\n",
      "batch: 400/1563 - train loss: 3.4217 - test loss: 16.6299 - train acc: 0.8150 - test acc: 0.4148 - 15m 25s\n",
      "batch: 500/1563 - train loss: 3.8775 - test loss: 16.3242 - train acc: 0.7878 - test acc: 0.4222 - 15m 30s\n",
      "batch: 600/1563 - train loss: 3.7621 - test loss: 16.9154 - train acc: 0.7900 - test acc: 0.4079 - 15m 35s\n",
      "batch: 700/1563 - train loss: 4.2656 - test loss: 16.7342 - train acc: 0.7550 - test acc: 0.4123 - 15m 40s\n",
      "batch: 800/1563 - train loss: 4.1840 - test loss: 16.9852 - train acc: 0.7731 - test acc: 0.4094 - 15m 44s\n",
      "batch: 900/1563 - train loss: 4.3606 - test loss: 17.1356 - train acc: 0.7578 - test acc: 0.4061 - 15m 49s\n",
      "batch: 1000/1563 - train loss: 4.8397 - test loss: 16.8747 - train acc: 0.7416 - test acc: 0.4170 - 15m 54s\n",
      "batch: 1100/1563 - train loss: 4.8683 - test loss: 16.8788 - train acc: 0.7331 - test acc: 0.4141 - 15m 59s\n",
      "batch: 1200/1563 - train loss: 4.9184 - test loss: 17.3886 - train acc: 0.7375 - test acc: 0.4062 - 16m 4s\n",
      "batch: 1300/1563 - train loss: 5.0565 - test loss: 16.8237 - train acc: 0.7303 - test acc: 0.4057 - 16m 8s\n",
      "batch: 1400/1563 - train loss: 5.1608 - test loss: 17.1565 - train acc: 0.7156 - test acc: 0.3991 - 16m 13s\n",
      "batch: 1500/1563 - train loss: 5.3254 - test loss: 16.4938 - train acc: 0.7182 - test acc: 0.4160 - 16m 18s\n",
      "batch: 1563/1563 - train loss: 5.4703 - test loss: 16.7074 - train acc: 0.7110 - test acc: 0.4048 - 16m 21s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.9979 - test loss: 16.3194 - train acc: 0.8362 - test acc: 0.4122 - 16m 26s\n",
      "batch: 200/1563 - train loss: 2.9972 - test loss: 17.4315 - train acc: 0.8393 - test acc: 0.4073 - 16m 31s\n",
      "batch: 300/1563 - train loss: 2.9927 - test loss: 17.6890 - train acc: 0.8325 - test acc: 0.4032 - 16m 36s\n",
      "batch: 400/1563 - train loss: 2.9134 - test loss: 16.9382 - train acc: 0.8384 - test acc: 0.4187 - 16m 41s\n",
      "batch: 500/1563 - train loss: 3.3170 - test loss: 17.6185 - train acc: 0.8194 - test acc: 0.4116 - 16m 45s\n",
      "batch: 600/1563 - train loss: 3.1855 - test loss: 17.2724 - train acc: 0.8203 - test acc: 0.4125 - 16m 50s\n",
      "batch: 700/1563 - train loss: 3.4847 - test loss: 17.9244 - train acc: 0.8021 - test acc: 0.4017 - 16m 55s\n",
      "batch: 800/1563 - train loss: 3.5808 - test loss: 17.5028 - train acc: 0.8030 - test acc: 0.4164 - 17m 0s\n",
      "batch: 900/1563 - train loss: 3.7976 - test loss: 17.3762 - train acc: 0.7884 - test acc: 0.4179 - 17m 5s\n",
      "batch: 1000/1563 - train loss: 3.9429 - test loss: 17.5203 - train acc: 0.7903 - test acc: 0.4115 - 17m 10s\n",
      "batch: 1100/1563 - train loss: 4.1502 - test loss: 18.2741 - train acc: 0.7715 - test acc: 0.3964 - 17m 14s\n",
      "batch: 1200/1563 - train loss: 4.2315 - test loss: 17.5341 - train acc: 0.7656 - test acc: 0.4150 - 17m 19s\n",
      "batch: 1300/1563 - train loss: 4.1932 - test loss: 17.9134 - train acc: 0.7637 - test acc: 0.3955 - 17m 24s\n",
      "batch: 1400/1563 - train loss: 4.2640 - test loss: 17.0227 - train acc: 0.7684 - test acc: 0.4114 - 17m 28s\n",
      "batch: 1500/1563 - train loss: 4.3681 - test loss: 17.6500 - train acc: 0.7619 - test acc: 0.4112 - 17m 34s\n",
      "batch: 1563/1563 - train loss: 4.7634 - test loss: 18.1911 - train acc: 0.7397 - test acc: 0.3828 - 17m 37s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.6384 - test loss: 17.6737 - train acc: 0.8512 - test acc: 0.4084 - 17m 42s\n",
      "batch: 200/1563 - train loss: 2.6367 - test loss: 17.2514 - train acc: 0.8506 - test acc: 0.4203 - 17m 47s\n",
      "batch: 300/1563 - train loss: 2.4951 - test loss: 17.5308 - train acc: 0.8587 - test acc: 0.4215 - 17m 52s\n",
      "batch: 400/1563 - train loss: 2.5108 - test loss: 17.8339 - train acc: 0.8634 - test acc: 0.4185 - 17m 57s\n",
      "batch: 500/1563 - train loss: 2.9164 - test loss: 17.8956 - train acc: 0.8362 - test acc: 0.4176 - 18m 1s\n",
      "batch: 600/1563 - train loss: 2.6682 - test loss: 18.7289 - train acc: 0.8519 - test acc: 0.4141 - 18m 6s\n",
      "batch: 700/1563 - train loss: 2.9608 - test loss: 17.9986 - train acc: 0.8328 - test acc: 0.4179 - 18m 11s\n",
      "batch: 800/1563 - train loss: 3.0590 - test loss: 18.0772 - train acc: 0.8280 - test acc: 0.4174 - 18m 16s\n",
      "batch: 900/1563 - train loss: 3.1640 - test loss: 17.7623 - train acc: 0.8272 - test acc: 0.4221 - 18m 21s\n",
      "batch: 1000/1563 - train loss: 3.4429 - test loss: 17.9164 - train acc: 0.8106 - test acc: 0.4141 - 18m 25s\n",
      "batch: 1100/1563 - train loss: 3.4365 - test loss: 18.3153 - train acc: 0.8028 - test acc: 0.4116 - 18m 30s\n",
      "batch: 1200/1563 - train loss: 3.6489 - test loss: 18.0930 - train acc: 0.8012 - test acc: 0.4114 - 18m 35s\n",
      "batch: 1300/1563 - train loss: 3.5249 - test loss: 18.1686 - train acc: 0.8025 - test acc: 0.4117 - 18m 40s\n",
      "batch: 1400/1563 - train loss: 3.8971 - test loss: 17.7314 - train acc: 0.7856 - test acc: 0.4091 - 18m 44s\n",
      "batch: 1500/1563 - train loss: 3.8748 - test loss: 17.7992 - train acc: 0.7818 - test acc: 0.4144 - 18m 49s\n",
      "batch: 1563/1563 - train loss: 3.9257 - test loss: 18.1362 - train acc: 0.7844 - test acc: 0.4085 - 18m 53s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.2179 - test loss: 17.5954 - train acc: 0.8773 - test acc: 0.4168 - 18m 58s\n",
      "batch: 200/1563 - train loss: 2.0348 - test loss: 17.9204 - train acc: 0.8860 - test acc: 0.4195 - 19m 3s\n",
      "batch: 300/1563 - train loss: 2.1329 - test loss: 18.1538 - train acc: 0.8800 - test acc: 0.4128 - 19m 8s\n",
      "batch: 400/1563 - train loss: 2.3400 - test loss: 18.6437 - train acc: 0.8707 - test acc: 0.4083 - 19m 12s\n",
      "batch: 500/1563 - train loss: 2.1805 - test loss: 18.3270 - train acc: 0.8760 - test acc: 0.4109 - 19m 17s\n",
      "batch: 600/1563 - train loss: 2.4471 - test loss: 18.3291 - train acc: 0.8675 - test acc: 0.4210 - 19m 22s\n",
      "batch: 700/1563 - train loss: 2.5995 - test loss: 18.4462 - train acc: 0.8509 - test acc: 0.4148 - 19m 26s\n",
      "batch: 800/1563 - train loss: 2.4742 - test loss: 18.7774 - train acc: 0.8638 - test acc: 0.4122 - 19m 31s\n",
      "batch: 900/1563 - train loss: 2.8303 - test loss: 19.6913 - train acc: 0.8399 - test acc: 0.3996 - 19m 36s\n",
      "batch: 1000/1563 - train loss: 2.9364 - test loss: 19.1256 - train acc: 0.8338 - test acc: 0.4076 - 19m 41s\n",
      "batch: 1100/1563 - train loss: 3.1124 - test loss: 18.9083 - train acc: 0.8300 - test acc: 0.4156 - 19m 46s\n",
      "batch: 1200/1563 - train loss: 3.0819 - test loss: 19.6795 - train acc: 0.8243 - test acc: 0.4102 - 19m 50s\n",
      "batch: 1300/1563 - train loss: 3.2743 - test loss: 19.9359 - train acc: 0.8241 - test acc: 0.3931 - 19m 55s\n",
      "batch: 1400/1563 - train loss: 3.2953 - test loss: 19.7039 - train acc: 0.8146 - test acc: 0.3964 - 20m 0s\n",
      "time is up! finishing training\n",
      "batch: 1401/1563 - train loss: 3.2962 - test loss: 19.9752 - train acc: 0.8150 - test acc: 0.3949 - 20m 3s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "testing - beta: 0.9 - part size: 8 - block upd: 64 - combination nº: 2\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 8 - nº part: 1176 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 73728 - partition: 8 - nº part: 9216 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 8192 - partition: 8 - nº part: 1024 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 294912 - partition: 8 - nº part: 36864 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 32768 - partition: 8 - nº part: 4096 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 1179648 - partition: 8 - nº part: 147456 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 131072 - partition: 8 - nº part: 16384 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 51200 - partition: 8 - nº part: 6400 - block updates: 64\n",
      "FisherPartitioner: param: 100 - partition: 8 - nº part: 13 - block updates: 13\n",
      "total partitions: 1403477 - effective block updates: 2557\n",
      "initializing inverses...\n",
      "partition 1/1403477\n",
      "partition 10000/1403477\n",
      "partition 20000/1403477\n",
      "partition 30000/1403477\n",
      "partition 40000/1403477\n",
      "partition 50000/1403477\n",
      "partition 60000/1403477\n",
      "partition 70000/1403477\n",
      "partition 80000/1403477\n",
      "partition 90000/1403477\n",
      "partition 100000/1403477\n",
      "partition 110000/1403477\n",
      "partition 120000/1403477\n",
      "partition 130000/1403477\n",
      "partition 140000/1403477\n",
      "partition 150000/1403477\n",
      "partition 160000/1403477\n",
      "partition 170000/1403477\n",
      "partition 180000/1403477\n",
      "partition 190000/1403477\n",
      "partition 200000/1403477\n",
      "partition 210000/1403477\n",
      "partition 220000/1403477\n",
      "partition 230000/1403477\n",
      "partition 240000/1403477\n",
      "partition 250000/1403477\n",
      "partition 260000/1403477\n",
      "partition 270000/1403477\n",
      "partition 280000/1403477\n",
      "partition 290000/1403477\n",
      "partition 300000/1403477\n",
      "partition 310000/1403477\n",
      "partition 320000/1403477\n",
      "partition 330000/1403477\n",
      "partition 340000/1403477\n",
      "partition 350000/1403477\n",
      "partition 360000/1403477\n",
      "partition 370000/1403477\n",
      "partition 380000/1403477\n",
      "partition 390000/1403477\n",
      "partition 400000/1403477\n",
      "partition 410000/1403477\n",
      "partition 420000/1403477\n",
      "partition 430000/1403477\n",
      "partition 440000/1403477\n",
      "partition 450000/1403477\n",
      "partition 460000/1403477\n",
      "partition 470000/1403477\n",
      "partition 480000/1403477\n",
      "partition 490000/1403477\n",
      "partition 500000/1403477\n",
      "partition 510000/1403477\n",
      "partition 520000/1403477\n",
      "partition 530000/1403477\n",
      "partition 540000/1403477\n",
      "partition 550000/1403477\n",
      "partition 560000/1403477\n",
      "partition 570000/1403477\n",
      "partition 580000/1403477\n",
      "partition 590000/1403477\n",
      "partition 600000/1403477\n",
      "partition 610000/1403477\n",
      "partition 620000/1403477\n",
      "partition 630000/1403477\n",
      "partition 640000/1403477\n",
      "partition 650000/1403477\n",
      "partition 660000/1403477\n",
      "partition 670000/1403477\n",
      "partition 680000/1403477\n",
      "partition 690000/1403477\n",
      "partition 700000/1403477\n",
      "partition 710000/1403477\n",
      "partition 720000/1403477\n",
      "partition 730000/1403477\n",
      "partition 740000/1403477\n",
      "partition 750000/1403477\n",
      "partition 760000/1403477\n",
      "partition 770000/1403477\n",
      "partition 780000/1403477\n",
      "partition 790000/1403477\n",
      "partition 800000/1403477\n",
      "partition 810000/1403477\n",
      "partition 820000/1403477\n",
      "partition 830000/1403477\n",
      "partition 840000/1403477\n",
      "partition 850000/1403477\n",
      "partition 860000/1403477\n",
      "partition 870000/1403477\n",
      "partition 880000/1403477\n",
      "partition 890000/1403477\n",
      "partition 900000/1403477\n",
      "partition 910000/1403477\n",
      "partition 920000/1403477\n",
      "partition 930000/1403477\n",
      "partition 940000/1403477\n",
      "partition 950000/1403477\n",
      "partition 960000/1403477\n",
      "partition 970000/1403477\n",
      "partition 980000/1403477\n",
      "partition 990000/1403477\n",
      "partition 1000000/1403477\n",
      "partition 1010000/1403477\n",
      "partition 1020000/1403477\n",
      "partition 1030000/1403477\n",
      "partition 1040000/1403477\n",
      "partition 1050000/1403477\n",
      "partition 1060000/1403477\n",
      "partition 1070000/1403477\n",
      "partition 1080000/1403477\n",
      "partition 1090000/1403477\n",
      "partition 1100000/1403477\n",
      "partition 1110000/1403477\n",
      "partition 1120000/1403477\n",
      "partition 1130000/1403477\n",
      "partition 1140000/1403477\n",
      "partition 1150000/1403477\n",
      "partition 1160000/1403477\n",
      "partition 1170000/1403477\n",
      "partition 1180000/1403477\n",
      "partition 1190000/1403477\n",
      "partition 1200000/1403477\n",
      "partition 1210000/1403477\n",
      "partition 1220000/1403477\n",
      "partition 1230000/1403477\n",
      "partition 1240000/1403477\n",
      "partition 1250000/1403477\n",
      "partition 1260000/1403477\n",
      "partition 1270000/1403477\n",
      "partition 1280000/1403477\n",
      "partition 1290000/1403477\n",
      "partition 1300000/1403477\n",
      "partition 1310000/1403477\n",
      "partition 1320000/1403477\n",
      "partition 1330000/1403477\n",
      "partition 1340000/1403477\n",
      "partition 1350000/1403477\n",
      "partition 1360000/1403477\n",
      "partition 1370000/1403477\n",
      "partition 1380000/1403477\n",
      "partition 1390000/1403477\n",
      "partition 1400000/1403477\n",
      "partition 1403477/1403477\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0847 - test loss: 25.2974 - train acc: 0.0380 - test acc: 0.0505 - 0m 1s\n",
      "batch: 200/1563 - train loss: 23.9854 - test loss: 23.5752 - train acc: 0.0636 - test acc: 0.0753 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.0401 - test loss: 22.1463 - train acc: 0.0833 - test acc: 0.1032 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.1966 - test loss: 23.0156 - train acc: 0.0987 - test acc: 0.0936 - 0m 15s\n",
      "batch: 500/1563 - train loss: 21.6042 - test loss: 21.1042 - train acc: 0.1115 - test acc: 0.1294 - 0m 20s\n",
      "batch: 600/1563 - train loss: 21.1362 - test loss: 21.2384 - train acc: 0.1238 - test acc: 0.1298 - 0m 25s\n",
      "batch: 700/1563 - train loss: 20.9231 - test loss: 20.7576 - train acc: 0.1321 - test acc: 0.1440 - 0m 30s\n",
      "batch: 800/1563 - train loss: 20.5602 - test loss: 20.0543 - train acc: 0.1434 - test acc: 0.1645 - 0m 34s\n",
      "batch: 900/1563 - train loss: 20.4009 - test loss: 19.7094 - train acc: 0.1469 - test acc: 0.1666 - 0m 39s\n",
      "batch: 1000/1563 - train loss: 19.9088 - test loss: 19.6519 - train acc: 0.1641 - test acc: 0.1685 - 0m 44s\n",
      "batch: 1100/1563 - train loss: 19.8439 - test loss: 19.0824 - train acc: 0.1585 - test acc: 0.1891 - 0m 48s\n",
      "batch: 1200/1563 - train loss: 19.1802 - test loss: 19.0839 - train acc: 0.1860 - test acc: 0.1894 - 0m 53s\n",
      "batch: 1300/1563 - train loss: 19.1658 - test loss: 18.8031 - train acc: 0.1797 - test acc: 0.1999 - 0m 58s\n",
      "batch: 1400/1563 - train loss: 18.9596 - test loss: 18.9127 - train acc: 0.1798 - test acc: 0.1894 - 1m 3s\n",
      "batch: 1500/1563 - train loss: 18.8513 - test loss: 18.5769 - train acc: 0.2010 - test acc: 0.2043 - 1m 7s\n",
      "batch: 1563/1563 - train loss: 18.6892 - test loss: 18.2152 - train acc: 0.2069 - test acc: 0.2124 - 1m 11s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.7231 - test loss: 18.1373 - train acc: 0.2431 - test acc: 0.2201 - 1m 16s\n",
      "batch: 200/1563 - train loss: 17.7353 - test loss: 18.6500 - train acc: 0.2359 - test acc: 0.2105 - 1m 21s\n",
      "batch: 300/1563 - train loss: 17.7204 - test loss: 17.7678 - train acc: 0.2197 - test acc: 0.2338 - 1m 25s\n",
      "batch: 400/1563 - train loss: 17.2337 - test loss: 18.3707 - train acc: 0.2441 - test acc: 0.2064 - 1m 30s\n",
      "batch: 500/1563 - train loss: 17.4447 - test loss: 17.3691 - train acc: 0.2419 - test acc: 0.2462 - 1m 35s\n",
      "batch: 600/1563 - train loss: 17.3217 - test loss: 18.2202 - train acc: 0.2522 - test acc: 0.2308 - 1m 40s\n",
      "batch: 700/1563 - train loss: 17.3719 - test loss: 17.7035 - train acc: 0.2472 - test acc: 0.2371 - 1m 44s\n",
      "batch: 800/1563 - train loss: 16.6527 - test loss: 16.8508 - train acc: 0.2703 - test acc: 0.2612 - 1m 49s\n",
      "batch: 900/1563 - train loss: 16.8956 - test loss: 18.6031 - train acc: 0.2428 - test acc: 0.2160 - 1m 54s\n",
      "batch: 1000/1563 - train loss: 16.9409 - test loss: 16.9486 - train acc: 0.2466 - test acc: 0.2573 - 1m 58s\n",
      "batch: 1100/1563 - train loss: 16.5948 - test loss: 16.6486 - train acc: 0.2685 - test acc: 0.2649 - 2m 3s\n",
      "batch: 1200/1563 - train loss: 16.7715 - test loss: 16.8519 - train acc: 0.2625 - test acc: 0.2615 - 2m 8s\n",
      "batch: 1300/1563 - train loss: 16.2222 - test loss: 17.3360 - train acc: 0.2803 - test acc: 0.2477 - 2m 13s\n",
      "batch: 1400/1563 - train loss: 16.0537 - test loss: 17.9520 - train acc: 0.2725 - test acc: 0.2404 - 2m 17s\n",
      "batch: 1500/1563 - train loss: 16.3389 - test loss: 16.0510 - train acc: 0.2750 - test acc: 0.2867 - 2m 22s\n",
      "batch: 1563/1563 - train loss: 16.2537 - test loss: 16.2872 - train acc: 0.2769 - test acc: 0.2791 - 2m 26s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.8905 - test loss: 15.8302 - train acc: 0.3294 - test acc: 0.3013 - 2m 31s\n",
      "batch: 200/1563 - train loss: 14.8426 - test loss: 16.8898 - train acc: 0.3369 - test acc: 0.2693 - 2m 35s\n",
      "batch: 300/1563 - train loss: 14.9001 - test loss: 16.4071 - train acc: 0.3197 - test acc: 0.2856 - 2m 40s\n",
      "batch: 400/1563 - train loss: 14.5517 - test loss: 15.4659 - train acc: 0.3356 - test acc: 0.3069 - 2m 45s\n",
      "batch: 500/1563 - train loss: 14.8339 - test loss: 16.0489 - train acc: 0.3206 - test acc: 0.2869 - 2m 49s\n",
      "batch: 600/1563 - train loss: 14.9289 - test loss: 15.5430 - train acc: 0.3318 - test acc: 0.3009 - 2m 54s\n",
      "batch: 700/1563 - train loss: 14.8321 - test loss: 16.3579 - train acc: 0.3284 - test acc: 0.2805 - 2m 58s\n",
      "batch: 800/1563 - train loss: 14.8363 - test loss: 15.0242 - train acc: 0.3331 - test acc: 0.3334 - 3m 3s\n",
      "batch: 900/1563 - train loss: 14.6419 - test loss: 15.2023 - train acc: 0.3337 - test acc: 0.3264 - 3m 8s\n",
      "batch: 1000/1563 - train loss: 14.3161 - test loss: 15.7998 - train acc: 0.3587 - test acc: 0.3085 - 3m 13s\n",
      "batch: 1100/1563 - train loss: 14.6589 - test loss: 15.1104 - train acc: 0.3307 - test acc: 0.3327 - 3m 18s\n",
      "batch: 1200/1563 - train loss: 14.9635 - test loss: 15.5949 - train acc: 0.3390 - test acc: 0.3170 - 3m 22s\n",
      "batch: 1300/1563 - train loss: 14.2599 - test loss: 14.4626 - train acc: 0.3562 - test acc: 0.3542 - 3m 27s\n",
      "batch: 1400/1563 - train loss: 14.1089 - test loss: 15.5936 - train acc: 0.3532 - test acc: 0.3134 - 3m 32s\n",
      "batch: 1500/1563 - train loss: 14.4764 - test loss: 15.9297 - train acc: 0.3425 - test acc: 0.3003 - 3m 36s\n",
      "batch: 1563/1563 - train loss: 14.3318 - test loss: 14.9083 - train acc: 0.3425 - test acc: 0.3338 - 3m 40s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.6505 - test loss: 14.4242 - train acc: 0.3953 - test acc: 0.3487 - 3m 45s\n",
      "batch: 200/1563 - train loss: 12.4498 - test loss: 14.8859 - train acc: 0.4223 - test acc: 0.3417 - 3m 50s\n",
      "batch: 300/1563 - train loss: 13.1109 - test loss: 16.3087 - train acc: 0.3907 - test acc: 0.2936 - 3m 55s\n",
      "batch: 400/1563 - train loss: 13.1718 - test loss: 15.0383 - train acc: 0.3906 - test acc: 0.3383 - 3m 59s\n",
      "batch: 500/1563 - train loss: 13.1773 - test loss: 14.8438 - train acc: 0.3944 - test acc: 0.3401 - 4m 4s\n",
      "batch: 600/1563 - train loss: 13.0657 - test loss: 15.5185 - train acc: 0.3984 - test acc: 0.3197 - 4m 9s\n",
      "batch: 700/1563 - train loss: 12.9592 - test loss: 14.6822 - train acc: 0.4044 - test acc: 0.3483 - 4m 13s\n",
      "batch: 800/1563 - train loss: 13.2604 - test loss: 14.3862 - train acc: 0.3954 - test acc: 0.3563 - 4m 18s\n",
      "batch: 900/1563 - train loss: 12.9564 - test loss: 14.4784 - train acc: 0.3982 - test acc: 0.3538 - 4m 23s\n",
      "batch: 1000/1563 - train loss: 13.0178 - test loss: 14.9441 - train acc: 0.4047 - test acc: 0.3414 - 4m 28s\n",
      "batch: 1100/1563 - train loss: 13.2002 - test loss: 14.2390 - train acc: 0.3925 - test acc: 0.3650 - 4m 32s\n",
      "batch: 1200/1563 - train loss: 12.9607 - test loss: 13.9986 - train acc: 0.3985 - test acc: 0.3722 - 4m 37s\n",
      "batch: 1300/1563 - train loss: 13.0738 - test loss: 14.4777 - train acc: 0.3947 - test acc: 0.3488 - 4m 42s\n",
      "batch: 1400/1563 - train loss: 13.2816 - test loss: 13.7886 - train acc: 0.3760 - test acc: 0.3733 - 4m 46s\n",
      "batch: 1500/1563 - train loss: 13.0226 - test loss: 15.6265 - train acc: 0.4006 - test acc: 0.3321 - 4m 51s\n",
      "batch: 1563/1563 - train loss: 13.2547 - test loss: 13.6225 - train acc: 0.3972 - test acc: 0.3769 - 4m 55s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.1992 - test loss: 14.7263 - train acc: 0.4650 - test acc: 0.3567 - 5m 0s\n",
      "batch: 200/1563 - train loss: 11.2039 - test loss: 14.3220 - train acc: 0.4634 - test acc: 0.3632 - 5m 5s\n",
      "batch: 300/1563 - train loss: 11.6461 - test loss: 13.6838 - train acc: 0.4519 - test acc: 0.3821 - 5m 9s\n",
      "batch: 400/1563 - train loss: 11.6224 - test loss: 14.7508 - train acc: 0.4513 - test acc: 0.3479 - 5m 14s\n",
      "batch: 500/1563 - train loss: 11.8446 - test loss: 14.7007 - train acc: 0.4410 - test acc: 0.3492 - 5m 19s\n",
      "batch: 600/1563 - train loss: 11.7490 - test loss: 15.2666 - train acc: 0.4547 - test acc: 0.3366 - 5m 24s\n",
      "batch: 700/1563 - train loss: 11.8333 - test loss: 13.9595 - train acc: 0.4501 - test acc: 0.3777 - 5m 28s\n",
      "batch: 800/1563 - train loss: 11.6631 - test loss: 15.3329 - train acc: 0.4381 - test acc: 0.3409 - 5m 33s\n",
      "batch: 900/1563 - train loss: 11.6755 - test loss: 13.9169 - train acc: 0.4501 - test acc: 0.3846 - 5m 38s\n",
      "batch: 1000/1563 - train loss: 11.6904 - test loss: 13.8700 - train acc: 0.4450 - test acc: 0.3839 - 5m 42s\n",
      "batch: 1100/1563 - train loss: 12.0925 - test loss: 13.7334 - train acc: 0.4325 - test acc: 0.3877 - 5m 47s\n",
      "batch: 1200/1563 - train loss: 11.6513 - test loss: 13.6889 - train acc: 0.4472 - test acc: 0.3838 - 5m 52s\n",
      "batch: 1300/1563 - train loss: 12.2312 - test loss: 13.7958 - train acc: 0.4309 - test acc: 0.3860 - 5m 57s\n",
      "batch: 1400/1563 - train loss: 11.9455 - test loss: 14.1727 - train acc: 0.4269 - test acc: 0.3705 - 6m 1s\n",
      "batch: 1500/1563 - train loss: 11.4927 - test loss: 13.4640 - train acc: 0.4525 - test acc: 0.3941 - 6m 6s\n",
      "batch: 1563/1563 - train loss: 11.6886 - test loss: 13.5489 - train acc: 0.4459 - test acc: 0.3961 - 6m 10s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.7938 - test loss: 14.2833 - train acc: 0.5231 - test acc: 0.3702 - 6m 14s\n",
      "batch: 200/1563 - train loss: 9.8252 - test loss: 13.3461 - train acc: 0.5231 - test acc: 0.4090 - 6m 19s\n",
      "batch: 300/1563 - train loss: 10.0180 - test loss: 14.0682 - train acc: 0.5116 - test acc: 0.3934 - 6m 24s\n",
      "batch: 400/1563 - train loss: 10.4053 - test loss: 15.5967 - train acc: 0.4916 - test acc: 0.3462 - 6m 29s\n",
      "batch: 500/1563 - train loss: 10.6666 - test loss: 14.3390 - train acc: 0.4791 - test acc: 0.3673 - 6m 33s\n",
      "batch: 600/1563 - train loss: 10.5570 - test loss: 13.9592 - train acc: 0.4890 - test acc: 0.3811 - 6m 38s\n",
      "batch: 700/1563 - train loss: 10.6574 - test loss: 14.4865 - train acc: 0.4878 - test acc: 0.3782 - 6m 43s\n",
      "batch: 800/1563 - train loss: 10.2164 - test loss: 14.4128 - train acc: 0.4959 - test acc: 0.3770 - 6m 48s\n",
      "batch: 900/1563 - train loss: 10.9869 - test loss: 13.3959 - train acc: 0.4753 - test acc: 0.4026 - 6m 52s\n",
      "batch: 1000/1563 - train loss: 10.7963 - test loss: 13.5856 - train acc: 0.4819 - test acc: 0.3963 - 6m 57s\n",
      "batch: 1100/1563 - train loss: 10.8202 - test loss: 13.9219 - train acc: 0.4868 - test acc: 0.3925 - 7m 2s\n",
      "batch: 1200/1563 - train loss: 10.8963 - test loss: 13.4597 - train acc: 0.4882 - test acc: 0.4035 - 7m 6s\n",
      "batch: 1300/1563 - train loss: 10.4675 - test loss: 13.2491 - train acc: 0.4947 - test acc: 0.4095 - 7m 11s\n",
      "batch: 1400/1563 - train loss: 10.7446 - test loss: 13.6319 - train acc: 0.4969 - test acc: 0.3960 - 7m 16s\n",
      "batch: 1500/1563 - train loss: 10.8117 - test loss: 13.2124 - train acc: 0.4898 - test acc: 0.4134 - 7m 21s\n",
      "batch: 1563/1563 - train loss: 11.0251 - test loss: 13.4848 - train acc: 0.4769 - test acc: 0.4048 - 7m 25s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.3665 - test loss: 13.9715 - train acc: 0.5940 - test acc: 0.4049 - 7m 29s\n",
      "batch: 200/1563 - train loss: 8.4635 - test loss: 14.2000 - train acc: 0.5841 - test acc: 0.3930 - 7m 34s\n",
      "batch: 300/1563 - train loss: 9.1349 - test loss: 14.1353 - train acc: 0.5431 - test acc: 0.3920 - 7m 39s\n",
      "batch: 400/1563 - train loss: 9.0478 - test loss: 13.7886 - train acc: 0.5522 - test acc: 0.4036 - 7m 43s\n",
      "batch: 500/1563 - train loss: 9.3035 - test loss: 13.7615 - train acc: 0.5375 - test acc: 0.4010 - 7m 48s\n",
      "batch: 600/1563 - train loss: 9.3119 - test loss: 13.6129 - train acc: 0.5412 - test acc: 0.4114 - 7m 53s\n",
      "batch: 700/1563 - train loss: 9.4400 - test loss: 13.6925 - train acc: 0.5384 - test acc: 0.4046 - 7m 58s\n",
      "batch: 800/1563 - train loss: 9.4577 - test loss: 13.3132 - train acc: 0.5371 - test acc: 0.4179 - 8m 2s\n",
      "batch: 900/1563 - train loss: 9.8476 - test loss: 13.9824 - train acc: 0.5256 - test acc: 0.3992 - 8m 7s\n",
      "batch: 1000/1563 - train loss: 9.7068 - test loss: 13.8420 - train acc: 0.5234 - test acc: 0.4026 - 8m 12s\n",
      "batch: 1100/1563 - train loss: 9.7190 - test loss: 13.5032 - train acc: 0.5253 - test acc: 0.4121 - 8m 16s\n",
      "batch: 1200/1563 - train loss: 10.0147 - test loss: 14.8865 - train acc: 0.5094 - test acc: 0.3771 - 8m 21s\n",
      "batch: 1300/1563 - train loss: 9.7370 - test loss: 13.6867 - train acc: 0.5222 - test acc: 0.4100 - 8m 26s\n",
      "batch: 1400/1563 - train loss: 10.1036 - test loss: 13.5160 - train acc: 0.5112 - test acc: 0.4049 - 8m 31s\n",
      "batch: 1500/1563 - train loss: 9.9185 - test loss: 13.4617 - train acc: 0.5087 - test acc: 0.4119 - 8m 36s\n",
      "batch: 1563/1563 - train loss: 10.1519 - test loss: 13.5011 - train acc: 0.5068 - test acc: 0.4115 - 8m 39s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.2030 - test loss: 13.4171 - train acc: 0.6491 - test acc: 0.4205 - 8m 44s\n",
      "batch: 200/1563 - train loss: 7.3888 - test loss: 14.1652 - train acc: 0.6300 - test acc: 0.4110 - 8m 49s\n",
      "batch: 300/1563 - train loss: 7.7750 - test loss: 14.5229 - train acc: 0.6066 - test acc: 0.3905 - 8m 54s\n",
      "batch: 400/1563 - train loss: 8.1006 - test loss: 13.8841 - train acc: 0.5912 - test acc: 0.4134 - 8m 59s\n",
      "batch: 500/1563 - train loss: 7.8176 - test loss: 13.6073 - train acc: 0.6078 - test acc: 0.4199 - 9m 4s\n",
      "batch: 600/1563 - train loss: 8.5017 - test loss: 15.5871 - train acc: 0.5725 - test acc: 0.3758 - 9m 8s\n",
      "batch: 700/1563 - train loss: 8.3986 - test loss: 13.8783 - train acc: 0.5765 - test acc: 0.4106 - 9m 13s\n",
      "batch: 800/1563 - train loss: 8.3392 - test loss: 15.9206 - train acc: 0.5822 - test acc: 0.3509 - 9m 18s\n",
      "batch: 900/1563 - train loss: 8.7777 - test loss: 14.4954 - train acc: 0.5687 - test acc: 0.3973 - 9m 23s\n",
      "batch: 1000/1563 - train loss: 9.1498 - test loss: 13.9267 - train acc: 0.5509 - test acc: 0.4011 - 9m 28s\n",
      "batch: 1100/1563 - train loss: 8.8006 - test loss: 13.6703 - train acc: 0.5569 - test acc: 0.4164 - 9m 33s\n",
      "batch: 1200/1563 - train loss: 8.5592 - test loss: 13.7859 - train acc: 0.5887 - test acc: 0.4188 - 9m 38s\n",
      "batch: 1300/1563 - train loss: 9.0224 - test loss: 13.6023 - train acc: 0.5607 - test acc: 0.4153 - 9m 43s\n",
      "batch: 1400/1563 - train loss: 8.9010 - test loss: 14.2852 - train acc: 0.5512 - test acc: 0.3891 - 9m 48s\n",
      "batch: 1500/1563 - train loss: 8.9071 - test loss: 13.7138 - train acc: 0.5559 - test acc: 0.4129 - 9m 53s\n",
      "batch: 1563/1563 - train loss: 9.0430 - test loss: 13.9738 - train acc: 0.5515 - test acc: 0.4089 - 9m 57s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.3314 - test loss: 13.7971 - train acc: 0.6763 - test acc: 0.4179 - 10m 2s\n",
      "batch: 200/1563 - train loss: 6.4271 - test loss: 15.4732 - train acc: 0.6709 - test acc: 0.3805 - 10m 7s\n",
      "batch: 300/1563 - train loss: 6.4696 - test loss: 14.0200 - train acc: 0.6710 - test acc: 0.4171 - 10m 12s\n",
      "batch: 400/1563 - train loss: 7.1666 - test loss: 14.6947 - train acc: 0.6366 - test acc: 0.4136 - 10m 16s\n",
      "batch: 500/1563 - train loss: 7.1546 - test loss: 14.2562 - train acc: 0.6328 - test acc: 0.4168 - 10m 21s\n",
      "batch: 600/1563 - train loss: 6.9151 - test loss: 14.6149 - train acc: 0.6406 - test acc: 0.4126 - 10m 26s\n",
      "batch: 700/1563 - train loss: 7.2286 - test loss: 14.7501 - train acc: 0.6275 - test acc: 0.4027 - 10m 31s\n",
      "batch: 800/1563 - train loss: 7.8273 - test loss: 14.0975 - train acc: 0.6059 - test acc: 0.4142 - 10m 36s\n",
      "batch: 900/1563 - train loss: 7.7460 - test loss: 14.7410 - train acc: 0.6137 - test acc: 0.4005 - 10m 41s\n",
      "batch: 1000/1563 - train loss: 7.7340 - test loss: 14.5062 - train acc: 0.6031 - test acc: 0.4072 - 10m 46s\n",
      "batch: 1100/1563 - train loss: 7.9975 - test loss: 14.7576 - train acc: 0.6025 - test acc: 0.3979 - 10m 50s\n",
      "batch: 1200/1563 - train loss: 8.1703 - test loss: 14.3366 - train acc: 0.5874 - test acc: 0.4088 - 10m 55s\n",
      "batch: 1300/1563 - train loss: 7.7796 - test loss: 14.2544 - train acc: 0.6002 - test acc: 0.4111 - 11m 0s\n",
      "batch: 1400/1563 - train loss: 7.8534 - test loss: 16.6803 - train acc: 0.6116 - test acc: 0.3646 - 11m 5s\n",
      "batch: 1500/1563 - train loss: 8.1218 - test loss: 13.7929 - train acc: 0.5913 - test acc: 0.4250 - 11m 10s\n",
      "batch: 1563/1563 - train loss: 8.0008 - test loss: 13.9938 - train acc: 0.5884 - test acc: 0.4151 - 11m 14s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.7231 - test loss: 13.7195 - train acc: 0.7072 - test acc: 0.4357 - 11m 18s\n",
      "batch: 200/1563 - train loss: 5.4716 - test loss: 15.1196 - train acc: 0.7113 - test acc: 0.4070 - 11m 23s\n",
      "batch: 300/1563 - train loss: 5.7406 - test loss: 15.6466 - train acc: 0.6982 - test acc: 0.3986 - 11m 28s\n",
      "batch: 400/1563 - train loss: 6.0068 - test loss: 15.1147 - train acc: 0.6816 - test acc: 0.4103 - 11m 33s\n",
      "batch: 500/1563 - train loss: 6.1034 - test loss: 14.5234 - train acc: 0.6873 - test acc: 0.4203 - 11m 37s\n",
      "batch: 600/1563 - train loss: 6.4525 - test loss: 14.7242 - train acc: 0.6622 - test acc: 0.4139 - 11m 42s\n",
      "batch: 700/1563 - train loss: 6.5399 - test loss: 14.8544 - train acc: 0.6600 - test acc: 0.4206 - 11m 47s\n",
      "batch: 800/1563 - train loss: 6.4500 - test loss: 15.8467 - train acc: 0.6647 - test acc: 0.3920 - 11m 51s\n",
      "batch: 900/1563 - train loss: 6.9004 - test loss: 14.9938 - train acc: 0.6459 - test acc: 0.4055 - 11m 56s\n",
      "batch: 1000/1563 - train loss: 6.9396 - test loss: 14.5874 - train acc: 0.6385 - test acc: 0.4158 - 12m 1s\n",
      "batch: 1100/1563 - train loss: 6.7852 - test loss: 15.1324 - train acc: 0.6473 - test acc: 0.4046 - 12m 6s\n",
      "batch: 1200/1563 - train loss: 6.8454 - test loss: 15.4210 - train acc: 0.6447 - test acc: 0.3930 - 12m 11s\n",
      "batch: 1300/1563 - train loss: 6.9081 - test loss: 15.4201 - train acc: 0.6391 - test acc: 0.3925 - 12m 15s\n",
      "batch: 1400/1563 - train loss: 7.2229 - test loss: 14.6822 - train acc: 0.6297 - test acc: 0.4175 - 12m 20s\n",
      "batch: 1500/1563 - train loss: 7.2200 - test loss: 14.6849 - train acc: 0.6344 - test acc: 0.4134 - 12m 25s\n",
      "batch: 1563/1563 - train loss: 7.1545 - test loss: 14.6264 - train acc: 0.6412 - test acc: 0.4169 - 12m 29s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.3876 - test loss: 14.3511 - train acc: 0.7784 - test acc: 0.4298 - 12m 34s\n",
      "batch: 200/1563 - train loss: 4.3093 - test loss: 15.0405 - train acc: 0.7725 - test acc: 0.4248 - 12m 39s\n",
      "batch: 300/1563 - train loss: 4.6146 - test loss: 15.3175 - train acc: 0.7544 - test acc: 0.4201 - 12m 44s\n",
      "batch: 400/1563 - train loss: 5.1825 - test loss: 15.6373 - train acc: 0.7269 - test acc: 0.4139 - 12m 48s\n",
      "batch: 500/1563 - train loss: 5.0459 - test loss: 15.8362 - train acc: 0.7278 - test acc: 0.4035 - 12m 53s\n",
      "batch: 600/1563 - train loss: 5.3289 - test loss: 15.9847 - train acc: 0.7200 - test acc: 0.4020 - 12m 58s\n",
      "batch: 700/1563 - train loss: 5.9528 - test loss: 15.3253 - train acc: 0.6973 - test acc: 0.4081 - 13m 2s\n",
      "batch: 800/1563 - train loss: 5.8669 - test loss: 16.9867 - train acc: 0.6891 - test acc: 0.3829 - 13m 8s\n",
      "batch: 900/1563 - train loss: 5.6644 - test loss: 15.3996 - train acc: 0.7019 - test acc: 0.4152 - 13m 12s\n",
      "batch: 1000/1563 - train loss: 6.1954 - test loss: 15.0443 - train acc: 0.6769 - test acc: 0.4192 - 13m 17s\n",
      "batch: 1100/1563 - train loss: 6.2770 - test loss: 15.8865 - train acc: 0.6800 - test acc: 0.3954 - 13m 22s\n",
      "batch: 1200/1563 - train loss: 6.4131 - test loss: 15.1716 - train acc: 0.6560 - test acc: 0.4164 - 13m 27s\n",
      "batch: 1300/1563 - train loss: 6.3136 - test loss: 16.0213 - train acc: 0.6719 - test acc: 0.3989 - 13m 31s\n",
      "batch: 1400/1563 - train loss: 6.5749 - test loss: 15.0421 - train acc: 0.6497 - test acc: 0.4172 - 13m 36s\n",
      "batch: 1500/1563 - train loss: 6.1707 - test loss: 15.1337 - train acc: 0.6732 - test acc: 0.4207 - 13m 41s\n",
      "batch: 1563/1563 - train loss: 6.1434 - test loss: 15.1674 - train acc: 0.6747 - test acc: 0.4116 - 13m 45s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.8767 - test loss: 15.2425 - train acc: 0.7968 - test acc: 0.4234 - 13m 50s\n",
      "batch: 200/1563 - train loss: 3.6317 - test loss: 15.1740 - train acc: 0.8031 - test acc: 0.4312 - 13m 55s\n",
      "batch: 300/1563 - train loss: 3.9130 - test loss: 15.9026 - train acc: 0.7884 - test acc: 0.4172 - 13m 59s\n",
      "batch: 400/1563 - train loss: 4.0638 - test loss: 15.9676 - train acc: 0.7790 - test acc: 0.4217 - 14m 4s\n",
      "batch: 500/1563 - train loss: 4.3992 - test loss: 16.4141 - train acc: 0.7622 - test acc: 0.4083 - 14m 9s\n",
      "batch: 600/1563 - train loss: 4.4779 - test loss: 16.0664 - train acc: 0.7603 - test acc: 0.4187 - 14m 14s\n",
      "batch: 700/1563 - train loss: 4.8883 - test loss: 16.2271 - train acc: 0.7353 - test acc: 0.4087 - 14m 18s\n",
      "batch: 800/1563 - train loss: 4.9223 - test loss: 15.7822 - train acc: 0.7294 - test acc: 0.4178 - 14m 23s\n",
      "batch: 900/1563 - train loss: 4.8883 - test loss: 16.5460 - train acc: 0.7363 - test acc: 0.4068 - 14m 28s\n",
      "batch: 1000/1563 - train loss: 4.9855 - test loss: 16.0710 - train acc: 0.7325 - test acc: 0.4107 - 14m 33s\n",
      "batch: 1100/1563 - train loss: 5.0979 - test loss: 16.1806 - train acc: 0.7231 - test acc: 0.4161 - 14m 38s\n",
      "batch: 1200/1563 - train loss: 5.3305 - test loss: 16.0658 - train acc: 0.7126 - test acc: 0.4184 - 14m 43s\n",
      "batch: 1300/1563 - train loss: 5.5131 - test loss: 15.7004 - train acc: 0.7022 - test acc: 0.4125 - 14m 48s\n",
      "batch: 1400/1563 - train loss: 5.5952 - test loss: 16.1814 - train acc: 0.7009 - test acc: 0.4048 - 14m 52s\n",
      "batch: 1500/1563 - train loss: 5.6429 - test loss: 16.6509 - train acc: 0.6979 - test acc: 0.4006 - 14m 57s\n",
      "batch: 1563/1563 - train loss: 5.7758 - test loss: 16.2403 - train acc: 0.6941 - test acc: 0.4036 - 15m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.2842 - test loss: 15.7542 - train acc: 0.8253 - test acc: 0.4258 - 15m 6s\n",
      "batch: 200/1563 - train loss: 3.2226 - test loss: 16.0857 - train acc: 0.8231 - test acc: 0.4207 - 15m 11s\n",
      "batch: 300/1563 - train loss: 3.2083 - test loss: 16.1474 - train acc: 0.8196 - test acc: 0.4245 - 15m 16s\n",
      "batch: 400/1563 - train loss: 3.3650 - test loss: 16.9914 - train acc: 0.8165 - test acc: 0.4137 - 15m 20s\n",
      "batch: 500/1563 - train loss: 3.7214 - test loss: 16.7010 - train acc: 0.8021 - test acc: 0.4207 - 15m 25s\n",
      "batch: 600/1563 - train loss: 3.7718 - test loss: 16.9930 - train acc: 0.7937 - test acc: 0.4102 - 15m 30s\n",
      "batch: 700/1563 - train loss: 4.1523 - test loss: 16.6154 - train acc: 0.7728 - test acc: 0.4213 - 15m 34s\n",
      "batch: 800/1563 - train loss: 4.1820 - test loss: 18.0949 - train acc: 0.7737 - test acc: 0.3851 - 15m 39s\n",
      "batch: 900/1563 - train loss: 4.6012 - test loss: 16.5233 - train acc: 0.7469 - test acc: 0.4175 - 15m 44s\n",
      "batch: 1000/1563 - train loss: 4.6786 - test loss: 16.8878 - train acc: 0.7488 - test acc: 0.4003 - 15m 49s\n",
      "batch: 1100/1563 - train loss: 4.6374 - test loss: 16.4013 - train acc: 0.7475 - test acc: 0.4136 - 15m 54s\n",
      "batch: 1200/1563 - train loss: 4.7324 - test loss: 16.6530 - train acc: 0.7537 - test acc: 0.4139 - 15m 58s\n",
      "batch: 1300/1563 - train loss: 4.8312 - test loss: 17.3678 - train acc: 0.7362 - test acc: 0.3992 - 16m 3s\n",
      "batch: 1400/1563 - train loss: 4.9815 - test loss: 16.3107 - train acc: 0.7350 - test acc: 0.4103 - 16m 8s\n",
      "batch: 1500/1563 - train loss: 4.9540 - test loss: 17.8539 - train acc: 0.7381 - test acc: 0.3880 - 16m 13s\n",
      "batch: 1563/1563 - train loss: 5.0609 - test loss: 17.6613 - train acc: 0.7312 - test acc: 0.3975 - 16m 17s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 3.0411 - test loss: 16.4721 - train acc: 0.8340 - test acc: 0.4188 - 16m 21s\n",
      "batch: 200/1563 - train loss: 2.5860 - test loss: 16.6125 - train acc: 0.8572 - test acc: 0.4258 - 16m 26s\n",
      "batch: 300/1563 - train loss: 2.7959 - test loss: 17.7497 - train acc: 0.8462 - test acc: 0.4043 - 16m 31s\n",
      "batch: 400/1563 - train loss: 2.9615 - test loss: 17.4883 - train acc: 0.8315 - test acc: 0.4148 - 16m 36s\n",
      "batch: 500/1563 - train loss: 3.1159 - test loss: 17.2231 - train acc: 0.8212 - test acc: 0.4232 - 16m 41s\n",
      "batch: 600/1563 - train loss: 3.4578 - test loss: 17.9116 - train acc: 0.8049 - test acc: 0.4141 - 16m 46s\n",
      "batch: 700/1563 - train loss: 3.3159 - test loss: 18.0137 - train acc: 0.8155 - test acc: 0.4116 - 16m 51s\n",
      "batch: 800/1563 - train loss: 3.6712 - test loss: 18.2905 - train acc: 0.7982 - test acc: 0.3971 - 16m 56s\n",
      "batch: 900/1563 - train loss: 3.7411 - test loss: 17.4811 - train acc: 0.7903 - test acc: 0.4133 - 17m 0s\n",
      "batch: 1000/1563 - train loss: 3.8713 - test loss: 17.5833 - train acc: 0.7840 - test acc: 0.4164 - 17m 5s\n",
      "batch: 1100/1563 - train loss: 4.0158 - test loss: 17.4010 - train acc: 0.7809 - test acc: 0.4144 - 17m 10s\n",
      "batch: 1200/1563 - train loss: 4.0931 - test loss: 17.3539 - train acc: 0.7793 - test acc: 0.4195 - 17m 14s\n",
      "batch: 1300/1563 - train loss: 4.0406 - test loss: 17.1397 - train acc: 0.7775 - test acc: 0.4186 - 17m 20s\n",
      "batch: 1400/1563 - train loss: 4.4613 - test loss: 17.6699 - train acc: 0.7603 - test acc: 0.4062 - 17m 24s\n",
      "batch: 1500/1563 - train loss: 4.2353 - test loss: 17.6340 - train acc: 0.7703 - test acc: 0.4059 - 17m 29s\n",
      "batch: 1563/1563 - train loss: 4.1338 - test loss: 17.4319 - train acc: 0.7769 - test acc: 0.4038 - 17m 33s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.4156 - test loss: 16.9753 - train acc: 0.8638 - test acc: 0.4249 - 17m 38s\n",
      "batch: 200/1563 - train loss: 2.2364 - test loss: 17.5604 - train acc: 0.8747 - test acc: 0.4182 - 17m 43s\n",
      "batch: 300/1563 - train loss: 2.4159 - test loss: 17.7494 - train acc: 0.8679 - test acc: 0.4200 - 17m 48s\n",
      "batch: 400/1563 - train loss: 2.4593 - test loss: 17.8830 - train acc: 0.8672 - test acc: 0.4218 - 17m 52s\n",
      "batch: 500/1563 - train loss: 2.4377 - test loss: 18.0982 - train acc: 0.8621 - test acc: 0.4173 - 17m 57s\n",
      "batch: 600/1563 - train loss: 2.9289 - test loss: 17.8184 - train acc: 0.8390 - test acc: 0.4128 - 18m 2s\n",
      "batch: 700/1563 - train loss: 2.7905 - test loss: 17.9718 - train acc: 0.8459 - test acc: 0.4180 - 18m 7s\n",
      "batch: 800/1563 - train loss: 3.2485 - test loss: 18.0211 - train acc: 0.8165 - test acc: 0.4145 - 18m 12s\n",
      "batch: 900/1563 - train loss: 2.9410 - test loss: 18.0803 - train acc: 0.8353 - test acc: 0.4216 - 18m 16s\n",
      "batch: 1000/1563 - train loss: 3.2381 - test loss: 18.1157 - train acc: 0.8218 - test acc: 0.4185 - 18m 21s\n",
      "batch: 1100/1563 - train loss: 3.6585 - test loss: 18.0191 - train acc: 0.7983 - test acc: 0.4198 - 18m 26s\n",
      "batch: 1200/1563 - train loss: 3.5530 - test loss: 17.7213 - train acc: 0.8031 - test acc: 0.4215 - 18m 31s\n",
      "batch: 1300/1563 - train loss: 3.8522 - test loss: 17.9444 - train acc: 0.7869 - test acc: 0.4076 - 18m 36s\n",
      "batch: 1400/1563 - train loss: 3.5507 - test loss: 17.5978 - train acc: 0.7893 - test acc: 0.4178 - 18m 40s\n",
      "batch: 1500/1563 - train loss: 3.9281 - test loss: 18.0091 - train acc: 0.7818 - test acc: 0.4122 - 18m 45s\n",
      "batch: 1563/1563 - train loss: 3.9037 - test loss: 18.4911 - train acc: 0.7790 - test acc: 0.4055 - 18m 49s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.1156 - test loss: 17.7768 - train acc: 0.8841 - test acc: 0.4202 - 18m 54s\n",
      "batch: 200/1563 - train loss: 1.9081 - test loss: 17.8539 - train acc: 0.8913 - test acc: 0.4222 - 18m 59s\n",
      "batch: 300/1563 - train loss: 1.8741 - test loss: 18.5345 - train acc: 0.8897 - test acc: 0.4163 - 19m 4s\n",
      "batch: 400/1563 - train loss: 2.1961 - test loss: 18.3534 - train acc: 0.8769 - test acc: 0.4131 - 19m 9s\n",
      "batch: 500/1563 - train loss: 2.1503 - test loss: 18.4577 - train acc: 0.8826 - test acc: 0.4184 - 19m 13s\n",
      "batch: 600/1563 - train loss: 2.3795 - test loss: 18.8099 - train acc: 0.8650 - test acc: 0.4159 - 19m 18s\n",
      "batch: 700/1563 - train loss: 2.5354 - test loss: 18.7956 - train acc: 0.8566 - test acc: 0.4173 - 19m 23s\n",
      "batch: 800/1563 - train loss: 2.4366 - test loss: 18.4711 - train acc: 0.8625 - test acc: 0.4222 - 19m 28s\n",
      "batch: 900/1563 - train loss: 2.5374 - test loss: 18.7720 - train acc: 0.8587 - test acc: 0.4164 - 19m 33s\n",
      "batch: 1000/1563 - train loss: 2.7009 - test loss: 19.3035 - train acc: 0.8440 - test acc: 0.4128 - 19m 37s\n",
      "batch: 1100/1563 - train loss: 2.9860 - test loss: 19.3882 - train acc: 0.8334 - test acc: 0.4057 - 19m 42s\n",
      "batch: 1200/1563 - train loss: 3.0290 - test loss: 19.3185 - train acc: 0.8197 - test acc: 0.4074 - 19m 47s\n",
      "batch: 1300/1563 - train loss: 3.1275 - test loss: 19.2165 - train acc: 0.8231 - test acc: 0.4140 - 19m 52s\n",
      "batch: 1400/1563 - train loss: 3.4328 - test loss: 18.8975 - train acc: 0.8136 - test acc: 0.4113 - 19m 57s\n",
      "time is up! finishing training\n",
      "batch: 1444/1563 - train loss: 3.5745 - test loss: 18.3878 - train acc: 0.8046 - test acc: 0.4234 - 20m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "testing - beta: 0.9 - part size: 8 - block upd: 64 - combination nº: 3\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 8 - nº part: 1176 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 73728 - partition: 8 - nº part: 9216 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 8192 - partition: 8 - nº part: 1024 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 294912 - partition: 8 - nº part: 36864 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 32768 - partition: 8 - nº part: 4096 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 1179648 - partition: 8 - nº part: 147456 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 131072 - partition: 8 - nº part: 16384 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 51200 - partition: 8 - nº part: 6400 - block updates: 64\n",
      "FisherPartitioner: param: 100 - partition: 8 - nº part: 13 - block updates: 13\n",
      "total partitions: 1403477 - effective block updates: 2557\n",
      "initializing inverses...\n",
      "partition 1/1403477\n",
      "partition 10000/1403477\n",
      "partition 20000/1403477\n",
      "partition 30000/1403477\n",
      "partition 40000/1403477\n",
      "partition 50000/1403477\n",
      "partition 60000/1403477\n",
      "partition 70000/1403477\n",
      "partition 80000/1403477\n",
      "partition 90000/1403477\n",
      "partition 100000/1403477\n",
      "partition 110000/1403477\n",
      "partition 120000/1403477\n",
      "partition 130000/1403477\n",
      "partition 140000/1403477\n",
      "partition 150000/1403477\n",
      "partition 160000/1403477\n",
      "partition 170000/1403477\n",
      "partition 180000/1403477\n",
      "partition 190000/1403477\n",
      "partition 200000/1403477\n",
      "partition 210000/1403477\n",
      "partition 220000/1403477\n",
      "partition 230000/1403477\n",
      "partition 240000/1403477\n",
      "partition 250000/1403477\n",
      "partition 260000/1403477\n",
      "partition 270000/1403477\n",
      "partition 280000/1403477\n",
      "partition 290000/1403477\n",
      "partition 300000/1403477\n",
      "partition 310000/1403477\n",
      "partition 320000/1403477\n",
      "partition 330000/1403477\n",
      "partition 340000/1403477\n",
      "partition 350000/1403477\n",
      "partition 360000/1403477\n",
      "partition 370000/1403477\n",
      "partition 380000/1403477\n",
      "partition 390000/1403477\n",
      "partition 400000/1403477\n",
      "partition 410000/1403477\n",
      "partition 420000/1403477\n",
      "partition 430000/1403477\n",
      "partition 440000/1403477\n",
      "partition 450000/1403477\n",
      "partition 460000/1403477\n",
      "partition 470000/1403477\n",
      "partition 480000/1403477\n",
      "partition 490000/1403477\n",
      "partition 500000/1403477\n",
      "partition 510000/1403477\n",
      "partition 520000/1403477\n",
      "partition 530000/1403477\n",
      "partition 540000/1403477\n",
      "partition 550000/1403477\n",
      "partition 560000/1403477\n",
      "partition 570000/1403477\n",
      "partition 580000/1403477\n",
      "partition 590000/1403477\n",
      "partition 600000/1403477\n",
      "partition 610000/1403477\n",
      "partition 620000/1403477\n",
      "partition 630000/1403477\n",
      "partition 640000/1403477\n",
      "partition 650000/1403477\n",
      "partition 660000/1403477\n",
      "partition 670000/1403477\n",
      "partition 680000/1403477\n",
      "partition 690000/1403477\n",
      "partition 700000/1403477\n",
      "partition 710000/1403477\n",
      "partition 720000/1403477\n",
      "partition 730000/1403477\n",
      "partition 740000/1403477\n",
      "partition 750000/1403477\n",
      "partition 760000/1403477\n",
      "partition 770000/1403477\n",
      "partition 780000/1403477\n",
      "partition 790000/1403477\n",
      "partition 800000/1403477\n",
      "partition 810000/1403477\n",
      "partition 820000/1403477\n",
      "partition 830000/1403477\n",
      "partition 840000/1403477\n",
      "partition 850000/1403477\n",
      "partition 860000/1403477\n",
      "partition 870000/1403477\n",
      "partition 880000/1403477\n",
      "partition 890000/1403477\n",
      "partition 900000/1403477\n",
      "partition 910000/1403477\n",
      "partition 920000/1403477\n",
      "partition 930000/1403477\n",
      "partition 940000/1403477\n",
      "partition 950000/1403477\n",
      "partition 960000/1403477\n",
      "partition 970000/1403477\n",
      "partition 980000/1403477\n",
      "partition 990000/1403477\n",
      "partition 1000000/1403477\n",
      "partition 1010000/1403477\n",
      "partition 1020000/1403477\n",
      "partition 1030000/1403477\n",
      "partition 1040000/1403477\n",
      "partition 1050000/1403477\n",
      "partition 1060000/1403477\n",
      "partition 1070000/1403477\n",
      "partition 1080000/1403477\n",
      "partition 1090000/1403477\n",
      "partition 1100000/1403477\n",
      "partition 1110000/1403477\n",
      "partition 1120000/1403477\n",
      "partition 1130000/1403477\n",
      "partition 1140000/1403477\n",
      "partition 1150000/1403477\n",
      "partition 1160000/1403477\n",
      "partition 1170000/1403477\n",
      "partition 1180000/1403477\n",
      "partition 1190000/1403477\n",
      "partition 1200000/1403477\n",
      "partition 1210000/1403477\n",
      "partition 1220000/1403477\n",
      "partition 1230000/1403477\n",
      "partition 1240000/1403477\n",
      "partition 1250000/1403477\n",
      "partition 1260000/1403477\n",
      "partition 1270000/1403477\n",
      "partition 1280000/1403477\n",
      "partition 1290000/1403477\n",
      "partition 1300000/1403477\n",
      "partition 1310000/1403477\n",
      "partition 1320000/1403477\n",
      "partition 1330000/1403477\n",
      "partition 1340000/1403477\n",
      "partition 1350000/1403477\n",
      "partition 1360000/1403477\n",
      "partition 1370000/1403477\n",
      "partition 1380000/1403477\n",
      "partition 1390000/1403477\n",
      "partition 1400000/1403477\n",
      "partition 1403477/1403477\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.2036 - test loss: 24.7637 - train acc: 0.0349 - test acc: 0.0531 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.2502 - test loss: 23.5520 - train acc: 0.0574 - test acc: 0.0685 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.4259 - test loss: 22.8910 - train acc: 0.0746 - test acc: 0.0807 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.4901 - test loss: 24.3474 - train acc: 0.0945 - test acc: 0.0667 - 0m 16s\n",
      "batch: 500/1563 - train loss: 22.1900 - test loss: 21.2189 - train acc: 0.1015 - test acc: 0.1168 - 0m 20s\n",
      "batch: 600/1563 - train loss: 21.4135 - test loss: 20.9826 - train acc: 0.1202 - test acc: 0.1275 - 0m 25s\n",
      "batch: 700/1563 - train loss: 21.2224 - test loss: 21.3059 - train acc: 0.1247 - test acc: 0.1171 - 0m 30s\n",
      "batch: 800/1563 - train loss: 20.5232 - test loss: 21.4873 - train acc: 0.1460 - test acc: 0.1299 - 0m 34s\n",
      "batch: 900/1563 - train loss: 20.1006 - test loss: 19.9032 - train acc: 0.1513 - test acc: 0.1521 - 0m 39s\n",
      "batch: 1000/1563 - train loss: 19.7260 - test loss: 20.1721 - train acc: 0.1660 - test acc: 0.1574 - 0m 44s\n",
      "batch: 1100/1563 - train loss: 19.5107 - test loss: 19.6040 - train acc: 0.1688 - test acc: 0.1745 - 0m 49s\n",
      "batch: 1200/1563 - train loss: 19.4088 - test loss: 20.0040 - train acc: 0.1775 - test acc: 0.1566 - 0m 53s\n",
      "batch: 1300/1563 - train loss: 19.1580 - test loss: 19.5924 - train acc: 0.1785 - test acc: 0.1771 - 0m 58s\n",
      "batch: 1400/1563 - train loss: 19.0044 - test loss: 18.2709 - train acc: 0.1894 - test acc: 0.2130 - 1m 3s\n",
      "batch: 1500/1563 - train loss: 18.4151 - test loss: 18.8418 - train acc: 0.2129 - test acc: 0.2030 - 1m 7s\n",
      "batch: 1563/1563 - train loss: 18.3038 - test loss: 19.6708 - train acc: 0.2154 - test acc: 0.1757 - 1m 11s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.7604 - test loss: 17.7172 - train acc: 0.2331 - test acc: 0.2313 - 1m 16s\n",
      "batch: 200/1563 - train loss: 17.6419 - test loss: 18.0608 - train acc: 0.2294 - test acc: 0.2248 - 1m 21s\n",
      "batch: 300/1563 - train loss: 17.3568 - test loss: 18.5568 - train acc: 0.2463 - test acc: 0.2090 - 1m 26s\n",
      "batch: 400/1563 - train loss: 17.5098 - test loss: 17.6596 - train acc: 0.2407 - test acc: 0.2401 - 1m 30s\n",
      "batch: 500/1563 - train loss: 17.4570 - test loss: 17.4824 - train acc: 0.2425 - test acc: 0.2437 - 1m 35s\n",
      "batch: 600/1563 - train loss: 17.2610 - test loss: 17.7923 - train acc: 0.2394 - test acc: 0.2302 - 1m 40s\n",
      "batch: 700/1563 - train loss: 16.8890 - test loss: 17.5458 - train acc: 0.2500 - test acc: 0.2425 - 1m 44s\n",
      "batch: 800/1563 - train loss: 16.8727 - test loss: 16.9861 - train acc: 0.2588 - test acc: 0.2524 - 1m 49s\n",
      "batch: 900/1563 - train loss: 16.7362 - test loss: 16.9321 - train acc: 0.2691 - test acc: 0.2553 - 1m 54s\n",
      "batch: 1000/1563 - train loss: 16.5057 - test loss: 17.8109 - train acc: 0.2656 - test acc: 0.2356 - 1m 59s\n",
      "batch: 1100/1563 - train loss: 16.4873 - test loss: 17.1770 - train acc: 0.2762 - test acc: 0.2529 - 2m 3s\n",
      "batch: 1200/1563 - train loss: 16.6736 - test loss: 17.5804 - train acc: 0.2746 - test acc: 0.2436 - 2m 8s\n",
      "batch: 1300/1563 - train loss: 16.6726 - test loss: 16.3559 - train acc: 0.2600 - test acc: 0.2853 - 2m 13s\n",
      "batch: 1400/1563 - train loss: 16.5111 - test loss: 17.0886 - train acc: 0.2637 - test acc: 0.2545 - 2m 18s\n",
      "batch: 1500/1563 - train loss: 15.8451 - test loss: 15.8089 - train acc: 0.2894 - test acc: 0.2982 - 2m 22s\n",
      "batch: 1563/1563 - train loss: 16.0008 - test loss: 18.4572 - train acc: 0.2837 - test acc: 0.2344 - 2m 26s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.7420 - test loss: 16.2123 - train acc: 0.3437 - test acc: 0.2950 - 2m 31s\n",
      "batch: 200/1563 - train loss: 14.7923 - test loss: 16.0528 - train acc: 0.3387 - test acc: 0.2876 - 2m 36s\n",
      "batch: 300/1563 - train loss: 15.0929 - test loss: 17.2878 - train acc: 0.3162 - test acc: 0.2586 - 2m 41s\n",
      "batch: 400/1563 - train loss: 14.8389 - test loss: 17.8122 - train acc: 0.3225 - test acc: 0.2687 - 2m 45s\n",
      "batch: 500/1563 - train loss: 14.8225 - test loss: 15.8042 - train acc: 0.3265 - test acc: 0.3025 - 2m 50s\n",
      "batch: 600/1563 - train loss: 14.9655 - test loss: 15.8267 - train acc: 0.3197 - test acc: 0.3007 - 2m 55s\n",
      "batch: 700/1563 - train loss: 14.8868 - test loss: 15.1818 - train acc: 0.3206 - test acc: 0.3242 - 3m 0s\n",
      "batch: 800/1563 - train loss: 14.8987 - test loss: 15.8113 - train acc: 0.3247 - test acc: 0.3006 - 3m 4s\n",
      "batch: 900/1563 - train loss: 14.4987 - test loss: 16.2889 - train acc: 0.3387 - test acc: 0.2862 - 3m 9s\n",
      "batch: 1000/1563 - train loss: 14.7452 - test loss: 16.1922 - train acc: 0.3274 - test acc: 0.2921 - 3m 14s\n",
      "batch: 1100/1563 - train loss: 14.5948 - test loss: 15.5194 - train acc: 0.3356 - test acc: 0.3108 - 3m 18s\n",
      "batch: 1200/1563 - train loss: 14.1217 - test loss: 15.0342 - train acc: 0.3640 - test acc: 0.3212 - 3m 23s\n",
      "batch: 1300/1563 - train loss: 14.6543 - test loss: 14.5334 - train acc: 0.3359 - test acc: 0.3472 - 3m 28s\n",
      "batch: 1400/1563 - train loss: 14.4648 - test loss: 14.7437 - train acc: 0.3503 - test acc: 0.3348 - 3m 33s\n",
      "batch: 1500/1563 - train loss: 14.4898 - test loss: 14.9577 - train acc: 0.3447 - test acc: 0.3287 - 3m 37s\n",
      "batch: 1563/1563 - train loss: 14.2746 - test loss: 14.8605 - train acc: 0.3547 - test acc: 0.3300 - 3m 41s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.6359 - test loss: 14.9223 - train acc: 0.4147 - test acc: 0.3344 - 3m 46s\n",
      "batch: 200/1563 - train loss: 12.7930 - test loss: 14.5451 - train acc: 0.4069 - test acc: 0.3525 - 3m 51s\n",
      "batch: 300/1563 - train loss: 13.0149 - test loss: 15.7952 - train acc: 0.3925 - test acc: 0.3203 - 3m 56s\n",
      "batch: 400/1563 - train loss: 13.0820 - test loss: 15.0258 - train acc: 0.3860 - test acc: 0.3387 - 4m 1s\n",
      "batch: 500/1563 - train loss: 13.1262 - test loss: 15.2911 - train acc: 0.3978 - test acc: 0.3286 - 4m 5s\n",
      "batch: 600/1563 - train loss: 13.1048 - test loss: 14.8135 - train acc: 0.3894 - test acc: 0.3432 - 4m 10s\n",
      "batch: 700/1563 - train loss: 13.0746 - test loss: 14.9147 - train acc: 0.3916 - test acc: 0.3430 - 4m 15s\n",
      "batch: 800/1563 - train loss: 13.1815 - test loss: 14.1712 - train acc: 0.3947 - test acc: 0.3613 - 4m 20s\n",
      "batch: 900/1563 - train loss: 12.9944 - test loss: 15.0476 - train acc: 0.4000 - test acc: 0.3344 - 4m 24s\n",
      "batch: 1000/1563 - train loss: 13.5339 - test loss: 14.5112 - train acc: 0.3728 - test acc: 0.3481 - 4m 29s\n",
      "batch: 1100/1563 - train loss: 13.1551 - test loss: 14.3762 - train acc: 0.4000 - test acc: 0.3577 - 4m 34s\n",
      "batch: 1200/1563 - train loss: 13.1393 - test loss: 14.7059 - train acc: 0.3956 - test acc: 0.3476 - 4m 39s\n",
      "batch: 1300/1563 - train loss: 13.5002 - test loss: 14.3593 - train acc: 0.3703 - test acc: 0.3587 - 4m 43s\n",
      "batch: 1400/1563 - train loss: 13.0511 - test loss: 13.7806 - train acc: 0.3910 - test acc: 0.3828 - 4m 48s\n",
      "batch: 1500/1563 - train loss: 12.9942 - test loss: 14.2714 - train acc: 0.4025 - test acc: 0.3593 - 4m 53s\n",
      "batch: 1563/1563 - train loss: 13.1049 - test loss: 16.6733 - train acc: 0.3959 - test acc: 0.2902 - 4m 57s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.1908 - test loss: 17.0856 - train acc: 0.4666 - test acc: 0.3030 - 5m 2s\n",
      "batch: 200/1563 - train loss: 11.4266 - test loss: 14.4677 - train acc: 0.4597 - test acc: 0.3563 - 5m 6s\n",
      "batch: 300/1563 - train loss: 11.3622 - test loss: 14.3823 - train acc: 0.4572 - test acc: 0.3686 - 5m 11s\n",
      "batch: 400/1563 - train loss: 11.7678 - test loss: 14.2391 - train acc: 0.4494 - test acc: 0.3680 - 5m 16s\n",
      "batch: 500/1563 - train loss: 11.6303 - test loss: 14.5328 - train acc: 0.4554 - test acc: 0.3613 - 5m 20s\n",
      "batch: 600/1563 - train loss: 11.3796 - test loss: 14.0762 - train acc: 0.4679 - test acc: 0.3755 - 5m 25s\n",
      "batch: 700/1563 - train loss: 11.6940 - test loss: 13.9193 - train acc: 0.4425 - test acc: 0.3790 - 5m 30s\n",
      "batch: 800/1563 - train loss: 12.0174 - test loss: 13.7038 - train acc: 0.4369 - test acc: 0.3840 - 5m 35s\n",
      "batch: 900/1563 - train loss: 11.7537 - test loss: 13.8046 - train acc: 0.4425 - test acc: 0.3860 - 5m 40s\n",
      "batch: 1000/1563 - train loss: 11.7598 - test loss: 13.8261 - train acc: 0.4466 - test acc: 0.3895 - 5m 44s\n",
      "batch: 1100/1563 - train loss: 12.0669 - test loss: 14.3314 - train acc: 0.4350 - test acc: 0.3621 - 5m 49s\n",
      "batch: 1200/1563 - train loss: 11.8421 - test loss: 14.0287 - train acc: 0.4438 - test acc: 0.3797 - 5m 53s\n",
      "batch: 1300/1563 - train loss: 12.1824 - test loss: 15.2694 - train acc: 0.4387 - test acc: 0.3469 - 5m 58s\n",
      "batch: 1400/1563 - train loss: 12.1511 - test loss: 14.1449 - train acc: 0.4388 - test acc: 0.3775 - 6m 3s\n",
      "batch: 1500/1563 - train loss: 11.9938 - test loss: 13.9317 - train acc: 0.4444 - test acc: 0.3815 - 6m 8s\n",
      "batch: 1563/1563 - train loss: 12.1142 - test loss: 13.5396 - train acc: 0.4360 - test acc: 0.3953 - 6m 12s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0740 - test loss: 14.1635 - train acc: 0.5113 - test acc: 0.3790 - 6m 16s\n",
      "batch: 200/1563 - train loss: 9.8814 - test loss: 13.5368 - train acc: 0.5202 - test acc: 0.4021 - 6m 21s\n",
      "batch: 300/1563 - train loss: 10.1856 - test loss: 14.3246 - train acc: 0.5056 - test acc: 0.3850 - 6m 26s\n",
      "batch: 400/1563 - train loss: 10.4110 - test loss: 13.7321 - train acc: 0.5091 - test acc: 0.3901 - 6m 31s\n",
      "batch: 500/1563 - train loss: 10.3215 - test loss: 14.1702 - train acc: 0.5075 - test acc: 0.3774 - 6m 36s\n",
      "batch: 600/1563 - train loss: 10.7121 - test loss: 14.9200 - train acc: 0.4906 - test acc: 0.3572 - 6m 40s\n",
      "batch: 700/1563 - train loss: 10.5168 - test loss: 14.0620 - train acc: 0.4953 - test acc: 0.3866 - 6m 45s\n",
      "batch: 800/1563 - train loss: 10.1668 - test loss: 13.8086 - train acc: 0.5103 - test acc: 0.3937 - 6m 50s\n",
      "batch: 900/1563 - train loss: 10.8642 - test loss: 13.6879 - train acc: 0.4819 - test acc: 0.3921 - 6m 54s\n",
      "batch: 1000/1563 - train loss: 10.8868 - test loss: 13.8191 - train acc: 0.4866 - test acc: 0.3936 - 6m 59s\n",
      "batch: 1100/1563 - train loss: 10.7446 - test loss: 14.8768 - train acc: 0.4885 - test acc: 0.3584 - 7m 4s\n",
      "batch: 1200/1563 - train loss: 10.4824 - test loss: 13.3908 - train acc: 0.5087 - test acc: 0.4084 - 7m 9s\n",
      "batch: 1300/1563 - train loss: 10.9045 - test loss: 14.7704 - train acc: 0.4772 - test acc: 0.3648 - 7m 13s\n",
      "batch: 1400/1563 - train loss: 10.9396 - test loss: 13.3407 - train acc: 0.4813 - test acc: 0.4055 - 7m 18s\n",
      "batch: 1500/1563 - train loss: 11.0904 - test loss: 13.8013 - train acc: 0.4588 - test acc: 0.3906 - 7m 23s\n",
      "batch: 1563/1563 - train loss: 10.7400 - test loss: 13.7620 - train acc: 0.4778 - test acc: 0.3917 - 7m 27s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.6978 - test loss: 14.4672 - train acc: 0.5675 - test acc: 0.3853 - 7m 31s\n",
      "batch: 200/1563 - train loss: 8.9072 - test loss: 13.4236 - train acc: 0.5665 - test acc: 0.4062 - 7m 37s\n",
      "batch: 300/1563 - train loss: 9.0207 - test loss: 14.3362 - train acc: 0.5512 - test acc: 0.3868 - 7m 42s\n",
      "batch: 400/1563 - train loss: 9.1602 - test loss: 13.6884 - train acc: 0.5491 - test acc: 0.4030 - 7m 46s\n",
      "batch: 500/1563 - train loss: 9.2039 - test loss: 14.2056 - train acc: 0.5525 - test acc: 0.3891 - 7m 51s\n",
      "batch: 600/1563 - train loss: 9.4945 - test loss: 13.9156 - train acc: 0.5244 - test acc: 0.3942 - 7m 56s\n",
      "batch: 700/1563 - train loss: 9.2281 - test loss: 13.5439 - train acc: 0.5453 - test acc: 0.4034 - 8m 0s\n",
      "batch: 800/1563 - train loss: 9.6020 - test loss: 15.1233 - train acc: 0.5282 - test acc: 0.3719 - 8m 5s\n",
      "batch: 900/1563 - train loss: 9.6830 - test loss: 13.7148 - train acc: 0.5122 - test acc: 0.3992 - 8m 10s\n",
      "batch: 1000/1563 - train loss: 9.8630 - test loss: 13.5299 - train acc: 0.5128 - test acc: 0.4123 - 8m 15s\n",
      "batch: 1100/1563 - train loss: 9.8011 - test loss: 13.3629 - train acc: 0.5290 - test acc: 0.4128 - 8m 19s\n",
      "batch: 1200/1563 - train loss: 9.7426 - test loss: 13.5060 - train acc: 0.5225 - test acc: 0.4151 - 8m 24s\n",
      "batch: 1300/1563 - train loss: 9.6930 - test loss: 13.7574 - train acc: 0.5250 - test acc: 0.3968 - 8m 29s\n",
      "batch: 1400/1563 - train loss: 9.8010 - test loss: 13.9746 - train acc: 0.5251 - test acc: 0.3998 - 8m 33s\n",
      "batch: 1500/1563 - train loss: 9.6348 - test loss: 13.5740 - train acc: 0.5401 - test acc: 0.4085 - 8m 38s\n",
      "batch: 1563/1563 - train loss: 9.7767 - test loss: 13.0434 - train acc: 0.5303 - test acc: 0.4265 - 8m 42s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.4048 - test loss: 13.2008 - train acc: 0.6216 - test acc: 0.4230 - 8m 47s\n",
      "batch: 200/1563 - train loss: 7.7922 - test loss: 13.6449 - train acc: 0.6090 - test acc: 0.4176 - 8m 52s\n",
      "batch: 300/1563 - train loss: 7.8840 - test loss: 13.6558 - train acc: 0.6107 - test acc: 0.4207 - 8m 56s\n",
      "batch: 400/1563 - train loss: 7.9976 - test loss: 13.7213 - train acc: 0.5965 - test acc: 0.4147 - 9m 1s\n",
      "batch: 500/1563 - train loss: 8.0403 - test loss: 14.5988 - train acc: 0.6049 - test acc: 0.3850 - 9m 6s\n",
      "batch: 600/1563 - train loss: 8.2841 - test loss: 13.8969 - train acc: 0.5750 - test acc: 0.4119 - 9m 11s\n",
      "batch: 700/1563 - train loss: 8.4834 - test loss: 14.6828 - train acc: 0.5659 - test acc: 0.3903 - 9m 16s\n",
      "batch: 800/1563 - train loss: 8.6398 - test loss: 15.1063 - train acc: 0.5512 - test acc: 0.3879 - 9m 20s\n",
      "batch: 900/1563 - train loss: 8.7237 - test loss: 14.1004 - train acc: 0.5540 - test acc: 0.3988 - 9m 25s\n",
      "batch: 1000/1563 - train loss: 8.3654 - test loss: 15.8673 - train acc: 0.5833 - test acc: 0.3591 - 9m 30s\n",
      "batch: 1100/1563 - train loss: 8.4703 - test loss: 13.6552 - train acc: 0.5737 - test acc: 0.4169 - 9m 34s\n",
      "batch: 1200/1563 - train loss: 8.6847 - test loss: 13.4829 - train acc: 0.5711 - test acc: 0.4243 - 9m 39s\n",
      "batch: 1300/1563 - train loss: 9.1073 - test loss: 13.8838 - train acc: 0.5493 - test acc: 0.4084 - 9m 44s\n",
      "batch: 1400/1563 - train loss: 9.0451 - test loss: 13.6157 - train acc: 0.5644 - test acc: 0.4148 - 9m 49s\n",
      "batch: 1500/1563 - train loss: 8.9065 - test loss: 13.3954 - train acc: 0.5571 - test acc: 0.4186 - 9m 54s\n",
      "batch: 1563/1563 - train loss: 8.7932 - test loss: 13.9782 - train acc: 0.5634 - test acc: 0.4012 - 9m 58s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.2280 - test loss: 13.7422 - train acc: 0.6825 - test acc: 0.4141 - 10m 2s\n",
      "batch: 200/1563 - train loss: 6.6278 - test loss: 13.7995 - train acc: 0.6538 - test acc: 0.4243 - 10m 7s\n",
      "batch: 300/1563 - train loss: 6.2335 - test loss: 14.9801 - train acc: 0.6788 - test acc: 0.3974 - 10m 12s\n",
      "batch: 400/1563 - train loss: 6.4544 - test loss: 14.9255 - train acc: 0.6631 - test acc: 0.3967 - 10m 17s\n",
      "batch: 500/1563 - train loss: 6.8060 - test loss: 14.4913 - train acc: 0.6485 - test acc: 0.4041 - 10m 22s\n",
      "batch: 600/1563 - train loss: 7.2162 - test loss: 14.4064 - train acc: 0.6363 - test acc: 0.4048 - 10m 26s\n",
      "batch: 700/1563 - train loss: 7.3623 - test loss: 14.0895 - train acc: 0.6246 - test acc: 0.4157 - 10m 31s\n",
      "batch: 800/1563 - train loss: 7.3970 - test loss: 14.2214 - train acc: 0.6128 - test acc: 0.4129 - 10m 36s\n",
      "batch: 900/1563 - train loss: 7.8958 - test loss: 14.3557 - train acc: 0.6063 - test acc: 0.4058 - 10m 40s\n",
      "batch: 1000/1563 - train loss: 7.6574 - test loss: 13.9871 - train acc: 0.6103 - test acc: 0.4132 - 10m 45s\n",
      "batch: 1100/1563 - train loss: 7.9138 - test loss: 13.8999 - train acc: 0.5962 - test acc: 0.4199 - 10m 50s\n",
      "batch: 1200/1563 - train loss: 7.9488 - test loss: 14.0277 - train acc: 0.5988 - test acc: 0.4172 - 10m 55s\n",
      "batch: 1300/1563 - train loss: 7.8673 - test loss: 14.0299 - train acc: 0.5981 - test acc: 0.4119 - 10m 59s\n",
      "batch: 1400/1563 - train loss: 8.0731 - test loss: 14.7033 - train acc: 0.6041 - test acc: 0.3959 - 11m 4s\n",
      "batch: 1500/1563 - train loss: 7.9524 - test loss: 13.9239 - train acc: 0.5991 - test acc: 0.4142 - 11m 9s\n",
      "batch: 1563/1563 - train loss: 8.1056 - test loss: 14.2525 - train acc: 0.5888 - test acc: 0.4118 - 11m 13s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.6205 - test loss: 14.0442 - train acc: 0.7104 - test acc: 0.4219 - 11m 18s\n",
      "batch: 200/1563 - train loss: 5.5097 - test loss: 14.1912 - train acc: 0.7137 - test acc: 0.4238 - 11m 23s\n",
      "batch: 300/1563 - train loss: 5.6051 - test loss: 14.5938 - train acc: 0.7007 - test acc: 0.4200 - 11m 28s\n",
      "batch: 400/1563 - train loss: 5.8109 - test loss: 14.8314 - train acc: 0.6850 - test acc: 0.4092 - 11m 32s\n",
      "batch: 500/1563 - train loss: 6.0288 - test loss: 14.7645 - train acc: 0.6875 - test acc: 0.4157 - 11m 37s\n",
      "batch: 600/1563 - train loss: 6.1840 - test loss: 16.2432 - train acc: 0.6770 - test acc: 0.3843 - 11m 42s\n",
      "batch: 700/1563 - train loss: 6.3718 - test loss: 15.1693 - train acc: 0.6676 - test acc: 0.4026 - 11m 47s\n",
      "batch: 800/1563 - train loss: 6.5673 - test loss: 14.8697 - train acc: 0.6478 - test acc: 0.4034 - 11m 52s\n",
      "batch: 900/1563 - train loss: 6.6189 - test loss: 14.5488 - train acc: 0.6560 - test acc: 0.4196 - 11m 56s\n",
      "batch: 1000/1563 - train loss: 6.8544 - test loss: 14.4866 - train acc: 0.6425 - test acc: 0.4149 - 12m 1s\n",
      "batch: 1100/1563 - train loss: 6.7416 - test loss: 14.6592 - train acc: 0.6506 - test acc: 0.4101 - 12m 6s\n",
      "batch: 1200/1563 - train loss: 7.3374 - test loss: 14.3108 - train acc: 0.6203 - test acc: 0.4176 - 12m 11s\n",
      "batch: 1300/1563 - train loss: 6.9467 - test loss: 14.3900 - train acc: 0.6528 - test acc: 0.4188 - 12m 15s\n",
      "batch: 1400/1563 - train loss: 7.2969 - test loss: 14.6614 - train acc: 0.6272 - test acc: 0.4033 - 12m 20s\n",
      "batch: 1500/1563 - train loss: 7.0314 - test loss: 14.5117 - train acc: 0.6338 - test acc: 0.4098 - 12m 25s\n",
      "batch: 1563/1563 - train loss: 7.0011 - test loss: 14.6647 - train acc: 0.6300 - test acc: 0.4124 - 12m 29s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.4086 - test loss: 14.6598 - train acc: 0.7725 - test acc: 0.4295 - 12m 34s\n",
      "batch: 200/1563 - train loss: 4.6659 - test loss: 15.1101 - train acc: 0.7537 - test acc: 0.4130 - 12m 38s\n",
      "batch: 300/1563 - train loss: 4.7622 - test loss: 15.9954 - train acc: 0.7481 - test acc: 0.3935 - 12m 43s\n",
      "batch: 400/1563 - train loss: 5.0407 - test loss: 15.3453 - train acc: 0.7250 - test acc: 0.4100 - 12m 48s\n",
      "batch: 500/1563 - train loss: 5.3169 - test loss: 15.0312 - train acc: 0.7228 - test acc: 0.4194 - 12m 53s\n",
      "batch: 600/1563 - train loss: 5.3335 - test loss: 15.4637 - train acc: 0.7272 - test acc: 0.4086 - 12m 58s\n",
      "batch: 700/1563 - train loss: 5.5481 - test loss: 15.0007 - train acc: 0.7113 - test acc: 0.4222 - 13m 2s\n",
      "batch: 800/1563 - train loss: 5.3900 - test loss: 15.1085 - train acc: 0.7235 - test acc: 0.4164 - 13m 7s\n",
      "batch: 900/1563 - train loss: 6.0211 - test loss: 15.8126 - train acc: 0.6854 - test acc: 0.4073 - 13m 12s\n",
      "batch: 1000/1563 - train loss: 5.8919 - test loss: 15.2211 - train acc: 0.6909 - test acc: 0.4167 - 13m 17s\n",
      "batch: 1100/1563 - train loss: 5.9416 - test loss: 15.3308 - train acc: 0.6828 - test acc: 0.4072 - 13m 22s\n",
      "batch: 1200/1563 - train loss: 6.0705 - test loss: 15.5102 - train acc: 0.6775 - test acc: 0.4048 - 13m 26s\n",
      "batch: 1300/1563 - train loss: 6.1959 - test loss: 15.0540 - train acc: 0.6766 - test acc: 0.4127 - 13m 31s\n",
      "batch: 1400/1563 - train loss: 6.7115 - test loss: 15.3697 - train acc: 0.6597 - test acc: 0.4042 - 13m 36s\n",
      "batch: 1500/1563 - train loss: 6.4138 - test loss: 14.8458 - train acc: 0.6673 - test acc: 0.4173 - 13m 41s\n",
      "batch: 1563/1563 - train loss: 6.5800 - test loss: 14.9203 - train acc: 0.6609 - test acc: 0.4133 - 13m 45s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.8521 - test loss: 14.9964 - train acc: 0.7949 - test acc: 0.4165 - 13m 49s\n",
      "batch: 200/1563 - train loss: 3.8972 - test loss: 15.3612 - train acc: 0.7884 - test acc: 0.4133 - 13m 54s\n",
      "batch: 300/1563 - train loss: 3.8493 - test loss: 15.3924 - train acc: 0.7881 - test acc: 0.4272 - 13m 59s\n",
      "batch: 400/1563 - train loss: 3.8597 - test loss: 15.7610 - train acc: 0.7949 - test acc: 0.4186 - 14m 4s\n",
      "batch: 500/1563 - train loss: 4.3193 - test loss: 15.9491 - train acc: 0.7641 - test acc: 0.4218 - 14m 8s\n",
      "batch: 600/1563 - train loss: 4.6922 - test loss: 15.8261 - train acc: 0.7475 - test acc: 0.4226 - 14m 13s\n",
      "batch: 700/1563 - train loss: 4.5911 - test loss: 15.7879 - train acc: 0.7428 - test acc: 0.4221 - 14m 18s\n",
      "batch: 800/1563 - train loss: 4.6422 - test loss: 17.8244 - train acc: 0.7510 - test acc: 0.3784 - 14m 23s\n",
      "batch: 900/1563 - train loss: 5.0990 - test loss: 15.5949 - train acc: 0.7234 - test acc: 0.4208 - 14m 28s\n",
      "batch: 1000/1563 - train loss: 5.1925 - test loss: 15.6161 - train acc: 0.7201 - test acc: 0.4194 - 14m 33s\n",
      "batch: 1100/1563 - train loss: 5.2404 - test loss: 16.1692 - train acc: 0.7206 - test acc: 0.4081 - 14m 37s\n",
      "batch: 1200/1563 - train loss: 5.0478 - test loss: 15.8966 - train acc: 0.7293 - test acc: 0.4100 - 14m 42s\n",
      "batch: 1300/1563 - train loss: 5.5930 - test loss: 15.5102 - train acc: 0.6991 - test acc: 0.4207 - 14m 47s\n",
      "batch: 1400/1563 - train loss: 5.5724 - test loss: 15.6533 - train acc: 0.7075 - test acc: 0.4160 - 14m 52s\n",
      "batch: 1500/1563 - train loss: 5.4607 - test loss: 15.4490 - train acc: 0.7025 - test acc: 0.4214 - 14m 57s\n",
      "batch: 1563/1563 - train loss: 5.5596 - test loss: 15.8296 - train acc: 0.7007 - test acc: 0.4137 - 15m 0s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.2840 - test loss: 15.8024 - train acc: 0.8224 - test acc: 0.4150 - 15m 5s\n",
      "batch: 200/1563 - train loss: 3.4188 - test loss: 16.5605 - train acc: 0.8084 - test acc: 0.4106 - 15m 10s\n",
      "batch: 300/1563 - train loss: 3.3455 - test loss: 16.0380 - train acc: 0.8215 - test acc: 0.4239 - 15m 15s\n",
      "batch: 400/1563 - train loss: 3.6186 - test loss: 16.1970 - train acc: 0.7965 - test acc: 0.4150 - 15m 19s\n",
      "batch: 500/1563 - train loss: 3.8105 - test loss: 16.9079 - train acc: 0.7881 - test acc: 0.4122 - 15m 24s\n",
      "batch: 600/1563 - train loss: 3.7693 - test loss: 17.8697 - train acc: 0.7924 - test acc: 0.3887 - 15m 29s\n",
      "batch: 700/1563 - train loss: 3.8851 - test loss: 16.5389 - train acc: 0.7865 - test acc: 0.4155 - 15m 34s\n",
      "batch: 800/1563 - train loss: 3.8885 - test loss: 16.7684 - train acc: 0.7866 - test acc: 0.4112 - 15m 38s\n",
      "batch: 900/1563 - train loss: 4.3389 - test loss: 18.3123 - train acc: 0.7643 - test acc: 0.3817 - 15m 43s\n",
      "batch: 1000/1563 - train loss: 4.4185 - test loss: 18.0538 - train acc: 0.7650 - test acc: 0.3866 - 15m 48s\n",
      "batch: 1100/1563 - train loss: 4.4462 - test loss: 16.8522 - train acc: 0.7584 - test acc: 0.4098 - 15m 53s\n",
      "batch: 1200/1563 - train loss: 4.6617 - test loss: 16.8244 - train acc: 0.7478 - test acc: 0.4123 - 15m 57s\n",
      "batch: 1300/1563 - train loss: 4.7201 - test loss: 16.2957 - train acc: 0.7384 - test acc: 0.4155 - 16m 3s\n",
      "batch: 1400/1563 - train loss: 5.0153 - test loss: 16.8519 - train acc: 0.7275 - test acc: 0.4107 - 16m 7s\n",
      "batch: 1500/1563 - train loss: 5.0365 - test loss: 16.3912 - train acc: 0.7259 - test acc: 0.4173 - 16m 12s\n",
      "batch: 1563/1563 - train loss: 5.1639 - test loss: 16.2275 - train acc: 0.7253 - test acc: 0.4167 - 16m 16s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8882 - test loss: 17.5570 - train acc: 0.8440 - test acc: 0.3994 - 16m 20s\n",
      "batch: 200/1563 - train loss: 2.7197 - test loss: 16.6539 - train acc: 0.8537 - test acc: 0.4255 - 16m 25s\n",
      "batch: 300/1563 - train loss: 2.7166 - test loss: 17.5799 - train acc: 0.8456 - test acc: 0.4066 - 16m 30s\n",
      "batch: 400/1563 - train loss: 3.0413 - test loss: 17.0532 - train acc: 0.8222 - test acc: 0.4237 - 16m 35s\n",
      "batch: 500/1563 - train loss: 3.3445 - test loss: 17.3064 - train acc: 0.8212 - test acc: 0.4141 - 16m 40s\n",
      "batch: 600/1563 - train loss: 3.4929 - test loss: 17.4063 - train acc: 0.8028 - test acc: 0.4090 - 16m 44s\n",
      "batch: 700/1563 - train loss: 3.4853 - test loss: 17.4666 - train acc: 0.8032 - test acc: 0.4155 - 16m 49s\n",
      "batch: 800/1563 - train loss: 3.6875 - test loss: 17.2268 - train acc: 0.7940 - test acc: 0.4163 - 16m 54s\n",
      "batch: 900/1563 - train loss: 3.9428 - test loss: 17.9538 - train acc: 0.7890 - test acc: 0.4034 - 16m 59s\n",
      "batch: 1000/1563 - train loss: 3.6107 - test loss: 18.0109 - train acc: 0.7968 - test acc: 0.4104 - 17m 4s\n",
      "batch: 1100/1563 - train loss: 3.7151 - test loss: 17.5264 - train acc: 0.7825 - test acc: 0.4124 - 17m 9s\n",
      "batch: 1200/1563 - train loss: 3.8619 - test loss: 17.4628 - train acc: 0.7912 - test acc: 0.4144 - 17m 14s\n",
      "batch: 1300/1563 - train loss: 4.0466 - test loss: 17.1195 - train acc: 0.7790 - test acc: 0.4216 - 17m 19s\n",
      "batch: 1400/1563 - train loss: 4.1983 - test loss: 17.4017 - train acc: 0.7656 - test acc: 0.4107 - 17m 23s\n",
      "batch: 1500/1563 - train loss: 3.9867 - test loss: 17.2168 - train acc: 0.7784 - test acc: 0.4194 - 17m 29s\n",
      "batch: 1563/1563 - train loss: 4.1945 - test loss: 17.0643 - train acc: 0.7747 - test acc: 0.4168 - 17m 33s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.3750 - test loss: 17.1461 - train acc: 0.8688 - test acc: 0.4213 - 17m 38s\n",
      "batch: 200/1563 - train loss: 2.5263 - test loss: 17.2570 - train acc: 0.8572 - test acc: 0.4225 - 17m 43s\n",
      "batch: 300/1563 - train loss: 2.2171 - test loss: 17.5003 - train acc: 0.8741 - test acc: 0.4224 - 17m 48s\n",
      "batch: 400/1563 - train loss: 2.4668 - test loss: 17.2982 - train acc: 0.8650 - test acc: 0.4317 - 17m 53s\n",
      "batch: 500/1563 - train loss: 2.4502 - test loss: 17.9165 - train acc: 0.8628 - test acc: 0.4226 - 17m 57s\n",
      "batch: 600/1563 - train loss: 2.6041 - test loss: 18.6959 - train acc: 0.8525 - test acc: 0.4078 - 18m 2s\n",
      "batch: 700/1563 - train loss: 2.9247 - test loss: 17.5638 - train acc: 0.8365 - test acc: 0.4215 - 18m 7s\n",
      "batch: 800/1563 - train loss: 3.1105 - test loss: 17.5758 - train acc: 0.8241 - test acc: 0.4221 - 18m 12s\n",
      "batch: 900/1563 - train loss: 2.9520 - test loss: 18.7967 - train acc: 0.8368 - test acc: 0.4040 - 18m 17s\n",
      "batch: 1000/1563 - train loss: 3.2812 - test loss: 18.7462 - train acc: 0.8227 - test acc: 0.4100 - 18m 22s\n",
      "batch: 1100/1563 - train loss: 3.3749 - test loss: 18.5004 - train acc: 0.8115 - test acc: 0.4070 - 18m 26s\n",
      "batch: 1200/1563 - train loss: 3.4231 - test loss: 17.7200 - train acc: 0.8090 - test acc: 0.4135 - 18m 31s\n",
      "batch: 1300/1563 - train loss: 3.4489 - test loss: 18.0534 - train acc: 0.8030 - test acc: 0.4117 - 18m 36s\n",
      "batch: 1400/1563 - train loss: 3.6211 - test loss: 17.8703 - train acc: 0.8009 - test acc: 0.4061 - 18m 41s\n",
      "batch: 1500/1563 - train loss: 3.8735 - test loss: 17.7349 - train acc: 0.7865 - test acc: 0.4099 - 18m 46s\n",
      "batch: 1563/1563 - train loss: 3.8357 - test loss: 18.0305 - train acc: 0.7878 - test acc: 0.4057 - 18m 50s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.1808 - test loss: 17.8915 - train acc: 0.8741 - test acc: 0.4227 - 18m 55s\n",
      "batch: 200/1563 - train loss: 2.1817 - test loss: 18.2368 - train acc: 0.8760 - test acc: 0.4187 - 19m 0s\n",
      "batch: 300/1563 - train loss: 2.0540 - test loss: 18.2922 - train acc: 0.8863 - test acc: 0.4221 - 19m 4s\n",
      "batch: 400/1563 - train loss: 2.0758 - test loss: 18.5537 - train acc: 0.8778 - test acc: 0.4108 - 19m 10s\n",
      "batch: 500/1563 - train loss: 2.3239 - test loss: 18.7256 - train acc: 0.8653 - test acc: 0.4108 - 19m 14s\n",
      "batch: 600/1563 - train loss: 2.2462 - test loss: 18.7874 - train acc: 0.8766 - test acc: 0.4220 - 19m 19s\n",
      "batch: 700/1563 - train loss: 2.3973 - test loss: 18.8496 - train acc: 0.8665 - test acc: 0.4169 - 19m 24s\n",
      "batch: 800/1563 - train loss: 2.6052 - test loss: 18.5086 - train acc: 0.8593 - test acc: 0.4221 - 19m 28s\n",
      "batch: 900/1563 - train loss: 2.6399 - test loss: 18.5936 - train acc: 0.8459 - test acc: 0.4204 - 19m 33s\n",
      "batch: 1000/1563 - train loss: 2.6775 - test loss: 18.7015 - train acc: 0.8462 - test acc: 0.4230 - 19m 37s\n",
      "batch: 1100/1563 - train loss: 2.8747 - test loss: 18.6107 - train acc: 0.8390 - test acc: 0.4237 - 19m 42s\n",
      "batch: 1200/1563 - train loss: 3.0341 - test loss: 18.9477 - train acc: 0.8275 - test acc: 0.4123 - 19m 47s\n",
      "batch: 1300/1563 - train loss: 3.1098 - test loss: 18.7550 - train acc: 0.8237 - test acc: 0.4149 - 19m 52s\n",
      "batch: 1400/1563 - train loss: 3.1498 - test loss: 18.9672 - train acc: 0.8153 - test acc: 0.4111 - 19m 56s\n",
      "time is up! finishing training\n",
      "batch: 1470/1563 - train loss: 3.1921 - test loss: 19.1098 - train acc: 0.8234 - test acc: 0.4067 - 20m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "testing - beta: 0.9 - part size: 8 - block upd: 64 - combination nº: 4\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 8 - nº part: 1176 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 73728 - partition: 8 - nº part: 9216 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 8192 - partition: 8 - nº part: 1024 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 294912 - partition: 8 - nº part: 36864 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 32768 - partition: 8 - nº part: 4096 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 1179648 - partition: 8 - nº part: 147456 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 131072 - partition: 8 - nº part: 16384 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 51200 - partition: 8 - nº part: 6400 - block updates: 64\n",
      "FisherPartitioner: param: 100 - partition: 8 - nº part: 13 - block updates: 13\n",
      "total partitions: 1403477 - effective block updates: 2557\n",
      "initializing inverses...\n",
      "partition 1/1403477\n",
      "partition 10000/1403477\n",
      "partition 20000/1403477\n",
      "partition 30000/1403477\n",
      "partition 40000/1403477\n",
      "partition 50000/1403477\n",
      "partition 60000/1403477\n",
      "partition 70000/1403477\n",
      "partition 80000/1403477\n",
      "partition 90000/1403477\n",
      "partition 100000/1403477\n",
      "partition 110000/1403477\n",
      "partition 120000/1403477\n",
      "partition 130000/1403477\n",
      "partition 140000/1403477\n",
      "partition 150000/1403477\n",
      "partition 160000/1403477\n",
      "partition 170000/1403477\n",
      "partition 180000/1403477\n",
      "partition 190000/1403477\n",
      "partition 200000/1403477\n",
      "partition 210000/1403477\n",
      "partition 220000/1403477\n",
      "partition 230000/1403477\n",
      "partition 240000/1403477\n",
      "partition 250000/1403477\n",
      "partition 260000/1403477\n",
      "partition 270000/1403477\n",
      "partition 280000/1403477\n",
      "partition 290000/1403477\n",
      "partition 300000/1403477\n",
      "partition 310000/1403477\n",
      "partition 320000/1403477\n",
      "partition 330000/1403477\n",
      "partition 340000/1403477\n",
      "partition 350000/1403477\n",
      "partition 360000/1403477\n",
      "partition 370000/1403477\n",
      "partition 380000/1403477\n",
      "partition 390000/1403477\n",
      "partition 400000/1403477\n",
      "partition 410000/1403477\n",
      "partition 420000/1403477\n",
      "partition 430000/1403477\n",
      "partition 440000/1403477\n",
      "partition 450000/1403477\n",
      "partition 460000/1403477\n",
      "partition 470000/1403477\n",
      "partition 480000/1403477\n",
      "partition 490000/1403477\n",
      "partition 500000/1403477\n",
      "partition 510000/1403477\n",
      "partition 520000/1403477\n",
      "partition 530000/1403477\n",
      "partition 540000/1403477\n",
      "partition 550000/1403477\n",
      "partition 560000/1403477\n",
      "partition 570000/1403477\n",
      "partition 580000/1403477\n",
      "partition 590000/1403477\n",
      "partition 600000/1403477\n",
      "partition 610000/1403477\n",
      "partition 620000/1403477\n",
      "partition 630000/1403477\n",
      "partition 640000/1403477\n",
      "partition 650000/1403477\n",
      "partition 660000/1403477\n",
      "partition 670000/1403477\n",
      "partition 680000/1403477\n",
      "partition 690000/1403477\n",
      "partition 700000/1403477\n",
      "partition 710000/1403477\n",
      "partition 720000/1403477\n",
      "partition 730000/1403477\n",
      "partition 740000/1403477\n",
      "partition 750000/1403477\n",
      "partition 760000/1403477\n",
      "partition 770000/1403477\n",
      "partition 780000/1403477\n",
      "partition 790000/1403477\n",
      "partition 800000/1403477\n",
      "partition 810000/1403477\n",
      "partition 820000/1403477\n",
      "partition 830000/1403477\n",
      "partition 840000/1403477\n",
      "partition 850000/1403477\n",
      "partition 860000/1403477\n",
      "partition 870000/1403477\n",
      "partition 880000/1403477\n",
      "partition 890000/1403477\n",
      "partition 900000/1403477\n",
      "partition 910000/1403477\n",
      "partition 920000/1403477\n",
      "partition 930000/1403477\n",
      "partition 940000/1403477\n",
      "partition 950000/1403477\n",
      "partition 960000/1403477\n",
      "partition 970000/1403477\n",
      "partition 980000/1403477\n",
      "partition 990000/1403477\n",
      "partition 1000000/1403477\n",
      "partition 1010000/1403477\n",
      "partition 1020000/1403477\n",
      "partition 1030000/1403477\n",
      "partition 1040000/1403477\n",
      "partition 1050000/1403477\n",
      "partition 1060000/1403477\n",
      "partition 1070000/1403477\n",
      "partition 1080000/1403477\n",
      "partition 1090000/1403477\n",
      "partition 1100000/1403477\n",
      "partition 1110000/1403477\n",
      "partition 1120000/1403477\n",
      "partition 1130000/1403477\n",
      "partition 1140000/1403477\n",
      "partition 1150000/1403477\n",
      "partition 1160000/1403477\n",
      "partition 1170000/1403477\n",
      "partition 1180000/1403477\n",
      "partition 1190000/1403477\n",
      "partition 1200000/1403477\n",
      "partition 1210000/1403477\n",
      "partition 1220000/1403477\n",
      "partition 1230000/1403477\n",
      "partition 1240000/1403477\n",
      "partition 1250000/1403477\n",
      "partition 1260000/1403477\n",
      "partition 1270000/1403477\n",
      "partition 1280000/1403477\n",
      "partition 1290000/1403477\n",
      "partition 1300000/1403477\n",
      "partition 1310000/1403477\n",
      "partition 1320000/1403477\n",
      "partition 1330000/1403477\n",
      "partition 1340000/1403477\n",
      "partition 1350000/1403477\n",
      "partition 1360000/1403477\n",
      "partition 1370000/1403477\n",
      "partition 1380000/1403477\n",
      "partition 1390000/1403477\n",
      "partition 1400000/1403477\n",
      "partition 1403477/1403477\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 25.9893 - test loss: 25.1642 - train acc: 0.0364 - test acc: 0.0533 - 0m 1s\n",
      "batch: 200/1563 - train loss: 23.8866 - test loss: 23.2849 - train acc: 0.0605 - test acc: 0.0847 - 0m 6s\n",
      "batch: 300/1563 - train loss: 22.6666 - test loss: 22.1771 - train acc: 0.0905 - test acc: 0.0985 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.1829 - test loss: 22.2600 - train acc: 0.1008 - test acc: 0.1014 - 0m 16s\n",
      "batch: 500/1563 - train loss: 21.5100 - test loss: 21.0893 - train acc: 0.1153 - test acc: 0.1266 - 0m 20s\n",
      "batch: 600/1563 - train loss: 20.6469 - test loss: 20.5392 - train acc: 0.1400 - test acc: 0.1426 - 0m 25s\n",
      "batch: 700/1563 - train loss: 20.6834 - test loss: 20.2417 - train acc: 0.1482 - test acc: 0.1546 - 0m 30s\n",
      "batch: 800/1563 - train loss: 20.4222 - test loss: 19.8870 - train acc: 0.1534 - test acc: 0.1670 - 0m 34s\n",
      "batch: 900/1563 - train loss: 20.2192 - test loss: 20.2314 - train acc: 0.1510 - test acc: 0.1478 - 0m 39s\n",
      "batch: 1000/1563 - train loss: 19.7518 - test loss: 20.1311 - train acc: 0.1713 - test acc: 0.1657 - 0m 44s\n",
      "batch: 1100/1563 - train loss: 19.5556 - test loss: 19.0664 - train acc: 0.1797 - test acc: 0.1935 - 0m 49s\n",
      "batch: 1200/1563 - train loss: 19.3346 - test loss: 19.6823 - train acc: 0.1791 - test acc: 0.1792 - 0m 53s\n",
      "batch: 1300/1563 - train loss: 18.7692 - test loss: 18.3936 - train acc: 0.1941 - test acc: 0.2167 - 0m 58s\n",
      "batch: 1400/1563 - train loss: 18.7667 - test loss: 20.1469 - train acc: 0.1957 - test acc: 0.1593 - 1m 3s\n",
      "batch: 1500/1563 - train loss: 18.5287 - test loss: 18.4688 - train acc: 0.2078 - test acc: 0.2090 - 1m 7s\n",
      "batch: 1563/1563 - train loss: 18.3759 - test loss: 17.9445 - train acc: 0.2041 - test acc: 0.2252 - 1m 11s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.6695 - test loss: 17.8679 - train acc: 0.2363 - test acc: 0.2220 - 1m 16s\n",
      "batch: 200/1563 - train loss: 17.7383 - test loss: 18.2319 - train acc: 0.2369 - test acc: 0.2186 - 1m 21s\n",
      "batch: 300/1563 - train loss: 17.4583 - test loss: 17.4002 - train acc: 0.2325 - test acc: 0.2421 - 1m 25s\n",
      "batch: 400/1563 - train loss: 17.3029 - test loss: 18.0498 - train acc: 0.2435 - test acc: 0.2277 - 1m 30s\n",
      "batch: 500/1563 - train loss: 17.4893 - test loss: 18.2981 - train acc: 0.2416 - test acc: 0.2235 - 1m 35s\n",
      "batch: 600/1563 - train loss: 17.1433 - test loss: 16.7226 - train acc: 0.2428 - test acc: 0.2674 - 1m 39s\n",
      "batch: 700/1563 - train loss: 17.1223 - test loss: 17.5546 - train acc: 0.2410 - test acc: 0.2306 - 1m 44s\n",
      "batch: 800/1563 - train loss: 16.5937 - test loss: 17.0304 - train acc: 0.2631 - test acc: 0.2559 - 1m 49s\n",
      "batch: 900/1563 - train loss: 16.9017 - test loss: 17.4066 - train acc: 0.2612 - test acc: 0.2485 - 1m 53s\n",
      "batch: 1000/1563 - train loss: 16.6038 - test loss: 16.9225 - train acc: 0.2709 - test acc: 0.2601 - 1m 58s\n",
      "batch: 1100/1563 - train loss: 16.5080 - test loss: 16.6482 - train acc: 0.2809 - test acc: 0.2718 - 2m 3s\n",
      "batch: 1200/1563 - train loss: 16.3630 - test loss: 17.3986 - train acc: 0.2757 - test acc: 0.2511 - 2m 8s\n",
      "batch: 1300/1563 - train loss: 16.4091 - test loss: 16.8611 - train acc: 0.2797 - test acc: 0.2611 - 2m 12s\n",
      "batch: 1400/1563 - train loss: 16.2901 - test loss: 16.0781 - train acc: 0.2747 - test acc: 0.2929 - 2m 17s\n",
      "batch: 1500/1563 - train loss: 16.2803 - test loss: 15.7850 - train acc: 0.2819 - test acc: 0.3019 - 2m 22s\n",
      "batch: 1563/1563 - train loss: 16.1276 - test loss: 16.5587 - train acc: 0.2928 - test acc: 0.2718 - 2m 26s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.8893 - test loss: 15.9639 - train acc: 0.3199 - test acc: 0.2955 - 2m 30s\n",
      "batch: 200/1563 - train loss: 14.7468 - test loss: 17.2709 - train acc: 0.3262 - test acc: 0.2593 - 2m 35s\n",
      "batch: 300/1563 - train loss: 14.9493 - test loss: 16.0526 - train acc: 0.3221 - test acc: 0.2936 - 2m 40s\n",
      "batch: 400/1563 - train loss: 15.1020 - test loss: 15.6718 - train acc: 0.3215 - test acc: 0.3044 - 2m 44s\n",
      "batch: 500/1563 - train loss: 14.8602 - test loss: 15.8052 - train acc: 0.3231 - test acc: 0.3031 - 2m 49s\n",
      "batch: 600/1563 - train loss: 14.9240 - test loss: 15.8574 - train acc: 0.3237 - test acc: 0.3016 - 2m 54s\n",
      "batch: 700/1563 - train loss: 14.7540 - test loss: 15.5764 - train acc: 0.3341 - test acc: 0.3097 - 2m 58s\n",
      "batch: 800/1563 - train loss: 14.9328 - test loss: 16.0180 - train acc: 0.3350 - test acc: 0.2961 - 3m 3s\n",
      "batch: 900/1563 - train loss: 14.8404 - test loss: 15.3292 - train acc: 0.3412 - test acc: 0.3200 - 3m 8s\n",
      "batch: 1000/1563 - train loss: 14.6033 - test loss: 15.5961 - train acc: 0.3412 - test acc: 0.3158 - 3m 13s\n",
      "batch: 1100/1563 - train loss: 14.5496 - test loss: 16.0020 - train acc: 0.3299 - test acc: 0.2941 - 3m 17s\n",
      "batch: 1200/1563 - train loss: 14.4641 - test loss: 15.7356 - train acc: 0.3462 - test acc: 0.3116 - 3m 22s\n",
      "batch: 1300/1563 - train loss: 14.6716 - test loss: 15.8923 - train acc: 0.3368 - test acc: 0.2993 - 3m 27s\n",
      "batch: 1400/1563 - train loss: 14.5429 - test loss: 14.6638 - train acc: 0.3378 - test acc: 0.3450 - 3m 31s\n",
      "batch: 1500/1563 - train loss: 14.7750 - test loss: 14.7048 - train acc: 0.3440 - test acc: 0.3423 - 3m 36s\n",
      "batch: 1563/1563 - train loss: 14.4776 - test loss: 15.3702 - train acc: 0.3547 - test acc: 0.3157 - 3m 40s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.5271 - test loss: 14.5961 - train acc: 0.4097 - test acc: 0.3475 - 3m 45s\n",
      "batch: 200/1563 - train loss: 13.0311 - test loss: 14.7582 - train acc: 0.3925 - test acc: 0.3470 - 3m 49s\n",
      "batch: 300/1563 - train loss: 13.1412 - test loss: 14.9258 - train acc: 0.3907 - test acc: 0.3432 - 3m 54s\n",
      "batch: 400/1563 - train loss: 13.2509 - test loss: 14.3957 - train acc: 0.3835 - test acc: 0.3585 - 3m 59s\n",
      "batch: 500/1563 - train loss: 13.1787 - test loss: 15.3407 - train acc: 0.3944 - test acc: 0.3273 - 4m 4s\n",
      "batch: 600/1563 - train loss: 13.0430 - test loss: 15.4010 - train acc: 0.4028 - test acc: 0.3210 - 4m 8s\n",
      "batch: 700/1563 - train loss: 13.2746 - test loss: 14.6409 - train acc: 0.3922 - test acc: 0.3488 - 4m 13s\n",
      "batch: 800/1563 - train loss: 13.1617 - test loss: 14.8611 - train acc: 0.3950 - test acc: 0.3394 - 4m 18s\n",
      "batch: 900/1563 - train loss: 12.9729 - test loss: 14.2949 - train acc: 0.4072 - test acc: 0.3596 - 4m 23s\n",
      "batch: 1000/1563 - train loss: 13.4001 - test loss: 15.0078 - train acc: 0.3912 - test acc: 0.3311 - 4m 28s\n",
      "batch: 1100/1563 - train loss: 13.3244 - test loss: 15.0478 - train acc: 0.3975 - test acc: 0.3362 - 4m 32s\n",
      "batch: 1200/1563 - train loss: 12.9445 - test loss: 14.2022 - train acc: 0.4006 - test acc: 0.3598 - 4m 37s\n",
      "batch: 1300/1563 - train loss: 12.8100 - test loss: 13.7611 - train acc: 0.4135 - test acc: 0.3809 - 4m 42s\n",
      "batch: 1400/1563 - train loss: 13.0028 - test loss: 14.0690 - train acc: 0.4069 - test acc: 0.3711 - 4m 46s\n",
      "batch: 1500/1563 - train loss: 13.0516 - test loss: 13.7527 - train acc: 0.4035 - test acc: 0.3771 - 4m 51s\n",
      "batch: 1563/1563 - train loss: 12.9262 - test loss: 14.1990 - train acc: 0.3984 - test acc: 0.3660 - 4m 55s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.1410 - test loss: 13.8041 - train acc: 0.4612 - test acc: 0.3778 - 5m 0s\n",
      "batch: 200/1563 - train loss: 11.5922 - test loss: 14.2943 - train acc: 0.4400 - test acc: 0.3722 - 5m 4s\n",
      "batch: 300/1563 - train loss: 11.6974 - test loss: 13.8566 - train acc: 0.4462 - test acc: 0.3749 - 5m 9s\n",
      "batch: 400/1563 - train loss: 11.6311 - test loss: 14.1319 - train acc: 0.4441 - test acc: 0.3697 - 5m 14s\n",
      "batch: 500/1563 - train loss: 12.0880 - test loss: 14.4266 - train acc: 0.4372 - test acc: 0.3645 - 5m 18s\n",
      "batch: 600/1563 - train loss: 11.5033 - test loss: 15.2323 - train acc: 0.4504 - test acc: 0.3420 - 5m 23s\n",
      "batch: 700/1563 - train loss: 11.8931 - test loss: 16.1224 - train acc: 0.4334 - test acc: 0.3191 - 5m 28s\n",
      "batch: 800/1563 - train loss: 12.0610 - test loss: 13.6427 - train acc: 0.4347 - test acc: 0.3859 - 5m 33s\n",
      "batch: 900/1563 - train loss: 11.8069 - test loss: 13.8861 - train acc: 0.4319 - test acc: 0.3781 - 5m 37s\n",
      "batch: 1000/1563 - train loss: 11.6151 - test loss: 14.8711 - train acc: 0.4497 - test acc: 0.3518 - 5m 42s\n",
      "batch: 1100/1563 - train loss: 11.6731 - test loss: 14.5521 - train acc: 0.4519 - test acc: 0.3636 - 5m 47s\n",
      "batch: 1200/1563 - train loss: 11.9498 - test loss: 13.7612 - train acc: 0.4388 - test acc: 0.3798 - 5m 52s\n",
      "batch: 1300/1563 - train loss: 11.9424 - test loss: 13.9761 - train acc: 0.4407 - test acc: 0.3774 - 5m 56s\n",
      "batch: 1400/1563 - train loss: 11.9961 - test loss: 15.8545 - train acc: 0.4372 - test acc: 0.3319 - 6m 1s\n",
      "batch: 1500/1563 - train loss: 11.9456 - test loss: 13.2026 - train acc: 0.4434 - test acc: 0.4009 - 6m 6s\n",
      "batch: 1563/1563 - train loss: 11.8619 - test loss: 13.5083 - train acc: 0.4500 - test acc: 0.3959 - 6m 10s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0617 - test loss: 13.3420 - train acc: 0.5157 - test acc: 0.4067 - 6m 15s\n",
      "batch: 200/1563 - train loss: 9.9308 - test loss: 14.5770 - train acc: 0.5222 - test acc: 0.3765 - 6m 20s\n",
      "batch: 300/1563 - train loss: 10.3181 - test loss: 13.3949 - train acc: 0.4997 - test acc: 0.4054 - 6m 24s\n",
      "batch: 400/1563 - train loss: 10.2972 - test loss: 14.1441 - train acc: 0.5091 - test acc: 0.3838 - 6m 29s\n",
      "batch: 500/1563 - train loss: 10.5341 - test loss: 13.5561 - train acc: 0.4947 - test acc: 0.3992 - 6m 34s\n",
      "batch: 600/1563 - train loss: 10.2320 - test loss: 13.6015 - train acc: 0.5082 - test acc: 0.3950 - 6m 38s\n",
      "batch: 700/1563 - train loss: 10.8742 - test loss: 13.6182 - train acc: 0.4813 - test acc: 0.3940 - 6m 43s\n",
      "batch: 800/1563 - train loss: 10.4536 - test loss: 13.6673 - train acc: 0.4878 - test acc: 0.3947 - 6m 48s\n",
      "batch: 900/1563 - train loss: 10.8297 - test loss: 13.7838 - train acc: 0.4659 - test acc: 0.3897 - 6m 53s\n",
      "batch: 1000/1563 - train loss: 10.7248 - test loss: 13.3424 - train acc: 0.4834 - test acc: 0.4022 - 6m 57s\n",
      "batch: 1100/1563 - train loss: 10.8115 - test loss: 13.5541 - train acc: 0.4756 - test acc: 0.3932 - 7m 2s\n",
      "batch: 1200/1563 - train loss: 11.1131 - test loss: 14.0848 - train acc: 0.4684 - test acc: 0.3810 - 7m 7s\n",
      "batch: 1300/1563 - train loss: 10.8197 - test loss: 13.8567 - train acc: 0.4866 - test acc: 0.3924 - 7m 12s\n",
      "batch: 1400/1563 - train loss: 10.9859 - test loss: 13.1795 - train acc: 0.4785 - test acc: 0.4087 - 7m 16s\n",
      "batch: 1500/1563 - train loss: 11.0375 - test loss: 13.7959 - train acc: 0.4772 - test acc: 0.3928 - 7m 21s\n",
      "batch: 1563/1563 - train loss: 10.8468 - test loss: 14.8728 - train acc: 0.4850 - test acc: 0.3539 - 7m 25s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.7691 - test loss: 13.5434 - train acc: 0.5712 - test acc: 0.4047 - 7m 30s\n",
      "batch: 200/1563 - train loss: 8.9188 - test loss: 13.5979 - train acc: 0.5625 - test acc: 0.4044 - 7m 35s\n",
      "batch: 300/1563 - train loss: 8.9743 - test loss: 14.6815 - train acc: 0.5503 - test acc: 0.3763 - 7m 39s\n",
      "batch: 400/1563 - train loss: 9.2574 - test loss: 13.6533 - train acc: 0.5576 - test acc: 0.4099 - 7m 44s\n",
      "batch: 500/1563 - train loss: 8.8915 - test loss: 13.5672 - train acc: 0.5503 - test acc: 0.4103 - 7m 49s\n",
      "batch: 600/1563 - train loss: 9.4530 - test loss: 14.1644 - train acc: 0.5366 - test acc: 0.3884 - 7m 54s\n",
      "batch: 700/1563 - train loss: 9.3871 - test loss: 13.9023 - train acc: 0.5384 - test acc: 0.4011 - 7m 58s\n",
      "batch: 800/1563 - train loss: 9.4893 - test loss: 15.1428 - train acc: 0.5324 - test acc: 0.3667 - 8m 3s\n",
      "batch: 900/1563 - train loss: 9.7565 - test loss: 13.7065 - train acc: 0.5247 - test acc: 0.4016 - 8m 8s\n",
      "batch: 1000/1563 - train loss: 9.6451 - test loss: 13.9115 - train acc: 0.5293 - test acc: 0.3964 - 8m 12s\n",
      "batch: 1100/1563 - train loss: 9.8227 - test loss: 13.3559 - train acc: 0.5322 - test acc: 0.4107 - 8m 17s\n",
      "batch: 1200/1563 - train loss: 9.8703 - test loss: 14.9274 - train acc: 0.5244 - test acc: 0.3689 - 8m 22s\n",
      "batch: 1300/1563 - train loss: 9.7291 - test loss: 13.4988 - train acc: 0.5213 - test acc: 0.4026 - 8m 27s\n",
      "batch: 1400/1563 - train loss: 9.9762 - test loss: 14.2848 - train acc: 0.4984 - test acc: 0.3958 - 8m 31s\n",
      "batch: 1500/1563 - train loss: 10.0374 - test loss: 13.2635 - train acc: 0.5128 - test acc: 0.4150 - 8m 36s\n",
      "batch: 1563/1563 - train loss: 9.6689 - test loss: 13.3257 - train acc: 0.5384 - test acc: 0.4168 - 8m 40s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.4606 - test loss: 13.5300 - train acc: 0.6228 - test acc: 0.4147 - 8m 45s\n",
      "batch: 200/1563 - train loss: 7.5166 - test loss: 13.9752 - train acc: 0.6200 - test acc: 0.4083 - 8m 49s\n",
      "batch: 300/1563 - train loss: 8.0162 - test loss: 13.7535 - train acc: 0.5879 - test acc: 0.4149 - 8m 55s\n",
      "batch: 400/1563 - train loss: 7.8661 - test loss: 14.1740 - train acc: 0.5972 - test acc: 0.4080 - 8m 59s\n",
      "batch: 500/1563 - train loss: 8.4385 - test loss: 14.1030 - train acc: 0.5781 - test acc: 0.4083 - 9m 4s\n",
      "batch: 600/1563 - train loss: 8.0131 - test loss: 15.9378 - train acc: 0.5894 - test acc: 0.3665 - 9m 9s\n",
      "batch: 700/1563 - train loss: 8.6343 - test loss: 13.7765 - train acc: 0.5694 - test acc: 0.4131 - 9m 13s\n",
      "batch: 800/1563 - train loss: 8.8121 - test loss: 14.2172 - train acc: 0.5600 - test acc: 0.4010 - 9m 18s\n",
      "batch: 900/1563 - train loss: 8.5993 - test loss: 13.7856 - train acc: 0.5693 - test acc: 0.4118 - 9m 23s\n",
      "batch: 1000/1563 - train loss: 8.6237 - test loss: 14.2733 - train acc: 0.5578 - test acc: 0.3981 - 9m 28s\n",
      "batch: 1100/1563 - train loss: 9.0594 - test loss: 13.8987 - train acc: 0.5463 - test acc: 0.4041 - 9m 32s\n",
      "batch: 1200/1563 - train loss: 8.7366 - test loss: 14.6800 - train acc: 0.5674 - test acc: 0.3913 - 9m 37s\n",
      "batch: 1300/1563 - train loss: 9.0618 - test loss: 13.4512 - train acc: 0.5431 - test acc: 0.4205 - 9m 42s\n",
      "batch: 1400/1563 - train loss: 8.7101 - test loss: 13.4052 - train acc: 0.5703 - test acc: 0.4249 - 9m 47s\n",
      "batch: 1500/1563 - train loss: 9.2859 - test loss: 13.2904 - train acc: 0.5415 - test acc: 0.4236 - 9m 51s\n",
      "batch: 1563/1563 - train loss: 9.4414 - test loss: 14.0731 - train acc: 0.5291 - test acc: 0.3961 - 9m 55s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4882 - test loss: 13.2865 - train acc: 0.6713 - test acc: 0.4301 - 10m 0s\n",
      "batch: 200/1563 - train loss: 6.4272 - test loss: 14.0309 - train acc: 0.6675 - test acc: 0.4104 - 10m 5s\n",
      "batch: 300/1563 - train loss: 6.7749 - test loss: 14.9492 - train acc: 0.6472 - test acc: 0.3968 - 10m 10s\n",
      "batch: 400/1563 - train loss: 6.7943 - test loss: 14.0359 - train acc: 0.6494 - test acc: 0.4165 - 10m 15s\n",
      "batch: 500/1563 - train loss: 7.4002 - test loss: 14.2807 - train acc: 0.6169 - test acc: 0.4080 - 10m 19s\n",
      "batch: 600/1563 - train loss: 7.5724 - test loss: 14.5400 - train acc: 0.6234 - test acc: 0.3984 - 10m 24s\n",
      "batch: 700/1563 - train loss: 7.6191 - test loss: 14.0485 - train acc: 0.6118 - test acc: 0.4123 - 10m 29s\n",
      "batch: 800/1563 - train loss: 7.5542 - test loss: 14.2544 - train acc: 0.6057 - test acc: 0.4117 - 10m 34s\n",
      "batch: 900/1563 - train loss: 7.6916 - test loss: 14.6764 - train acc: 0.6131 - test acc: 0.3966 - 10m 39s\n",
      "batch: 1000/1563 - train loss: 7.9354 - test loss: 14.2302 - train acc: 0.6025 - test acc: 0.4150 - 10m 43s\n",
      "batch: 1100/1563 - train loss: 7.7088 - test loss: 13.9694 - train acc: 0.6103 - test acc: 0.4201 - 10m 48s\n",
      "batch: 1200/1563 - train loss: 7.8906 - test loss: 14.1670 - train acc: 0.6002 - test acc: 0.4133 - 10m 53s\n",
      "batch: 1300/1563 - train loss: 7.8996 - test loss: 14.7458 - train acc: 0.5994 - test acc: 0.3968 - 10m 58s\n",
      "batch: 1400/1563 - train loss: 8.2488 - test loss: 14.0483 - train acc: 0.5862 - test acc: 0.4095 - 11m 3s\n",
      "batch: 1500/1563 - train loss: 8.3285 - test loss: 16.4642 - train acc: 0.5835 - test acc: 0.3604 - 11m 8s\n",
      "batch: 1563/1563 - train loss: 8.4003 - test loss: 14.3753 - train acc: 0.5815 - test acc: 0.4012 - 11m 12s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.9296 - test loss: 14.0639 - train acc: 0.7078 - test acc: 0.4189 - 11m 16s\n",
      "batch: 200/1563 - train loss: 5.5817 - test loss: 14.2563 - train acc: 0.7116 - test acc: 0.4164 - 11m 21s\n",
      "batch: 300/1563 - train loss: 5.5504 - test loss: 14.9540 - train acc: 0.7050 - test acc: 0.4100 - 11m 26s\n",
      "batch: 400/1563 - train loss: 6.0611 - test loss: 14.6532 - train acc: 0.6819 - test acc: 0.4124 - 11m 31s\n",
      "batch: 500/1563 - train loss: 6.1421 - test loss: 15.0268 - train acc: 0.6725 - test acc: 0.4099 - 11m 36s\n",
      "batch: 600/1563 - train loss: 6.3647 - test loss: 14.4255 - train acc: 0.6625 - test acc: 0.4224 - 11m 40s\n",
      "batch: 700/1563 - train loss: 6.3042 - test loss: 15.2175 - train acc: 0.6726 - test acc: 0.4009 - 11m 45s\n",
      "batch: 800/1563 - train loss: 6.5226 - test loss: 15.4665 - train acc: 0.6560 - test acc: 0.3931 - 11m 50s\n",
      "batch: 900/1563 - train loss: 6.9922 - test loss: 14.8573 - train acc: 0.6356 - test acc: 0.4120 - 11m 55s\n",
      "batch: 1000/1563 - train loss: 6.9287 - test loss: 14.7615 - train acc: 0.6412 - test acc: 0.4112 - 12m 0s\n",
      "batch: 1100/1563 - train loss: 6.9324 - test loss: 14.6087 - train acc: 0.6469 - test acc: 0.4115 - 12m 5s\n",
      "batch: 1200/1563 - train loss: 7.0491 - test loss: 14.7827 - train acc: 0.6338 - test acc: 0.4131 - 12m 10s\n",
      "batch: 1300/1563 - train loss: 7.2179 - test loss: 14.7743 - train acc: 0.6216 - test acc: 0.4143 - 12m 15s\n",
      "batch: 1400/1563 - train loss: 7.4249 - test loss: 14.4108 - train acc: 0.6237 - test acc: 0.4142 - 12m 20s\n",
      "batch: 1500/1563 - train loss: 7.4224 - test loss: 14.5389 - train acc: 0.6216 - test acc: 0.4110 - 12m 24s\n",
      "batch: 1563/1563 - train loss: 7.3794 - test loss: 14.7885 - train acc: 0.6184 - test acc: 0.3960 - 12m 29s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.8430 - test loss: 14.6141 - train acc: 0.7403 - test acc: 0.4214 - 12m 34s\n",
      "batch: 200/1563 - train loss: 4.4765 - test loss: 14.9999 - train acc: 0.7584 - test acc: 0.4156 - 12m 39s\n",
      "batch: 300/1563 - train loss: 4.8156 - test loss: 14.8674 - train acc: 0.7469 - test acc: 0.4215 - 12m 44s\n",
      "batch: 400/1563 - train loss: 5.1180 - test loss: 15.3633 - train acc: 0.7269 - test acc: 0.4087 - 12m 49s\n",
      "batch: 500/1563 - train loss: 5.4556 - test loss: 15.1626 - train acc: 0.7172 - test acc: 0.4140 - 12m 54s\n",
      "batch: 600/1563 - train loss: 5.3736 - test loss: 15.4287 - train acc: 0.7200 - test acc: 0.4179 - 12m 58s\n",
      "batch: 700/1563 - train loss: 5.6326 - test loss: 15.5066 - train acc: 0.6997 - test acc: 0.4111 - 13m 3s\n",
      "batch: 800/1563 - train loss: 5.9218 - test loss: 15.1374 - train acc: 0.6885 - test acc: 0.4236 - 13m 9s\n",
      "batch: 900/1563 - train loss: 5.8582 - test loss: 15.8944 - train acc: 0.6904 - test acc: 0.4005 - 13m 14s\n",
      "batch: 1000/1563 - train loss: 5.8205 - test loss: 16.4450 - train acc: 0.6907 - test acc: 0.3891 - 13m 18s\n",
      "batch: 1100/1563 - train loss: 6.3121 - test loss: 15.5918 - train acc: 0.6700 - test acc: 0.4039 - 13m 23s\n",
      "batch: 1200/1563 - train loss: 6.0880 - test loss: 15.5136 - train acc: 0.6860 - test acc: 0.4017 - 13m 28s\n",
      "batch: 1300/1563 - train loss: 6.4184 - test loss: 16.0734 - train acc: 0.6603 - test acc: 0.4036 - 13m 33s\n",
      "batch: 1400/1563 - train loss: 6.6017 - test loss: 14.8695 - train acc: 0.6579 - test acc: 0.4235 - 13m 39s\n",
      "batch: 1500/1563 - train loss: 6.4293 - test loss: 14.7847 - train acc: 0.6697 - test acc: 0.4178 - 13m 43s\n",
      "batch: 1563/1563 - train loss: 6.5473 - test loss: 15.1583 - train acc: 0.6619 - test acc: 0.4104 - 13m 48s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.2329 - test loss: 15.2612 - train acc: 0.7724 - test acc: 0.4158 - 13m 53s\n",
      "batch: 200/1563 - train loss: 3.9091 - test loss: 15.0863 - train acc: 0.7943 - test acc: 0.4310 - 13m 57s\n",
      "batch: 300/1563 - train loss: 4.1630 - test loss: 15.7983 - train acc: 0.7687 - test acc: 0.4168 - 14m 2s\n",
      "batch: 400/1563 - train loss: 4.2597 - test loss: 15.7368 - train acc: 0.7650 - test acc: 0.4219 - 14m 7s\n",
      "batch: 500/1563 - train loss: 4.4575 - test loss: 15.9831 - train acc: 0.7641 - test acc: 0.4131 - 14m 12s\n",
      "batch: 600/1563 - train loss: 4.7233 - test loss: 16.4713 - train acc: 0.7441 - test acc: 0.4101 - 14m 17s\n",
      "batch: 700/1563 - train loss: 4.9450 - test loss: 16.3083 - train acc: 0.7312 - test acc: 0.4064 - 14m 22s\n",
      "batch: 800/1563 - train loss: 5.1884 - test loss: 16.0529 - train acc: 0.7262 - test acc: 0.4068 - 14m 26s\n",
      "batch: 900/1563 - train loss: 5.3125 - test loss: 15.9225 - train acc: 0.7181 - test acc: 0.4116 - 14m 31s\n",
      "batch: 1000/1563 - train loss: 5.3937 - test loss: 16.1481 - train acc: 0.7119 - test acc: 0.4089 - 14m 36s\n",
      "batch: 1100/1563 - train loss: 5.3857 - test loss: 17.4133 - train acc: 0.7119 - test acc: 0.3778 - 14m 41s\n",
      "batch: 1200/1563 - train loss: 5.5484 - test loss: 16.0675 - train acc: 0.7016 - test acc: 0.4080 - 14m 46s\n",
      "batch: 1300/1563 - train loss: 5.4767 - test loss: 16.5102 - train acc: 0.7084 - test acc: 0.3932 - 14m 51s\n",
      "batch: 1400/1563 - train loss: 5.6035 - test loss: 15.4933 - train acc: 0.7006 - test acc: 0.4170 - 14m 56s\n",
      "batch: 1500/1563 - train loss: 5.9210 - test loss: 15.9324 - train acc: 0.6916 - test acc: 0.4095 - 15m 0s\n",
      "batch: 1563/1563 - train loss: 5.8261 - test loss: 15.2823 - train acc: 0.6888 - test acc: 0.4192 - 15m 4s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.3315 - test loss: 15.6744 - train acc: 0.8140 - test acc: 0.4257 - 15m 9s\n",
      "batch: 200/1563 - train loss: 3.4063 - test loss: 15.7239 - train acc: 0.8090 - test acc: 0.4281 - 15m 14s\n",
      "batch: 300/1563 - train loss: 3.4153 - test loss: 16.4789 - train acc: 0.8162 - test acc: 0.4139 - 15m 19s\n",
      "batch: 400/1563 - train loss: 3.6110 - test loss: 16.0446 - train acc: 0.8018 - test acc: 0.4218 - 15m 24s\n",
      "batch: 500/1563 - train loss: 3.8119 - test loss: 16.8429 - train acc: 0.7862 - test acc: 0.4149 - 15m 29s\n",
      "batch: 600/1563 - train loss: 3.9910 - test loss: 16.6845 - train acc: 0.7827 - test acc: 0.4107 - 15m 33s\n",
      "batch: 700/1563 - train loss: 4.2013 - test loss: 16.7519 - train acc: 0.7734 - test acc: 0.4135 - 15m 38s\n",
      "batch: 800/1563 - train loss: 4.2893 - test loss: 16.5245 - train acc: 0.7644 - test acc: 0.4177 - 15m 43s\n",
      "batch: 900/1563 - train loss: 4.4004 - test loss: 16.7014 - train acc: 0.7656 - test acc: 0.4156 - 15m 48s\n",
      "batch: 1000/1563 - train loss: 4.5721 - test loss: 17.7917 - train acc: 0.7540 - test acc: 0.3908 - 15m 53s\n",
      "batch: 1100/1563 - train loss: 4.9446 - test loss: 16.9172 - train acc: 0.7294 - test acc: 0.4032 - 15m 58s\n",
      "batch: 1200/1563 - train loss: 4.8722 - test loss: 16.6119 - train acc: 0.7328 - test acc: 0.4063 - 16m 3s\n",
      "batch: 1300/1563 - train loss: 4.9533 - test loss: 16.9079 - train acc: 0.7322 - test acc: 0.4042 - 16m 7s\n",
      "batch: 1400/1563 - train loss: 5.2821 - test loss: 16.0576 - train acc: 0.7172 - test acc: 0.4164 - 16m 12s\n",
      "batch: 1500/1563 - train loss: 5.2138 - test loss: 16.1018 - train acc: 0.7216 - test acc: 0.4134 - 16m 17s\n",
      "batch: 1563/1563 - train loss: 5.1612 - test loss: 16.0215 - train acc: 0.7235 - test acc: 0.4188 - 16m 21s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 3.1276 - test loss: 15.9737 - train acc: 0.8296 - test acc: 0.4240 - 16m 26s\n",
      "batch: 200/1563 - train loss: 3.1398 - test loss: 16.2515 - train acc: 0.8287 - test acc: 0.4246 - 16m 31s\n",
      "batch: 300/1563 - train loss: 3.0834 - test loss: 16.7743 - train acc: 0.8353 - test acc: 0.4221 - 16m 36s\n",
      "batch: 400/1563 - train loss: 3.0962 - test loss: 17.1405 - train acc: 0.8212 - test acc: 0.4123 - 16m 40s\n",
      "batch: 500/1563 - train loss: 3.3550 - test loss: 17.3936 - train acc: 0.8118 - test acc: 0.4071 - 16m 45s\n",
      "batch: 600/1563 - train loss: 3.3270 - test loss: 17.2760 - train acc: 0.8144 - test acc: 0.4184 - 16m 50s\n",
      "batch: 700/1563 - train loss: 3.7424 - test loss: 17.2806 - train acc: 0.7911 - test acc: 0.4183 - 16m 55s\n",
      "batch: 800/1563 - train loss: 3.7547 - test loss: 17.5861 - train acc: 0.7896 - test acc: 0.4035 - 17m 0s\n",
      "batch: 900/1563 - train loss: 3.5406 - test loss: 17.1021 - train acc: 0.8028 - test acc: 0.4255 - 17m 4s\n",
      "batch: 1000/1563 - train loss: 3.8862 - test loss: 17.3155 - train acc: 0.7859 - test acc: 0.4157 - 17m 9s\n",
      "batch: 1100/1563 - train loss: 3.8740 - test loss: 18.0203 - train acc: 0.7887 - test acc: 0.4005 - 17m 14s\n",
      "batch: 1200/1563 - train loss: 4.0715 - test loss: 17.4317 - train acc: 0.7696 - test acc: 0.4122 - 17m 20s\n",
      "batch: 1300/1563 - train loss: 4.2416 - test loss: 18.4726 - train acc: 0.7609 - test acc: 0.3847 - 17m 24s\n",
      "batch: 1400/1563 - train loss: 4.1251 - test loss: 17.1149 - train acc: 0.7787 - test acc: 0.4141 - 17m 29s\n",
      "batch: 1500/1563 - train loss: 4.5832 - test loss: 17.0307 - train acc: 0.7503 - test acc: 0.4131 - 17m 34s\n",
      "batch: 1563/1563 - train loss: 4.3494 - test loss: 17.1605 - train acc: 0.7534 - test acc: 0.4113 - 17m 38s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.5291 - test loss: 17.3022 - train acc: 0.8566 - test acc: 0.4269 - 17m 43s\n",
      "batch: 200/1563 - train loss: 2.4097 - test loss: 17.1701 - train acc: 0.8684 - test acc: 0.4260 - 17m 48s\n",
      "batch: 300/1563 - train loss: 2.6839 - test loss: 17.2169 - train acc: 0.8503 - test acc: 0.4245 - 17m 53s\n",
      "batch: 400/1563 - train loss: 2.6708 - test loss: 17.3878 - train acc: 0.8469 - test acc: 0.4225 - 17m 57s\n",
      "batch: 500/1563 - train loss: 2.6812 - test loss: 18.1748 - train acc: 0.8359 - test acc: 0.4178 - 18m 2s\n",
      "batch: 600/1563 - train loss: 2.8131 - test loss: 18.4097 - train acc: 0.8390 - test acc: 0.4096 - 18m 7s\n",
      "batch: 700/1563 - train loss: 3.0687 - test loss: 17.7416 - train acc: 0.8328 - test acc: 0.4194 - 18m 12s\n",
      "batch: 800/1563 - train loss: 3.2568 - test loss: 18.4014 - train acc: 0.8113 - test acc: 0.4155 - 18m 17s\n",
      "batch: 900/1563 - train loss: 3.4223 - test loss: 17.9945 - train acc: 0.8059 - test acc: 0.4225 - 18m 22s\n",
      "batch: 1000/1563 - train loss: 3.3081 - test loss: 18.0096 - train acc: 0.8199 - test acc: 0.4130 - 18m 27s\n",
      "batch: 1100/1563 - train loss: 3.5995 - test loss: 18.0190 - train acc: 0.7946 - test acc: 0.4073 - 18m 32s\n",
      "batch: 1200/1563 - train loss: 3.5840 - test loss: 18.5413 - train acc: 0.7969 - test acc: 0.4094 - 18m 36s\n",
      "batch: 1300/1563 - train loss: 3.7445 - test loss: 17.8641 - train acc: 0.7934 - test acc: 0.4146 - 18m 41s\n",
      "batch: 1400/1563 - train loss: 3.8611 - test loss: 17.5848 - train acc: 0.7784 - test acc: 0.4130 - 18m 46s\n",
      "batch: 1500/1563 - train loss: 3.9613 - test loss: 17.9538 - train acc: 0.7812 - test acc: 0.4086 - 18m 51s\n",
      "batch: 1563/1563 - train loss: 4.0865 - test loss: 18.0014 - train acc: 0.7684 - test acc: 0.4149 - 18m 55s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.2203 - test loss: 17.3332 - train acc: 0.8738 - test acc: 0.4305 - 19m 0s\n",
      "batch: 200/1563 - train loss: 2.1856 - test loss: 17.7904 - train acc: 0.8788 - test acc: 0.4230 - 19m 4s\n",
      "batch: 300/1563 - train loss: 2.1440 - test loss: 17.6087 - train acc: 0.8737 - test acc: 0.4272 - 19m 9s\n",
      "batch: 400/1563 - train loss: 2.1273 - test loss: 17.9913 - train acc: 0.8794 - test acc: 0.4310 - 19m 14s\n",
      "batch: 500/1563 - train loss: 2.1412 - test loss: 18.4194 - train acc: 0.8812 - test acc: 0.4240 - 19m 19s\n",
      "batch: 600/1563 - train loss: 2.5988 - test loss: 18.7828 - train acc: 0.8562 - test acc: 0.4200 - 19m 24s\n",
      "batch: 700/1563 - train loss: 2.6364 - test loss: 18.3453 - train acc: 0.8450 - test acc: 0.4251 - 19m 29s\n",
      "batch: 800/1563 - train loss: 2.4067 - test loss: 18.8317 - train acc: 0.8581 - test acc: 0.4205 - 19m 34s\n",
      "batch: 900/1563 - train loss: 2.9707 - test loss: 19.2804 - train acc: 0.8306 - test acc: 0.4094 - 19m 39s\n",
      "batch: 1000/1563 - train loss: 3.0021 - test loss: 18.5065 - train acc: 0.8331 - test acc: 0.4189 - 19m 43s\n",
      "batch: 1100/1563 - train loss: 2.8782 - test loss: 18.2765 - train acc: 0.8350 - test acc: 0.4253 - 19m 48s\n",
      "batch: 1200/1563 - train loss: 2.8999 - test loss: 18.9772 - train acc: 0.8296 - test acc: 0.4085 - 19m 53s\n",
      "batch: 1300/1563 - train loss: 3.2975 - test loss: 18.7346 - train acc: 0.8181 - test acc: 0.4143 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 1301/1563 - train loss: 3.3040 - test loss: 18.5968 - train acc: 0.8168 - test acc: 0.4168 - 20m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "testing - beta: 0.9 - part size: 8 - block upd: 64 - combination nº: 5\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 8 - nº part: 1176 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 73728 - partition: 8 - nº part: 9216 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 8192 - partition: 8 - nº part: 1024 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 294912 - partition: 8 - nº part: 36864 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 32768 - partition: 8 - nº part: 4096 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 1179648 - partition: 8 - nº part: 147456 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 131072 - partition: 8 - nº part: 16384 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 51200 - partition: 8 - nº part: 6400 - block updates: 64\n",
      "FisherPartitioner: param: 100 - partition: 8 - nº part: 13 - block updates: 13\n",
      "total partitions: 1403477 - effective block updates: 2557\n",
      "initializing inverses...\n",
      "partition 1/1403477\n",
      "partition 10000/1403477\n",
      "partition 20000/1403477\n",
      "partition 30000/1403477\n",
      "partition 40000/1403477\n",
      "partition 50000/1403477\n",
      "partition 60000/1403477\n",
      "partition 70000/1403477\n",
      "partition 80000/1403477\n",
      "partition 90000/1403477\n",
      "partition 100000/1403477\n",
      "partition 110000/1403477\n",
      "partition 120000/1403477\n",
      "partition 130000/1403477\n",
      "partition 140000/1403477\n",
      "partition 150000/1403477\n",
      "partition 160000/1403477\n",
      "partition 170000/1403477\n",
      "partition 180000/1403477\n",
      "partition 190000/1403477\n",
      "partition 200000/1403477\n",
      "partition 210000/1403477\n",
      "partition 220000/1403477\n",
      "partition 230000/1403477\n",
      "partition 240000/1403477\n",
      "partition 250000/1403477\n",
      "partition 260000/1403477\n",
      "partition 270000/1403477\n",
      "partition 280000/1403477\n",
      "partition 290000/1403477\n",
      "partition 300000/1403477\n",
      "partition 310000/1403477\n",
      "partition 320000/1403477\n",
      "partition 330000/1403477\n",
      "partition 340000/1403477\n",
      "partition 350000/1403477\n",
      "partition 360000/1403477\n",
      "partition 370000/1403477\n",
      "partition 380000/1403477\n",
      "partition 390000/1403477\n",
      "partition 400000/1403477\n",
      "partition 410000/1403477\n",
      "partition 420000/1403477\n",
      "partition 430000/1403477\n",
      "partition 440000/1403477\n",
      "partition 450000/1403477\n",
      "partition 460000/1403477\n",
      "partition 470000/1403477\n",
      "partition 480000/1403477\n",
      "partition 490000/1403477\n",
      "partition 500000/1403477\n",
      "partition 510000/1403477\n",
      "partition 520000/1403477\n",
      "partition 530000/1403477\n",
      "partition 540000/1403477\n",
      "partition 550000/1403477\n",
      "partition 560000/1403477\n",
      "partition 570000/1403477\n",
      "partition 580000/1403477\n",
      "partition 590000/1403477\n",
      "partition 600000/1403477\n",
      "partition 610000/1403477\n",
      "partition 620000/1403477\n",
      "partition 630000/1403477\n",
      "partition 640000/1403477\n",
      "partition 650000/1403477\n",
      "partition 660000/1403477\n",
      "partition 670000/1403477\n",
      "partition 680000/1403477\n",
      "partition 690000/1403477\n",
      "partition 700000/1403477\n",
      "partition 710000/1403477\n",
      "partition 720000/1403477\n",
      "partition 730000/1403477\n",
      "partition 740000/1403477\n",
      "partition 750000/1403477\n",
      "partition 760000/1403477\n",
      "partition 770000/1403477\n",
      "partition 780000/1403477\n",
      "partition 790000/1403477\n",
      "partition 800000/1403477\n",
      "partition 810000/1403477\n",
      "partition 820000/1403477\n",
      "partition 830000/1403477\n",
      "partition 840000/1403477\n",
      "partition 850000/1403477\n",
      "partition 860000/1403477\n",
      "partition 870000/1403477\n",
      "partition 880000/1403477\n",
      "partition 890000/1403477\n",
      "partition 900000/1403477\n",
      "partition 910000/1403477\n",
      "partition 920000/1403477\n",
      "partition 930000/1403477\n",
      "partition 940000/1403477\n",
      "partition 950000/1403477\n",
      "partition 960000/1403477\n",
      "partition 970000/1403477\n",
      "partition 980000/1403477\n",
      "partition 990000/1403477\n",
      "partition 1000000/1403477\n",
      "partition 1010000/1403477\n",
      "partition 1020000/1403477\n",
      "partition 1030000/1403477\n",
      "partition 1040000/1403477\n",
      "partition 1050000/1403477\n",
      "partition 1060000/1403477\n",
      "partition 1070000/1403477\n",
      "partition 1080000/1403477\n",
      "partition 1090000/1403477\n",
      "partition 1100000/1403477\n",
      "partition 1110000/1403477\n",
      "partition 1120000/1403477\n",
      "partition 1130000/1403477\n",
      "partition 1140000/1403477\n",
      "partition 1150000/1403477\n",
      "partition 1160000/1403477\n",
      "partition 1170000/1403477\n",
      "partition 1180000/1403477\n",
      "partition 1190000/1403477\n",
      "partition 1200000/1403477\n",
      "partition 1210000/1403477\n",
      "partition 1220000/1403477\n",
      "partition 1230000/1403477\n",
      "partition 1240000/1403477\n",
      "partition 1250000/1403477\n",
      "partition 1260000/1403477\n",
      "partition 1270000/1403477\n",
      "partition 1280000/1403477\n",
      "partition 1290000/1403477\n",
      "partition 1300000/1403477\n",
      "partition 1310000/1403477\n",
      "partition 1320000/1403477\n",
      "partition 1330000/1403477\n",
      "partition 1340000/1403477\n",
      "partition 1350000/1403477\n",
      "partition 1360000/1403477\n",
      "partition 1370000/1403477\n",
      "partition 1380000/1403477\n",
      "partition 1390000/1403477\n",
      "partition 1400000/1403477\n",
      "partition 1403477/1403477\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0651 - test loss: 24.6594 - train acc: 0.0411 - test acc: 0.0565 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.7912 - test loss: 23.2622 - train acc: 0.0671 - test acc: 0.0802 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.1204 - test loss: 23.3082 - train acc: 0.0752 - test acc: 0.0780 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.2513 - test loss: 21.8169 - train acc: 0.0890 - test acc: 0.1090 - 0m 16s\n",
      "batch: 500/1563 - train loss: 21.6179 - test loss: 21.1412 - train acc: 0.1209 - test acc: 0.1274 - 0m 21s\n",
      "batch: 600/1563 - train loss: 21.0455 - test loss: 21.0205 - train acc: 0.1271 - test acc: 0.1334 - 0m 26s\n",
      "batch: 700/1563 - train loss: 20.6714 - test loss: 21.2699 - train acc: 0.1422 - test acc: 0.1272 - 0m 30s\n",
      "batch: 800/1563 - train loss: 20.3835 - test loss: 19.7143 - train acc: 0.1516 - test acc: 0.1606 - 0m 35s\n",
      "batch: 900/1563 - train loss: 19.9908 - test loss: 19.9454 - train acc: 0.1653 - test acc: 0.1691 - 0m 40s\n",
      "batch: 1000/1563 - train loss: 20.0147 - test loss: 19.4035 - train acc: 0.1535 - test acc: 0.1794 - 0m 44s\n",
      "batch: 1100/1563 - train loss: 19.6081 - test loss: 19.1392 - train acc: 0.1607 - test acc: 0.1819 - 0m 49s\n",
      "batch: 1200/1563 - train loss: 19.1362 - test loss: 19.1816 - train acc: 0.1907 - test acc: 0.1943 - 0m 54s\n",
      "batch: 1300/1563 - train loss: 19.3190 - test loss: 18.9299 - train acc: 0.1806 - test acc: 0.2015 - 0m 59s\n",
      "batch: 1400/1563 - train loss: 18.8308 - test loss: 18.8609 - train acc: 0.1881 - test acc: 0.1918 - 1m 3s\n",
      "batch: 1500/1563 - train loss: 18.5793 - test loss: 17.9781 - train acc: 0.1998 - test acc: 0.2238 - 1m 8s\n",
      "batch: 1563/1563 - train loss: 18.3633 - test loss: 19.9339 - train acc: 0.2079 - test acc: 0.1760 - 1m 12s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.7690 - test loss: 18.7382 - train acc: 0.2272 - test acc: 0.2003 - 1m 17s\n",
      "batch: 200/1563 - train loss: 17.5054 - test loss: 18.4155 - train acc: 0.2407 - test acc: 0.2128 - 1m 22s\n",
      "batch: 300/1563 - train loss: 17.3133 - test loss: 17.7816 - train acc: 0.2341 - test acc: 0.2324 - 1m 26s\n",
      "batch: 400/1563 - train loss: 17.5781 - test loss: 18.2650 - train acc: 0.2341 - test acc: 0.2213 - 1m 31s\n",
      "batch: 500/1563 - train loss: 17.2678 - test loss: 17.6012 - train acc: 0.2569 - test acc: 0.2407 - 1m 36s\n",
      "batch: 600/1563 - train loss: 17.0631 - test loss: 17.1941 - train acc: 0.2535 - test acc: 0.2459 - 1m 40s\n",
      "batch: 700/1563 - train loss: 16.9288 - test loss: 17.4104 - train acc: 0.2475 - test acc: 0.2463 - 1m 45s\n",
      "batch: 800/1563 - train loss: 16.9513 - test loss: 16.7521 - train acc: 0.2482 - test acc: 0.2652 - 1m 50s\n",
      "batch: 900/1563 - train loss: 16.8786 - test loss: 16.7298 - train acc: 0.2556 - test acc: 0.2670 - 1m 55s\n",
      "batch: 1000/1563 - train loss: 16.5450 - test loss: 16.7132 - train acc: 0.2715 - test acc: 0.2656 - 2m 0s\n",
      "batch: 1100/1563 - train loss: 17.0064 - test loss: 17.7014 - train acc: 0.2637 - test acc: 0.2423 - 2m 5s\n",
      "batch: 1200/1563 - train loss: 16.3666 - test loss: 17.4556 - train acc: 0.2700 - test acc: 0.2467 - 2m 9s\n",
      "batch: 1300/1563 - train loss: 16.3627 - test loss: 17.5856 - train acc: 0.2800 - test acc: 0.2384 - 2m 14s\n",
      "batch: 1400/1563 - train loss: 16.7061 - test loss: 16.5430 - train acc: 0.2697 - test acc: 0.2680 - 2m 19s\n",
      "batch: 1500/1563 - train loss: 16.2605 - test loss: 16.6017 - train acc: 0.2896 - test acc: 0.2730 - 2m 23s\n",
      "batch: 1563/1563 - train loss: 16.0567 - test loss: 16.3634 - train acc: 0.2877 - test acc: 0.2805 - 2m 28s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.0164 - test loss: 16.3553 - train acc: 0.3265 - test acc: 0.2859 - 2m 32s\n",
      "batch: 200/1563 - train loss: 14.8038 - test loss: 16.0548 - train acc: 0.3188 - test acc: 0.2950 - 2m 37s\n",
      "batch: 300/1563 - train loss: 14.9748 - test loss: 16.5237 - train acc: 0.3169 - test acc: 0.2791 - 2m 42s\n",
      "batch: 400/1563 - train loss: 15.0493 - test loss: 15.8481 - train acc: 0.3234 - test acc: 0.3063 - 2m 46s\n",
      "batch: 500/1563 - train loss: 15.0336 - test loss: 17.3140 - train acc: 0.3262 - test acc: 0.2506 - 2m 51s\n",
      "batch: 600/1563 - train loss: 14.9601 - test loss: 15.8874 - train acc: 0.3172 - test acc: 0.2945 - 2m 56s\n",
      "batch: 700/1563 - train loss: 15.1167 - test loss: 15.5859 - train acc: 0.3281 - test acc: 0.3066 - 3m 1s\n",
      "batch: 800/1563 - train loss: 14.9341 - test loss: 15.6896 - train acc: 0.3325 - test acc: 0.3068 - 3m 5s\n",
      "batch: 900/1563 - train loss: 14.9073 - test loss: 15.9446 - train acc: 0.3160 - test acc: 0.2917 - 3m 10s\n",
      "batch: 1000/1563 - train loss: 14.9968 - test loss: 16.6176 - train acc: 0.3228 - test acc: 0.2827 - 3m 15s\n",
      "batch: 1100/1563 - train loss: 14.8916 - test loss: 15.4050 - train acc: 0.3297 - test acc: 0.3192 - 3m 19s\n",
      "batch: 1200/1563 - train loss: 14.6794 - test loss: 15.0560 - train acc: 0.3356 - test acc: 0.3304 - 3m 25s\n",
      "batch: 1300/1563 - train loss: 14.6503 - test loss: 15.2428 - train acc: 0.3521 - test acc: 0.3250 - 3m 29s\n",
      "batch: 1400/1563 - train loss: 14.5006 - test loss: 14.7504 - train acc: 0.3465 - test acc: 0.3426 - 3m 34s\n",
      "batch: 1500/1563 - train loss: 14.4393 - test loss: 15.0371 - train acc: 0.3437 - test acc: 0.3273 - 3m 39s\n",
      "batch: 1563/1563 - train loss: 14.5866 - test loss: 15.9371 - train acc: 0.3440 - test acc: 0.3039 - 3m 43s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.0764 - test loss: 16.3356 - train acc: 0.3947 - test acc: 0.2944 - 3m 47s\n",
      "batch: 200/1563 - train loss: 12.9708 - test loss: 14.6054 - train acc: 0.3916 - test acc: 0.3458 - 3m 52s\n",
      "batch: 300/1563 - train loss: 13.2202 - test loss: 15.2202 - train acc: 0.3869 - test acc: 0.3353 - 3m 57s\n",
      "batch: 400/1563 - train loss: 13.2828 - test loss: 14.9850 - train acc: 0.3947 - test acc: 0.3350 - 4m 2s\n",
      "batch: 500/1563 - train loss: 13.4954 - test loss: 14.9210 - train acc: 0.3759 - test acc: 0.3368 - 4m 7s\n",
      "batch: 600/1563 - train loss: 13.2773 - test loss: 14.4661 - train acc: 0.3803 - test acc: 0.3494 - 4m 11s\n",
      "batch: 700/1563 - train loss: 13.3554 - test loss: 14.3428 - train acc: 0.3872 - test acc: 0.3545 - 4m 16s\n",
      "batch: 800/1563 - train loss: 13.4938 - test loss: 14.7454 - train acc: 0.3765 - test acc: 0.3410 - 4m 21s\n",
      "batch: 900/1563 - train loss: 13.4989 - test loss: 15.7210 - train acc: 0.3797 - test acc: 0.3142 - 4m 26s\n",
      "batch: 1000/1563 - train loss: 13.1400 - test loss: 14.8376 - train acc: 0.4028 - test acc: 0.3369 - 4m 31s\n",
      "batch: 1100/1563 - train loss: 13.2853 - test loss: 15.8905 - train acc: 0.3747 - test acc: 0.3089 - 4m 35s\n",
      "batch: 1200/1563 - train loss: 13.2133 - test loss: 14.3470 - train acc: 0.3894 - test acc: 0.3513 - 4m 40s\n",
      "batch: 1300/1563 - train loss: 13.1344 - test loss: 13.8639 - train acc: 0.4069 - test acc: 0.3739 - 4m 45s\n",
      "batch: 1400/1563 - train loss: 13.3042 - test loss: 14.9675 - train acc: 0.3862 - test acc: 0.3305 - 4m 50s\n",
      "batch: 1500/1563 - train loss: 12.9832 - test loss: 15.5948 - train acc: 0.3922 - test acc: 0.3182 - 4m 54s\n",
      "batch: 1563/1563 - train loss: 13.2985 - test loss: 14.3174 - train acc: 0.3934 - test acc: 0.3576 - 4m 58s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3373 - test loss: 15.8722 - train acc: 0.4565 - test acc: 0.3314 - 5m 3s\n",
      "batch: 200/1563 - train loss: 11.6802 - test loss: 14.6287 - train acc: 0.4537 - test acc: 0.3517 - 5m 8s\n",
      "batch: 300/1563 - train loss: 11.8441 - test loss: 13.9887 - train acc: 0.4391 - test acc: 0.3725 - 5m 13s\n",
      "batch: 400/1563 - train loss: 11.8234 - test loss: 14.8153 - train acc: 0.4478 - test acc: 0.3509 - 5m 17s\n",
      "batch: 500/1563 - train loss: 11.9290 - test loss: 15.5297 - train acc: 0.4488 - test acc: 0.3306 - 5m 22s\n",
      "batch: 600/1563 - train loss: 11.7408 - test loss: 14.4286 - train acc: 0.4534 - test acc: 0.3625 - 5m 27s\n",
      "batch: 700/1563 - train loss: 11.9887 - test loss: 13.9024 - train acc: 0.4363 - test acc: 0.3701 - 5m 32s\n",
      "batch: 800/1563 - train loss: 12.0234 - test loss: 14.0283 - train acc: 0.4301 - test acc: 0.3754 - 5m 36s\n",
      "batch: 900/1563 - train loss: 12.0765 - test loss: 13.7351 - train acc: 0.4388 - test acc: 0.3901 - 5m 41s\n",
      "batch: 1000/1563 - train loss: 11.8467 - test loss: 14.1043 - train acc: 0.4403 - test acc: 0.3696 - 5m 46s\n",
      "batch: 1100/1563 - train loss: 12.1680 - test loss: 13.6076 - train acc: 0.4297 - test acc: 0.3868 - 5m 50s\n",
      "batch: 1200/1563 - train loss: 12.0080 - test loss: 14.4571 - train acc: 0.4337 - test acc: 0.3568 - 5m 55s\n",
      "batch: 1300/1563 - train loss: 11.8374 - test loss: 13.7101 - train acc: 0.4407 - test acc: 0.3850 - 6m 0s\n",
      "batch: 1400/1563 - train loss: 12.2025 - test loss: 13.4950 - train acc: 0.4341 - test acc: 0.3928 - 6m 5s\n",
      "batch: 1500/1563 - train loss: 12.0458 - test loss: 13.7231 - train acc: 0.4413 - test acc: 0.3810 - 6m 10s\n",
      "batch: 1563/1563 - train loss: 12.3298 - test loss: 13.3758 - train acc: 0.4119 - test acc: 0.3960 - 6m 13s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.1909 - test loss: 13.4410 - train acc: 0.5035 - test acc: 0.3966 - 6m 18s\n",
      "batch: 200/1563 - train loss: 10.0560 - test loss: 13.2660 - train acc: 0.5212 - test acc: 0.4072 - 6m 23s\n",
      "batch: 300/1563 - train loss: 10.5130 - test loss: 13.7710 - train acc: 0.4969 - test acc: 0.3862 - 6m 28s\n",
      "batch: 400/1563 - train loss: 10.1975 - test loss: 14.6725 - train acc: 0.5044 - test acc: 0.3690 - 6m 33s\n",
      "batch: 500/1563 - train loss: 10.8095 - test loss: 14.2311 - train acc: 0.4816 - test acc: 0.3816 - 6m 37s\n",
      "batch: 600/1563 - train loss: 10.7240 - test loss: 13.3716 - train acc: 0.4863 - test acc: 0.4020 - 6m 42s\n",
      "batch: 700/1563 - train loss: 11.1068 - test loss: 13.4081 - train acc: 0.4647 - test acc: 0.3991 - 6m 47s\n",
      "batch: 800/1563 - train loss: 10.7809 - test loss: 13.4807 - train acc: 0.4857 - test acc: 0.3955 - 6m 52s\n",
      "batch: 900/1563 - train loss: 10.6702 - test loss: 15.1594 - train acc: 0.4829 - test acc: 0.3596 - 6m 56s\n",
      "batch: 1000/1563 - train loss: 11.1835 - test loss: 13.7727 - train acc: 0.4666 - test acc: 0.3884 - 7m 1s\n",
      "batch: 1100/1563 - train loss: 10.8425 - test loss: 13.5960 - train acc: 0.4813 - test acc: 0.3962 - 7m 6s\n",
      "batch: 1200/1563 - train loss: 10.8151 - test loss: 13.2345 - train acc: 0.4763 - test acc: 0.4012 - 7m 11s\n",
      "batch: 1300/1563 - train loss: 10.9503 - test loss: 16.1191 - train acc: 0.4772 - test acc: 0.3348 - 7m 16s\n",
      "batch: 1400/1563 - train loss: 10.9795 - test loss: 13.6156 - train acc: 0.4853 - test acc: 0.3963 - 7m 20s\n",
      "batch: 1500/1563 - train loss: 11.1579 - test loss: 13.3332 - train acc: 0.4634 - test acc: 0.4044 - 7m 25s\n",
      "batch: 1563/1563 - train loss: 11.2062 - test loss: 13.3151 - train acc: 0.4591 - test acc: 0.3966 - 7m 29s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.8326 - test loss: 13.4230 - train acc: 0.5675 - test acc: 0.4057 - 7m 34s\n",
      "batch: 200/1563 - train loss: 8.9394 - test loss: 14.6557 - train acc: 0.5572 - test acc: 0.3766 - 7m 39s\n",
      "batch: 300/1563 - train loss: 9.2019 - test loss: 13.8165 - train acc: 0.5509 - test acc: 0.3949 - 7m 44s\n",
      "batch: 400/1563 - train loss: 9.2226 - test loss: 13.9365 - train acc: 0.5491 - test acc: 0.4010 - 7m 48s\n",
      "batch: 500/1563 - train loss: 9.3973 - test loss: 13.5687 - train acc: 0.5444 - test acc: 0.4102 - 7m 53s\n",
      "batch: 600/1563 - train loss: 9.3967 - test loss: 14.0193 - train acc: 0.5393 - test acc: 0.3933 - 7m 58s\n",
      "batch: 700/1563 - train loss: 9.9447 - test loss: 13.5354 - train acc: 0.5175 - test acc: 0.4053 - 8m 2s\n",
      "batch: 800/1563 - train loss: 9.8565 - test loss: 14.2533 - train acc: 0.5147 - test acc: 0.3870 - 8m 8s\n",
      "batch: 900/1563 - train loss: 10.0406 - test loss: 13.2015 - train acc: 0.5147 - test acc: 0.4155 - 8m 12s\n",
      "batch: 1000/1563 - train loss: 9.7270 - test loss: 14.0802 - train acc: 0.5243 - test acc: 0.3916 - 8m 17s\n",
      "batch: 1100/1563 - train loss: 10.2810 - test loss: 13.3118 - train acc: 0.5040 - test acc: 0.4107 - 8m 22s\n",
      "batch: 1200/1563 - train loss: 9.9692 - test loss: 13.6185 - train acc: 0.5175 - test acc: 0.4021 - 8m 26s\n",
      "batch: 1300/1563 - train loss: 10.1029 - test loss: 13.6786 - train acc: 0.5034 - test acc: 0.4033 - 8m 31s\n",
      "batch: 1400/1563 - train loss: 10.0807 - test loss: 14.0302 - train acc: 0.5044 - test acc: 0.3874 - 8m 36s\n",
      "batch: 1500/1563 - train loss: 10.0829 - test loss: 14.2983 - train acc: 0.5141 - test acc: 0.3852 - 8m 41s\n",
      "batch: 1563/1563 - train loss: 10.1233 - test loss: 13.6034 - train acc: 0.5031 - test acc: 0.4014 - 8m 45s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.9170 - test loss: 13.9498 - train acc: 0.6090 - test acc: 0.4027 - 8m 50s\n",
      "batch: 200/1563 - train loss: 7.3986 - test loss: 14.4820 - train acc: 0.6284 - test acc: 0.3963 - 8m 54s\n",
      "batch: 300/1563 - train loss: 7.9066 - test loss: 15.3464 - train acc: 0.6013 - test acc: 0.3629 - 8m 59s\n",
      "batch: 400/1563 - train loss: 8.3903 - test loss: 14.0506 - train acc: 0.5834 - test acc: 0.4009 - 9m 4s\n",
      "batch: 500/1563 - train loss: 8.3557 - test loss: 15.9429 - train acc: 0.5788 - test acc: 0.3566 - 9m 9s\n",
      "batch: 600/1563 - train loss: 8.7211 - test loss: 13.9220 - train acc: 0.5628 - test acc: 0.3999 - 9m 14s\n",
      "batch: 700/1563 - train loss: 8.7347 - test loss: 14.5167 - train acc: 0.5647 - test acc: 0.3854 - 9m 19s\n",
      "batch: 800/1563 - train loss: 8.8653 - test loss: 14.0011 - train acc: 0.5568 - test acc: 0.4046 - 9m 23s\n",
      "batch: 900/1563 - train loss: 8.9270 - test loss: 14.0386 - train acc: 0.5575 - test acc: 0.3993 - 9m 28s\n",
      "batch: 1000/1563 - train loss: 9.1545 - test loss: 13.7689 - train acc: 0.5563 - test acc: 0.4060 - 9m 33s\n",
      "batch: 1100/1563 - train loss: 9.0413 - test loss: 14.0126 - train acc: 0.5499 - test acc: 0.4029 - 9m 37s\n",
      "batch: 1200/1563 - train loss: 9.1737 - test loss: 13.6264 - train acc: 0.5553 - test acc: 0.4095 - 9m 42s\n",
      "batch: 1300/1563 - train loss: 9.1462 - test loss: 13.2629 - train acc: 0.5566 - test acc: 0.4176 - 9m 47s\n",
      "batch: 1400/1563 - train loss: 9.0294 - test loss: 13.7377 - train acc: 0.5537 - test acc: 0.4015 - 9m 52s\n",
      "batch: 1500/1563 - train loss: 8.8707 - test loss: 13.7140 - train acc: 0.5641 - test acc: 0.4084 - 9m 57s\n",
      "batch: 1563/1563 - train loss: 8.9209 - test loss: 13.0533 - train acc: 0.5556 - test acc: 0.4246 - 10m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.5960 - test loss: 13.8134 - train acc: 0.6606 - test acc: 0.4134 - 10m 5s\n",
      "batch: 200/1563 - train loss: 6.7628 - test loss: 13.8771 - train acc: 0.6593 - test acc: 0.4132 - 10m 10s\n",
      "batch: 300/1563 - train loss: 7.0911 - test loss: 14.0378 - train acc: 0.6406 - test acc: 0.4124 - 10m 15s\n",
      "batch: 400/1563 - train loss: 7.0947 - test loss: 14.2945 - train acc: 0.6341 - test acc: 0.4122 - 10m 20s\n",
      "batch: 500/1563 - train loss: 7.2276 - test loss: 14.7437 - train acc: 0.6306 - test acc: 0.3923 - 10m 25s\n",
      "batch: 600/1563 - train loss: 7.5863 - test loss: 14.5553 - train acc: 0.6134 - test acc: 0.3889 - 10m 30s\n",
      "batch: 700/1563 - train loss: 7.7487 - test loss: 14.2927 - train acc: 0.6088 - test acc: 0.4056 - 10m 34s\n",
      "batch: 800/1563 - train loss: 8.1018 - test loss: 14.4866 - train acc: 0.5806 - test acc: 0.3932 - 10m 39s\n",
      "batch: 900/1563 - train loss: 7.8284 - test loss: 14.6257 - train acc: 0.6053 - test acc: 0.3967 - 10m 44s\n",
      "batch: 1000/1563 - train loss: 8.2202 - test loss: 14.2824 - train acc: 0.5781 - test acc: 0.4039 - 10m 49s\n",
      "batch: 1100/1563 - train loss: 8.1202 - test loss: 13.8810 - train acc: 0.5931 - test acc: 0.4078 - 10m 54s\n",
      "batch: 1200/1563 - train loss: 8.1635 - test loss: 15.0796 - train acc: 0.5938 - test acc: 0.3879 - 10m 58s\n",
      "batch: 1300/1563 - train loss: 8.4116 - test loss: 14.5089 - train acc: 0.5888 - test acc: 0.3988 - 11m 3s\n",
      "batch: 1400/1563 - train loss: 8.2130 - test loss: 14.7422 - train acc: 0.5887 - test acc: 0.4000 - 11m 8s\n",
      "batch: 1500/1563 - train loss: 8.4338 - test loss: 13.9752 - train acc: 0.5765 - test acc: 0.4131 - 11m 12s\n",
      "batch: 1563/1563 - train loss: 8.4747 - test loss: 13.9541 - train acc: 0.5841 - test acc: 0.4128 - 11m 17s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.6998 - test loss: 14.2709 - train acc: 0.7082 - test acc: 0.4061 - 11m 22s\n",
      "batch: 200/1563 - train loss: 5.6651 - test loss: 14.0494 - train acc: 0.7084 - test acc: 0.4244 - 11m 26s\n",
      "batch: 300/1563 - train loss: 5.7287 - test loss: 14.2897 - train acc: 0.7051 - test acc: 0.4204 - 11m 31s\n",
      "batch: 400/1563 - train loss: 6.0686 - test loss: 14.6328 - train acc: 0.6863 - test acc: 0.4121 - 11m 36s\n",
      "batch: 500/1563 - train loss: 6.3957 - test loss: 14.5693 - train acc: 0.6740 - test acc: 0.4127 - 11m 41s\n",
      "batch: 600/1563 - train loss: 6.3618 - test loss: 14.6093 - train acc: 0.6709 - test acc: 0.4155 - 11m 45s\n",
      "batch: 700/1563 - train loss: 6.8976 - test loss: 14.3813 - train acc: 0.6375 - test acc: 0.4172 - 11m 51s\n",
      "batch: 800/1563 - train loss: 6.7724 - test loss: 14.3734 - train acc: 0.6450 - test acc: 0.4162 - 11m 55s\n",
      "batch: 900/1563 - train loss: 7.0934 - test loss: 14.6096 - train acc: 0.6419 - test acc: 0.4104 - 12m 0s\n",
      "batch: 1000/1563 - train loss: 7.0564 - test loss: 14.4539 - train acc: 0.6310 - test acc: 0.4130 - 12m 5s\n",
      "batch: 1100/1563 - train loss: 7.1039 - test loss: 14.2143 - train acc: 0.6419 - test acc: 0.4150 - 12m 10s\n",
      "batch: 1200/1563 - train loss: 7.4517 - test loss: 15.0926 - train acc: 0.6188 - test acc: 0.4003 - 12m 14s\n",
      "batch: 1300/1563 - train loss: 7.5019 - test loss: 14.4790 - train acc: 0.6125 - test acc: 0.4123 - 12m 20s\n",
      "batch: 1400/1563 - train loss: 7.4880 - test loss: 14.9477 - train acc: 0.6187 - test acc: 0.4017 - 12m 24s\n",
      "batch: 1500/1563 - train loss: 7.3927 - test loss: 14.2978 - train acc: 0.6243 - test acc: 0.4125 - 12m 29s\n",
      "batch: 1563/1563 - train loss: 7.6856 - test loss: 14.3904 - train acc: 0.6078 - test acc: 0.4079 - 12m 33s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.9961 - test loss: 14.0948 - train acc: 0.7400 - test acc: 0.4200 - 12m 38s\n",
      "batch: 200/1563 - train loss: 5.0944 - test loss: 14.6606 - train acc: 0.7375 - test acc: 0.4203 - 12m 43s\n",
      "batch: 300/1563 - train loss: 5.2190 - test loss: 15.7393 - train acc: 0.7266 - test acc: 0.3908 - 12m 47s\n",
      "batch: 400/1563 - train loss: 5.1525 - test loss: 14.9494 - train acc: 0.7288 - test acc: 0.4199 - 12m 52s\n",
      "batch: 500/1563 - train loss: 5.6281 - test loss: 15.1936 - train acc: 0.7032 - test acc: 0.4038 - 12m 57s\n",
      "batch: 600/1563 - train loss: 5.9752 - test loss: 15.3307 - train acc: 0.6900 - test acc: 0.4103 - 13m 2s\n",
      "batch: 700/1563 - train loss: 5.7925 - test loss: 14.7671 - train acc: 0.6988 - test acc: 0.4197 - 13m 7s\n",
      "batch: 800/1563 - train loss: 5.9584 - test loss: 15.6168 - train acc: 0.6894 - test acc: 0.4081 - 13m 11s\n",
      "batch: 900/1563 - train loss: 6.0662 - test loss: 14.9840 - train acc: 0.6772 - test acc: 0.4155 - 13m 16s\n",
      "batch: 1000/1563 - train loss: 6.0379 - test loss: 15.5329 - train acc: 0.6813 - test acc: 0.4034 - 13m 21s\n",
      "batch: 1100/1563 - train loss: 6.3915 - test loss: 15.1630 - train acc: 0.6641 - test acc: 0.4078 - 13m 26s\n",
      "batch: 1200/1563 - train loss: 6.5787 - test loss: 15.7356 - train acc: 0.6479 - test acc: 0.3934 - 13m 31s\n",
      "batch: 1300/1563 - train loss: 6.2235 - test loss: 15.3761 - train acc: 0.6678 - test acc: 0.4023 - 13m 35s\n",
      "batch: 1400/1563 - train loss: 6.7056 - test loss: 14.5882 - train acc: 0.6535 - test acc: 0.4137 - 13m 40s\n",
      "batch: 1500/1563 - train loss: 6.5806 - test loss: 15.1727 - train acc: 0.6582 - test acc: 0.4049 - 13m 45s\n",
      "batch: 1563/1563 - train loss: 6.6705 - test loss: 15.3511 - train acc: 0.6551 - test acc: 0.4093 - 13m 49s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.1119 - test loss: 16.0990 - train acc: 0.7856 - test acc: 0.4008 - 13m 54s\n",
      "batch: 200/1563 - train loss: 4.2902 - test loss: 15.6342 - train acc: 0.7738 - test acc: 0.4120 - 13m 59s\n",
      "batch: 300/1563 - train loss: 4.2753 - test loss: 15.6101 - train acc: 0.7669 - test acc: 0.4147 - 14m 4s\n",
      "batch: 400/1563 - train loss: 4.3568 - test loss: 16.4205 - train acc: 0.7678 - test acc: 0.4027 - 14m 8s\n",
      "batch: 500/1563 - train loss: 5.0434 - test loss: 16.4468 - train acc: 0.7309 - test acc: 0.3942 - 14m 13s\n",
      "batch: 600/1563 - train loss: 5.1334 - test loss: 16.0846 - train acc: 0.7253 - test acc: 0.4002 - 14m 18s\n",
      "batch: 700/1563 - train loss: 5.0113 - test loss: 15.5016 - train acc: 0.7297 - test acc: 0.4131 - 14m 23s\n",
      "batch: 800/1563 - train loss: 5.1558 - test loss: 15.5578 - train acc: 0.7212 - test acc: 0.4171 - 14m 28s\n",
      "batch: 900/1563 - train loss: 5.1434 - test loss: 15.8418 - train acc: 0.7194 - test acc: 0.4101 - 14m 32s\n",
      "batch: 1000/1563 - train loss: 5.4539 - test loss: 15.4564 - train acc: 0.7135 - test acc: 0.4185 - 14m 37s\n",
      "batch: 1100/1563 - train loss: 5.6207 - test loss: 15.4089 - train acc: 0.7006 - test acc: 0.4128 - 14m 42s\n",
      "batch: 1200/1563 - train loss: 5.5112 - test loss: 15.5424 - train acc: 0.7137 - test acc: 0.4109 - 14m 47s\n",
      "batch: 1300/1563 - train loss: 5.9059 - test loss: 16.2901 - train acc: 0.6822 - test acc: 0.3992 - 14m 52s\n",
      "batch: 1400/1563 - train loss: 6.0965 - test loss: 15.2334 - train acc: 0.6744 - test acc: 0.4153 - 14m 57s\n",
      "batch: 1500/1563 - train loss: 5.7750 - test loss: 15.2369 - train acc: 0.6898 - test acc: 0.4179 - 15m 1s\n",
      "batch: 1563/1563 - train loss: 5.9727 - test loss: 15.8295 - train acc: 0.6829 - test acc: 0.4015 - 15m 5s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.5533 - test loss: 15.1320 - train acc: 0.8094 - test acc: 0.4270 - 15m 10s\n",
      "batch: 200/1563 - train loss: 3.4496 - test loss: 15.5631 - train acc: 0.8205 - test acc: 0.4248 - 15m 15s\n",
      "batch: 300/1563 - train loss: 3.6057 - test loss: 15.8626 - train acc: 0.8084 - test acc: 0.4198 - 15m 20s\n",
      "batch: 400/1563 - train loss: 3.8467 - test loss: 16.0941 - train acc: 0.7900 - test acc: 0.4236 - 15m 24s\n",
      "batch: 500/1563 - train loss: 3.8929 - test loss: 16.4252 - train acc: 0.7865 - test acc: 0.4197 - 15m 29s\n",
      "batch: 600/1563 - train loss: 4.2052 - test loss: 16.2752 - train acc: 0.7678 - test acc: 0.4101 - 15m 34s\n",
      "batch: 700/1563 - train loss: 4.2130 - test loss: 16.3399 - train acc: 0.7612 - test acc: 0.4217 - 15m 39s\n",
      "batch: 800/1563 - train loss: 4.4573 - test loss: 16.2686 - train acc: 0.7540 - test acc: 0.4168 - 15m 44s\n",
      "batch: 900/1563 - train loss: 4.5194 - test loss: 16.8189 - train acc: 0.7562 - test acc: 0.4080 - 15m 49s\n",
      "batch: 1000/1563 - train loss: 5.0631 - test loss: 16.3725 - train acc: 0.7281 - test acc: 0.4126 - 15m 53s\n",
      "batch: 1100/1563 - train loss: 4.7380 - test loss: 16.3570 - train acc: 0.7413 - test acc: 0.4158 - 15m 58s\n",
      "batch: 1200/1563 - train loss: 5.1384 - test loss: 16.1466 - train acc: 0.7169 - test acc: 0.4198 - 16m 3s\n",
      "batch: 1300/1563 - train loss: 5.1770 - test loss: 15.7822 - train acc: 0.7241 - test acc: 0.4217 - 16m 8s\n",
      "batch: 1400/1563 - train loss: 4.9546 - test loss: 15.8421 - train acc: 0.7328 - test acc: 0.4182 - 16m 12s\n",
      "batch: 1500/1563 - train loss: 5.3303 - test loss: 16.1375 - train acc: 0.7138 - test acc: 0.4079 - 16m 17s\n",
      "batch: 1563/1563 - train loss: 5.2444 - test loss: 16.0280 - train acc: 0.7178 - test acc: 0.4173 - 16m 21s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8780 - test loss: 15.8200 - train acc: 0.8456 - test acc: 0.4251 - 16m 26s\n",
      "batch: 200/1563 - train loss: 2.9492 - test loss: 16.2569 - train acc: 0.8418 - test acc: 0.4182 - 16m 31s\n",
      "batch: 300/1563 - train loss: 3.1226 - test loss: 16.6632 - train acc: 0.8274 - test acc: 0.4158 - 16m 36s\n",
      "batch: 400/1563 - train loss: 2.8589 - test loss: 16.7341 - train acc: 0.8497 - test acc: 0.4170 - 16m 40s\n",
      "batch: 500/1563 - train loss: 3.2356 - test loss: 17.4661 - train acc: 0.8275 - test acc: 0.4105 - 16m 45s\n",
      "batch: 600/1563 - train loss: 3.3972 - test loss: 16.5833 - train acc: 0.8181 - test acc: 0.4250 - 16m 50s\n",
      "batch: 700/1563 - train loss: 3.6122 - test loss: 16.8167 - train acc: 0.8075 - test acc: 0.4210 - 16m 55s\n",
      "batch: 800/1563 - train loss: 3.7845 - test loss: 19.1749 - train acc: 0.7953 - test acc: 0.3770 - 16m 59s\n",
      "batch: 900/1563 - train loss: 3.8836 - test loss: 16.8244 - train acc: 0.7877 - test acc: 0.4183 - 17m 4s\n",
      "batch: 1000/1563 - train loss: 3.9518 - test loss: 17.3736 - train acc: 0.7853 - test acc: 0.4096 - 17m 9s\n",
      "batch: 1100/1563 - train loss: 4.0913 - test loss: 17.8928 - train acc: 0.7756 - test acc: 0.3933 - 17m 14s\n",
      "batch: 1200/1563 - train loss: 4.4217 - test loss: 17.4370 - train acc: 0.7540 - test acc: 0.4026 - 17m 19s\n",
      "batch: 1300/1563 - train loss: 4.4201 - test loss: 19.9953 - train acc: 0.7534 - test acc: 0.3601 - 17m 23s\n",
      "batch: 1400/1563 - train loss: 4.5734 - test loss: 18.1850 - train acc: 0.7474 - test acc: 0.3956 - 17m 28s\n",
      "batch: 1500/1563 - train loss: 4.4126 - test loss: 16.8665 - train acc: 0.7612 - test acc: 0.4234 - 17m 34s\n",
      "batch: 1563/1563 - train loss: 4.5965 - test loss: 17.1701 - train acc: 0.7590 - test acc: 0.4064 - 17m 37s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.7284 - test loss: 16.6842 - train acc: 0.8471 - test acc: 0.4217 - 17m 42s\n",
      "batch: 200/1563 - train loss: 2.5932 - test loss: 17.0813 - train acc: 0.8566 - test acc: 0.4182 - 17m 47s\n",
      "batch: 300/1563 - train loss: 2.6852 - test loss: 16.7946 - train acc: 0.8475 - test acc: 0.4237 - 17m 52s\n",
      "batch: 400/1563 - train loss: 2.8061 - test loss: 17.1608 - train acc: 0.8372 - test acc: 0.4216 - 17m 57s\n",
      "batch: 500/1563 - train loss: 3.0599 - test loss: 17.2747 - train acc: 0.8203 - test acc: 0.4236 - 18m 1s\n",
      "batch: 600/1563 - train loss: 2.9746 - test loss: 17.1478 - train acc: 0.8346 - test acc: 0.4179 - 18m 6s\n",
      "batch: 700/1563 - train loss: 3.1642 - test loss: 17.8189 - train acc: 0.8259 - test acc: 0.4175 - 18m 11s\n",
      "batch: 800/1563 - train loss: 3.1502 - test loss: 17.7705 - train acc: 0.8303 - test acc: 0.4184 - 18m 16s\n",
      "batch: 900/1563 - train loss: 3.2958 - test loss: 17.3642 - train acc: 0.8271 - test acc: 0.4214 - 18m 21s\n",
      "batch: 1000/1563 - train loss: 3.3492 - test loss: 18.3070 - train acc: 0.8072 - test acc: 0.4092 - 18m 26s\n",
      "batch: 1100/1563 - train loss: 3.4897 - test loss: 18.4261 - train acc: 0.8075 - test acc: 0.4071 - 18m 30s\n",
      "batch: 1200/1563 - train loss: 3.7282 - test loss: 17.5622 - train acc: 0.7931 - test acc: 0.4163 - 18m 35s\n",
      "batch: 1300/1563 - train loss: 3.5467 - test loss: 18.7151 - train acc: 0.8015 - test acc: 0.3932 - 18m 40s\n",
      "batch: 1400/1563 - train loss: 4.1386 - test loss: 17.7418 - train acc: 0.7738 - test acc: 0.4144 - 18m 45s\n",
      "batch: 1500/1563 - train loss: 3.8423 - test loss: 18.0870 - train acc: 0.7918 - test acc: 0.4035 - 18m 50s\n",
      "batch: 1563/1563 - train loss: 3.7825 - test loss: 17.7388 - train acc: 0.7984 - test acc: 0.4069 - 18m 54s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.5976 - test loss: 17.1894 - train acc: 0.8531 - test acc: 0.4227 - 18m 59s\n",
      "batch: 200/1563 - train loss: 2.2313 - test loss: 17.3876 - train acc: 0.8791 - test acc: 0.4282 - 19m 3s\n",
      "batch: 300/1563 - train loss: 2.2076 - test loss: 17.7900 - train acc: 0.8766 - test acc: 0.4234 - 19m 9s\n",
      "batch: 400/1563 - train loss: 2.3394 - test loss: 17.6972 - train acc: 0.8672 - test acc: 0.4289 - 19m 13s\n",
      "batch: 500/1563 - train loss: 2.2972 - test loss: 18.3920 - train acc: 0.8690 - test acc: 0.4157 - 19m 18s\n",
      "batch: 600/1563 - train loss: 2.5512 - test loss: 17.7825 - train acc: 0.8516 - test acc: 0.4233 - 19m 23s\n",
      "batch: 700/1563 - train loss: 2.6551 - test loss: 18.4682 - train acc: 0.8481 - test acc: 0.4115 - 19m 28s\n",
      "batch: 800/1563 - train loss: 2.4798 - test loss: 18.4580 - train acc: 0.8649 - test acc: 0.4213 - 19m 32s\n",
      "batch: 900/1563 - train loss: 3.1958 - test loss: 18.2757 - train acc: 0.8162 - test acc: 0.4234 - 19m 37s\n",
      "batch: 1000/1563 - train loss: 3.0690 - test loss: 18.1188 - train acc: 0.8293 - test acc: 0.4234 - 19m 42s\n",
      "batch: 1100/1563 - train loss: 3.1966 - test loss: 17.8935 - train acc: 0.8231 - test acc: 0.4254 - 19m 47s\n",
      "batch: 1200/1563 - train loss: 2.9921 - test loss: 17.8917 - train acc: 0.8312 - test acc: 0.4166 - 19m 52s\n",
      "batch: 1300/1563 - train loss: 3.3698 - test loss: 18.4289 - train acc: 0.8012 - test acc: 0.4114 - 19m 56s\n",
      "time is up! finishing training\n",
      "batch: 1362/1563 - train loss: 3.4851 - test loss: 18.5057 - train acc: 0.7978 - test acc: 0.4164 - 20m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "testing - beta: 0.9 - part size: 8 - block upd: 64 - combination nº: 6\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 8 - nº part: 1176 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 73728 - partition: 8 - nº part: 9216 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 8192 - partition: 8 - nº part: 1024 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 294912 - partition: 8 - nº part: 36864 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 32768 - partition: 8 - nº part: 4096 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 1179648 - partition: 8 - nº part: 147456 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 131072 - partition: 8 - nº part: 16384 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 51200 - partition: 8 - nº part: 6400 - block updates: 64\n",
      "FisherPartitioner: param: 100 - partition: 8 - nº part: 13 - block updates: 13\n",
      "total partitions: 1403477 - effective block updates: 2557\n",
      "initializing inverses...\n",
      "partition 1/1403477\n",
      "partition 10000/1403477\n",
      "partition 20000/1403477\n",
      "partition 30000/1403477\n",
      "partition 40000/1403477\n",
      "partition 50000/1403477\n",
      "partition 60000/1403477\n",
      "partition 70000/1403477\n",
      "partition 80000/1403477\n",
      "partition 90000/1403477\n",
      "partition 100000/1403477\n",
      "partition 110000/1403477\n",
      "partition 120000/1403477\n",
      "partition 130000/1403477\n",
      "partition 140000/1403477\n",
      "partition 150000/1403477\n",
      "partition 160000/1403477\n",
      "partition 170000/1403477\n",
      "partition 180000/1403477\n",
      "partition 190000/1403477\n",
      "partition 200000/1403477\n",
      "partition 210000/1403477\n",
      "partition 220000/1403477\n",
      "partition 230000/1403477\n",
      "partition 240000/1403477\n",
      "partition 250000/1403477\n",
      "partition 260000/1403477\n",
      "partition 270000/1403477\n",
      "partition 280000/1403477\n",
      "partition 290000/1403477\n",
      "partition 300000/1403477\n",
      "partition 310000/1403477\n",
      "partition 320000/1403477\n",
      "partition 330000/1403477\n",
      "partition 340000/1403477\n",
      "partition 350000/1403477\n",
      "partition 360000/1403477\n",
      "partition 370000/1403477\n",
      "partition 380000/1403477\n",
      "partition 390000/1403477\n",
      "partition 400000/1403477\n",
      "partition 410000/1403477\n",
      "partition 420000/1403477\n",
      "partition 430000/1403477\n",
      "partition 440000/1403477\n",
      "partition 450000/1403477\n",
      "partition 460000/1403477\n",
      "partition 470000/1403477\n",
      "partition 480000/1403477\n",
      "partition 490000/1403477\n",
      "partition 500000/1403477\n",
      "partition 510000/1403477\n",
      "partition 520000/1403477\n",
      "partition 530000/1403477\n",
      "partition 540000/1403477\n",
      "partition 550000/1403477\n",
      "partition 560000/1403477\n",
      "partition 570000/1403477\n",
      "partition 580000/1403477\n",
      "partition 590000/1403477\n",
      "partition 600000/1403477\n",
      "partition 610000/1403477\n",
      "partition 620000/1403477\n",
      "partition 630000/1403477\n",
      "partition 640000/1403477\n",
      "partition 650000/1403477\n",
      "partition 660000/1403477\n",
      "partition 670000/1403477\n",
      "partition 680000/1403477\n",
      "partition 690000/1403477\n",
      "partition 700000/1403477\n",
      "partition 710000/1403477\n",
      "partition 720000/1403477\n",
      "partition 730000/1403477\n",
      "partition 740000/1403477\n",
      "partition 750000/1403477\n",
      "partition 760000/1403477\n",
      "partition 770000/1403477\n",
      "partition 780000/1403477\n",
      "partition 790000/1403477\n",
      "partition 800000/1403477\n",
      "partition 810000/1403477\n",
      "partition 820000/1403477\n",
      "partition 830000/1403477\n",
      "partition 840000/1403477\n",
      "partition 850000/1403477\n",
      "partition 860000/1403477\n",
      "partition 870000/1403477\n",
      "partition 880000/1403477\n",
      "partition 890000/1403477\n",
      "partition 900000/1403477\n",
      "partition 910000/1403477\n",
      "partition 920000/1403477\n",
      "partition 930000/1403477\n",
      "partition 940000/1403477\n",
      "partition 950000/1403477\n",
      "partition 960000/1403477\n",
      "partition 970000/1403477\n",
      "partition 980000/1403477\n",
      "partition 990000/1403477\n",
      "partition 1000000/1403477\n",
      "partition 1010000/1403477\n",
      "partition 1020000/1403477\n",
      "partition 1030000/1403477\n",
      "partition 1040000/1403477\n",
      "partition 1050000/1403477\n",
      "partition 1060000/1403477\n",
      "partition 1070000/1403477\n",
      "partition 1080000/1403477\n",
      "partition 1090000/1403477\n",
      "partition 1100000/1403477\n",
      "partition 1110000/1403477\n",
      "partition 1120000/1403477\n",
      "partition 1130000/1403477\n",
      "partition 1140000/1403477\n",
      "partition 1150000/1403477\n",
      "partition 1160000/1403477\n",
      "partition 1170000/1403477\n",
      "partition 1180000/1403477\n",
      "partition 1190000/1403477\n",
      "partition 1200000/1403477\n",
      "partition 1210000/1403477\n",
      "partition 1220000/1403477\n",
      "partition 1230000/1403477\n",
      "partition 1240000/1403477\n",
      "partition 1250000/1403477\n",
      "partition 1260000/1403477\n",
      "partition 1270000/1403477\n",
      "partition 1280000/1403477\n",
      "partition 1290000/1403477\n",
      "partition 1300000/1403477\n",
      "partition 1310000/1403477\n",
      "partition 1320000/1403477\n",
      "partition 1330000/1403477\n",
      "partition 1340000/1403477\n",
      "partition 1350000/1403477\n",
      "partition 1360000/1403477\n",
      "partition 1370000/1403477\n",
      "partition 1380000/1403477\n",
      "partition 1390000/1403477\n",
      "partition 1400000/1403477\n",
      "partition 1403477/1403477\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.2396 - test loss: 24.3375 - train acc: 0.0333 - test acc: 0.0539 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.9435 - test loss: 23.2763 - train acc: 0.0667 - test acc: 0.0783 - 0m 6s\n",
      "batch: 300/1563 - train loss: 22.9488 - test loss: 22.4893 - train acc: 0.0789 - test acc: 0.0886 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.3841 - test loss: 21.6190 - train acc: 0.0971 - test acc: 0.1118 - 0m 16s\n",
      "batch: 500/1563 - train loss: 21.4786 - test loss: 21.3178 - train acc: 0.1147 - test acc: 0.1263 - 0m 20s\n",
      "batch: 600/1563 - train loss: 21.1116 - test loss: 20.6154 - train acc: 0.1303 - test acc: 0.1406 - 0m 25s\n",
      "batch: 700/1563 - train loss: 20.8160 - test loss: 20.1351 - train acc: 0.1353 - test acc: 0.1584 - 0m 30s\n",
      "batch: 800/1563 - train loss: 20.5846 - test loss: 20.1834 - train acc: 0.1369 - test acc: 0.1550 - 0m 35s\n",
      "batch: 900/1563 - train loss: 20.0904 - test loss: 19.7936 - train acc: 0.1538 - test acc: 0.1609 - 0m 39s\n",
      "batch: 1000/1563 - train loss: 20.0932 - test loss: 19.2086 - train acc: 0.1553 - test acc: 0.1843 - 0m 44s\n",
      "batch: 1100/1563 - train loss: 19.6639 - test loss: 19.0711 - train acc: 0.1626 - test acc: 0.1847 - 0m 49s\n",
      "batch: 1200/1563 - train loss: 19.1244 - test loss: 19.4089 - train acc: 0.1866 - test acc: 0.1809 - 0m 53s\n",
      "batch: 1300/1563 - train loss: 18.9617 - test loss: 19.1139 - train acc: 0.1888 - test acc: 0.1941 - 0m 58s\n",
      "batch: 1400/1563 - train loss: 19.0185 - test loss: 18.7587 - train acc: 0.1907 - test acc: 0.1902 - 1m 3s\n",
      "batch: 1500/1563 - train loss: 18.9109 - test loss: 19.3297 - train acc: 0.1963 - test acc: 0.1782 - 1m 8s\n",
      "batch: 1563/1563 - train loss: 19.0306 - test loss: 18.0591 - train acc: 0.1835 - test acc: 0.2209 - 1m 12s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.8507 - test loss: 18.8963 - train acc: 0.2128 - test acc: 0.2053 - 1m 16s\n",
      "batch: 200/1563 - train loss: 17.6233 - test loss: 18.1273 - train acc: 0.2232 - test acc: 0.2243 - 1m 21s\n",
      "batch: 300/1563 - train loss: 17.5642 - test loss: 17.7006 - train acc: 0.2341 - test acc: 0.2395 - 1m 26s\n",
      "batch: 400/1563 - train loss: 17.4041 - test loss: 18.8982 - train acc: 0.2363 - test acc: 0.2036 - 1m 31s\n",
      "batch: 500/1563 - train loss: 17.3267 - test loss: 18.8144 - train acc: 0.2466 - test acc: 0.2048 - 1m 35s\n",
      "batch: 600/1563 - train loss: 17.0541 - test loss: 17.2499 - train acc: 0.2513 - test acc: 0.2456 - 1m 40s\n",
      "batch: 700/1563 - train loss: 16.9510 - test loss: 17.3571 - train acc: 0.2488 - test acc: 0.2462 - 1m 45s\n",
      "batch: 800/1563 - train loss: 16.6976 - test loss: 17.8733 - train acc: 0.2603 - test acc: 0.2401 - 1m 50s\n",
      "batch: 900/1563 - train loss: 16.7771 - test loss: 17.2870 - train acc: 0.2615 - test acc: 0.2483 - 1m 54s\n",
      "batch: 1000/1563 - train loss: 16.3475 - test loss: 19.2567 - train acc: 0.2828 - test acc: 0.1924 - 1m 59s\n",
      "batch: 1100/1563 - train loss: 16.3574 - test loss: 16.6210 - train acc: 0.2772 - test acc: 0.2688 - 2m 4s\n",
      "batch: 1200/1563 - train loss: 16.1832 - test loss: 16.1304 - train acc: 0.2772 - test acc: 0.2833 - 2m 9s\n",
      "batch: 1300/1563 - train loss: 16.2839 - test loss: 16.6208 - train acc: 0.2743 - test acc: 0.2683 - 2m 14s\n",
      "batch: 1400/1563 - train loss: 16.0371 - test loss: 16.3008 - train acc: 0.2962 - test acc: 0.2887 - 2m 19s\n",
      "batch: 1500/1563 - train loss: 15.8555 - test loss: 16.6076 - train acc: 0.2956 - test acc: 0.2768 - 2m 23s\n",
      "batch: 1563/1563 - train loss: 16.1099 - test loss: 17.3688 - train acc: 0.2818 - test acc: 0.2484 - 2m 27s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.1663 - test loss: 16.7854 - train acc: 0.3178 - test acc: 0.2741 - 2m 32s\n",
      "batch: 200/1563 - train loss: 15.0831 - test loss: 18.9710 - train acc: 0.3256 - test acc: 0.2238 - 2m 37s\n",
      "batch: 300/1563 - train loss: 14.9135 - test loss: 15.6222 - train acc: 0.3309 - test acc: 0.3106 - 2m 42s\n",
      "batch: 400/1563 - train loss: 14.8319 - test loss: 16.1160 - train acc: 0.3287 - test acc: 0.2974 - 2m 46s\n",
      "batch: 500/1563 - train loss: 14.7204 - test loss: 16.1981 - train acc: 0.3344 - test acc: 0.2875 - 2m 51s\n",
      "batch: 600/1563 - train loss: 15.3052 - test loss: 15.4448 - train acc: 0.3137 - test acc: 0.3116 - 2m 56s\n",
      "batch: 700/1563 - train loss: 14.6408 - test loss: 15.0974 - train acc: 0.3384 - test acc: 0.3242 - 3m 1s\n",
      "batch: 800/1563 - train loss: 14.4467 - test loss: 15.4510 - train acc: 0.3412 - test acc: 0.3190 - 3m 5s\n",
      "batch: 900/1563 - train loss: 14.6434 - test loss: 14.8705 - train acc: 0.3353 - test acc: 0.3344 - 3m 10s\n",
      "batch: 1000/1563 - train loss: 14.2993 - test loss: 16.5257 - train acc: 0.3553 - test acc: 0.2798 - 3m 15s\n",
      "batch: 1100/1563 - train loss: 14.2472 - test loss: 16.3227 - train acc: 0.3603 - test acc: 0.2921 - 3m 20s\n",
      "batch: 1200/1563 - train loss: 14.4664 - test loss: 15.9463 - train acc: 0.3463 - test acc: 0.3033 - 3m 24s\n",
      "batch: 1300/1563 - train loss: 14.1860 - test loss: 15.9993 - train acc: 0.3537 - test acc: 0.3033 - 3m 29s\n",
      "batch: 1400/1563 - train loss: 14.2833 - test loss: 14.8098 - train acc: 0.3519 - test acc: 0.3327 - 3m 34s\n",
      "batch: 1500/1563 - train loss: 14.3279 - test loss: 15.8182 - train acc: 0.3493 - test acc: 0.3063 - 3m 38s\n",
      "batch: 1563/1563 - train loss: 14.3508 - test loss: 15.5707 - train acc: 0.3381 - test acc: 0.3127 - 3m 43s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.8248 - test loss: 15.6062 - train acc: 0.4150 - test acc: 0.3162 - 3m 47s\n",
      "batch: 200/1563 - train loss: 12.6186 - test loss: 14.6779 - train acc: 0.4072 - test acc: 0.3476 - 3m 52s\n",
      "batch: 300/1563 - train loss: 13.1899 - test loss: 15.6591 - train acc: 0.3932 - test acc: 0.3213 - 3m 57s\n",
      "batch: 400/1563 - train loss: 13.0979 - test loss: 15.0920 - train acc: 0.3935 - test acc: 0.3263 - 4m 1s\n",
      "batch: 500/1563 - train loss: 13.2896 - test loss: 15.4993 - train acc: 0.3700 - test acc: 0.3161 - 4m 6s\n",
      "batch: 600/1563 - train loss: 13.2804 - test loss: 16.6568 - train acc: 0.3850 - test acc: 0.2959 - 4m 11s\n",
      "batch: 700/1563 - train loss: 13.3074 - test loss: 14.4873 - train acc: 0.3828 - test acc: 0.3520 - 4m 16s\n",
      "batch: 800/1563 - train loss: 13.0622 - test loss: 15.1178 - train acc: 0.3888 - test acc: 0.3293 - 4m 20s\n",
      "batch: 900/1563 - train loss: 12.8818 - test loss: 14.8071 - train acc: 0.4025 - test acc: 0.3308 - 4m 25s\n",
      "batch: 1000/1563 - train loss: 13.1545 - test loss: 14.4710 - train acc: 0.4022 - test acc: 0.3534 - 4m 30s\n",
      "batch: 1100/1563 - train loss: 13.1079 - test loss: 14.9175 - train acc: 0.3891 - test acc: 0.3353 - 4m 34s\n",
      "batch: 1200/1563 - train loss: 13.2866 - test loss: 15.9080 - train acc: 0.3894 - test acc: 0.3110 - 4m 39s\n",
      "batch: 1300/1563 - train loss: 13.1431 - test loss: 13.7622 - train acc: 0.3935 - test acc: 0.3762 - 4m 44s\n",
      "batch: 1400/1563 - train loss: 13.1174 - test loss: 15.2263 - train acc: 0.3944 - test acc: 0.3314 - 4m 49s\n",
      "batch: 1500/1563 - train loss: 12.9485 - test loss: 14.1583 - train acc: 0.4072 - test acc: 0.3635 - 4m 54s\n",
      "batch: 1563/1563 - train loss: 13.0231 - test loss: 14.1150 - train acc: 0.4084 - test acc: 0.3658 - 4m 58s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.4868 - test loss: 13.9423 - train acc: 0.4544 - test acc: 0.3783 - 5m 3s\n",
      "batch: 200/1563 - train loss: 11.5565 - test loss: 15.7437 - train acc: 0.4541 - test acc: 0.3273 - 5m 7s\n",
      "batch: 300/1563 - train loss: 11.4640 - test loss: 13.6344 - train acc: 0.4599 - test acc: 0.3893 - 5m 12s\n",
      "batch: 400/1563 - train loss: 11.9047 - test loss: 14.2303 - train acc: 0.4397 - test acc: 0.3624 - 5m 17s\n",
      "batch: 500/1563 - train loss: 11.9070 - test loss: 14.5229 - train acc: 0.4269 - test acc: 0.3560 - 5m 22s\n",
      "batch: 600/1563 - train loss: 12.0128 - test loss: 14.9068 - train acc: 0.4325 - test acc: 0.3569 - 5m 26s\n",
      "batch: 700/1563 - train loss: 11.9498 - test loss: 13.7491 - train acc: 0.4335 - test acc: 0.3807 - 5m 31s\n",
      "batch: 800/1563 - train loss: 11.9090 - test loss: 14.5117 - train acc: 0.4359 - test acc: 0.3582 - 5m 36s\n",
      "batch: 900/1563 - train loss: 11.8668 - test loss: 13.5816 - train acc: 0.4369 - test acc: 0.3875 - 5m 40s\n",
      "batch: 1000/1563 - train loss: 11.9690 - test loss: 14.1478 - train acc: 0.4384 - test acc: 0.3758 - 5m 45s\n",
      "batch: 1100/1563 - train loss: 11.6938 - test loss: 13.7685 - train acc: 0.4500 - test acc: 0.3794 - 5m 50s\n",
      "batch: 1200/1563 - train loss: 12.1169 - test loss: 14.6511 - train acc: 0.4216 - test acc: 0.3559 - 5m 55s\n",
      "batch: 1300/1563 - train loss: 11.8191 - test loss: 13.4480 - train acc: 0.4509 - test acc: 0.3933 - 6m 0s\n",
      "batch: 1400/1563 - train loss: 12.0762 - test loss: 13.8895 - train acc: 0.4269 - test acc: 0.3744 - 6m 4s\n",
      "batch: 1500/1563 - train loss: 11.7966 - test loss: 13.8480 - train acc: 0.4457 - test acc: 0.3796 - 6m 9s\n",
      "batch: 1563/1563 - train loss: 11.7798 - test loss: 15.4863 - train acc: 0.4463 - test acc: 0.3338 - 6m 13s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.7878 - test loss: 13.4290 - train acc: 0.5243 - test acc: 0.3980 - 6m 18s\n",
      "batch: 200/1563 - train loss: 9.9147 - test loss: 14.7255 - train acc: 0.5103 - test acc: 0.3586 - 6m 23s\n",
      "batch: 300/1563 - train loss: 10.5739 - test loss: 13.9776 - train acc: 0.5019 - test acc: 0.3769 - 6m 28s\n",
      "batch: 400/1563 - train loss: 10.4851 - test loss: 14.6633 - train acc: 0.5028 - test acc: 0.3647 - 6m 32s\n",
      "batch: 500/1563 - train loss: 10.2884 - test loss: 14.2103 - train acc: 0.5081 - test acc: 0.3799 - 6m 37s\n",
      "batch: 600/1563 - train loss: 10.8113 - test loss: 14.1999 - train acc: 0.4884 - test acc: 0.3830 - 6m 42s\n",
      "batch: 700/1563 - train loss: 10.6475 - test loss: 13.7297 - train acc: 0.4798 - test acc: 0.3886 - 6m 46s\n",
      "batch: 800/1563 - train loss: 10.7989 - test loss: 13.4833 - train acc: 0.4822 - test acc: 0.3961 - 6m 52s\n",
      "batch: 900/1563 - train loss: 10.5376 - test loss: 13.9936 - train acc: 0.4878 - test acc: 0.3865 - 6m 56s\n",
      "batch: 1000/1563 - train loss: 10.7861 - test loss: 13.7860 - train acc: 0.4807 - test acc: 0.3902 - 7m 1s\n",
      "batch: 1100/1563 - train loss: 10.9576 - test loss: 13.8430 - train acc: 0.4628 - test acc: 0.3900 - 7m 6s\n",
      "batch: 1200/1563 - train loss: 10.9140 - test loss: 13.8656 - train acc: 0.4731 - test acc: 0.3832 - 7m 11s\n",
      "batch: 1300/1563 - train loss: 11.2109 - test loss: 13.2105 - train acc: 0.4709 - test acc: 0.4060 - 7m 15s\n",
      "batch: 1400/1563 - train loss: 11.0825 - test loss: 14.0402 - train acc: 0.4782 - test acc: 0.3871 - 7m 20s\n",
      "batch: 1500/1563 - train loss: 10.8373 - test loss: 13.9498 - train acc: 0.4719 - test acc: 0.3931 - 7m 25s\n",
      "batch: 1563/1563 - train loss: 10.8462 - test loss: 15.2378 - train acc: 0.4731 - test acc: 0.3541 - 7m 29s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.9260 - test loss: 14.0092 - train acc: 0.5565 - test acc: 0.3928 - 7m 34s\n",
      "batch: 200/1563 - train loss: 9.1581 - test loss: 14.7950 - train acc: 0.5450 - test acc: 0.3746 - 7m 39s\n",
      "batch: 300/1563 - train loss: 9.0568 - test loss: 13.5674 - train acc: 0.5687 - test acc: 0.4019 - 7m 43s\n",
      "batch: 400/1563 - train loss: 9.4852 - test loss: 13.5497 - train acc: 0.5331 - test acc: 0.4144 - 7m 48s\n",
      "batch: 500/1563 - train loss: 9.6069 - test loss: 13.5305 - train acc: 0.5250 - test acc: 0.4107 - 7m 53s\n",
      "batch: 600/1563 - train loss: 9.4371 - test loss: 13.8168 - train acc: 0.5481 - test acc: 0.4030 - 7m 58s\n",
      "batch: 700/1563 - train loss: 9.6095 - test loss: 13.9227 - train acc: 0.5306 - test acc: 0.4002 - 8m 3s\n",
      "batch: 800/1563 - train loss: 9.6097 - test loss: 14.1197 - train acc: 0.5272 - test acc: 0.3911 - 8m 7s\n",
      "batch: 900/1563 - train loss: 9.6387 - test loss: 15.6263 - train acc: 0.5175 - test acc: 0.3459 - 8m 12s\n",
      "batch: 1000/1563 - train loss: 9.7749 - test loss: 13.4402 - train acc: 0.5175 - test acc: 0.4033 - 8m 17s\n",
      "batch: 1100/1563 - train loss: 9.9220 - test loss: 14.4498 - train acc: 0.5125 - test acc: 0.3843 - 8m 22s\n",
      "batch: 1200/1563 - train loss: 9.7824 - test loss: 13.4725 - train acc: 0.5256 - test acc: 0.4116 - 8m 27s\n",
      "batch: 1300/1563 - train loss: 9.6870 - test loss: 13.7778 - train acc: 0.5288 - test acc: 0.3979 - 8m 32s\n",
      "batch: 1400/1563 - train loss: 9.9310 - test loss: 14.0246 - train acc: 0.5110 - test acc: 0.3940 - 8m 37s\n",
      "batch: 1500/1563 - train loss: 10.0070 - test loss: 13.4028 - train acc: 0.4954 - test acc: 0.4097 - 8m 41s\n",
      "batch: 1563/1563 - train loss: 10.1193 - test loss: 13.2144 - train acc: 0.5053 - test acc: 0.4121 - 8m 45s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.6440 - test loss: 13.3690 - train acc: 0.6140 - test acc: 0.4120 - 8m 50s\n",
      "batch: 200/1563 - train loss: 7.8134 - test loss: 13.4803 - train acc: 0.6034 - test acc: 0.4140 - 8m 55s\n",
      "batch: 300/1563 - train loss: 8.1806 - test loss: 13.9732 - train acc: 0.5900 - test acc: 0.3989 - 9m 0s\n",
      "batch: 400/1563 - train loss: 8.1260 - test loss: 13.9436 - train acc: 0.5947 - test acc: 0.4039 - 9m 5s\n",
      "batch: 500/1563 - train loss: 8.4145 - test loss: 14.2563 - train acc: 0.5731 - test acc: 0.3907 - 9m 9s\n",
      "batch: 600/1563 - train loss: 8.6691 - test loss: 14.2211 - train acc: 0.5668 - test acc: 0.4049 - 9m 14s\n",
      "batch: 700/1563 - train loss: 8.6239 - test loss: 13.6072 - train acc: 0.5640 - test acc: 0.4166 - 9m 19s\n",
      "batch: 800/1563 - train loss: 8.7448 - test loss: 13.8296 - train acc: 0.5653 - test acc: 0.4023 - 9m 24s\n",
      "batch: 900/1563 - train loss: 8.9015 - test loss: 14.1413 - train acc: 0.5581 - test acc: 0.3945 - 9m 29s\n",
      "batch: 1000/1563 - train loss: 8.7815 - test loss: 14.0995 - train acc: 0.5569 - test acc: 0.3969 - 9m 33s\n",
      "batch: 1100/1563 - train loss: 8.6765 - test loss: 14.1705 - train acc: 0.5722 - test acc: 0.3979 - 9m 38s\n",
      "batch: 1200/1563 - train loss: 8.9126 - test loss: 13.4327 - train acc: 0.5559 - test acc: 0.4168 - 9m 43s\n",
      "batch: 1300/1563 - train loss: 8.7807 - test loss: 14.0057 - train acc: 0.5659 - test acc: 0.4012 - 9m 47s\n",
      "batch: 1400/1563 - train loss: 9.1133 - test loss: 13.7880 - train acc: 0.5453 - test acc: 0.4115 - 9m 52s\n",
      "batch: 1500/1563 - train loss: 9.1961 - test loss: 13.5995 - train acc: 0.5372 - test acc: 0.4088 - 9m 57s\n",
      "batch: 1563/1563 - train loss: 9.0596 - test loss: 14.0339 - train acc: 0.5455 - test acc: 0.4094 - 10m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.3854 - test loss: 13.7894 - train acc: 0.6688 - test acc: 0.4246 - 10m 6s\n",
      "batch: 200/1563 - train loss: 6.6781 - test loss: 14.3872 - train acc: 0.6510 - test acc: 0.4014 - 10m 11s\n",
      "batch: 300/1563 - train loss: 6.8145 - test loss: 14.1270 - train acc: 0.6541 - test acc: 0.4078 - 10m 16s\n",
      "batch: 400/1563 - train loss: 7.0102 - test loss: 13.8796 - train acc: 0.6425 - test acc: 0.4153 - 10m 21s\n",
      "batch: 500/1563 - train loss: 7.0825 - test loss: 14.9090 - train acc: 0.6335 - test acc: 0.3940 - 10m 25s\n",
      "batch: 600/1563 - train loss: 7.4914 - test loss: 14.8327 - train acc: 0.6206 - test acc: 0.3979 - 10m 31s\n",
      "batch: 700/1563 - train loss: 7.7824 - test loss: 14.0452 - train acc: 0.5997 - test acc: 0.4056 - 10m 36s\n",
      "batch: 800/1563 - train loss: 7.6808 - test loss: 14.2216 - train acc: 0.6093 - test acc: 0.4115 - 10m 41s\n",
      "batch: 900/1563 - train loss: 8.0474 - test loss: 14.0513 - train acc: 0.6000 - test acc: 0.4116 - 10m 45s\n",
      "batch: 1000/1563 - train loss: 8.0767 - test loss: 14.0946 - train acc: 0.5849 - test acc: 0.4069 - 10m 50s\n",
      "batch: 1100/1563 - train loss: 8.1052 - test loss: 13.7603 - train acc: 0.6016 - test acc: 0.4219 - 10m 55s\n",
      "batch: 1200/1563 - train loss: 8.2709 - test loss: 14.1123 - train acc: 0.5866 - test acc: 0.4117 - 11m 0s\n",
      "batch: 1300/1563 - train loss: 8.1015 - test loss: 14.1714 - train acc: 0.5906 - test acc: 0.4082 - 11m 5s\n",
      "batch: 1400/1563 - train loss: 8.3544 - test loss: 13.6110 - train acc: 0.5862 - test acc: 0.4283 - 11m 10s\n",
      "batch: 1500/1563 - train loss: 8.2106 - test loss: 14.0147 - train acc: 0.5972 - test acc: 0.4127 - 11m 15s\n",
      "batch: 1563/1563 - train loss: 8.3511 - test loss: 13.6090 - train acc: 0.5862 - test acc: 0.4228 - 11m 19s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.6788 - test loss: 14.2475 - train acc: 0.6984 - test acc: 0.4166 - 11m 24s\n",
      "batch: 200/1563 - train loss: 5.8873 - test loss: 15.0351 - train acc: 0.6931 - test acc: 0.3981 - 11m 29s\n",
      "batch: 300/1563 - train loss: 5.8136 - test loss: 14.9946 - train acc: 0.6982 - test acc: 0.4031 - 11m 34s\n",
      "batch: 400/1563 - train loss: 6.3223 - test loss: 14.8372 - train acc: 0.6760 - test acc: 0.4147 - 11m 39s\n",
      "batch: 500/1563 - train loss: 6.3333 - test loss: 16.5172 - train acc: 0.6725 - test acc: 0.3752 - 11m 44s\n",
      "batch: 600/1563 - train loss: 6.6448 - test loss: 14.5197 - train acc: 0.6563 - test acc: 0.4189 - 11m 49s\n",
      "batch: 700/1563 - train loss: 6.7010 - test loss: 15.3599 - train acc: 0.6484 - test acc: 0.3959 - 11m 54s\n",
      "batch: 800/1563 - train loss: 6.9837 - test loss: 15.0839 - train acc: 0.6406 - test acc: 0.4042 - 11m 58s\n",
      "batch: 900/1563 - train loss: 6.9635 - test loss: 14.5465 - train acc: 0.6385 - test acc: 0.4156 - 12m 3s\n",
      "batch: 1000/1563 - train loss: 7.0399 - test loss: 14.7132 - train acc: 0.6387 - test acc: 0.4081 - 12m 9s\n",
      "batch: 1100/1563 - train loss: 7.1192 - test loss: 13.9028 - train acc: 0.6325 - test acc: 0.4340 - 12m 13s\n",
      "batch: 1200/1563 - train loss: 7.0090 - test loss: 14.4652 - train acc: 0.6338 - test acc: 0.4158 - 12m 18s\n",
      "batch: 1300/1563 - train loss: 7.4209 - test loss: 14.6612 - train acc: 0.6197 - test acc: 0.4028 - 12m 23s\n",
      "batch: 1400/1563 - train loss: 7.3304 - test loss: 14.4771 - train acc: 0.6194 - test acc: 0.4082 - 12m 28s\n",
      "batch: 1500/1563 - train loss: 7.4377 - test loss: 14.5543 - train acc: 0.6132 - test acc: 0.4020 - 12m 32s\n",
      "batch: 1563/1563 - train loss: 7.3883 - test loss: 14.0969 - train acc: 0.6184 - test acc: 0.4173 - 12m 37s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.7787 - test loss: 14.6731 - train acc: 0.7528 - test acc: 0.4184 - 12m 41s\n",
      "batch: 200/1563 - train loss: 4.6720 - test loss: 14.3407 - train acc: 0.7494 - test acc: 0.4294 - 12m 46s\n",
      "batch: 300/1563 - train loss: 4.8016 - test loss: 14.9312 - train acc: 0.7487 - test acc: 0.4209 - 12m 51s\n",
      "batch: 400/1563 - train loss: 5.0957 - test loss: 15.2451 - train acc: 0.7340 - test acc: 0.4158 - 12m 56s\n",
      "batch: 500/1563 - train loss: 5.4663 - test loss: 15.1170 - train acc: 0.7050 - test acc: 0.4101 - 13m 0s\n",
      "batch: 600/1563 - train loss: 5.5276 - test loss: 15.4359 - train acc: 0.7057 - test acc: 0.4085 - 13m 5s\n",
      "batch: 700/1563 - train loss: 6.0006 - test loss: 14.9022 - train acc: 0.6881 - test acc: 0.4184 - 13m 10s\n",
      "batch: 800/1563 - train loss: 5.8431 - test loss: 15.6817 - train acc: 0.6972 - test acc: 0.4083 - 13m 15s\n",
      "batch: 900/1563 - train loss: 6.2357 - test loss: 16.2067 - train acc: 0.6719 - test acc: 0.3951 - 13m 20s\n",
      "batch: 1000/1563 - train loss: 6.0100 - test loss: 15.0194 - train acc: 0.6844 - test acc: 0.4174 - 13m 24s\n",
      "batch: 1100/1563 - train loss: 6.4371 - test loss: 16.9908 - train acc: 0.6738 - test acc: 0.3672 - 13m 29s\n",
      "batch: 1200/1563 - train loss: 6.3535 - test loss: 15.4651 - train acc: 0.6675 - test acc: 0.4053 - 13m 34s\n",
      "batch: 1300/1563 - train loss: 6.5718 - test loss: 15.8255 - train acc: 0.6590 - test acc: 0.4028 - 13m 39s\n",
      "batch: 1400/1563 - train loss: 6.4885 - test loss: 14.8936 - train acc: 0.6623 - test acc: 0.4091 - 13m 44s\n",
      "batch: 1500/1563 - train loss: 6.6142 - test loss: 14.4301 - train acc: 0.6544 - test acc: 0.4229 - 13m 48s\n",
      "batch: 1563/1563 - train loss: 6.5714 - test loss: 14.9560 - train acc: 0.6434 - test acc: 0.4078 - 13m 52s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.2478 - test loss: 15.3399 - train acc: 0.7724 - test acc: 0.4155 - 13m 57s\n",
      "batch: 200/1563 - train loss: 4.0535 - test loss: 15.9837 - train acc: 0.7859 - test acc: 0.4095 - 14m 2s\n",
      "batch: 300/1563 - train loss: 4.3049 - test loss: 15.4472 - train acc: 0.7684 - test acc: 0.4161 - 14m 7s\n",
      "batch: 400/1563 - train loss: 4.0920 - test loss: 15.6119 - train acc: 0.7815 - test acc: 0.4136 - 14m 12s\n",
      "batch: 500/1563 - train loss: 4.6785 - test loss: 15.7048 - train acc: 0.7343 - test acc: 0.4215 - 14m 17s\n",
      "batch: 600/1563 - train loss: 4.6893 - test loss: 17.1688 - train acc: 0.7497 - test acc: 0.3840 - 14m 21s\n",
      "batch: 700/1563 - train loss: 4.9807 - test loss: 15.5056 - train acc: 0.7332 - test acc: 0.4183 - 14m 26s\n",
      "batch: 800/1563 - train loss: 5.2509 - test loss: 16.6863 - train acc: 0.7184 - test acc: 0.3975 - 14m 31s\n",
      "batch: 900/1563 - train loss: 5.4238 - test loss: 15.8902 - train acc: 0.7113 - test acc: 0.4115 - 14m 36s\n",
      "batch: 1000/1563 - train loss: 5.5232 - test loss: 15.9934 - train acc: 0.7097 - test acc: 0.4112 - 14m 41s\n",
      "batch: 1100/1563 - train loss: 5.6943 - test loss: 15.8324 - train acc: 0.6910 - test acc: 0.4023 - 14m 46s\n",
      "batch: 1200/1563 - train loss: 5.5403 - test loss: 15.9472 - train acc: 0.7044 - test acc: 0.4036 - 14m 50s\n",
      "batch: 1300/1563 - train loss: 5.7234 - test loss: 16.3155 - train acc: 0.6919 - test acc: 0.4092 - 14m 55s\n",
      "batch: 1400/1563 - train loss: 5.7844 - test loss: 15.2527 - train acc: 0.6881 - test acc: 0.4187 - 15m 0s\n",
      "batch: 1500/1563 - train loss: 5.8758 - test loss: 16.1961 - train acc: 0.6981 - test acc: 0.3998 - 15m 5s\n",
      "batch: 1563/1563 - train loss: 5.8819 - test loss: 16.7496 - train acc: 0.6900 - test acc: 0.3893 - 15m 9s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.6610 - test loss: 15.4868 - train acc: 0.8068 - test acc: 0.4233 - 15m 14s\n",
      "batch: 200/1563 - train loss: 3.5186 - test loss: 15.9958 - train acc: 0.8062 - test acc: 0.4198 - 15m 18s\n",
      "batch: 300/1563 - train loss: 3.7176 - test loss: 16.0797 - train acc: 0.8000 - test acc: 0.4148 - 15m 23s\n",
      "batch: 400/1563 - train loss: 3.7608 - test loss: 15.9238 - train acc: 0.8012 - test acc: 0.4255 - 15m 28s\n",
      "batch: 500/1563 - train loss: 3.9301 - test loss: 16.2435 - train acc: 0.7853 - test acc: 0.4237 - 15m 33s\n",
      "batch: 600/1563 - train loss: 3.9337 - test loss: 16.0705 - train acc: 0.7887 - test acc: 0.4242 - 15m 38s\n",
      "batch: 700/1563 - train loss: 4.1397 - test loss: 16.3086 - train acc: 0.7766 - test acc: 0.4239 - 15m 43s\n",
      "batch: 800/1563 - train loss: 4.0165 - test loss: 16.6916 - train acc: 0.7715 - test acc: 0.4060 - 15m 47s\n",
      "batch: 900/1563 - train loss: 4.3818 - test loss: 16.4418 - train acc: 0.7575 - test acc: 0.4246 - 15m 52s\n",
      "batch: 1000/1563 - train loss: 4.7431 - test loss: 16.4968 - train acc: 0.7388 - test acc: 0.4134 - 15m 57s\n",
      "batch: 1100/1563 - train loss: 5.0569 - test loss: 16.4716 - train acc: 0.7279 - test acc: 0.4112 - 16m 2s\n",
      "batch: 1200/1563 - train loss: 5.0220 - test loss: 16.0399 - train acc: 0.7260 - test acc: 0.4214 - 16m 7s\n",
      "batch: 1300/1563 - train loss: 5.0876 - test loss: 16.0132 - train acc: 0.7234 - test acc: 0.4161 - 16m 12s\n",
      "batch: 1400/1563 - train loss: 5.2010 - test loss: 15.9420 - train acc: 0.7169 - test acc: 0.4268 - 16m 17s\n",
      "batch: 1500/1563 - train loss: 5.0204 - test loss: 17.2844 - train acc: 0.7253 - test acc: 0.3988 - 16m 22s\n",
      "batch: 1563/1563 - train loss: 5.2465 - test loss: 16.0118 - train acc: 0.7125 - test acc: 0.4103 - 16m 26s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 3.1978 - test loss: 16.0800 - train acc: 0.8209 - test acc: 0.4264 - 16m 30s\n",
      "batch: 200/1563 - train loss: 3.1113 - test loss: 16.3431 - train acc: 0.8291 - test acc: 0.4280 - 16m 35s\n",
      "batch: 300/1563 - train loss: 3.1868 - test loss: 16.7738 - train acc: 0.8284 - test acc: 0.4103 - 16m 40s\n",
      "batch: 400/1563 - train loss: 2.9041 - test loss: 16.4611 - train acc: 0.8403 - test acc: 0.4324 - 16m 45s\n",
      "batch: 500/1563 - train loss: 3.3013 - test loss: 16.7631 - train acc: 0.8209 - test acc: 0.4187 - 16m 50s\n",
      "batch: 600/1563 - train loss: 3.7389 - test loss: 18.2050 - train acc: 0.7900 - test acc: 0.3922 - 16m 55s\n",
      "batch: 700/1563 - train loss: 3.7403 - test loss: 17.2337 - train acc: 0.7943 - test acc: 0.4093 - 17m 0s\n",
      "batch: 800/1563 - train loss: 4.0098 - test loss: 17.2929 - train acc: 0.7737 - test acc: 0.4142 - 17m 4s\n",
      "batch: 900/1563 - train loss: 4.0768 - test loss: 17.1528 - train acc: 0.7687 - test acc: 0.4129 - 17m 9s\n",
      "batch: 1000/1563 - train loss: 3.9398 - test loss: 17.5749 - train acc: 0.7831 - test acc: 0.4130 - 17m 14s\n",
      "batch: 1100/1563 - train loss: 4.2704 - test loss: 17.0115 - train acc: 0.7724 - test acc: 0.4157 - 17m 19s\n",
      "batch: 1200/1563 - train loss: 4.7000 - test loss: 16.7146 - train acc: 0.7447 - test acc: 0.4175 - 17m 24s\n",
      "batch: 1300/1563 - train loss: 4.3496 - test loss: 17.0687 - train acc: 0.7616 - test acc: 0.4141 - 17m 29s\n",
      "batch: 1400/1563 - train loss: 4.4362 - test loss: 16.3728 - train acc: 0.7569 - test acc: 0.4230 - 17m 33s\n",
      "batch: 1500/1563 - train loss: 4.4616 - test loss: 16.5038 - train acc: 0.7541 - test acc: 0.4186 - 17m 38s\n",
      "batch: 1563/1563 - train loss: 4.4169 - test loss: 16.8370 - train acc: 0.7594 - test acc: 0.4158 - 17m 42s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.4824 - test loss: 16.7511 - train acc: 0.8703 - test acc: 0.4227 - 17m 47s\n",
      "batch: 200/1563 - train loss: 2.6073 - test loss: 16.7787 - train acc: 0.8597 - test acc: 0.4232 - 17m 52s\n",
      "batch: 300/1563 - train loss: 2.5490 - test loss: 17.2312 - train acc: 0.8593 - test acc: 0.4233 - 17m 57s\n",
      "batch: 400/1563 - train loss: 2.7565 - test loss: 17.9608 - train acc: 0.8409 - test acc: 0.4140 - 18m 2s\n",
      "batch: 500/1563 - train loss: 2.8388 - test loss: 17.9387 - train acc: 0.8400 - test acc: 0.4205 - 18m 7s\n",
      "batch: 600/1563 - train loss: 2.7265 - test loss: 17.7157 - train acc: 0.8484 - test acc: 0.4178 - 18m 12s\n",
      "batch: 700/1563 - train loss: 3.0039 - test loss: 18.3855 - train acc: 0.8358 - test acc: 0.4062 - 18m 16s\n",
      "batch: 800/1563 - train loss: 3.0061 - test loss: 17.7018 - train acc: 0.8336 - test acc: 0.4178 - 18m 21s\n",
      "batch: 900/1563 - train loss: 3.3630 - test loss: 18.9203 - train acc: 0.8209 - test acc: 0.3973 - 18m 26s\n",
      "batch: 1000/1563 - train loss: 3.5781 - test loss: 18.1252 - train acc: 0.7990 - test acc: 0.4142 - 18m 31s\n",
      "batch: 1100/1563 - train loss: 3.5835 - test loss: 17.7033 - train acc: 0.7974 - test acc: 0.4201 - 18m 36s\n",
      "batch: 1200/1563 - train loss: 3.8426 - test loss: 18.0326 - train acc: 0.7971 - test acc: 0.4063 - 18m 41s\n",
      "batch: 1300/1563 - train loss: 3.9125 - test loss: 17.5205 - train acc: 0.7840 - test acc: 0.4214 - 18m 46s\n",
      "batch: 1400/1563 - train loss: 3.9092 - test loss: 18.1313 - train acc: 0.7894 - test acc: 0.4040 - 18m 50s\n",
      "batch: 1500/1563 - train loss: 3.8339 - test loss: 19.0440 - train acc: 0.7899 - test acc: 0.3832 - 18m 56s\n",
      "batch: 1563/1563 - train loss: 3.9691 - test loss: 18.4075 - train acc: 0.7803 - test acc: 0.4045 - 19m 0s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.3075 - test loss: 17.1923 - train acc: 0.8719 - test acc: 0.4198 - 19m 4s\n",
      "batch: 200/1563 - train loss: 2.0236 - test loss: 17.9988 - train acc: 0.8854 - test acc: 0.4140 - 19m 9s\n",
      "batch: 300/1563 - train loss: 2.2816 - test loss: 17.7782 - train acc: 0.8734 - test acc: 0.4242 - 19m 14s\n",
      "batch: 400/1563 - train loss: 2.2510 - test loss: 18.3099 - train acc: 0.8773 - test acc: 0.4155 - 19m 19s\n",
      "batch: 500/1563 - train loss: 2.4019 - test loss: 18.1364 - train acc: 0.8600 - test acc: 0.4249 - 19m 24s\n",
      "batch: 600/1563 - train loss: 2.5301 - test loss: 18.1749 - train acc: 0.8566 - test acc: 0.4259 - 19m 29s\n",
      "batch: 700/1563 - train loss: 2.6899 - test loss: 18.2382 - train acc: 0.8468 - test acc: 0.4209 - 19m 33s\n",
      "batch: 800/1563 - train loss: 2.8104 - test loss: 19.9932 - train acc: 0.8447 - test acc: 0.3994 - 19m 38s\n",
      "batch: 900/1563 - train loss: 2.8463 - test loss: 18.4722 - train acc: 0.8374 - test acc: 0.4221 - 19m 43s\n",
      "batch: 1000/1563 - train loss: 2.9613 - test loss: 18.4237 - train acc: 0.8325 - test acc: 0.4226 - 19m 48s\n",
      "batch: 1100/1563 - train loss: 3.1684 - test loss: 18.4990 - train acc: 0.8228 - test acc: 0.4270 - 19m 53s\n",
      "batch: 1200/1563 - train loss: 3.2151 - test loss: 18.7274 - train acc: 0.8221 - test acc: 0.4125 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 1208/1563 - train loss: 3.2588 - test loss: 19.8471 - train acc: 0.8212 - test acc: 0.3930 - 20m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "testing - beta: 0.9 - part size: 8 - block upd: 64 - combination nº: 7\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 8 - nº part: 1176 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 73728 - partition: 8 - nº part: 9216 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 8192 - partition: 8 - nº part: 1024 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 294912 - partition: 8 - nº part: 36864 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 32768 - partition: 8 - nº part: 4096 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 1179648 - partition: 8 - nº part: 147456 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 131072 - partition: 8 - nº part: 16384 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 51200 - partition: 8 - nº part: 6400 - block updates: 64\n",
      "FisherPartitioner: param: 100 - partition: 8 - nº part: 13 - block updates: 13\n",
      "total partitions: 1403477 - effective block updates: 2557\n",
      "initializing inverses...\n",
      "partition 1/1403477\n",
      "partition 10000/1403477\n",
      "partition 20000/1403477\n",
      "partition 30000/1403477\n",
      "partition 40000/1403477\n",
      "partition 50000/1403477\n",
      "partition 60000/1403477\n",
      "partition 70000/1403477\n",
      "partition 80000/1403477\n",
      "partition 90000/1403477\n",
      "partition 100000/1403477\n",
      "partition 110000/1403477\n",
      "partition 120000/1403477\n",
      "partition 130000/1403477\n",
      "partition 140000/1403477\n",
      "partition 150000/1403477\n",
      "partition 160000/1403477\n",
      "partition 170000/1403477\n",
      "partition 180000/1403477\n",
      "partition 190000/1403477\n",
      "partition 200000/1403477\n",
      "partition 210000/1403477\n",
      "partition 220000/1403477\n",
      "partition 230000/1403477\n",
      "partition 240000/1403477\n",
      "partition 250000/1403477\n",
      "partition 260000/1403477\n",
      "partition 270000/1403477\n",
      "partition 280000/1403477\n",
      "partition 290000/1403477\n",
      "partition 300000/1403477\n",
      "partition 310000/1403477\n",
      "partition 320000/1403477\n",
      "partition 330000/1403477\n",
      "partition 340000/1403477\n",
      "partition 350000/1403477\n",
      "partition 360000/1403477\n",
      "partition 370000/1403477\n",
      "partition 380000/1403477\n",
      "partition 390000/1403477\n",
      "partition 400000/1403477\n",
      "partition 410000/1403477\n",
      "partition 420000/1403477\n",
      "partition 430000/1403477\n",
      "partition 440000/1403477\n",
      "partition 450000/1403477\n",
      "partition 460000/1403477\n",
      "partition 470000/1403477\n",
      "partition 480000/1403477\n",
      "partition 490000/1403477\n",
      "partition 500000/1403477\n",
      "partition 510000/1403477\n",
      "partition 520000/1403477\n",
      "partition 530000/1403477\n",
      "partition 540000/1403477\n",
      "partition 550000/1403477\n",
      "partition 560000/1403477\n",
      "partition 570000/1403477\n",
      "partition 580000/1403477\n",
      "partition 590000/1403477\n",
      "partition 600000/1403477\n",
      "partition 610000/1403477\n",
      "partition 620000/1403477\n",
      "partition 630000/1403477\n",
      "partition 640000/1403477\n",
      "partition 650000/1403477\n",
      "partition 660000/1403477\n",
      "partition 670000/1403477\n",
      "partition 680000/1403477\n",
      "partition 690000/1403477\n",
      "partition 700000/1403477\n",
      "partition 710000/1403477\n",
      "partition 720000/1403477\n",
      "partition 730000/1403477\n",
      "partition 740000/1403477\n",
      "partition 750000/1403477\n",
      "partition 760000/1403477\n",
      "partition 770000/1403477\n",
      "partition 780000/1403477\n",
      "partition 790000/1403477\n",
      "partition 800000/1403477\n",
      "partition 810000/1403477\n",
      "partition 820000/1403477\n",
      "partition 830000/1403477\n",
      "partition 840000/1403477\n",
      "partition 850000/1403477\n",
      "partition 860000/1403477\n",
      "partition 870000/1403477\n",
      "partition 880000/1403477\n",
      "partition 890000/1403477\n",
      "partition 900000/1403477\n",
      "partition 910000/1403477\n",
      "partition 920000/1403477\n",
      "partition 930000/1403477\n",
      "partition 940000/1403477\n",
      "partition 950000/1403477\n",
      "partition 960000/1403477\n",
      "partition 970000/1403477\n",
      "partition 980000/1403477\n",
      "partition 990000/1403477\n",
      "partition 1000000/1403477\n",
      "partition 1010000/1403477\n",
      "partition 1020000/1403477\n",
      "partition 1030000/1403477\n",
      "partition 1040000/1403477\n",
      "partition 1050000/1403477\n",
      "partition 1060000/1403477\n",
      "partition 1070000/1403477\n",
      "partition 1080000/1403477\n",
      "partition 1090000/1403477\n",
      "partition 1100000/1403477\n",
      "partition 1110000/1403477\n",
      "partition 1120000/1403477\n",
      "partition 1130000/1403477\n",
      "partition 1140000/1403477\n",
      "partition 1150000/1403477\n",
      "partition 1160000/1403477\n",
      "partition 1170000/1403477\n",
      "partition 1180000/1403477\n",
      "partition 1190000/1403477\n",
      "partition 1200000/1403477\n",
      "partition 1210000/1403477\n",
      "partition 1220000/1403477\n",
      "partition 1230000/1403477\n",
      "partition 1240000/1403477\n",
      "partition 1250000/1403477\n",
      "partition 1260000/1403477\n",
      "partition 1270000/1403477\n",
      "partition 1280000/1403477\n",
      "partition 1290000/1403477\n",
      "partition 1300000/1403477\n",
      "partition 1310000/1403477\n",
      "partition 1320000/1403477\n",
      "partition 1330000/1403477\n",
      "partition 1340000/1403477\n",
      "partition 1350000/1403477\n",
      "partition 1360000/1403477\n",
      "partition 1370000/1403477\n",
      "partition 1380000/1403477\n",
      "partition 1390000/1403477\n",
      "partition 1400000/1403477\n",
      "partition 1403477/1403477\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1783 - test loss: 24.9393 - train acc: 0.0430 - test acc: 0.0545 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.9402 - test loss: 23.1156 - train acc: 0.0555 - test acc: 0.0787 - 0m 6s\n",
      "batch: 300/1563 - train loss: 22.7617 - test loss: 22.7548 - train acc: 0.0805 - test acc: 0.1008 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.3119 - test loss: 22.2345 - train acc: 0.0912 - test acc: 0.1010 - 0m 16s\n",
      "batch: 500/1563 - train loss: 21.6624 - test loss: 21.1703 - train acc: 0.1099 - test acc: 0.1245 - 0m 21s\n",
      "batch: 600/1563 - train loss: 21.2636 - test loss: 22.5692 - train acc: 0.1325 - test acc: 0.1168 - 0m 26s\n",
      "batch: 700/1563 - train loss: 20.9374 - test loss: 20.9980 - train acc: 0.1394 - test acc: 0.1348 - 0m 30s\n",
      "batch: 800/1563 - train loss: 20.3511 - test loss: 20.9824 - train acc: 0.1457 - test acc: 0.1337 - 0m 35s\n",
      "batch: 900/1563 - train loss: 20.2831 - test loss: 19.7154 - train acc: 0.1550 - test acc: 0.1684 - 0m 40s\n",
      "batch: 1000/1563 - train loss: 19.8767 - test loss: 19.6955 - train acc: 0.1591 - test acc: 0.1685 - 0m 44s\n",
      "batch: 1100/1563 - train loss: 19.3235 - test loss: 19.2192 - train acc: 0.1804 - test acc: 0.1909 - 0m 50s\n",
      "batch: 1200/1563 - train loss: 19.4410 - test loss: 21.1184 - train acc: 0.1772 - test acc: 0.1348 - 0m 54s\n",
      "batch: 1300/1563 - train loss: 18.9768 - test loss: 18.9956 - train acc: 0.1813 - test acc: 0.1864 - 0m 59s\n",
      "batch: 1400/1563 - train loss: 18.5442 - test loss: 18.9806 - train acc: 0.1960 - test acc: 0.1867 - 1m 4s\n",
      "batch: 1500/1563 - train loss: 18.8878 - test loss: 18.8623 - train acc: 0.1923 - test acc: 0.1983 - 1m 8s\n",
      "batch: 1563/1563 - train loss: 18.5584 - test loss: 18.4854 - train acc: 0.1994 - test acc: 0.2088 - 1m 12s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.8319 - test loss: 17.8668 - train acc: 0.2266 - test acc: 0.2311 - 1m 17s\n",
      "batch: 200/1563 - train loss: 17.7574 - test loss: 17.6116 - train acc: 0.2322 - test acc: 0.2408 - 1m 22s\n",
      "batch: 300/1563 - train loss: 17.6705 - test loss: 17.6454 - train acc: 0.2369 - test acc: 0.2356 - 1m 26s\n",
      "batch: 400/1563 - train loss: 17.3277 - test loss: 17.9462 - train acc: 0.2388 - test acc: 0.2367 - 1m 31s\n",
      "batch: 500/1563 - train loss: 17.5229 - test loss: 17.2990 - train acc: 0.2273 - test acc: 0.2479 - 1m 36s\n",
      "batch: 600/1563 - train loss: 16.8953 - test loss: 17.4996 - train acc: 0.2641 - test acc: 0.2435 - 1m 41s\n",
      "batch: 700/1563 - train loss: 17.4339 - test loss: 16.6343 - train acc: 0.2453 - test acc: 0.2731 - 1m 45s\n",
      "batch: 800/1563 - train loss: 16.8636 - test loss: 16.7785 - train acc: 0.2594 - test acc: 0.2587 - 1m 50s\n",
      "batch: 900/1563 - train loss: 16.5995 - test loss: 16.4300 - train acc: 0.2750 - test acc: 0.2821 - 1m 55s\n",
      "batch: 1000/1563 - train loss: 16.8258 - test loss: 16.4302 - train acc: 0.2459 - test acc: 0.2780 - 2m 0s\n",
      "batch: 1100/1563 - train loss: 16.4070 - test loss: 17.1718 - train acc: 0.2734 - test acc: 0.2523 - 2m 5s\n",
      "batch: 1200/1563 - train loss: 16.4164 - test loss: 16.4225 - train acc: 0.2715 - test acc: 0.2785 - 2m 9s\n",
      "batch: 1300/1563 - train loss: 16.4777 - test loss: 16.2644 - train acc: 0.2734 - test acc: 0.2788 - 2m 14s\n",
      "batch: 1400/1563 - train loss: 16.4915 - test loss: 15.9273 - train acc: 0.2547 - test acc: 0.2957 - 2m 18s\n",
      "batch: 1500/1563 - train loss: 16.2636 - test loss: 15.9612 - train acc: 0.2781 - test acc: 0.2915 - 2m 23s\n",
      "batch: 1563/1563 - train loss: 16.0716 - test loss: 15.9627 - train acc: 0.2806 - test acc: 0.2904 - 2m 27s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.7440 - test loss: 16.6132 - train acc: 0.3312 - test acc: 0.2747 - 2m 32s\n",
      "batch: 200/1563 - train loss: 15.0736 - test loss: 15.7213 - train acc: 0.3197 - test acc: 0.3006 - 2m 37s\n",
      "batch: 300/1563 - train loss: 15.3484 - test loss: 15.8863 - train acc: 0.3059 - test acc: 0.3017 - 2m 41s\n",
      "batch: 400/1563 - train loss: 14.7055 - test loss: 15.5086 - train acc: 0.3372 - test acc: 0.3113 - 2m 46s\n",
      "batch: 500/1563 - train loss: 15.1422 - test loss: 16.6823 - train acc: 0.3287 - test acc: 0.2781 - 2m 51s\n",
      "batch: 600/1563 - train loss: 14.7616 - test loss: 16.4082 - train acc: 0.3382 - test acc: 0.2891 - 2m 56s\n",
      "batch: 700/1563 - train loss: 14.7517 - test loss: 15.8341 - train acc: 0.3399 - test acc: 0.2970 - 3m 0s\n",
      "batch: 800/1563 - train loss: 14.8273 - test loss: 15.6083 - train acc: 0.3487 - test acc: 0.3128 - 3m 5s\n",
      "batch: 900/1563 - train loss: 14.8246 - test loss: 15.1758 - train acc: 0.3334 - test acc: 0.3249 - 3m 10s\n",
      "batch: 1000/1563 - train loss: 14.6031 - test loss: 15.2194 - train acc: 0.3366 - test acc: 0.3242 - 3m 15s\n",
      "batch: 1100/1563 - train loss: 15.2295 - test loss: 15.3793 - train acc: 0.3241 - test acc: 0.3166 - 3m 19s\n",
      "batch: 1200/1563 - train loss: 14.3887 - test loss: 14.7450 - train acc: 0.3606 - test acc: 0.3345 - 3m 24s\n",
      "batch: 1300/1563 - train loss: 14.7174 - test loss: 15.4971 - train acc: 0.3302 - test acc: 0.3188 - 3m 29s\n",
      "batch: 1400/1563 - train loss: 14.5690 - test loss: 15.8764 - train acc: 0.3378 - test acc: 0.3044 - 3m 34s\n",
      "batch: 1500/1563 - train loss: 14.4260 - test loss: 16.1749 - train acc: 0.3500 - test acc: 0.2932 - 3m 38s\n",
      "batch: 1563/1563 - train loss: 14.5401 - test loss: 14.8586 - train acc: 0.3446 - test acc: 0.3416 - 3m 43s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.8969 - test loss: 15.7359 - train acc: 0.3950 - test acc: 0.3121 - 3m 47s\n",
      "batch: 200/1563 - train loss: 12.9049 - test loss: 14.6430 - train acc: 0.3984 - test acc: 0.3477 - 3m 52s\n",
      "batch: 300/1563 - train loss: 12.7770 - test loss: 14.8632 - train acc: 0.4063 - test acc: 0.3436 - 3m 58s\n",
      "batch: 400/1563 - train loss: 13.1861 - test loss: 14.7089 - train acc: 0.3953 - test acc: 0.3439 - 4m 2s\n",
      "batch: 500/1563 - train loss: 13.1891 - test loss: 15.1437 - train acc: 0.3897 - test acc: 0.3371 - 4m 7s\n",
      "batch: 600/1563 - train loss: 13.2829 - test loss: 15.9367 - train acc: 0.3860 - test acc: 0.2969 - 4m 12s\n",
      "batch: 700/1563 - train loss: 13.2694 - test loss: 14.3980 - train acc: 0.3909 - test acc: 0.3562 - 4m 17s\n",
      "batch: 800/1563 - train loss: 13.1091 - test loss: 15.3919 - train acc: 0.4022 - test acc: 0.3253 - 4m 22s\n",
      "batch: 900/1563 - train loss: 13.3130 - test loss: 14.1858 - train acc: 0.3919 - test acc: 0.3634 - 4m 27s\n",
      "batch: 1000/1563 - train loss: 13.0717 - test loss: 14.6058 - train acc: 0.3869 - test acc: 0.3518 - 4m 31s\n",
      "batch: 1100/1563 - train loss: 13.4014 - test loss: 14.0874 - train acc: 0.3894 - test acc: 0.3615 - 4m 36s\n",
      "batch: 1200/1563 - train loss: 13.2193 - test loss: 14.4665 - train acc: 0.3909 - test acc: 0.3475 - 4m 41s\n",
      "batch: 1300/1563 - train loss: 13.1037 - test loss: 15.0273 - train acc: 0.3862 - test acc: 0.3423 - 4m 45s\n",
      "batch: 1400/1563 - train loss: 12.8632 - test loss: 13.7325 - train acc: 0.4016 - test acc: 0.3807 - 4m 50s\n",
      "batch: 1500/1563 - train loss: 12.7529 - test loss: 13.8744 - train acc: 0.4047 - test acc: 0.3760 - 4m 55s\n",
      "batch: 1563/1563 - train loss: 12.9635 - test loss: 14.8335 - train acc: 0.3950 - test acc: 0.3454 - 4m 59s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.5901 - test loss: 14.5966 - train acc: 0.4566 - test acc: 0.3579 - 5m 4s\n",
      "batch: 200/1563 - train loss: 11.3532 - test loss: 14.8025 - train acc: 0.4578 - test acc: 0.3475 - 5m 9s\n",
      "batch: 300/1563 - train loss: 11.5394 - test loss: 13.8648 - train acc: 0.4519 - test acc: 0.3790 - 5m 13s\n",
      "batch: 400/1563 - train loss: 11.5202 - test loss: 14.1262 - train acc: 0.4638 - test acc: 0.3698 - 5m 18s\n",
      "batch: 500/1563 - train loss: 11.6078 - test loss: 16.9643 - train acc: 0.4437 - test acc: 0.3071 - 5m 23s\n",
      "batch: 600/1563 - train loss: 11.9113 - test loss: 13.8250 - train acc: 0.4381 - test acc: 0.3792 - 5m 27s\n",
      "batch: 700/1563 - train loss: 12.2266 - test loss: 14.0773 - train acc: 0.4269 - test acc: 0.3769 - 5m 33s\n",
      "batch: 800/1563 - train loss: 11.9945 - test loss: 14.4106 - train acc: 0.4366 - test acc: 0.3620 - 5m 37s\n",
      "batch: 900/1563 - train loss: 11.9704 - test loss: 13.6196 - train acc: 0.4288 - test acc: 0.3913 - 5m 42s\n",
      "batch: 1000/1563 - train loss: 11.8584 - test loss: 13.8129 - train acc: 0.4401 - test acc: 0.3827 - 5m 47s\n",
      "batch: 1100/1563 - train loss: 11.8350 - test loss: 13.8133 - train acc: 0.4547 - test acc: 0.3837 - 5m 51s\n",
      "batch: 1200/1563 - train loss: 11.9243 - test loss: 13.7482 - train acc: 0.4379 - test acc: 0.3804 - 5m 56s\n",
      "batch: 1300/1563 - train loss: 12.0074 - test loss: 14.1196 - train acc: 0.4344 - test acc: 0.3653 - 6m 1s\n",
      "batch: 1400/1563 - train loss: 11.6997 - test loss: 13.5478 - train acc: 0.4534 - test acc: 0.3879 - 6m 6s\n",
      "batch: 1500/1563 - train loss: 11.8484 - test loss: 13.7692 - train acc: 0.4406 - test acc: 0.3819 - 6m 10s\n",
      "batch: 1563/1563 - train loss: 12.0175 - test loss: 14.1671 - train acc: 0.4425 - test acc: 0.3732 - 6m 14s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.9499 - test loss: 13.7803 - train acc: 0.5175 - test acc: 0.3885 - 6m 19s\n",
      "batch: 200/1563 - train loss: 9.9295 - test loss: 14.1431 - train acc: 0.5194 - test acc: 0.3783 - 6m 23s\n",
      "batch: 300/1563 - train loss: 10.2388 - test loss: 14.0197 - train acc: 0.5081 - test acc: 0.3879 - 6m 28s\n",
      "batch: 400/1563 - train loss: 10.2600 - test loss: 13.7675 - train acc: 0.5106 - test acc: 0.3978 - 6m 33s\n",
      "batch: 500/1563 - train loss: 10.5174 - test loss: 13.7547 - train acc: 0.4912 - test acc: 0.3833 - 6m 38s\n",
      "batch: 600/1563 - train loss: 10.5243 - test loss: 13.8843 - train acc: 0.4929 - test acc: 0.3883 - 6m 43s\n",
      "batch: 700/1563 - train loss: 10.5725 - test loss: 13.6484 - train acc: 0.4888 - test acc: 0.3899 - 6m 47s\n",
      "batch: 800/1563 - train loss: 10.7282 - test loss: 13.6479 - train acc: 0.4728 - test acc: 0.3998 - 6m 52s\n",
      "batch: 900/1563 - train loss: 10.9498 - test loss: 13.7813 - train acc: 0.4663 - test acc: 0.3871 - 6m 57s\n",
      "batch: 1000/1563 - train loss: 10.7487 - test loss: 13.7945 - train acc: 0.4822 - test acc: 0.3889 - 7m 2s\n",
      "batch: 1100/1563 - train loss: 10.8572 - test loss: 13.9937 - train acc: 0.4778 - test acc: 0.3831 - 7m 7s\n",
      "batch: 1200/1563 - train loss: 10.5895 - test loss: 13.9123 - train acc: 0.4887 - test acc: 0.3910 - 7m 11s\n",
      "batch: 1300/1563 - train loss: 11.0698 - test loss: 14.5279 - train acc: 0.4706 - test acc: 0.3684 - 7m 16s\n",
      "batch: 1400/1563 - train loss: 10.8050 - test loss: 13.2922 - train acc: 0.4800 - test acc: 0.4019 - 7m 21s\n",
      "batch: 1500/1563 - train loss: 10.6998 - test loss: 13.5155 - train acc: 0.4921 - test acc: 0.4008 - 7m 25s\n",
      "batch: 1563/1563 - train loss: 10.6558 - test loss: 13.2799 - train acc: 0.4937 - test acc: 0.4072 - 7m 29s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.5430 - test loss: 13.6392 - train acc: 0.5919 - test acc: 0.4025 - 7m 34s\n",
      "batch: 200/1563 - train loss: 8.5270 - test loss: 13.6320 - train acc: 0.5806 - test acc: 0.4035 - 7m 39s\n",
      "batch: 300/1563 - train loss: 8.9928 - test loss: 14.8806 - train acc: 0.5491 - test acc: 0.3817 - 7m 44s\n",
      "batch: 400/1563 - train loss: 8.9062 - test loss: 14.1477 - train acc: 0.5575 - test acc: 0.3896 - 7m 48s\n",
      "batch: 500/1563 - train loss: 9.2545 - test loss: 13.8968 - train acc: 0.5525 - test acc: 0.3960 - 7m 53s\n",
      "batch: 600/1563 - train loss: 9.2104 - test loss: 13.6499 - train acc: 0.5531 - test acc: 0.4061 - 7m 58s\n",
      "batch: 700/1563 - train loss: 9.4728 - test loss: 14.0496 - train acc: 0.5340 - test acc: 0.3964 - 8m 2s\n",
      "batch: 800/1563 - train loss: 9.4903 - test loss: 13.7112 - train acc: 0.5205 - test acc: 0.3996 - 8m 7s\n",
      "batch: 900/1563 - train loss: 9.6728 - test loss: 14.1546 - train acc: 0.5360 - test acc: 0.3870 - 8m 12s\n",
      "batch: 1000/1563 - train loss: 9.6790 - test loss: 13.6227 - train acc: 0.5365 - test acc: 0.4066 - 8m 17s\n",
      "batch: 1100/1563 - train loss: 9.8175 - test loss: 14.6837 - train acc: 0.5184 - test acc: 0.3656 - 8m 21s\n",
      "batch: 1200/1563 - train loss: 9.7337 - test loss: 13.9052 - train acc: 0.5287 - test acc: 0.4011 - 8m 26s\n",
      "batch: 1300/1563 - train loss: 10.1479 - test loss: 13.0889 - train acc: 0.5025 - test acc: 0.4221 - 8m 31s\n",
      "batch: 1400/1563 - train loss: 9.9960 - test loss: 13.9539 - train acc: 0.5184 - test acc: 0.3862 - 8m 35s\n",
      "batch: 1500/1563 - train loss: 9.8458 - test loss: 13.5212 - train acc: 0.5213 - test acc: 0.4142 - 8m 41s\n",
      "batch: 1563/1563 - train loss: 9.4752 - test loss: 13.5885 - train acc: 0.5421 - test acc: 0.4031 - 8m 45s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.4972 - test loss: 13.8351 - train acc: 0.6257 - test acc: 0.4082 - 8m 49s\n",
      "batch: 200/1563 - train loss: 7.6046 - test loss: 13.8347 - train acc: 0.6112 - test acc: 0.4108 - 8m 54s\n",
      "batch: 300/1563 - train loss: 7.9251 - test loss: 14.2514 - train acc: 0.6049 - test acc: 0.4026 - 8m 59s\n",
      "batch: 400/1563 - train loss: 8.1639 - test loss: 14.1523 - train acc: 0.5953 - test acc: 0.4064 - 9m 3s\n",
      "batch: 500/1563 - train loss: 8.0599 - test loss: 13.8027 - train acc: 0.5944 - test acc: 0.4102 - 9m 8s\n",
      "batch: 600/1563 - train loss: 8.0874 - test loss: 14.0086 - train acc: 0.5928 - test acc: 0.4051 - 9m 13s\n",
      "batch: 700/1563 - train loss: 8.3508 - test loss: 14.2527 - train acc: 0.5913 - test acc: 0.4000 - 9m 18s\n",
      "batch: 800/1563 - train loss: 8.5770 - test loss: 13.6641 - train acc: 0.5793 - test acc: 0.4137 - 9m 23s\n",
      "batch: 900/1563 - train loss: 8.3861 - test loss: 13.6130 - train acc: 0.5872 - test acc: 0.4136 - 9m 27s\n",
      "batch: 1000/1563 - train loss: 8.6474 - test loss: 14.0810 - train acc: 0.5597 - test acc: 0.3947 - 9m 32s\n",
      "batch: 1100/1563 - train loss: 8.7204 - test loss: 13.7439 - train acc: 0.5700 - test acc: 0.4106 - 9m 37s\n",
      "batch: 1200/1563 - train loss: 8.6587 - test loss: 14.3771 - train acc: 0.5671 - test acc: 0.4017 - 9m 42s\n",
      "batch: 1300/1563 - train loss: 8.9985 - test loss: 14.1017 - train acc: 0.5609 - test acc: 0.4005 - 9m 46s\n",
      "batch: 1400/1563 - train loss: 8.8229 - test loss: 13.6094 - train acc: 0.5675 - test acc: 0.4171 - 9m 51s\n",
      "batch: 1500/1563 - train loss: 8.7206 - test loss: 13.4085 - train acc: 0.5777 - test acc: 0.4257 - 9m 56s\n",
      "batch: 1563/1563 - train loss: 8.8584 - test loss: 13.6750 - train acc: 0.5609 - test acc: 0.4129 - 10m 0s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.5320 - test loss: 13.6295 - train acc: 0.6713 - test acc: 0.4264 - 10m 4s\n",
      "batch: 200/1563 - train loss: 6.3247 - test loss: 14.0197 - train acc: 0.6741 - test acc: 0.4169 - 10m 9s\n",
      "batch: 300/1563 - train loss: 6.6037 - test loss: 14.3358 - train acc: 0.6497 - test acc: 0.4077 - 10m 14s\n",
      "batch: 400/1563 - train loss: 6.8682 - test loss: 14.6734 - train acc: 0.6532 - test acc: 0.4003 - 10m 19s\n",
      "batch: 500/1563 - train loss: 6.9494 - test loss: 14.4814 - train acc: 0.6450 - test acc: 0.4082 - 10m 23s\n",
      "batch: 600/1563 - train loss: 6.9278 - test loss: 14.6794 - train acc: 0.6422 - test acc: 0.4058 - 10m 28s\n",
      "batch: 700/1563 - train loss: 7.5556 - test loss: 14.8353 - train acc: 0.6259 - test acc: 0.3989 - 10m 33s\n",
      "batch: 800/1563 - train loss: 7.5121 - test loss: 14.4385 - train acc: 0.6309 - test acc: 0.4067 - 10m 37s\n",
      "batch: 900/1563 - train loss: 7.7026 - test loss: 14.3512 - train acc: 0.6190 - test acc: 0.4081 - 10m 42s\n",
      "batch: 1000/1563 - train loss: 7.8943 - test loss: 14.2025 - train acc: 0.6026 - test acc: 0.4121 - 10m 47s\n",
      "batch: 1100/1563 - train loss: 7.9892 - test loss: 14.0788 - train acc: 0.5918 - test acc: 0.4178 - 10m 52s\n",
      "batch: 1200/1563 - train loss: 7.7987 - test loss: 13.8930 - train acc: 0.6015 - test acc: 0.4176 - 10m 57s\n",
      "batch: 1300/1563 - train loss: 7.6961 - test loss: 13.8787 - train acc: 0.6109 - test acc: 0.4217 - 11m 1s\n",
      "batch: 1400/1563 - train loss: 8.1192 - test loss: 14.4091 - train acc: 0.5968 - test acc: 0.4033 - 11m 6s\n",
      "batch: 1500/1563 - train loss: 8.1656 - test loss: 13.8611 - train acc: 0.5862 - test acc: 0.4155 - 11m 11s\n",
      "batch: 1563/1563 - train loss: 7.9373 - test loss: 13.8770 - train acc: 0.6028 - test acc: 0.4229 - 11m 15s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.3691 - test loss: 13.9329 - train acc: 0.7231 - test acc: 0.4278 - 11m 20s\n",
      "batch: 200/1563 - train loss: 5.2009 - test loss: 14.7308 - train acc: 0.7316 - test acc: 0.4168 - 11m 25s\n",
      "batch: 300/1563 - train loss: 5.2504 - test loss: 14.3977 - train acc: 0.7262 - test acc: 0.4234 - 11m 29s\n",
      "batch: 400/1563 - train loss: 5.8647 - test loss: 15.3153 - train acc: 0.7028 - test acc: 0.3982 - 11m 34s\n",
      "batch: 500/1563 - train loss: 5.8323 - test loss: 15.0256 - train acc: 0.6935 - test acc: 0.4145 - 11m 39s\n",
      "batch: 600/1563 - train loss: 6.4295 - test loss: 15.1616 - train acc: 0.6662 - test acc: 0.4007 - 11m 43s\n",
      "batch: 700/1563 - train loss: 6.5518 - test loss: 14.6154 - train acc: 0.6625 - test acc: 0.4165 - 11m 49s\n",
      "batch: 800/1563 - train loss: 6.3275 - test loss: 14.9276 - train acc: 0.6710 - test acc: 0.4059 - 11m 53s\n",
      "batch: 900/1563 - train loss: 6.5895 - test loss: 14.7263 - train acc: 0.6656 - test acc: 0.4191 - 11m 58s\n",
      "batch: 1000/1563 - train loss: 6.9789 - test loss: 14.7120 - train acc: 0.6400 - test acc: 0.4122 - 12m 3s\n",
      "batch: 1100/1563 - train loss: 6.7994 - test loss: 14.8100 - train acc: 0.6485 - test acc: 0.4041 - 12m 7s\n",
      "batch: 1200/1563 - train loss: 7.1018 - test loss: 15.6721 - train acc: 0.6378 - test acc: 0.3968 - 12m 12s\n",
      "batch: 1300/1563 - train loss: 7.3341 - test loss: 14.9507 - train acc: 0.6290 - test acc: 0.3985 - 12m 17s\n",
      "batch: 1400/1563 - train loss: 6.9287 - test loss: 14.7133 - train acc: 0.6528 - test acc: 0.4135 - 12m 22s\n",
      "batch: 1500/1563 - train loss: 7.0845 - test loss: 15.5068 - train acc: 0.6281 - test acc: 0.3806 - 12m 27s\n",
      "batch: 1563/1563 - train loss: 7.1378 - test loss: 14.9347 - train acc: 0.6351 - test acc: 0.4032 - 12m 31s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.4457 - test loss: 14.7924 - train acc: 0.7609 - test acc: 0.4196 - 12m 36s\n",
      "batch: 200/1563 - train loss: 4.4759 - test loss: 14.7378 - train acc: 0.7560 - test acc: 0.4239 - 12m 40s\n",
      "batch: 300/1563 - train loss: 4.3057 - test loss: 15.5961 - train acc: 0.7737 - test acc: 0.4114 - 12m 45s\n",
      "batch: 400/1563 - train loss: 4.7601 - test loss: 15.6928 - train acc: 0.7391 - test acc: 0.4112 - 12m 50s\n",
      "batch: 500/1563 - train loss: 5.2766 - test loss: 15.5790 - train acc: 0.7238 - test acc: 0.4079 - 12m 55s\n",
      "batch: 600/1563 - train loss: 5.4742 - test loss: 15.6672 - train acc: 0.7122 - test acc: 0.4058 - 12m 59s\n",
      "batch: 700/1563 - train loss: 5.3680 - test loss: 16.1253 - train acc: 0.7166 - test acc: 0.3992 - 13m 4s\n",
      "batch: 800/1563 - train loss: 5.5254 - test loss: 15.6928 - train acc: 0.7013 - test acc: 0.4044 - 13m 9s\n",
      "batch: 900/1563 - train loss: 5.8982 - test loss: 15.2216 - train acc: 0.6960 - test acc: 0.4134 - 13m 14s\n",
      "batch: 1000/1563 - train loss: 5.8549 - test loss: 15.2456 - train acc: 0.6935 - test acc: 0.4203 - 13m 18s\n",
      "batch: 1100/1563 - train loss: 5.8418 - test loss: 15.3316 - train acc: 0.6866 - test acc: 0.4190 - 13m 23s\n",
      "batch: 1200/1563 - train loss: 6.1014 - test loss: 15.3049 - train acc: 0.6870 - test acc: 0.4130 - 13m 28s\n",
      "batch: 1300/1563 - train loss: 6.1906 - test loss: 15.2202 - train acc: 0.6691 - test acc: 0.4168 - 13m 33s\n",
      "batch: 1400/1563 - train loss: 6.3761 - test loss: 15.3764 - train acc: 0.6601 - test acc: 0.4045 - 13m 37s\n",
      "batch: 1500/1563 - train loss: 6.4834 - test loss: 15.6410 - train acc: 0.6694 - test acc: 0.4065 - 13m 42s\n",
      "batch: 1563/1563 - train loss: 6.5511 - test loss: 15.7802 - train acc: 0.6644 - test acc: 0.4032 - 13m 46s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.9955 - test loss: 15.2142 - train acc: 0.7906 - test acc: 0.4233 - 13m 51s\n",
      "batch: 200/1563 - train loss: 3.9399 - test loss: 15.5574 - train acc: 0.7806 - test acc: 0.4210 - 13m 56s\n",
      "batch: 300/1563 - train loss: 4.0112 - test loss: 15.6742 - train acc: 0.7815 - test acc: 0.4190 - 14m 1s\n",
      "batch: 400/1563 - train loss: 4.1572 - test loss: 16.3075 - train acc: 0.7766 - test acc: 0.4098 - 14m 5s\n",
      "batch: 500/1563 - train loss: 4.3380 - test loss: 16.1398 - train acc: 0.7693 - test acc: 0.4126 - 14m 10s\n",
      "batch: 600/1563 - train loss: 4.3947 - test loss: 16.0847 - train acc: 0.7572 - test acc: 0.4135 - 14m 15s\n",
      "batch: 700/1563 - train loss: 4.4260 - test loss: 16.3654 - train acc: 0.7659 - test acc: 0.4064 - 14m 20s\n",
      "batch: 800/1563 - train loss: 4.7943 - test loss: 16.3067 - train acc: 0.7434 - test acc: 0.4111 - 14m 24s\n",
      "batch: 900/1563 - train loss: 4.9545 - test loss: 16.2081 - train acc: 0.7366 - test acc: 0.4205 - 14m 29s\n",
      "batch: 1000/1563 - train loss: 5.1313 - test loss: 16.2801 - train acc: 0.7144 - test acc: 0.4088 - 14m 34s\n",
      "batch: 1100/1563 - train loss: 5.4109 - test loss: 15.8813 - train acc: 0.7003 - test acc: 0.4161 - 14m 38s\n",
      "batch: 1200/1563 - train loss: 5.1853 - test loss: 16.7127 - train acc: 0.7241 - test acc: 0.3962 - 14m 43s\n",
      "batch: 1300/1563 - train loss: 5.5842 - test loss: 16.3085 - train acc: 0.7128 - test acc: 0.4068 - 14m 48s\n",
      "batch: 1400/1563 - train loss: 5.7819 - test loss: 16.0103 - train acc: 0.6907 - test acc: 0.4110 - 14m 53s\n",
      "batch: 1500/1563 - train loss: 5.5725 - test loss: 15.6590 - train acc: 0.7001 - test acc: 0.4159 - 14m 58s\n",
      "batch: 1563/1563 - train loss: 5.5136 - test loss: 15.6334 - train acc: 0.7091 - test acc: 0.4208 - 15m 2s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.3592 - test loss: 15.5973 - train acc: 0.8200 - test acc: 0.4245 - 15m 6s\n",
      "batch: 200/1563 - train loss: 3.1550 - test loss: 15.6881 - train acc: 0.8337 - test acc: 0.4277 - 15m 11s\n",
      "batch: 300/1563 - train loss: 3.1845 - test loss: 16.4439 - train acc: 0.8321 - test acc: 0.4132 - 15m 16s\n",
      "batch: 400/1563 - train loss: 3.5139 - test loss: 16.6035 - train acc: 0.8062 - test acc: 0.4183 - 15m 21s\n",
      "batch: 500/1563 - train loss: 3.5131 - test loss: 16.4856 - train acc: 0.8012 - test acc: 0.4237 - 15m 25s\n",
      "batch: 600/1563 - train loss: 3.8402 - test loss: 17.4993 - train acc: 0.7943 - test acc: 0.4101 - 15m 30s\n",
      "batch: 700/1563 - train loss: 4.1560 - test loss: 16.5139 - train acc: 0.7756 - test acc: 0.4185 - 15m 35s\n",
      "batch: 800/1563 - train loss: 4.1759 - test loss: 17.2925 - train acc: 0.7728 - test acc: 0.4029 - 15m 40s\n",
      "batch: 900/1563 - train loss: 4.3928 - test loss: 16.7595 - train acc: 0.7562 - test acc: 0.4139 - 15m 44s\n",
      "batch: 1000/1563 - train loss: 4.6817 - test loss: 16.4766 - train acc: 0.7425 - test acc: 0.4145 - 15m 49s\n",
      "batch: 1100/1563 - train loss: 4.8149 - test loss: 17.1257 - train acc: 0.7434 - test acc: 0.3999 - 15m 54s\n",
      "batch: 1200/1563 - train loss: 5.0043 - test loss: 16.6831 - train acc: 0.7291 - test acc: 0.4065 - 15m 59s\n",
      "batch: 1300/1563 - train loss: 4.7950 - test loss: 16.4916 - train acc: 0.7463 - test acc: 0.4088 - 16m 4s\n",
      "batch: 1400/1563 - train loss: 4.8825 - test loss: 17.9587 - train acc: 0.7268 - test acc: 0.3914 - 16m 9s\n",
      "batch: 1500/1563 - train loss: 5.0147 - test loss: 16.4989 - train acc: 0.7300 - test acc: 0.4078 - 16m 13s\n",
      "batch: 1563/1563 - train loss: 5.1375 - test loss: 16.5694 - train acc: 0.7234 - test acc: 0.4065 - 16m 17s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8636 - test loss: 16.1805 - train acc: 0.8418 - test acc: 0.4254 - 16m 22s\n",
      "batch: 200/1563 - train loss: 2.7785 - test loss: 16.9750 - train acc: 0.8440 - test acc: 0.4246 - 16m 27s\n",
      "batch: 300/1563 - train loss: 2.7767 - test loss: 16.8056 - train acc: 0.8456 - test acc: 0.4214 - 16m 32s\n",
      "batch: 400/1563 - train loss: 2.9772 - test loss: 17.1215 - train acc: 0.8387 - test acc: 0.4169 - 16m 37s\n",
      "batch: 500/1563 - train loss: 3.1071 - test loss: 16.8804 - train acc: 0.8268 - test acc: 0.4198 - 16m 41s\n",
      "batch: 600/1563 - train loss: 3.2124 - test loss: 17.0788 - train acc: 0.8187 - test acc: 0.4234 - 16m 46s\n",
      "batch: 700/1563 - train loss: 3.3796 - test loss: 17.9191 - train acc: 0.8159 - test acc: 0.4098 - 16m 51s\n",
      "batch: 800/1563 - train loss: 3.4702 - test loss: 17.1866 - train acc: 0.8121 - test acc: 0.4171 - 16m 55s\n",
      "batch: 900/1563 - train loss: 3.6545 - test loss: 17.4244 - train acc: 0.7937 - test acc: 0.4186 - 17m 0s\n",
      "batch: 1000/1563 - train loss: 3.8226 - test loss: 17.5374 - train acc: 0.7893 - test acc: 0.4164 - 17m 5s\n",
      "batch: 1100/1563 - train loss: 3.6778 - test loss: 17.4963 - train acc: 0.7900 - test acc: 0.4126 - 17m 10s\n",
      "batch: 1200/1563 - train loss: 3.9639 - test loss: 17.3187 - train acc: 0.7784 - test acc: 0.4184 - 17m 15s\n",
      "batch: 1300/1563 - train loss: 3.9493 - test loss: 17.3050 - train acc: 0.7859 - test acc: 0.4160 - 17m 19s\n",
      "batch: 1400/1563 - train loss: 4.3291 - test loss: 17.4625 - train acc: 0.7634 - test acc: 0.4171 - 17m 24s\n",
      "batch: 1500/1563 - train loss: 4.2637 - test loss: 17.5752 - train acc: 0.7659 - test acc: 0.4127 - 17m 29s\n",
      "batch: 1563/1563 - train loss: 4.4060 - test loss: 17.7072 - train acc: 0.7650 - test acc: 0.4086 - 17m 33s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.6618 - test loss: 16.9705 - train acc: 0.8512 - test acc: 0.4260 - 17m 38s\n",
      "batch: 200/1563 - train loss: 2.4188 - test loss: 17.3139 - train acc: 0.8719 - test acc: 0.4250 - 17m 42s\n",
      "batch: 300/1563 - train loss: 2.5909 - test loss: 17.8754 - train acc: 0.8540 - test acc: 0.4156 - 17m 47s\n",
      "batch: 400/1563 - train loss: 2.3843 - test loss: 17.7117 - train acc: 0.8619 - test acc: 0.4207 - 17m 52s\n",
      "batch: 500/1563 - train loss: 2.6635 - test loss: 17.9846 - train acc: 0.8477 - test acc: 0.4179 - 17m 57s\n",
      "batch: 600/1563 - train loss: 2.9643 - test loss: 18.0977 - train acc: 0.8337 - test acc: 0.4127 - 18m 2s\n",
      "batch: 700/1563 - train loss: 3.1795 - test loss: 18.9564 - train acc: 0.8203 - test acc: 0.4056 - 18m 7s\n",
      "batch: 800/1563 - train loss: 3.1400 - test loss: 18.0339 - train acc: 0.8174 - test acc: 0.4178 - 18m 12s\n",
      "batch: 900/1563 - train loss: 3.0543 - test loss: 18.0069 - train acc: 0.8211 - test acc: 0.4175 - 18m 16s\n",
      "batch: 1000/1563 - train loss: 3.2465 - test loss: 17.9038 - train acc: 0.8159 - test acc: 0.4184 - 18m 21s\n",
      "batch: 1100/1563 - train loss: 3.2519 - test loss: 18.6357 - train acc: 0.8119 - test acc: 0.4081 - 18m 26s\n",
      "batch: 1200/1563 - train loss: 3.4574 - test loss: 18.4522 - train acc: 0.8081 - test acc: 0.4094 - 18m 30s\n",
      "batch: 1300/1563 - train loss: 3.5715 - test loss: 18.3333 - train acc: 0.7968 - test acc: 0.4053 - 18m 35s\n",
      "batch: 1400/1563 - train loss: 3.5745 - test loss: 17.8148 - train acc: 0.8000 - test acc: 0.4116 - 18m 40s\n",
      "batch: 1500/1563 - train loss: 3.7684 - test loss: 17.6274 - train acc: 0.7871 - test acc: 0.4219 - 18m 45s\n",
      "batch: 1563/1563 - train loss: 3.8684 - test loss: 18.0061 - train acc: 0.7800 - test acc: 0.4169 - 18m 49s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.1152 - test loss: 18.1292 - train acc: 0.8798 - test acc: 0.4163 - 18m 54s\n",
      "batch: 200/1563 - train loss: 2.0491 - test loss: 18.1836 - train acc: 0.8872 - test acc: 0.4270 - 18m 58s\n",
      "batch: 300/1563 - train loss: 2.1043 - test loss: 18.1161 - train acc: 0.8885 - test acc: 0.4168 - 19m 3s\n",
      "batch: 400/1563 - train loss: 2.0084 - test loss: 18.7243 - train acc: 0.8825 - test acc: 0.4169 - 19m 8s\n",
      "batch: 500/1563 - train loss: 2.0967 - test loss: 18.5613 - train acc: 0.8784 - test acc: 0.4209 - 19m 13s\n",
      "batch: 600/1563 - train loss: 2.1987 - test loss: 19.2128 - train acc: 0.8754 - test acc: 0.4120 - 19m 18s\n",
      "batch: 700/1563 - train loss: 2.4171 - test loss: 18.5767 - train acc: 0.8609 - test acc: 0.4212 - 19m 23s\n",
      "batch: 800/1563 - train loss: 2.4663 - test loss: 18.8697 - train acc: 0.8525 - test acc: 0.4111 - 19m 27s\n",
      "batch: 900/1563 - train loss: 2.6774 - test loss: 18.9006 - train acc: 0.8480 - test acc: 0.4211 - 19m 32s\n",
      "batch: 1000/1563 - train loss: 2.6912 - test loss: 19.1509 - train acc: 0.8453 - test acc: 0.4172 - 19m 37s\n",
      "batch: 1100/1563 - train loss: 2.8331 - test loss: 18.7844 - train acc: 0.8318 - test acc: 0.4166 - 19m 42s\n",
      "batch: 1200/1563 - train loss: 3.0553 - test loss: 19.1336 - train acc: 0.8234 - test acc: 0.4109 - 19m 47s\n",
      "batch: 1300/1563 - train loss: 3.1049 - test loss: 18.5166 - train acc: 0.8234 - test acc: 0.4276 - 19m 52s\n",
      "batch: 1400/1563 - train loss: 3.1757 - test loss: 18.7771 - train acc: 0.8178 - test acc: 0.4134 - 19m 56s\n",
      "time is up! finishing training\n",
      "batch: 1473/1563 - train loss: 3.2635 - test loss: 18.8971 - train acc: 0.8084 - test acc: 0.4105 - 20m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "testing - beta: 0.9 - part size: 8 - block upd: 64 - combination nº: 8\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 8 - nº part: 1176 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 73728 - partition: 8 - nº part: 9216 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 8192 - partition: 8 - nº part: 1024 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 294912 - partition: 8 - nº part: 36864 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 32768 - partition: 8 - nº part: 4096 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 1179648 - partition: 8 - nº part: 147456 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 131072 - partition: 8 - nº part: 16384 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 51200 - partition: 8 - nº part: 6400 - block updates: 64\n",
      "FisherPartitioner: param: 100 - partition: 8 - nº part: 13 - block updates: 13\n",
      "total partitions: 1403477 - effective block updates: 2557\n",
      "initializing inverses...\n",
      "partition 1/1403477\n",
      "partition 10000/1403477\n",
      "partition 20000/1403477\n",
      "partition 30000/1403477\n",
      "partition 40000/1403477\n",
      "partition 50000/1403477\n",
      "partition 60000/1403477\n",
      "partition 70000/1403477\n",
      "partition 80000/1403477\n",
      "partition 90000/1403477\n",
      "partition 100000/1403477\n",
      "partition 110000/1403477\n",
      "partition 120000/1403477\n",
      "partition 130000/1403477\n",
      "partition 140000/1403477\n",
      "partition 150000/1403477\n",
      "partition 160000/1403477\n",
      "partition 170000/1403477\n",
      "partition 180000/1403477\n",
      "partition 190000/1403477\n",
      "partition 200000/1403477\n",
      "partition 210000/1403477\n",
      "partition 220000/1403477\n",
      "partition 230000/1403477\n",
      "partition 240000/1403477\n",
      "partition 250000/1403477\n",
      "partition 260000/1403477\n",
      "partition 270000/1403477\n",
      "partition 280000/1403477\n",
      "partition 290000/1403477\n",
      "partition 300000/1403477\n",
      "partition 310000/1403477\n",
      "partition 320000/1403477\n",
      "partition 330000/1403477\n",
      "partition 340000/1403477\n",
      "partition 350000/1403477\n",
      "partition 360000/1403477\n",
      "partition 370000/1403477\n",
      "partition 380000/1403477\n",
      "partition 390000/1403477\n",
      "partition 400000/1403477\n",
      "partition 410000/1403477\n",
      "partition 420000/1403477\n",
      "partition 430000/1403477\n",
      "partition 440000/1403477\n",
      "partition 450000/1403477\n",
      "partition 460000/1403477\n",
      "partition 470000/1403477\n",
      "partition 480000/1403477\n",
      "partition 490000/1403477\n",
      "partition 500000/1403477\n",
      "partition 510000/1403477\n",
      "partition 520000/1403477\n",
      "partition 530000/1403477\n",
      "partition 540000/1403477\n",
      "partition 550000/1403477\n",
      "partition 560000/1403477\n",
      "partition 570000/1403477\n",
      "partition 580000/1403477\n",
      "partition 590000/1403477\n",
      "partition 600000/1403477\n",
      "partition 610000/1403477\n",
      "partition 620000/1403477\n",
      "partition 630000/1403477\n",
      "partition 640000/1403477\n",
      "partition 650000/1403477\n",
      "partition 660000/1403477\n",
      "partition 670000/1403477\n",
      "partition 680000/1403477\n",
      "partition 690000/1403477\n",
      "partition 700000/1403477\n",
      "partition 710000/1403477\n",
      "partition 720000/1403477\n",
      "partition 730000/1403477\n",
      "partition 740000/1403477\n",
      "partition 750000/1403477\n",
      "partition 760000/1403477\n",
      "partition 770000/1403477\n",
      "partition 780000/1403477\n",
      "partition 790000/1403477\n",
      "partition 800000/1403477\n",
      "partition 810000/1403477\n",
      "partition 820000/1403477\n",
      "partition 830000/1403477\n",
      "partition 840000/1403477\n",
      "partition 850000/1403477\n",
      "partition 860000/1403477\n",
      "partition 870000/1403477\n",
      "partition 880000/1403477\n",
      "partition 890000/1403477\n",
      "partition 900000/1403477\n",
      "partition 910000/1403477\n",
      "partition 920000/1403477\n",
      "partition 930000/1403477\n",
      "partition 940000/1403477\n",
      "partition 950000/1403477\n",
      "partition 960000/1403477\n",
      "partition 970000/1403477\n",
      "partition 980000/1403477\n",
      "partition 990000/1403477\n",
      "partition 1000000/1403477\n",
      "partition 1010000/1403477\n",
      "partition 1020000/1403477\n",
      "partition 1030000/1403477\n",
      "partition 1040000/1403477\n",
      "partition 1050000/1403477\n",
      "partition 1060000/1403477\n",
      "partition 1070000/1403477\n",
      "partition 1080000/1403477\n",
      "partition 1090000/1403477\n",
      "partition 1100000/1403477\n",
      "partition 1110000/1403477\n",
      "partition 1120000/1403477\n",
      "partition 1130000/1403477\n",
      "partition 1140000/1403477\n",
      "partition 1150000/1403477\n",
      "partition 1160000/1403477\n",
      "partition 1170000/1403477\n",
      "partition 1180000/1403477\n",
      "partition 1190000/1403477\n",
      "partition 1200000/1403477\n",
      "partition 1210000/1403477\n",
      "partition 1220000/1403477\n",
      "partition 1230000/1403477\n",
      "partition 1240000/1403477\n",
      "partition 1250000/1403477\n",
      "partition 1260000/1403477\n",
      "partition 1270000/1403477\n",
      "partition 1280000/1403477\n",
      "partition 1290000/1403477\n",
      "partition 1300000/1403477\n",
      "partition 1310000/1403477\n",
      "partition 1320000/1403477\n",
      "partition 1330000/1403477\n",
      "partition 1340000/1403477\n",
      "partition 1350000/1403477\n",
      "partition 1360000/1403477\n",
      "partition 1370000/1403477\n",
      "partition 1380000/1403477\n",
      "partition 1390000/1403477\n",
      "partition 1400000/1403477\n",
      "partition 1403477/1403477\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1335 - test loss: 24.6450 - train acc: 0.0407 - test acc: 0.0482 - 0m 2s\n",
      "batch: 200/1563 - train loss: 24.1755 - test loss: 22.9209 - train acc: 0.0570 - test acc: 0.0847 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.3172 - test loss: 22.5935 - train acc: 0.0698 - test acc: 0.0838 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.4515 - test loss: 23.2439 - train acc: 0.0893 - test acc: 0.0844 - 0m 15s\n",
      "batch: 500/1563 - train loss: 22.0042 - test loss: 21.7722 - train acc: 0.1165 - test acc: 0.1098 - 0m 20s\n",
      "batch: 600/1563 - train loss: 21.6247 - test loss: 21.0055 - train acc: 0.1128 - test acc: 0.1304 - 0m 25s\n",
      "batch: 700/1563 - train loss: 20.7451 - test loss: 20.7460 - train acc: 0.1368 - test acc: 0.1396 - 0m 30s\n",
      "batch: 800/1563 - train loss: 20.7961 - test loss: 20.8313 - train acc: 0.1343 - test acc: 0.1414 - 0m 35s\n",
      "batch: 900/1563 - train loss: 20.2482 - test loss: 20.9886 - train acc: 0.1453 - test acc: 0.1373 - 0m 39s\n",
      "batch: 1000/1563 - train loss: 19.9307 - test loss: 19.7375 - train acc: 0.1591 - test acc: 0.1692 - 0m 44s\n",
      "batch: 1100/1563 - train loss: 19.4041 - test loss: 19.4855 - train acc: 0.1792 - test acc: 0.1752 - 0m 48s\n",
      "batch: 1200/1563 - train loss: 19.2409 - test loss: 19.0205 - train acc: 0.1913 - test acc: 0.1918 - 0m 53s\n",
      "batch: 1300/1563 - train loss: 18.9408 - test loss: 19.0156 - train acc: 0.1910 - test acc: 0.1937 - 0m 58s\n",
      "batch: 1400/1563 - train loss: 18.7879 - test loss: 19.3606 - train acc: 0.1944 - test acc: 0.1807 - 1m 3s\n",
      "batch: 1500/1563 - train loss: 18.5967 - test loss: 18.4584 - train acc: 0.2028 - test acc: 0.2024 - 1m 7s\n",
      "batch: 1563/1563 - train loss: 18.3582 - test loss: 18.6281 - train acc: 0.2107 - test acc: 0.1986 - 1m 11s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.9358 - test loss: 18.1663 - train acc: 0.2241 - test acc: 0.2199 - 1m 16s\n",
      "batch: 200/1563 - train loss: 17.6895 - test loss: 19.3676 - train acc: 0.2291 - test acc: 0.1873 - 1m 21s\n",
      "batch: 300/1563 - train loss: 17.8170 - test loss: 17.5775 - train acc: 0.2204 - test acc: 0.2384 - 1m 25s\n",
      "batch: 400/1563 - train loss: 17.5404 - test loss: 17.5581 - train acc: 0.2181 - test acc: 0.2342 - 1m 30s\n",
      "batch: 500/1563 - train loss: 17.5565 - test loss: 17.7290 - train acc: 0.2248 - test acc: 0.2249 - 1m 35s\n",
      "batch: 600/1563 - train loss: 17.2168 - test loss: 18.6934 - train acc: 0.2522 - test acc: 0.2073 - 1m 40s\n",
      "batch: 700/1563 - train loss: 17.0906 - test loss: 17.0666 - train acc: 0.2446 - test acc: 0.2616 - 1m 44s\n",
      "batch: 800/1563 - train loss: 16.8867 - test loss: 22.4311 - train acc: 0.2666 - test acc: 0.1351 - 1m 49s\n",
      "batch: 900/1563 - train loss: 16.7567 - test loss: 16.5531 - train acc: 0.2625 - test acc: 0.2694 - 1m 54s\n",
      "batch: 1000/1563 - train loss: 16.7998 - test loss: 16.6892 - train acc: 0.2622 - test acc: 0.2704 - 1m 58s\n",
      "batch: 1100/1563 - train loss: 16.5543 - test loss: 16.4815 - train acc: 0.2765 - test acc: 0.2700 - 2m 3s\n",
      "batch: 1200/1563 - train loss: 16.3510 - test loss: 16.7720 - train acc: 0.2934 - test acc: 0.2701 - 2m 8s\n",
      "batch: 1300/1563 - train loss: 16.4393 - test loss: 16.4150 - train acc: 0.2696 - test acc: 0.2763 - 2m 13s\n",
      "batch: 1400/1563 - train loss: 16.3738 - test loss: 16.6339 - train acc: 0.2722 - test acc: 0.2685 - 2m 17s\n",
      "batch: 1500/1563 - train loss: 16.0834 - test loss: 15.9566 - train acc: 0.2853 - test acc: 0.2921 - 2m 22s\n",
      "batch: 1563/1563 - train loss: 16.1817 - test loss: 17.0388 - train acc: 0.2825 - test acc: 0.2580 - 2m 26s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.0013 - test loss: 15.9439 - train acc: 0.3244 - test acc: 0.2943 - 2m 31s\n",
      "batch: 200/1563 - train loss: 14.9949 - test loss: 17.3652 - train acc: 0.3294 - test acc: 0.2626 - 2m 36s\n",
      "batch: 300/1563 - train loss: 15.4007 - test loss: 15.8003 - train acc: 0.3150 - test acc: 0.2992 - 2m 40s\n",
      "batch: 400/1563 - train loss: 14.8532 - test loss: 16.0870 - train acc: 0.3119 - test acc: 0.2916 - 2m 45s\n",
      "batch: 500/1563 - train loss: 14.8001 - test loss: 17.1909 - train acc: 0.3306 - test acc: 0.2646 - 2m 50s\n",
      "batch: 600/1563 - train loss: 14.8381 - test loss: 16.6708 - train acc: 0.3281 - test acc: 0.2772 - 2m 54s\n",
      "batch: 700/1563 - train loss: 14.9168 - test loss: 15.5535 - train acc: 0.3287 - test acc: 0.3042 - 2m 59s\n",
      "batch: 800/1563 - train loss: 15.0047 - test loss: 15.2752 - train acc: 0.3253 - test acc: 0.3250 - 3m 4s\n",
      "batch: 900/1563 - train loss: 14.8258 - test loss: 15.5661 - train acc: 0.3378 - test acc: 0.3008 - 3m 9s\n",
      "batch: 1000/1563 - train loss: 14.5219 - test loss: 14.9385 - train acc: 0.3334 - test acc: 0.3350 - 3m 14s\n",
      "batch: 1100/1563 - train loss: 14.5696 - test loss: 15.9836 - train acc: 0.3306 - test acc: 0.2967 - 3m 18s\n",
      "batch: 1200/1563 - train loss: 14.3910 - test loss: 15.4301 - train acc: 0.3497 - test acc: 0.3202 - 3m 23s\n",
      "batch: 1300/1563 - train loss: 14.5728 - test loss: 14.8464 - train acc: 0.3460 - test acc: 0.3351 - 3m 27s\n",
      "batch: 1400/1563 - train loss: 14.3942 - test loss: 15.4623 - train acc: 0.3425 - test acc: 0.3172 - 3m 32s\n",
      "batch: 1500/1563 - train loss: 14.3810 - test loss: 15.3851 - train acc: 0.3549 - test acc: 0.3100 - 3m 37s\n",
      "batch: 1563/1563 - train loss: 14.2373 - test loss: 15.2082 - train acc: 0.3543 - test acc: 0.3212 - 3m 41s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.8734 - test loss: 14.8079 - train acc: 0.4031 - test acc: 0.3318 - 3m 46s\n",
      "batch: 200/1563 - train loss: 13.2077 - test loss: 14.8242 - train acc: 0.3878 - test acc: 0.3358 - 3m 50s\n",
      "batch: 300/1563 - train loss: 13.1576 - test loss: 14.2776 - train acc: 0.3906 - test acc: 0.3573 - 3m 55s\n",
      "batch: 400/1563 - train loss: 12.8276 - test loss: 14.9219 - train acc: 0.4072 - test acc: 0.3355 - 4m 0s\n",
      "batch: 500/1563 - train loss: 13.4045 - test loss: 14.3777 - train acc: 0.3813 - test acc: 0.3542 - 4m 4s\n",
      "batch: 600/1563 - train loss: 13.2079 - test loss: 14.3968 - train acc: 0.3869 - test acc: 0.3563 - 4m 9s\n",
      "batch: 700/1563 - train loss: 12.9976 - test loss: 14.6057 - train acc: 0.3932 - test acc: 0.3447 - 4m 14s\n",
      "batch: 800/1563 - train loss: 13.0826 - test loss: 15.2239 - train acc: 0.4010 - test acc: 0.3307 - 4m 19s\n",
      "batch: 900/1563 - train loss: 13.2565 - test loss: 14.1298 - train acc: 0.3862 - test acc: 0.3627 - 4m 23s\n",
      "batch: 1000/1563 - train loss: 12.9762 - test loss: 13.9487 - train acc: 0.3978 - test acc: 0.3688 - 4m 28s\n",
      "batch: 1100/1563 - train loss: 12.7807 - test loss: 15.4127 - train acc: 0.4041 - test acc: 0.3357 - 4m 33s\n",
      "batch: 1200/1563 - train loss: 13.1341 - test loss: 14.5516 - train acc: 0.3947 - test acc: 0.3556 - 4m 37s\n",
      "batch: 1300/1563 - train loss: 13.2808 - test loss: 14.0453 - train acc: 0.3991 - test acc: 0.3718 - 4m 42s\n",
      "batch: 1400/1563 - train loss: 13.0744 - test loss: 13.8317 - train acc: 0.3959 - test acc: 0.3734 - 4m 47s\n",
      "batch: 1500/1563 - train loss: 12.9453 - test loss: 15.3202 - train acc: 0.4031 - test acc: 0.3319 - 4m 52s\n",
      "batch: 1563/1563 - train loss: 12.9653 - test loss: 14.2045 - train acc: 0.4003 - test acc: 0.3595 - 4m 56s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3576 - test loss: 14.4950 - train acc: 0.4513 - test acc: 0.3564 - 5m 0s\n",
      "batch: 200/1563 - train loss: 11.3120 - test loss: 14.7333 - train acc: 0.4560 - test acc: 0.3513 - 5m 5s\n",
      "batch: 300/1563 - train loss: 11.3612 - test loss: 14.4261 - train acc: 0.4566 - test acc: 0.3564 - 5m 10s\n",
      "batch: 400/1563 - train loss: 11.4568 - test loss: 13.9994 - train acc: 0.4688 - test acc: 0.3771 - 5m 15s\n",
      "batch: 500/1563 - train loss: 11.5375 - test loss: 15.0837 - train acc: 0.4488 - test acc: 0.3389 - 5m 20s\n",
      "batch: 600/1563 - train loss: 11.5079 - test loss: 14.6512 - train acc: 0.4487 - test acc: 0.3570 - 5m 24s\n",
      "batch: 700/1563 - train loss: 12.2177 - test loss: 13.9402 - train acc: 0.4288 - test acc: 0.3705 - 5m 29s\n",
      "batch: 800/1563 - train loss: 11.9648 - test loss: 14.3134 - train acc: 0.4335 - test acc: 0.3684 - 5m 34s\n",
      "batch: 900/1563 - train loss: 12.0128 - test loss: 14.6968 - train acc: 0.4413 - test acc: 0.3513 - 5m 38s\n",
      "batch: 1000/1563 - train loss: 11.8040 - test loss: 14.7329 - train acc: 0.4354 - test acc: 0.3522 - 5m 43s\n",
      "batch: 1100/1563 - train loss: 11.7925 - test loss: 13.6918 - train acc: 0.4450 - test acc: 0.3886 - 5m 48s\n",
      "batch: 1200/1563 - train loss: 11.7732 - test loss: 13.5470 - train acc: 0.4388 - test acc: 0.3904 - 5m 53s\n",
      "batch: 1300/1563 - train loss: 11.9478 - test loss: 13.8086 - train acc: 0.4372 - test acc: 0.3841 - 5m 57s\n",
      "batch: 1400/1563 - train loss: 12.1368 - test loss: 14.0607 - train acc: 0.4354 - test acc: 0.3760 - 6m 2s\n",
      "batch: 1500/1563 - train loss: 11.9122 - test loss: 14.2167 - train acc: 0.4382 - test acc: 0.3740 - 6m 6s\n",
      "batch: 1563/1563 - train loss: 11.8774 - test loss: 14.5468 - train acc: 0.4397 - test acc: 0.3667 - 6m 10s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.7543 - test loss: 13.7725 - train acc: 0.5262 - test acc: 0.3876 - 6m 15s\n",
      "batch: 200/1563 - train loss: 9.8811 - test loss: 14.0874 - train acc: 0.5162 - test acc: 0.3787 - 6m 20s\n",
      "batch: 300/1563 - train loss: 10.1375 - test loss: 15.2704 - train acc: 0.5163 - test acc: 0.3479 - 6m 24s\n",
      "batch: 400/1563 - train loss: 10.3484 - test loss: 14.1553 - train acc: 0.5050 - test acc: 0.3800 - 6m 29s\n",
      "batch: 500/1563 - train loss: 10.5303 - test loss: 13.6869 - train acc: 0.5010 - test acc: 0.3917 - 6m 34s\n",
      "batch: 600/1563 - train loss: 10.3126 - test loss: 13.2364 - train acc: 0.5047 - test acc: 0.4055 - 6m 38s\n",
      "batch: 700/1563 - train loss: 10.5422 - test loss: 13.4999 - train acc: 0.5066 - test acc: 0.4009 - 6m 43s\n",
      "batch: 800/1563 - train loss: 10.4866 - test loss: 14.7613 - train acc: 0.4903 - test acc: 0.3668 - 6m 48s\n",
      "batch: 900/1563 - train loss: 10.6128 - test loss: 14.0118 - train acc: 0.4935 - test acc: 0.3873 - 6m 53s\n",
      "batch: 1000/1563 - train loss: 10.9747 - test loss: 13.8295 - train acc: 0.4747 - test acc: 0.3882 - 6m 58s\n",
      "batch: 1100/1563 - train loss: 11.1987 - test loss: 14.0209 - train acc: 0.4650 - test acc: 0.3819 - 7m 3s\n",
      "batch: 1200/1563 - train loss: 10.7353 - test loss: 13.1479 - train acc: 0.4825 - test acc: 0.4120 - 7m 7s\n",
      "batch: 1300/1563 - train loss: 10.9990 - test loss: 14.5045 - train acc: 0.4794 - test acc: 0.3669 - 7m 12s\n",
      "batch: 1400/1563 - train loss: 11.0366 - test loss: 13.7058 - train acc: 0.4675 - test acc: 0.3914 - 7m 17s\n",
      "batch: 1500/1563 - train loss: 10.6562 - test loss: 13.2307 - train acc: 0.4813 - test acc: 0.4046 - 7m 22s\n",
      "batch: 1563/1563 - train loss: 10.9817 - test loss: 13.7114 - train acc: 0.4791 - test acc: 0.3878 - 7m 26s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.4457 - test loss: 13.6421 - train acc: 0.5913 - test acc: 0.4127 - 7m 31s\n",
      "batch: 200/1563 - train loss: 8.5582 - test loss: 13.7192 - train acc: 0.5653 - test acc: 0.3988 - 7m 36s\n",
      "batch: 300/1563 - train loss: 8.9353 - test loss: 13.9571 - train acc: 0.5597 - test acc: 0.3888 - 7m 40s\n",
      "batch: 400/1563 - train loss: 9.0962 - test loss: 13.6994 - train acc: 0.5419 - test acc: 0.4016 - 7m 45s\n",
      "batch: 500/1563 - train loss: 9.0463 - test loss: 13.8035 - train acc: 0.5615 - test acc: 0.4017 - 7m 50s\n",
      "batch: 600/1563 - train loss: 9.3249 - test loss: 13.6933 - train acc: 0.5366 - test acc: 0.4050 - 7m 55s\n",
      "batch: 700/1563 - train loss: 9.4956 - test loss: 14.5936 - train acc: 0.5350 - test acc: 0.3779 - 8m 0s\n",
      "batch: 800/1563 - train loss: 9.5353 - test loss: 13.6587 - train acc: 0.5272 - test acc: 0.4010 - 8m 5s\n",
      "batch: 900/1563 - train loss: 9.6020 - test loss: 13.5699 - train acc: 0.5381 - test acc: 0.4032 - 8m 10s\n",
      "batch: 1000/1563 - train loss: 9.5606 - test loss: 14.0290 - train acc: 0.5297 - test acc: 0.3970 - 8m 14s\n",
      "batch: 1100/1563 - train loss: 9.7880 - test loss: 14.2044 - train acc: 0.5184 - test acc: 0.3859 - 8m 19s\n",
      "batch: 1200/1563 - train loss: 10.0162 - test loss: 14.4625 - train acc: 0.5125 - test acc: 0.3845 - 8m 24s\n",
      "batch: 1300/1563 - train loss: 9.7431 - test loss: 13.4153 - train acc: 0.5188 - test acc: 0.4143 - 8m 29s\n",
      "batch: 1400/1563 - train loss: 10.0492 - test loss: 13.7373 - train acc: 0.5012 - test acc: 0.3968 - 8m 34s\n",
      "batch: 1500/1563 - train loss: 9.7738 - test loss: 13.3173 - train acc: 0.5241 - test acc: 0.4170 - 8m 38s\n",
      "batch: 1563/1563 - train loss: 9.5556 - test loss: 13.2274 - train acc: 0.5347 - test acc: 0.4171 - 8m 42s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.2838 - test loss: 13.7924 - train acc: 0.6447 - test acc: 0.4075 - 8m 47s\n",
      "batch: 200/1563 - train loss: 7.4198 - test loss: 13.8081 - train acc: 0.6253 - test acc: 0.4176 - 8m 52s\n",
      "batch: 300/1563 - train loss: 7.7095 - test loss: 13.9430 - train acc: 0.6175 - test acc: 0.4069 - 8m 57s\n",
      "batch: 400/1563 - train loss: 7.7018 - test loss: 14.1245 - train acc: 0.6071 - test acc: 0.4108 - 9m 2s\n",
      "batch: 500/1563 - train loss: 8.4719 - test loss: 13.8585 - train acc: 0.5750 - test acc: 0.4074 - 9m 6s\n",
      "batch: 600/1563 - train loss: 8.2964 - test loss: 14.0362 - train acc: 0.5799 - test acc: 0.4053 - 9m 11s\n",
      "batch: 700/1563 - train loss: 8.3902 - test loss: 13.9089 - train acc: 0.5712 - test acc: 0.4104 - 9m 16s\n",
      "batch: 800/1563 - train loss: 8.6128 - test loss: 14.7094 - train acc: 0.5734 - test acc: 0.3910 - 9m 20s\n",
      "batch: 900/1563 - train loss: 8.5721 - test loss: 14.5783 - train acc: 0.5775 - test acc: 0.3905 - 9m 25s\n",
      "batch: 1000/1563 - train loss: 8.4759 - test loss: 15.2549 - train acc: 0.5756 - test acc: 0.3725 - 9m 30s\n",
      "batch: 1100/1563 - train loss: 8.8333 - test loss: 13.9807 - train acc: 0.5606 - test acc: 0.4047 - 9m 35s\n",
      "batch: 1200/1563 - train loss: 8.8502 - test loss: 13.8447 - train acc: 0.5581 - test acc: 0.4084 - 9m 39s\n",
      "batch: 1300/1563 - train loss: 8.7955 - test loss: 13.5600 - train acc: 0.5653 - test acc: 0.4159 - 9m 44s\n",
      "batch: 1400/1563 - train loss: 8.9943 - test loss: 13.5359 - train acc: 0.5541 - test acc: 0.4148 - 9m 49s\n",
      "batch: 1500/1563 - train loss: 8.6243 - test loss: 14.5609 - train acc: 0.5709 - test acc: 0.3856 - 9m 53s\n",
      "batch: 1563/1563 - train loss: 8.8925 - test loss: 13.7868 - train acc: 0.5668 - test acc: 0.4184 - 9m 58s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.2697 - test loss: 14.4777 - train acc: 0.6763 - test acc: 0.4036 - 10m 2s\n",
      "batch: 200/1563 - train loss: 6.1536 - test loss: 14.2019 - train acc: 0.6841 - test acc: 0.4123 - 10m 7s\n",
      "batch: 300/1563 - train loss: 6.5762 - test loss: 15.2065 - train acc: 0.6616 - test acc: 0.3895 - 10m 12s\n",
      "batch: 400/1563 - train loss: 6.7325 - test loss: 16.4584 - train acc: 0.6581 - test acc: 0.3748 - 10m 16s\n",
      "batch: 500/1563 - train loss: 6.8149 - test loss: 14.9086 - train acc: 0.6497 - test acc: 0.3994 - 10m 21s\n",
      "batch: 600/1563 - train loss: 7.5053 - test loss: 14.4597 - train acc: 0.6131 - test acc: 0.4035 - 10m 26s\n",
      "batch: 700/1563 - train loss: 7.3296 - test loss: 14.6232 - train acc: 0.6231 - test acc: 0.4023 - 10m 31s\n",
      "batch: 800/1563 - train loss: 7.5276 - test loss: 15.6978 - train acc: 0.6092 - test acc: 0.3814 - 10m 36s\n",
      "batch: 900/1563 - train loss: 7.7804 - test loss: 14.1988 - train acc: 0.6112 - test acc: 0.4120 - 10m 40s\n",
      "batch: 1000/1563 - train loss: 7.5409 - test loss: 14.2786 - train acc: 0.6087 - test acc: 0.4038 - 10m 45s\n",
      "batch: 1100/1563 - train loss: 7.9323 - test loss: 14.4290 - train acc: 0.6047 - test acc: 0.3951 - 10m 50s\n",
      "batch: 1200/1563 - train loss: 8.0323 - test loss: 15.3079 - train acc: 0.5900 - test acc: 0.3879 - 10m 54s\n",
      "batch: 1300/1563 - train loss: 7.9231 - test loss: 14.2167 - train acc: 0.5972 - test acc: 0.4131 - 10m 59s\n",
      "batch: 1400/1563 - train loss: 7.9955 - test loss: 15.2051 - train acc: 0.5900 - test acc: 0.3831 - 11m 4s\n",
      "batch: 1500/1563 - train loss: 8.2537 - test loss: 13.8004 - train acc: 0.5884 - test acc: 0.4200 - 11m 9s\n",
      "batch: 1563/1563 - train loss: 8.1290 - test loss: 14.0754 - train acc: 0.5869 - test acc: 0.4066 - 11m 13s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.2853 - test loss: 14.3802 - train acc: 0.7319 - test acc: 0.4111 - 11m 17s\n",
      "batch: 200/1563 - train loss: 5.2576 - test loss: 14.7607 - train acc: 0.7278 - test acc: 0.4160 - 11m 22s\n",
      "batch: 300/1563 - train loss: 5.8203 - test loss: 15.0160 - train acc: 0.6916 - test acc: 0.4104 - 11m 27s\n",
      "batch: 400/1563 - train loss: 5.4782 - test loss: 15.0075 - train acc: 0.7166 - test acc: 0.4085 - 11m 32s\n",
      "batch: 500/1563 - train loss: 6.1246 - test loss: 14.7513 - train acc: 0.6747 - test acc: 0.4108 - 11m 37s\n",
      "batch: 600/1563 - train loss: 6.0027 - test loss: 15.4536 - train acc: 0.6907 - test acc: 0.4013 - 11m 41s\n",
      "batch: 700/1563 - train loss: 6.2128 - test loss: 14.9327 - train acc: 0.6716 - test acc: 0.4127 - 11m 46s\n",
      "batch: 800/1563 - train loss: 6.5895 - test loss: 14.7776 - train acc: 0.6557 - test acc: 0.4133 - 11m 51s\n",
      "batch: 900/1563 - train loss: 6.7595 - test loss: 14.5383 - train acc: 0.6607 - test acc: 0.4166 - 11m 55s\n",
      "batch: 1000/1563 - train loss: 6.7565 - test loss: 14.7524 - train acc: 0.6388 - test acc: 0.4115 - 12m 0s\n",
      "batch: 1100/1563 - train loss: 7.0386 - test loss: 14.4470 - train acc: 0.6372 - test acc: 0.4170 - 12m 5s\n",
      "batch: 1200/1563 - train loss: 6.9424 - test loss: 14.6943 - train acc: 0.6387 - test acc: 0.4109 - 12m 10s\n",
      "batch: 1300/1563 - train loss: 6.9895 - test loss: 14.5913 - train acc: 0.6385 - test acc: 0.4151 - 12m 15s\n",
      "batch: 1400/1563 - train loss: 6.9254 - test loss: 14.5840 - train acc: 0.6469 - test acc: 0.4074 - 12m 19s\n",
      "batch: 1500/1563 - train loss: 7.4096 - test loss: 14.2325 - train acc: 0.6294 - test acc: 0.4220 - 12m 24s\n",
      "batch: 1563/1563 - train loss: 7.2155 - test loss: 14.5866 - train acc: 0.6284 - test acc: 0.4095 - 12m 28s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.4207 - test loss: 14.4716 - train acc: 0.7756 - test acc: 0.4294 - 12m 33s\n",
      "batch: 200/1563 - train loss: 4.5648 - test loss: 15.5871 - train acc: 0.7628 - test acc: 0.4005 - 12m 38s\n",
      "batch: 300/1563 - train loss: 4.7075 - test loss: 15.1607 - train acc: 0.7410 - test acc: 0.4199 - 12m 42s\n",
      "batch: 400/1563 - train loss: 4.6658 - test loss: 14.8744 - train acc: 0.7500 - test acc: 0.4281 - 12m 47s\n",
      "batch: 500/1563 - train loss: 5.0237 - test loss: 15.2715 - train acc: 0.7366 - test acc: 0.4188 - 12m 52s\n",
      "batch: 600/1563 - train loss: 5.2210 - test loss: 15.7120 - train acc: 0.7184 - test acc: 0.4102 - 12m 56s\n",
      "batch: 700/1563 - train loss: 5.6893 - test loss: 15.3348 - train acc: 0.6981 - test acc: 0.4130 - 13m 1s\n",
      "batch: 800/1563 - train loss: 5.5132 - test loss: 15.5323 - train acc: 0.7047 - test acc: 0.4120 - 13m 6s\n",
      "batch: 900/1563 - train loss: 5.7584 - test loss: 15.3104 - train acc: 0.6904 - test acc: 0.4131 - 13m 11s\n",
      "batch: 1000/1563 - train loss: 5.7806 - test loss: 15.8171 - train acc: 0.6988 - test acc: 0.3999 - 13m 16s\n",
      "batch: 1100/1563 - train loss: 6.0431 - test loss: 15.3951 - train acc: 0.6787 - test acc: 0.4090 - 13m 20s\n",
      "batch: 1200/1563 - train loss: 6.3285 - test loss: 15.4907 - train acc: 0.6666 - test acc: 0.4035 - 13m 25s\n",
      "batch: 1300/1563 - train loss: 6.1886 - test loss: 15.1920 - train acc: 0.6759 - test acc: 0.4147 - 13m 30s\n",
      "batch: 1400/1563 - train loss: 6.3406 - test loss: 15.5765 - train acc: 0.6673 - test acc: 0.4067 - 13m 35s\n",
      "batch: 1500/1563 - train loss: 6.2174 - test loss: 15.0207 - train acc: 0.6766 - test acc: 0.4182 - 13m 40s\n",
      "batch: 1563/1563 - train loss: 6.5465 - test loss: 15.4482 - train acc: 0.6581 - test acc: 0.4085 - 13m 43s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.9427 - test loss: 16.0861 - train acc: 0.7888 - test acc: 0.3918 - 13m 48s\n",
      "batch: 200/1563 - train loss: 3.8447 - test loss: 15.2164 - train acc: 0.7956 - test acc: 0.4225 - 13m 53s\n",
      "batch: 300/1563 - train loss: 4.0007 - test loss: 15.8843 - train acc: 0.7862 - test acc: 0.4192 - 13m 58s\n",
      "batch: 400/1563 - train loss: 3.9191 - test loss: 16.2371 - train acc: 0.7840 - test acc: 0.4145 - 14m 2s\n",
      "batch: 500/1563 - train loss: 4.3619 - test loss: 16.0291 - train acc: 0.7619 - test acc: 0.4190 - 14m 8s\n",
      "batch: 600/1563 - train loss: 4.4005 - test loss: 17.1683 - train acc: 0.7549 - test acc: 0.3903 - 14m 12s\n",
      "batch: 700/1563 - train loss: 4.7460 - test loss: 16.1003 - train acc: 0.7353 - test acc: 0.4139 - 14m 17s\n",
      "batch: 800/1563 - train loss: 5.0467 - test loss: 16.1988 - train acc: 0.7319 - test acc: 0.4124 - 14m 22s\n",
      "batch: 900/1563 - train loss: 4.8911 - test loss: 16.7883 - train acc: 0.7344 - test acc: 0.4042 - 14m 26s\n",
      "batch: 1000/1563 - train loss: 4.9747 - test loss: 16.0527 - train acc: 0.7319 - test acc: 0.4216 - 14m 31s\n",
      "batch: 1100/1563 - train loss: 5.4613 - test loss: 16.5746 - train acc: 0.7063 - test acc: 0.4035 - 14m 36s\n",
      "batch: 1200/1563 - train loss: 5.2326 - test loss: 15.7413 - train acc: 0.7222 - test acc: 0.4211 - 14m 41s\n",
      "batch: 1300/1563 - train loss: 5.5324 - test loss: 16.2991 - train acc: 0.7081 - test acc: 0.4047 - 14m 46s\n",
      "batch: 1400/1563 - train loss: 5.5367 - test loss: 15.7075 - train acc: 0.7141 - test acc: 0.4202 - 14m 50s\n",
      "batch: 1500/1563 - train loss: 5.3891 - test loss: 15.8125 - train acc: 0.7159 - test acc: 0.4198 - 14m 55s\n",
      "batch: 1563/1563 - train loss: 5.4073 - test loss: 15.7547 - train acc: 0.7156 - test acc: 0.4177 - 14m 59s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.3458 - test loss: 16.1998 - train acc: 0.8134 - test acc: 0.4187 - 15m 4s\n",
      "batch: 200/1563 - train loss: 3.1691 - test loss: 16.6396 - train acc: 0.8278 - test acc: 0.4215 - 15m 8s\n",
      "batch: 300/1563 - train loss: 3.2350 - test loss: 16.3574 - train acc: 0.8284 - test acc: 0.4224 - 15m 14s\n",
      "batch: 400/1563 - train loss: 3.3200 - test loss: 17.3413 - train acc: 0.8165 - test acc: 0.4106 - 15m 18s\n",
      "batch: 500/1563 - train loss: 3.4832 - test loss: 16.5785 - train acc: 0.8031 - test acc: 0.4154 - 15m 23s\n",
      "batch: 600/1563 - train loss: 3.8214 - test loss: 17.1803 - train acc: 0.7962 - test acc: 0.3996 - 15m 28s\n",
      "batch: 700/1563 - train loss: 3.9843 - test loss: 17.1043 - train acc: 0.7815 - test acc: 0.4082 - 15m 32s\n",
      "batch: 800/1563 - train loss: 4.2934 - test loss: 17.0163 - train acc: 0.7622 - test acc: 0.4180 - 15m 37s\n",
      "batch: 900/1563 - train loss: 4.1151 - test loss: 16.3223 - train acc: 0.7766 - test acc: 0.4237 - 15m 42s\n",
      "batch: 1000/1563 - train loss: 4.2919 - test loss: 17.4846 - train acc: 0.7678 - test acc: 0.4076 - 15m 47s\n",
      "batch: 1100/1563 - train loss: 4.4985 - test loss: 16.7619 - train acc: 0.7587 - test acc: 0.4122 - 15m 52s\n",
      "batch: 1200/1563 - train loss: 4.8321 - test loss: 16.8388 - train acc: 0.7366 - test acc: 0.4042 - 15m 56s\n",
      "batch: 1300/1563 - train loss: 4.6999 - test loss: 16.4256 - train acc: 0.7400 - test acc: 0.4074 - 16m 1s\n",
      "batch: 1400/1563 - train loss: 4.4986 - test loss: 16.2191 - train acc: 0.7532 - test acc: 0.4075 - 16m 6s\n",
      "batch: 1500/1563 - train loss: 4.8143 - test loss: 16.4337 - train acc: 0.7387 - test acc: 0.4069 - 16m 11s\n",
      "batch: 1563/1563 - train loss: 4.8342 - test loss: 16.5196 - train acc: 0.7378 - test acc: 0.4176 - 16m 15s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.7737 - test loss: 16.3944 - train acc: 0.8531 - test acc: 0.4260 - 16m 20s\n",
      "batch: 200/1563 - train loss: 2.6698 - test loss: 16.8498 - train acc: 0.8487 - test acc: 0.4235 - 16m 24s\n",
      "batch: 300/1563 - train loss: 2.7047 - test loss: 17.3997 - train acc: 0.8544 - test acc: 0.4128 - 16m 29s\n",
      "batch: 400/1563 - train loss: 2.9158 - test loss: 17.3302 - train acc: 0.8431 - test acc: 0.4159 - 16m 34s\n",
      "batch: 500/1563 - train loss: 2.9975 - test loss: 18.1255 - train acc: 0.8281 - test acc: 0.4040 - 16m 39s\n",
      "batch: 600/1563 - train loss: 3.4422 - test loss: 17.7937 - train acc: 0.8056 - test acc: 0.4135 - 16m 43s\n",
      "batch: 700/1563 - train loss: 3.4975 - test loss: 17.4313 - train acc: 0.8021 - test acc: 0.4130 - 16m 48s\n",
      "batch: 800/1563 - train loss: 3.5525 - test loss: 17.9713 - train acc: 0.8049 - test acc: 0.4036 - 16m 53s\n",
      "batch: 900/1563 - train loss: 3.5628 - test loss: 17.2984 - train acc: 0.8069 - test acc: 0.4252 - 16m 58s\n",
      "batch: 1000/1563 - train loss: 3.7407 - test loss: 17.8862 - train acc: 0.7953 - test acc: 0.4031 - 17m 2s\n",
      "batch: 1100/1563 - train loss: 3.9600 - test loss: 17.8399 - train acc: 0.7884 - test acc: 0.4117 - 17m 7s\n",
      "batch: 1200/1563 - train loss: 4.0502 - test loss: 17.6281 - train acc: 0.7824 - test acc: 0.4162 - 17m 12s\n",
      "batch: 1300/1563 - train loss: 4.0963 - test loss: 17.9041 - train acc: 0.7712 - test acc: 0.3994 - 17m 17s\n",
      "batch: 1400/1563 - train loss: 4.2297 - test loss: 17.4875 - train acc: 0.7675 - test acc: 0.4186 - 17m 22s\n",
      "batch: 1500/1563 - train loss: 4.2404 - test loss: 17.2939 - train acc: 0.7635 - test acc: 0.4063 - 17m 27s\n",
      "batch: 1563/1563 - train loss: 4.3628 - test loss: 17.3176 - train acc: 0.7584 - test acc: 0.4111 - 17m 31s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.4326 - test loss: 17.3341 - train acc: 0.8619 - test acc: 0.4176 - 17m 35s\n",
      "batch: 200/1563 - train loss: 2.1665 - test loss: 17.3772 - train acc: 0.8763 - test acc: 0.4191 - 17m 40s\n",
      "batch: 300/1563 - train loss: 2.5497 - test loss: 17.7832 - train acc: 0.8516 - test acc: 0.4179 - 17m 45s\n",
      "batch: 400/1563 - train loss: 2.3685 - test loss: 17.6556 - train acc: 0.8713 - test acc: 0.4300 - 17m 50s\n",
      "batch: 500/1563 - train loss: 2.5184 - test loss: 17.8356 - train acc: 0.8615 - test acc: 0.4217 - 17m 55s\n",
      "batch: 600/1563 - train loss: 2.5002 - test loss: 18.5678 - train acc: 0.8509 - test acc: 0.4093 - 17m 59s\n",
      "batch: 700/1563 - train loss: 2.8875 - test loss: 18.5025 - train acc: 0.8334 - test acc: 0.4050 - 18m 4s\n",
      "batch: 800/1563 - train loss: 2.9119 - test loss: 18.2373 - train acc: 0.8330 - test acc: 0.4132 - 18m 9s\n",
      "batch: 900/1563 - train loss: 3.0451 - test loss: 18.1930 - train acc: 0.8327 - test acc: 0.4149 - 18m 13s\n",
      "batch: 1000/1563 - train loss: 3.2999 - test loss: 18.2326 - train acc: 0.8137 - test acc: 0.4142 - 18m 18s\n",
      "batch: 1100/1563 - train loss: 3.1576 - test loss: 18.1788 - train acc: 0.8278 - test acc: 0.4170 - 18m 23s\n",
      "batch: 1200/1563 - train loss: 3.6451 - test loss: 18.1726 - train acc: 0.7968 - test acc: 0.4077 - 18m 28s\n",
      "batch: 1300/1563 - train loss: 3.6573 - test loss: 18.1496 - train acc: 0.8015 - test acc: 0.4122 - 18m 33s\n",
      "batch: 1400/1563 - train loss: 3.7585 - test loss: 18.2339 - train acc: 0.7890 - test acc: 0.4148 - 18m 37s\n",
      "batch: 1500/1563 - train loss: 3.6840 - test loss: 18.0084 - train acc: 0.8037 - test acc: 0.4147 - 18m 42s\n",
      "batch: 1563/1563 - train loss: 3.8886 - test loss: 18.5487 - train acc: 0.7890 - test acc: 0.4029 - 18m 46s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.9999 - test loss: 18.1055 - train acc: 0.8882 - test acc: 0.4206 - 18m 51s\n",
      "batch: 200/1563 - train loss: 1.8591 - test loss: 18.0959 - train acc: 0.8969 - test acc: 0.4202 - 18m 56s\n",
      "batch: 300/1563 - train loss: 1.9645 - test loss: 18.0094 - train acc: 0.8923 - test acc: 0.4199 - 19m 1s\n",
      "batch: 400/1563 - train loss: 2.0110 - test loss: 18.5257 - train acc: 0.8825 - test acc: 0.4187 - 19m 6s\n",
      "batch: 500/1563 - train loss: 2.2392 - test loss: 18.4330 - train acc: 0.8713 - test acc: 0.4210 - 19m 10s\n",
      "batch: 600/1563 - train loss: 2.3454 - test loss: 18.4063 - train acc: 0.8662 - test acc: 0.4230 - 19m 15s\n",
      "batch: 700/1563 - train loss: 2.3627 - test loss: 19.1500 - train acc: 0.8625 - test acc: 0.4111 - 19m 20s\n",
      "batch: 800/1563 - train loss: 2.6566 - test loss: 19.3437 - train acc: 0.8562 - test acc: 0.4032 - 19m 25s\n",
      "batch: 900/1563 - train loss: 2.6630 - test loss: 19.4508 - train acc: 0.8519 - test acc: 0.4078 - 19m 29s\n",
      "batch: 1000/1563 - train loss: 2.6414 - test loss: 18.7492 - train acc: 0.8528 - test acc: 0.4217 - 19m 34s\n",
      "batch: 1100/1563 - train loss: 2.7010 - test loss: 18.5105 - train acc: 0.8496 - test acc: 0.4159 - 19m 39s\n",
      "batch: 1200/1563 - train loss: 2.9095 - test loss: 18.9617 - train acc: 0.8356 - test acc: 0.4069 - 19m 44s\n",
      "batch: 1300/1563 - train loss: 3.2282 - test loss: 18.8318 - train acc: 0.8215 - test acc: 0.4144 - 19m 48s\n",
      "batch: 1400/1563 - train loss: 3.4183 - test loss: 18.4816 - train acc: 0.7985 - test acc: 0.4162 - 19m 53s\n",
      "batch: 1500/1563 - train loss: 3.4277 - test loss: 18.8442 - train acc: 0.8127 - test acc: 0.4012 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 1501/1563 - train loss: 3.4728 - test loss: 18.8731 - train acc: 0.8105 - test acc: 0.4008 - 20m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "testing - beta: 0.9 - part size: 8 - block upd: 64 - combination nº: 9\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 8 - nº part: 1176 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 73728 - partition: 8 - nº part: 9216 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 8192 - partition: 8 - nº part: 1024 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 294912 - partition: 8 - nº part: 36864 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 32768 - partition: 8 - nº part: 4096 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 1179648 - partition: 8 - nº part: 147456 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 131072 - partition: 8 - nº part: 16384 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 51200 - partition: 8 - nº part: 6400 - block updates: 64\n",
      "FisherPartitioner: param: 100 - partition: 8 - nº part: 13 - block updates: 13\n",
      "total partitions: 1403477 - effective block updates: 2557\n",
      "initializing inverses...\n",
      "partition 1/1403477\n",
      "partition 10000/1403477\n",
      "partition 20000/1403477\n",
      "partition 30000/1403477\n",
      "partition 40000/1403477\n",
      "partition 50000/1403477\n",
      "partition 60000/1403477\n",
      "partition 70000/1403477\n",
      "partition 80000/1403477\n",
      "partition 90000/1403477\n",
      "partition 100000/1403477\n",
      "partition 110000/1403477\n",
      "partition 120000/1403477\n",
      "partition 130000/1403477\n",
      "partition 140000/1403477\n",
      "partition 150000/1403477\n",
      "partition 160000/1403477\n",
      "partition 170000/1403477\n",
      "partition 180000/1403477\n",
      "partition 190000/1403477\n",
      "partition 200000/1403477\n",
      "partition 210000/1403477\n",
      "partition 220000/1403477\n",
      "partition 230000/1403477\n",
      "partition 240000/1403477\n",
      "partition 250000/1403477\n",
      "partition 260000/1403477\n",
      "partition 270000/1403477\n",
      "partition 280000/1403477\n",
      "partition 290000/1403477\n",
      "partition 300000/1403477\n",
      "partition 310000/1403477\n",
      "partition 320000/1403477\n",
      "partition 330000/1403477\n",
      "partition 340000/1403477\n",
      "partition 350000/1403477\n",
      "partition 360000/1403477\n",
      "partition 370000/1403477\n",
      "partition 380000/1403477\n",
      "partition 390000/1403477\n",
      "partition 400000/1403477\n",
      "partition 410000/1403477\n",
      "partition 420000/1403477\n",
      "partition 430000/1403477\n",
      "partition 440000/1403477\n",
      "partition 450000/1403477\n",
      "partition 460000/1403477\n",
      "partition 470000/1403477\n",
      "partition 480000/1403477\n",
      "partition 490000/1403477\n",
      "partition 500000/1403477\n",
      "partition 510000/1403477\n",
      "partition 520000/1403477\n",
      "partition 530000/1403477\n",
      "partition 540000/1403477\n",
      "partition 550000/1403477\n",
      "partition 560000/1403477\n",
      "partition 570000/1403477\n",
      "partition 580000/1403477\n",
      "partition 590000/1403477\n",
      "partition 600000/1403477\n",
      "partition 610000/1403477\n",
      "partition 620000/1403477\n",
      "partition 630000/1403477\n",
      "partition 640000/1403477\n",
      "partition 650000/1403477\n",
      "partition 660000/1403477\n",
      "partition 670000/1403477\n",
      "partition 680000/1403477\n",
      "partition 690000/1403477\n",
      "partition 700000/1403477\n",
      "partition 710000/1403477\n",
      "partition 720000/1403477\n",
      "partition 730000/1403477\n",
      "partition 740000/1403477\n",
      "partition 750000/1403477\n",
      "partition 760000/1403477\n",
      "partition 770000/1403477\n",
      "partition 780000/1403477\n",
      "partition 790000/1403477\n",
      "partition 800000/1403477\n",
      "partition 810000/1403477\n",
      "partition 820000/1403477\n",
      "partition 830000/1403477\n",
      "partition 840000/1403477\n",
      "partition 850000/1403477\n",
      "partition 860000/1403477\n",
      "partition 870000/1403477\n",
      "partition 880000/1403477\n",
      "partition 890000/1403477\n",
      "partition 900000/1403477\n",
      "partition 910000/1403477\n",
      "partition 920000/1403477\n",
      "partition 930000/1403477\n",
      "partition 940000/1403477\n",
      "partition 950000/1403477\n",
      "partition 960000/1403477\n",
      "partition 970000/1403477\n",
      "partition 980000/1403477\n",
      "partition 990000/1403477\n",
      "partition 1000000/1403477\n",
      "partition 1010000/1403477\n",
      "partition 1020000/1403477\n",
      "partition 1030000/1403477\n",
      "partition 1040000/1403477\n",
      "partition 1050000/1403477\n",
      "partition 1060000/1403477\n",
      "partition 1070000/1403477\n",
      "partition 1080000/1403477\n",
      "partition 1090000/1403477\n",
      "partition 1100000/1403477\n",
      "partition 1110000/1403477\n",
      "partition 1120000/1403477\n",
      "partition 1130000/1403477\n",
      "partition 1140000/1403477\n",
      "partition 1150000/1403477\n",
      "partition 1160000/1403477\n",
      "partition 1170000/1403477\n",
      "partition 1180000/1403477\n",
      "partition 1190000/1403477\n",
      "partition 1200000/1403477\n",
      "partition 1210000/1403477\n",
      "partition 1220000/1403477\n",
      "partition 1230000/1403477\n",
      "partition 1240000/1403477\n",
      "partition 1250000/1403477\n",
      "partition 1260000/1403477\n",
      "partition 1270000/1403477\n",
      "partition 1280000/1403477\n",
      "partition 1290000/1403477\n",
      "partition 1300000/1403477\n",
      "partition 1310000/1403477\n",
      "partition 1320000/1403477\n",
      "partition 1330000/1403477\n",
      "partition 1340000/1403477\n",
      "partition 1350000/1403477\n",
      "partition 1360000/1403477\n",
      "partition 1370000/1403477\n",
      "partition 1380000/1403477\n",
      "partition 1390000/1403477\n",
      "partition 1400000/1403477\n",
      "partition 1403477/1403477\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0513 - test loss: 24.5881 - train acc: 0.0386 - test acc: 0.0530 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.8881 - test loss: 22.9087 - train acc: 0.0677 - test acc: 0.0716 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.1561 - test loss: 23.3236 - train acc: 0.0689 - test acc: 0.0823 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.4826 - test loss: 21.8951 - train acc: 0.0852 - test acc: 0.1043 - 0m 16s\n",
      "batch: 500/1563 - train loss: 21.5613 - test loss: 21.0939 - train acc: 0.1166 - test acc: 0.1234 - 0m 21s\n",
      "batch: 600/1563 - train loss: 21.0887 - test loss: 20.9344 - train acc: 0.1296 - test acc: 0.1272 - 0m 26s\n",
      "batch: 700/1563 - train loss: 20.8272 - test loss: 20.1189 - train acc: 0.1310 - test acc: 0.1571 - 0m 30s\n",
      "batch: 800/1563 - train loss: 20.3747 - test loss: 20.1955 - train acc: 0.1462 - test acc: 0.1653 - 0m 35s\n",
      "batch: 900/1563 - train loss: 20.1628 - test loss: 19.8256 - train acc: 0.1566 - test acc: 0.1733 - 0m 40s\n",
      "batch: 1000/1563 - train loss: 19.7148 - test loss: 19.3587 - train acc: 0.1653 - test acc: 0.1826 - 0m 45s\n",
      "batch: 1100/1563 - train loss: 19.6004 - test loss: 19.5967 - train acc: 0.1716 - test acc: 0.1709 - 0m 50s\n",
      "batch: 1200/1563 - train loss: 19.2434 - test loss: 18.8022 - train acc: 0.1722 - test acc: 0.1947 - 0m 54s\n",
      "batch: 1300/1563 - train loss: 19.0024 - test loss: 18.8177 - train acc: 0.1919 - test acc: 0.2053 - 0m 59s\n",
      "batch: 1400/1563 - train loss: 18.7959 - test loss: 18.5868 - train acc: 0.2020 - test acc: 0.2043 - 1m 4s\n",
      "batch: 1500/1563 - train loss: 18.8294 - test loss: 18.0753 - train acc: 0.1970 - test acc: 0.2180 - 1m 9s\n",
      "batch: 1563/1563 - train loss: 18.5130 - test loss: 18.7730 - train acc: 0.2010 - test acc: 0.1961 - 1m 13s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.7923 - test loss: 18.0213 - train acc: 0.2304 - test acc: 0.2210 - 1m 18s\n",
      "batch: 200/1563 - train loss: 17.3802 - test loss: 17.9850 - train acc: 0.2372 - test acc: 0.2213 - 1m 22s\n",
      "batch: 300/1563 - train loss: 17.4091 - test loss: 17.8166 - train acc: 0.2391 - test acc: 0.2293 - 1m 27s\n",
      "batch: 400/1563 - train loss: 17.6023 - test loss: 18.1143 - train acc: 0.2309 - test acc: 0.2182 - 1m 32s\n",
      "batch: 500/1563 - train loss: 17.1201 - test loss: 18.1488 - train acc: 0.2456 - test acc: 0.2295 - 1m 36s\n",
      "batch: 600/1563 - train loss: 17.3105 - test loss: 17.2212 - train acc: 0.2497 - test acc: 0.2527 - 1m 41s\n",
      "batch: 700/1563 - train loss: 17.2473 - test loss: 17.2308 - train acc: 0.2472 - test acc: 0.2520 - 1m 46s\n",
      "batch: 800/1563 - train loss: 16.5894 - test loss: 17.3924 - train acc: 0.2699 - test acc: 0.2424 - 1m 51s\n",
      "batch: 900/1563 - train loss: 16.8078 - test loss: 17.1761 - train acc: 0.2622 - test acc: 0.2556 - 1m 55s\n",
      "batch: 1000/1563 - train loss: 16.5955 - test loss: 16.4601 - train acc: 0.2632 - test acc: 0.2736 - 2m 0s\n",
      "batch: 1100/1563 - train loss: 16.3900 - test loss: 16.6897 - train acc: 0.2759 - test acc: 0.2670 - 2m 5s\n",
      "batch: 1200/1563 - train loss: 16.4057 - test loss: 17.0898 - train acc: 0.2738 - test acc: 0.2495 - 2m 9s\n",
      "batch: 1300/1563 - train loss: 16.4063 - test loss: 16.5836 - train acc: 0.2838 - test acc: 0.2743 - 2m 14s\n",
      "batch: 1400/1563 - train loss: 15.9794 - test loss: 16.0509 - train acc: 0.2884 - test acc: 0.2954 - 2m 19s\n",
      "batch: 1500/1563 - train loss: 16.3347 - test loss: 16.5996 - train acc: 0.2793 - test acc: 0.2708 - 2m 24s\n",
      "batch: 1563/1563 - train loss: 16.1529 - test loss: 16.9739 - train acc: 0.2862 - test acc: 0.2596 - 2m 27s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.9455 - test loss: 16.2166 - train acc: 0.3262 - test acc: 0.2882 - 2m 32s\n",
      "batch: 200/1563 - train loss: 14.8809 - test loss: 17.9150 - train acc: 0.3303 - test acc: 0.2378 - 2m 37s\n",
      "batch: 300/1563 - train loss: 14.8024 - test loss: 17.5306 - train acc: 0.3260 - test acc: 0.2565 - 2m 41s\n",
      "batch: 400/1563 - train loss: 14.5285 - test loss: 16.7700 - train acc: 0.3325 - test acc: 0.2773 - 2m 46s\n",
      "batch: 500/1563 - train loss: 14.6488 - test loss: 16.4167 - train acc: 0.3272 - test acc: 0.2897 - 2m 51s\n",
      "batch: 600/1563 - train loss: 14.7769 - test loss: 15.9806 - train acc: 0.3359 - test acc: 0.3027 - 2m 56s\n",
      "batch: 700/1563 - train loss: 14.9738 - test loss: 15.1677 - train acc: 0.3287 - test acc: 0.3183 - 3m 0s\n",
      "batch: 800/1563 - train loss: 14.4660 - test loss: 16.0450 - train acc: 0.3416 - test acc: 0.2987 - 3m 5s\n",
      "batch: 900/1563 - train loss: 14.5960 - test loss: 15.7280 - train acc: 0.3547 - test acc: 0.3143 - 3m 10s\n",
      "batch: 1000/1563 - train loss: 14.7005 - test loss: 15.4127 - train acc: 0.3262 - test acc: 0.3194 - 3m 14s\n",
      "batch: 1100/1563 - train loss: 14.4347 - test loss: 15.8059 - train acc: 0.3424 - test acc: 0.3102 - 3m 19s\n",
      "batch: 1200/1563 - train loss: 14.5612 - test loss: 14.7124 - train acc: 0.3293 - test acc: 0.3313 - 3m 24s\n",
      "batch: 1300/1563 - train loss: 14.4627 - test loss: 14.6469 - train acc: 0.3506 - test acc: 0.3449 - 3m 29s\n",
      "batch: 1400/1563 - train loss: 14.4553 - test loss: 16.0631 - train acc: 0.3529 - test acc: 0.3037 - 3m 33s\n",
      "batch: 1500/1563 - train loss: 14.3475 - test loss: 14.3852 - train acc: 0.3488 - test acc: 0.3522 - 3m 38s\n",
      "batch: 1563/1563 - train loss: 14.3193 - test loss: 15.0321 - train acc: 0.3497 - test acc: 0.3309 - 3m 42s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.8373 - test loss: 14.7936 - train acc: 0.4028 - test acc: 0.3462 - 3m 47s\n",
      "batch: 200/1563 - train loss: 12.8997 - test loss: 15.0935 - train acc: 0.4015 - test acc: 0.3284 - 3m 52s\n",
      "batch: 300/1563 - train loss: 12.6463 - test loss: 16.5241 - train acc: 0.4091 - test acc: 0.3049 - 3m 56s\n",
      "batch: 400/1563 - train loss: 13.3364 - test loss: 14.7041 - train acc: 0.3844 - test acc: 0.3403 - 4m 1s\n",
      "batch: 500/1563 - train loss: 13.1974 - test loss: 15.0901 - train acc: 0.3794 - test acc: 0.3330 - 4m 6s\n",
      "batch: 600/1563 - train loss: 13.2081 - test loss: 14.8138 - train acc: 0.4006 - test acc: 0.3395 - 4m 10s\n",
      "batch: 700/1563 - train loss: 13.4090 - test loss: 16.0394 - train acc: 0.3785 - test acc: 0.3025 - 4m 15s\n",
      "batch: 800/1563 - train loss: 13.1235 - test loss: 14.3853 - train acc: 0.3909 - test acc: 0.3536 - 4m 20s\n",
      "batch: 900/1563 - train loss: 13.0704 - test loss: 15.1966 - train acc: 0.3928 - test acc: 0.3315 - 4m 25s\n",
      "batch: 1000/1563 - train loss: 13.0796 - test loss: 14.6043 - train acc: 0.3878 - test acc: 0.3443 - 4m 29s\n",
      "batch: 1100/1563 - train loss: 12.9827 - test loss: 14.7387 - train acc: 0.3953 - test acc: 0.3435 - 4m 34s\n",
      "batch: 1200/1563 - train loss: 12.9835 - test loss: 15.0274 - train acc: 0.4047 - test acc: 0.3320 - 4m 39s\n",
      "batch: 1300/1563 - train loss: 13.2451 - test loss: 14.3305 - train acc: 0.3950 - test acc: 0.3624 - 4m 43s\n",
      "batch: 1400/1563 - train loss: 12.7357 - test loss: 13.6059 - train acc: 0.3981 - test acc: 0.3831 - 4m 48s\n",
      "batch: 1500/1563 - train loss: 13.1338 - test loss: 14.0801 - train acc: 0.3841 - test acc: 0.3681 - 4m 52s\n",
      "batch: 1563/1563 - train loss: 12.9971 - test loss: 13.9280 - train acc: 0.4034 - test acc: 0.3678 - 4m 57s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.2687 - test loss: 16.4789 - train acc: 0.4757 - test acc: 0.3093 - 5m 1s\n",
      "batch: 200/1563 - train loss: 11.7386 - test loss: 14.5418 - train acc: 0.4450 - test acc: 0.3545 - 5m 6s\n",
      "batch: 300/1563 - train loss: 11.7241 - test loss: 20.5957 - train acc: 0.4441 - test acc: 0.2448 - 5m 11s\n",
      "batch: 400/1563 - train loss: 11.8185 - test loss: 14.2444 - train acc: 0.4441 - test acc: 0.3670 - 5m 15s\n",
      "batch: 500/1563 - train loss: 11.6794 - test loss: 14.0614 - train acc: 0.4394 - test acc: 0.3794 - 5m 20s\n",
      "batch: 600/1563 - train loss: 11.6529 - test loss: 13.7252 - train acc: 0.4538 - test acc: 0.3808 - 5m 25s\n",
      "batch: 700/1563 - train loss: 11.3714 - test loss: 16.7130 - train acc: 0.4516 - test acc: 0.3195 - 5m 30s\n",
      "batch: 800/1563 - train loss: 11.8926 - test loss: 13.9899 - train acc: 0.4393 - test acc: 0.3770 - 5m 34s\n",
      "batch: 900/1563 - train loss: 11.7142 - test loss: 14.7990 - train acc: 0.4378 - test acc: 0.3470 - 5m 39s\n",
      "batch: 1000/1563 - train loss: 12.0723 - test loss: 14.4953 - train acc: 0.4341 - test acc: 0.3634 - 5m 44s\n",
      "batch: 1100/1563 - train loss: 11.6708 - test loss: 15.3665 - train acc: 0.4535 - test acc: 0.3426 - 5m 48s\n",
      "batch: 1200/1563 - train loss: 11.9686 - test loss: 14.4230 - train acc: 0.4322 - test acc: 0.3603 - 5m 53s\n",
      "batch: 1300/1563 - train loss: 11.8478 - test loss: 13.6779 - train acc: 0.4379 - test acc: 0.3877 - 5m 58s\n",
      "batch: 1400/1563 - train loss: 11.8037 - test loss: 13.7313 - train acc: 0.4419 - test acc: 0.3817 - 6m 3s\n",
      "batch: 1500/1563 - train loss: 12.0118 - test loss: 13.0835 - train acc: 0.4344 - test acc: 0.4097 - 6m 7s\n",
      "batch: 1563/1563 - train loss: 11.9446 - test loss: 16.7025 - train acc: 0.4388 - test acc: 0.3104 - 6m 11s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.6453 - test loss: 14.4675 - train acc: 0.5397 - test acc: 0.3627 - 6m 16s\n",
      "batch: 200/1563 - train loss: 10.1780 - test loss: 13.5116 - train acc: 0.5016 - test acc: 0.3904 - 6m 20s\n",
      "batch: 300/1563 - train loss: 10.1591 - test loss: 15.1230 - train acc: 0.5088 - test acc: 0.3542 - 6m 25s\n",
      "batch: 400/1563 - train loss: 10.5762 - test loss: 13.8136 - train acc: 0.4894 - test acc: 0.3873 - 6m 31s\n",
      "batch: 500/1563 - train loss: 10.3959 - test loss: 13.8313 - train acc: 0.5066 - test acc: 0.3869 - 6m 35s\n",
      "batch: 600/1563 - train loss: 10.5663 - test loss: 14.2655 - train acc: 0.4984 - test acc: 0.3824 - 6m 40s\n",
      "batch: 700/1563 - train loss: 10.6236 - test loss: 14.6823 - train acc: 0.4838 - test acc: 0.3632 - 6m 44s\n",
      "batch: 800/1563 - train loss: 10.4586 - test loss: 13.3789 - train acc: 0.4972 - test acc: 0.4037 - 6m 49s\n",
      "batch: 900/1563 - train loss: 10.7676 - test loss: 13.5695 - train acc: 0.4916 - test acc: 0.3971 - 6m 54s\n",
      "batch: 1000/1563 - train loss: 10.8619 - test loss: 13.3605 - train acc: 0.4766 - test acc: 0.4052 - 6m 58s\n",
      "batch: 1100/1563 - train loss: 11.1128 - test loss: 14.0440 - train acc: 0.4719 - test acc: 0.3858 - 7m 3s\n",
      "batch: 1200/1563 - train loss: 10.8755 - test loss: 13.6663 - train acc: 0.4769 - test acc: 0.3923 - 7m 8s\n",
      "batch: 1300/1563 - train loss: 10.9874 - test loss: 13.1350 - train acc: 0.4687 - test acc: 0.4080 - 7m 13s\n",
      "batch: 1400/1563 - train loss: 10.8411 - test loss: 15.5598 - train acc: 0.4763 - test acc: 0.3312 - 7m 18s\n",
      "batch: 1500/1563 - train loss: 11.0070 - test loss: 13.4164 - train acc: 0.4787 - test acc: 0.4009 - 7m 22s\n",
      "batch: 1563/1563 - train loss: 10.8139 - test loss: 13.9686 - train acc: 0.4896 - test acc: 0.3814 - 7m 26s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.5373 - test loss: 13.9721 - train acc: 0.5753 - test acc: 0.3893 - 7m 31s\n",
      "batch: 200/1563 - train loss: 8.7419 - test loss: 13.9162 - train acc: 0.5618 - test acc: 0.3954 - 7m 36s\n",
      "batch: 300/1563 - train loss: 8.8130 - test loss: 14.6125 - train acc: 0.5709 - test acc: 0.3781 - 7m 40s\n",
      "batch: 400/1563 - train loss: 8.7523 - test loss: 13.9904 - train acc: 0.5578 - test acc: 0.3948 - 7m 45s\n",
      "batch: 500/1563 - train loss: 9.1176 - test loss: 14.9627 - train acc: 0.5462 - test acc: 0.3747 - 7m 49s\n",
      "batch: 600/1563 - train loss: 9.4938 - test loss: 13.6181 - train acc: 0.5328 - test acc: 0.4014 - 7m 54s\n",
      "batch: 700/1563 - train loss: 9.5604 - test loss: 13.4626 - train acc: 0.5415 - test acc: 0.4113 - 7m 59s\n",
      "batch: 800/1563 - train loss: 9.7129 - test loss: 14.1824 - train acc: 0.5213 - test acc: 0.3920 - 8m 4s\n",
      "batch: 900/1563 - train loss: 9.5001 - test loss: 13.4821 - train acc: 0.5287 - test acc: 0.4028 - 8m 9s\n",
      "batch: 1000/1563 - train loss: 9.9025 - test loss: 14.5593 - train acc: 0.5156 - test acc: 0.3800 - 8m 13s\n",
      "batch: 1100/1563 - train loss: 9.9505 - test loss: 13.7679 - train acc: 0.5194 - test acc: 0.4020 - 8m 18s\n",
      "batch: 1200/1563 - train loss: 9.8226 - test loss: 13.3966 - train acc: 0.5278 - test acc: 0.4015 - 8m 22s\n",
      "batch: 1300/1563 - train loss: 10.0877 - test loss: 12.9791 - train acc: 0.5137 - test acc: 0.4169 - 8m 27s\n",
      "batch: 1400/1563 - train loss: 10.0542 - test loss: 13.5187 - train acc: 0.5047 - test acc: 0.4066 - 8m 32s\n",
      "batch: 1500/1563 - train loss: 9.9177 - test loss: 13.7257 - train acc: 0.5119 - test acc: 0.3981 - 8m 37s\n",
      "batch: 1563/1563 - train loss: 9.9021 - test loss: 12.8580 - train acc: 0.5144 - test acc: 0.4211 - 8m 41s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.5530 - test loss: 13.0022 - train acc: 0.6216 - test acc: 0.4284 - 8m 46s\n",
      "batch: 200/1563 - train loss: 7.5590 - test loss: 14.0275 - train acc: 0.6166 - test acc: 0.4070 - 8m 50s\n",
      "batch: 300/1563 - train loss: 8.0575 - test loss: 14.3087 - train acc: 0.5913 - test acc: 0.3900 - 8m 55s\n",
      "batch: 400/1563 - train loss: 7.7883 - test loss: 16.0584 - train acc: 0.6084 - test acc: 0.3514 - 9m 0s\n",
      "batch: 500/1563 - train loss: 8.1932 - test loss: 14.0578 - train acc: 0.5881 - test acc: 0.4005 - 9m 4s\n",
      "batch: 600/1563 - train loss: 8.5254 - test loss: 14.4844 - train acc: 0.5694 - test acc: 0.3884 - 9m 9s\n",
      "batch: 700/1563 - train loss: 8.5929 - test loss: 15.3203 - train acc: 0.5680 - test acc: 0.3654 - 9m 14s\n",
      "batch: 800/1563 - train loss: 8.6015 - test loss: 14.3404 - train acc: 0.5819 - test acc: 0.3937 - 9m 18s\n",
      "batch: 900/1563 - train loss: 8.6496 - test loss: 14.3597 - train acc: 0.5638 - test acc: 0.3895 - 9m 23s\n",
      "batch: 1000/1563 - train loss: 8.9273 - test loss: 13.9252 - train acc: 0.5640 - test acc: 0.4056 - 9m 28s\n",
      "batch: 1100/1563 - train loss: 8.6180 - test loss: 13.5767 - train acc: 0.5643 - test acc: 0.4134 - 9m 32s\n",
      "batch: 1200/1563 - train loss: 8.9720 - test loss: 13.4737 - train acc: 0.5519 - test acc: 0.4177 - 9m 37s\n",
      "batch: 1300/1563 - train loss: 8.9052 - test loss: 13.4502 - train acc: 0.5649 - test acc: 0.4163 - 9m 42s\n",
      "batch: 1400/1563 - train loss: 8.5945 - test loss: 13.6161 - train acc: 0.5719 - test acc: 0.4139 - 9m 47s\n",
      "batch: 1500/1563 - train loss: 9.2307 - test loss: 13.6514 - train acc: 0.5465 - test acc: 0.4055 - 9m 51s\n",
      "batch: 1563/1563 - train loss: 9.2886 - test loss: 13.3892 - train acc: 0.5337 - test acc: 0.4126 - 9m 55s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.6047 - test loss: 13.9721 - train acc: 0.6694 - test acc: 0.4128 - 10m 0s\n",
      "batch: 200/1563 - train loss: 6.5056 - test loss: 14.2365 - train acc: 0.6716 - test acc: 0.4114 - 10m 5s\n",
      "batch: 300/1563 - train loss: 6.7788 - test loss: 14.2863 - train acc: 0.6516 - test acc: 0.4008 - 10m 10s\n",
      "batch: 400/1563 - train loss: 6.7971 - test loss: 13.7908 - train acc: 0.6525 - test acc: 0.4315 - 10m 14s\n",
      "batch: 500/1563 - train loss: 7.1144 - test loss: 14.1264 - train acc: 0.6362 - test acc: 0.4194 - 10m 19s\n",
      "batch: 600/1563 - train loss: 7.5835 - test loss: 14.5438 - train acc: 0.6137 - test acc: 0.4077 - 10m 24s\n",
      "batch: 700/1563 - train loss: 7.3940 - test loss: 14.8195 - train acc: 0.6213 - test acc: 0.3955 - 10m 28s\n",
      "batch: 800/1563 - train loss: 7.7847 - test loss: 13.7350 - train acc: 0.6144 - test acc: 0.4195 - 10m 33s\n",
      "batch: 900/1563 - train loss: 7.8718 - test loss: 14.0325 - train acc: 0.5984 - test acc: 0.4150 - 10m 38s\n",
      "batch: 1000/1563 - train loss: 7.7704 - test loss: 14.3668 - train acc: 0.6113 - test acc: 0.4123 - 10m 43s\n",
      "batch: 1100/1563 - train loss: 7.9729 - test loss: 14.8349 - train acc: 0.5943 - test acc: 0.3881 - 10m 47s\n",
      "batch: 1200/1563 - train loss: 7.8470 - test loss: 14.1878 - train acc: 0.5981 - test acc: 0.4079 - 10m 52s\n",
      "batch: 1300/1563 - train loss: 8.0068 - test loss: 13.7929 - train acc: 0.5918 - test acc: 0.4230 - 10m 57s\n",
      "batch: 1400/1563 - train loss: 8.0397 - test loss: 13.8302 - train acc: 0.5987 - test acc: 0.4193 - 11m 1s\n",
      "batch: 1500/1563 - train loss: 8.1232 - test loss: 13.6246 - train acc: 0.5837 - test acc: 0.4190 - 11m 6s\n",
      "batch: 1563/1563 - train loss: 7.9599 - test loss: 14.2222 - train acc: 0.5978 - test acc: 0.4097 - 11m 10s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.4907 - test loss: 13.9840 - train acc: 0.7163 - test acc: 0.4260 - 11m 15s\n",
      "batch: 200/1563 - train loss: 5.5984 - test loss: 14.9625 - train acc: 0.7048 - test acc: 0.4090 - 11m 20s\n",
      "batch: 300/1563 - train loss: 5.8168 - test loss: 14.7061 - train acc: 0.6922 - test acc: 0.4147 - 11m 24s\n",
      "batch: 400/1563 - train loss: 5.9043 - test loss: 14.3037 - train acc: 0.6960 - test acc: 0.4216 - 11m 29s\n",
      "batch: 500/1563 - train loss: 6.0896 - test loss: 14.4286 - train acc: 0.6791 - test acc: 0.4249 - 11m 34s\n",
      "batch: 600/1563 - train loss: 6.1897 - test loss: 14.3081 - train acc: 0.6735 - test acc: 0.4223 - 11m 38s\n",
      "batch: 700/1563 - train loss: 6.4054 - test loss: 15.0818 - train acc: 0.6754 - test acc: 0.4031 - 11m 43s\n",
      "batch: 800/1563 - train loss: 6.6753 - test loss: 14.6028 - train acc: 0.6473 - test acc: 0.4122 - 11m 48s\n",
      "batch: 900/1563 - train loss: 6.9998 - test loss: 14.8759 - train acc: 0.6368 - test acc: 0.4045 - 11m 53s\n",
      "batch: 1000/1563 - train loss: 6.9452 - test loss: 14.6245 - train acc: 0.6438 - test acc: 0.4091 - 11m 57s\n",
      "batch: 1100/1563 - train loss: 7.3444 - test loss: 15.2731 - train acc: 0.6298 - test acc: 0.3868 - 12m 2s\n",
      "batch: 1200/1563 - train loss: 6.7927 - test loss: 15.0111 - train acc: 0.6481 - test acc: 0.3919 - 12m 7s\n",
      "batch: 1300/1563 - train loss: 7.0551 - test loss: 14.4860 - train acc: 0.6325 - test acc: 0.4120 - 12m 11s\n",
      "batch: 1400/1563 - train loss: 7.2990 - test loss: 13.9713 - train acc: 0.6303 - test acc: 0.4274 - 12m 17s\n",
      "batch: 1500/1563 - train loss: 7.3022 - test loss: 14.3737 - train acc: 0.6307 - test acc: 0.4147 - 12m 21s\n",
      "batch: 1563/1563 - train loss: 7.5420 - test loss: 15.3469 - train acc: 0.6045 - test acc: 0.3874 - 12m 25s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.9806 - test loss: 14.9715 - train acc: 0.7400 - test acc: 0.4102 - 12m 30s\n",
      "batch: 200/1563 - train loss: 4.6864 - test loss: 15.6622 - train acc: 0.7578 - test acc: 0.4064 - 12m 34s\n",
      "batch: 300/1563 - train loss: 4.7578 - test loss: 15.5466 - train acc: 0.7534 - test acc: 0.4173 - 12m 39s\n",
      "batch: 400/1563 - train loss: 5.1605 - test loss: 15.9200 - train acc: 0.7184 - test acc: 0.3987 - 12m 44s\n",
      "batch: 500/1563 - train loss: 5.0750 - test loss: 17.6431 - train acc: 0.7235 - test acc: 0.3643 - 12m 49s\n",
      "batch: 600/1563 - train loss: 5.5823 - test loss: 15.3563 - train acc: 0.7141 - test acc: 0.4132 - 12m 54s\n",
      "batch: 700/1563 - train loss: 5.1864 - test loss: 15.4440 - train acc: 0.7215 - test acc: 0.4117 - 12m 58s\n",
      "batch: 800/1563 - train loss: 5.8908 - test loss: 15.3049 - train acc: 0.6897 - test acc: 0.4119 - 13m 3s\n",
      "batch: 900/1563 - train loss: 5.8550 - test loss: 14.9030 - train acc: 0.6891 - test acc: 0.4202 - 13m 8s\n",
      "batch: 1000/1563 - train loss: 6.1242 - test loss: 15.4835 - train acc: 0.6731 - test acc: 0.4048 - 13m 13s\n",
      "batch: 1100/1563 - train loss: 6.4013 - test loss: 15.6038 - train acc: 0.6591 - test acc: 0.3908 - 13m 17s\n",
      "batch: 1200/1563 - train loss: 6.3731 - test loss: 15.1329 - train acc: 0.6682 - test acc: 0.4137 - 13m 22s\n",
      "batch: 1300/1563 - train loss: 6.4017 - test loss: 16.4095 - train acc: 0.6616 - test acc: 0.3883 - 13m 27s\n",
      "batch: 1400/1563 - train loss: 6.7426 - test loss: 15.1695 - train acc: 0.6500 - test acc: 0.3986 - 13m 32s\n",
      "batch: 1500/1563 - train loss: 6.4819 - test loss: 15.7410 - train acc: 0.6547 - test acc: 0.3891 - 13m 36s\n",
      "batch: 1563/1563 - train loss: 6.4316 - test loss: 15.2890 - train acc: 0.6641 - test acc: 0.4078 - 13m 41s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.0523 - test loss: 15.5623 - train acc: 0.7809 - test acc: 0.4147 - 13m 45s\n",
      "batch: 200/1563 - train loss: 4.0083 - test loss: 15.2141 - train acc: 0.7925 - test acc: 0.4216 - 13m 50s\n",
      "batch: 300/1563 - train loss: 4.2134 - test loss: 15.8930 - train acc: 0.7766 - test acc: 0.4178 - 13m 55s\n",
      "batch: 400/1563 - train loss: 4.3773 - test loss: 16.4936 - train acc: 0.7669 - test acc: 0.4002 - 14m 0s\n",
      "batch: 500/1563 - train loss: 4.4263 - test loss: 15.6347 - train acc: 0.7540 - test acc: 0.4222 - 14m 5s\n",
      "batch: 600/1563 - train loss: 4.6019 - test loss: 15.7251 - train acc: 0.7487 - test acc: 0.4209 - 14m 10s\n",
      "batch: 700/1563 - train loss: 4.7642 - test loss: 16.1727 - train acc: 0.7466 - test acc: 0.4058 - 14m 15s\n",
      "batch: 800/1563 - train loss: 5.0122 - test loss: 16.1956 - train acc: 0.7369 - test acc: 0.4148 - 14m 19s\n",
      "batch: 900/1563 - train loss: 5.2703 - test loss: 15.9402 - train acc: 0.7188 - test acc: 0.4092 - 14m 25s\n",
      "batch: 1000/1563 - train loss: 5.2429 - test loss: 16.3629 - train acc: 0.7191 - test acc: 0.4003 - 14m 30s\n",
      "batch: 1100/1563 - train loss: 5.2497 - test loss: 15.7895 - train acc: 0.7163 - test acc: 0.4153 - 14m 34s\n",
      "batch: 1200/1563 - train loss: 5.2307 - test loss: 16.3908 - train acc: 0.7213 - test acc: 0.4020 - 14m 39s\n",
      "batch: 1300/1563 - train loss: 5.6167 - test loss: 16.1141 - train acc: 0.6935 - test acc: 0.4106 - 14m 45s\n",
      "batch: 1400/1563 - train loss: 5.6650 - test loss: 15.8785 - train acc: 0.7003 - test acc: 0.4143 - 14m 50s\n",
      "batch: 1500/1563 - train loss: 5.9370 - test loss: 15.4358 - train acc: 0.6750 - test acc: 0.4254 - 14m 55s\n",
      "batch: 1563/1563 - train loss: 5.8645 - test loss: 15.7728 - train acc: 0.6941 - test acc: 0.4101 - 14m 59s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.5987 - test loss: 15.8496 - train acc: 0.8059 - test acc: 0.4151 - 15m 4s\n",
      "batch: 200/1563 - train loss: 3.5129 - test loss: 16.1317 - train acc: 0.8043 - test acc: 0.4070 - 15m 8s\n",
      "batch: 300/1563 - train loss: 3.6375 - test loss: 16.6160 - train acc: 0.8052 - test acc: 0.4129 - 15m 13s\n",
      "batch: 400/1563 - train loss: 3.7653 - test loss: 16.1092 - train acc: 0.7862 - test acc: 0.4263 - 15m 18s\n",
      "batch: 500/1563 - train loss: 3.9784 - test loss: 18.0480 - train acc: 0.7812 - test acc: 0.3909 - 15m 23s\n",
      "batch: 600/1563 - train loss: 4.2144 - test loss: 17.3891 - train acc: 0.7740 - test acc: 0.3988 - 15m 28s\n",
      "batch: 700/1563 - train loss: 4.3856 - test loss: 16.6994 - train acc: 0.7633 - test acc: 0.4056 - 15m 33s\n",
      "batch: 800/1563 - train loss: 4.4684 - test loss: 16.4744 - train acc: 0.7500 - test acc: 0.4165 - 15m 38s\n",
      "batch: 900/1563 - train loss: 4.5478 - test loss: 17.1483 - train acc: 0.7444 - test acc: 0.4019 - 15m 42s\n",
      "batch: 1000/1563 - train loss: 4.6556 - test loss: 16.9539 - train acc: 0.7472 - test acc: 0.4086 - 15m 47s\n",
      "batch: 1100/1563 - train loss: 4.6616 - test loss: 16.0831 - train acc: 0.7487 - test acc: 0.4200 - 15m 52s\n",
      "batch: 1200/1563 - train loss: 4.8322 - test loss: 17.4327 - train acc: 0.7331 - test acc: 0.3892 - 15m 57s\n",
      "batch: 1300/1563 - train loss: 4.9405 - test loss: 16.2104 - train acc: 0.7368 - test acc: 0.4238 - 16m 2s\n",
      "batch: 1400/1563 - train loss: 4.8628 - test loss: 16.3353 - train acc: 0.7303 - test acc: 0.4191 - 16m 6s\n",
      "batch: 1500/1563 - train loss: 4.9448 - test loss: 15.8928 - train acc: 0.7341 - test acc: 0.4310 - 16m 11s\n",
      "batch: 1563/1563 - train loss: 4.8954 - test loss: 16.6261 - train acc: 0.7344 - test acc: 0.4023 - 16m 15s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.9280 - test loss: 16.3120 - train acc: 0.8403 - test acc: 0.4250 - 16m 20s\n",
      "batch: 200/1563 - train loss: 2.8075 - test loss: 16.2028 - train acc: 0.8497 - test acc: 0.4285 - 16m 24s\n",
      "batch: 300/1563 - train loss: 2.9095 - test loss: 17.1054 - train acc: 0.8384 - test acc: 0.4083 - 16m 29s\n",
      "batch: 400/1563 - train loss: 2.9426 - test loss: 17.5734 - train acc: 0.8362 - test acc: 0.4092 - 16m 34s\n",
      "batch: 500/1563 - train loss: 3.1042 - test loss: 18.1032 - train acc: 0.8306 - test acc: 0.3972 - 16m 39s\n",
      "batch: 600/1563 - train loss: 3.3235 - test loss: 17.9280 - train acc: 0.8106 - test acc: 0.4111 - 16m 44s\n",
      "batch: 700/1563 - train loss: 3.4211 - test loss: 17.1428 - train acc: 0.8008 - test acc: 0.4133 - 16m 48s\n",
      "batch: 800/1563 - train loss: 3.4935 - test loss: 17.2567 - train acc: 0.8081 - test acc: 0.4208 - 16m 53s\n",
      "batch: 900/1563 - train loss: 3.8570 - test loss: 17.9358 - train acc: 0.7900 - test acc: 0.4031 - 16m 58s\n",
      "batch: 1000/1563 - train loss: 3.8371 - test loss: 17.1655 - train acc: 0.7956 - test acc: 0.4153 - 17m 3s\n",
      "batch: 1100/1563 - train loss: 3.9883 - test loss: 16.7021 - train acc: 0.7822 - test acc: 0.4226 - 17m 7s\n",
      "batch: 1200/1563 - train loss: 4.1970 - test loss: 17.6392 - train acc: 0.7740 - test acc: 0.4014 - 17m 12s\n",
      "batch: 1300/1563 - train loss: 4.1379 - test loss: 16.6825 - train acc: 0.7724 - test acc: 0.4211 - 17m 17s\n",
      "batch: 1400/1563 - train loss: 4.1739 - test loss: 16.9346 - train acc: 0.7712 - test acc: 0.4190 - 17m 22s\n",
      "batch: 1500/1563 - train loss: 4.4276 - test loss: 18.4275 - train acc: 0.7581 - test acc: 0.3932 - 17m 26s\n",
      "batch: 1563/1563 - train loss: 4.4581 - test loss: 16.6793 - train acc: 0.7519 - test acc: 0.4207 - 17m 31s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.8162 - test loss: 16.8380 - train acc: 0.8462 - test acc: 0.4209 - 17m 36s\n",
      "batch: 200/1563 - train loss: 2.4623 - test loss: 17.3217 - train acc: 0.8688 - test acc: 0.4234 - 17m 40s\n",
      "batch: 300/1563 - train loss: 2.3969 - test loss: 17.4534 - train acc: 0.8694 - test acc: 0.4248 - 17m 45s\n",
      "batch: 400/1563 - train loss: 2.3639 - test loss: 17.7042 - train acc: 0.8606 - test acc: 0.4226 - 17m 50s\n",
      "batch: 500/1563 - train loss: 2.7252 - test loss: 17.7495 - train acc: 0.8521 - test acc: 0.4267 - 17m 54s\n",
      "batch: 600/1563 - train loss: 2.8243 - test loss: 18.2644 - train acc: 0.8434 - test acc: 0.4106 - 17m 59s\n",
      "batch: 700/1563 - train loss: 2.8030 - test loss: 17.6285 - train acc: 0.8387 - test acc: 0.4223 - 18m 4s\n",
      "batch: 800/1563 - train loss: 2.9957 - test loss: 18.0422 - train acc: 0.8350 - test acc: 0.4194 - 18m 9s\n",
      "batch: 900/1563 - train loss: 3.2574 - test loss: 18.0584 - train acc: 0.8203 - test acc: 0.4140 - 18m 14s\n",
      "batch: 1000/1563 - train loss: 3.2565 - test loss: 18.0263 - train acc: 0.8109 - test acc: 0.4193 - 18m 19s\n",
      "batch: 1100/1563 - train loss: 3.5285 - test loss: 17.7881 - train acc: 0.8022 - test acc: 0.4208 - 18m 23s\n",
      "batch: 1200/1563 - train loss: 3.3927 - test loss: 18.4762 - train acc: 0.8072 - test acc: 0.4158 - 18m 28s\n",
      "batch: 1300/1563 - train loss: 3.7850 - test loss: 18.5145 - train acc: 0.7968 - test acc: 0.3995 - 18m 33s\n",
      "batch: 1400/1563 - train loss: 3.9437 - test loss: 17.7154 - train acc: 0.7800 - test acc: 0.4195 - 18m 38s\n",
      "batch: 1500/1563 - train loss: 3.6774 - test loss: 17.3270 - train acc: 0.7943 - test acc: 0.4188 - 18m 42s\n",
      "batch: 1563/1563 - train loss: 3.7125 - test loss: 17.8585 - train acc: 0.7961 - test acc: 0.4138 - 18m 46s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.2636 - test loss: 17.8938 - train acc: 0.8757 - test acc: 0.4106 - 18m 51s\n",
      "batch: 200/1563 - train loss: 2.0926 - test loss: 17.5352 - train acc: 0.8835 - test acc: 0.4314 - 18m 56s\n",
      "batch: 300/1563 - train loss: 2.1619 - test loss: 17.6028 - train acc: 0.8766 - test acc: 0.4259 - 19m 0s\n",
      "batch: 400/1563 - train loss: 2.0976 - test loss: 18.0045 - train acc: 0.8722 - test acc: 0.4232 - 19m 6s\n",
      "batch: 500/1563 - train loss: 2.1994 - test loss: 17.9999 - train acc: 0.8735 - test acc: 0.4251 - 19m 10s\n",
      "batch: 600/1563 - train loss: 2.4525 - test loss: 18.4876 - train acc: 0.8641 - test acc: 0.4146 - 19m 15s\n",
      "batch: 700/1563 - train loss: 2.4383 - test loss: 18.6990 - train acc: 0.8562 - test acc: 0.4207 - 19m 20s\n",
      "batch: 800/1563 - train loss: 2.6079 - test loss: 18.4926 - train acc: 0.8522 - test acc: 0.4228 - 19m 25s\n",
      "batch: 900/1563 - train loss: 2.7042 - test loss: 18.6997 - train acc: 0.8441 - test acc: 0.4169 - 19m 29s\n",
      "batch: 1000/1563 - train loss: 2.9237 - test loss: 18.9889 - train acc: 0.8387 - test acc: 0.4193 - 19m 34s\n",
      "batch: 1100/1563 - train loss: 2.9003 - test loss: 18.5770 - train acc: 0.8318 - test acc: 0.4224 - 19m 39s\n",
      "batch: 1200/1563 - train loss: 2.8310 - test loss: 18.8955 - train acc: 0.8425 - test acc: 0.4114 - 19m 44s\n",
      "batch: 1300/1563 - train loss: 3.2175 - test loss: 19.4210 - train acc: 0.8199 - test acc: 0.4030 - 19m 49s\n",
      "batch: 1400/1563 - train loss: 3.3247 - test loss: 18.9326 - train acc: 0.8164 - test acc: 0.4129 - 19m 53s\n",
      "batch: 1500/1563 - train loss: 3.4499 - test loss: 18.8969 - train acc: 0.8106 - test acc: 0.4064 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 1501/1563 - train loss: 3.4278 - test loss: 18.8119 - train acc: 0.8118 - test acc: 0.4076 - 20m 1s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "testing - beta: 0.9 - part size: 8 - block upd: 64 - combination nº: 10\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 8 - nº part: 1176 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 36864 - partition: 8 - nº part: 4608 - block updates: 64\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 64 - partition: 8 - nº part: 8 - block updates: 8\n",
      "FisherPartitioner: param: 73728 - partition: 8 - nº part: 9216 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 8192 - partition: 8 - nº part: 1024 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 147456 - partition: 8 - nº part: 18432 - block updates: 64\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 128 - partition: 8 - nº part: 16 - block updates: 16\n",
      "FisherPartitioner: param: 294912 - partition: 8 - nº part: 36864 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 32768 - partition: 8 - nº part: 4096 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 589824 - partition: 8 - nº part: 73728 - block updates: 64\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 256 - partition: 8 - nº part: 32 - block updates: 32\n",
      "FisherPartitioner: param: 1179648 - partition: 8 - nº part: 147456 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 131072 - partition: 8 - nº part: 16384 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 2359296 - partition: 8 - nº part: 294912 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 512 - partition: 8 - nº part: 64 - block updates: 64\n",
      "FisherPartitioner: param: 51200 - partition: 8 - nº part: 6400 - block updates: 64\n",
      "FisherPartitioner: param: 100 - partition: 8 - nº part: 13 - block updates: 13\n",
      "total partitions: 1403477 - effective block updates: 2557\n",
      "initializing inverses...\n",
      "partition 1/1403477\n",
      "partition 10000/1403477\n",
      "partition 20000/1403477\n",
      "partition 30000/1403477\n",
      "partition 40000/1403477\n",
      "partition 50000/1403477\n",
      "partition 60000/1403477\n",
      "partition 70000/1403477\n",
      "partition 80000/1403477\n",
      "partition 90000/1403477\n",
      "partition 100000/1403477\n",
      "partition 110000/1403477\n",
      "partition 120000/1403477\n",
      "partition 130000/1403477\n",
      "partition 140000/1403477\n",
      "partition 150000/1403477\n",
      "partition 160000/1403477\n",
      "partition 170000/1403477\n",
      "partition 180000/1403477\n",
      "partition 190000/1403477\n",
      "partition 200000/1403477\n",
      "partition 210000/1403477\n",
      "partition 220000/1403477\n",
      "partition 230000/1403477\n",
      "partition 240000/1403477\n",
      "partition 250000/1403477\n",
      "partition 260000/1403477\n",
      "partition 270000/1403477\n",
      "partition 280000/1403477\n",
      "partition 290000/1403477\n",
      "partition 300000/1403477\n",
      "partition 310000/1403477\n",
      "partition 320000/1403477\n",
      "partition 330000/1403477\n",
      "partition 340000/1403477\n",
      "partition 350000/1403477\n",
      "partition 360000/1403477\n",
      "partition 370000/1403477\n",
      "partition 380000/1403477\n",
      "partition 390000/1403477\n",
      "partition 400000/1403477\n",
      "partition 410000/1403477\n",
      "partition 420000/1403477\n",
      "partition 430000/1403477\n",
      "partition 440000/1403477\n",
      "partition 450000/1403477\n",
      "partition 460000/1403477\n",
      "partition 470000/1403477\n",
      "partition 480000/1403477\n",
      "partition 490000/1403477\n",
      "partition 500000/1403477\n",
      "partition 510000/1403477\n",
      "partition 520000/1403477\n",
      "partition 530000/1403477\n",
      "partition 540000/1403477\n",
      "partition 550000/1403477\n",
      "partition 560000/1403477\n",
      "partition 570000/1403477\n",
      "partition 580000/1403477\n",
      "partition 590000/1403477\n",
      "partition 600000/1403477\n",
      "partition 610000/1403477\n",
      "partition 620000/1403477\n",
      "partition 630000/1403477\n",
      "partition 640000/1403477\n",
      "partition 650000/1403477\n",
      "partition 660000/1403477\n",
      "partition 670000/1403477\n",
      "partition 680000/1403477\n",
      "partition 690000/1403477\n",
      "partition 700000/1403477\n",
      "partition 710000/1403477\n",
      "partition 720000/1403477\n",
      "partition 730000/1403477\n",
      "partition 740000/1403477\n",
      "partition 750000/1403477\n",
      "partition 760000/1403477\n",
      "partition 770000/1403477\n",
      "partition 780000/1403477\n",
      "partition 790000/1403477\n",
      "partition 800000/1403477\n",
      "partition 810000/1403477\n",
      "partition 820000/1403477\n",
      "partition 830000/1403477\n",
      "partition 840000/1403477\n",
      "partition 850000/1403477\n",
      "partition 860000/1403477\n",
      "partition 870000/1403477\n",
      "partition 880000/1403477\n",
      "partition 890000/1403477\n",
      "partition 900000/1403477\n",
      "partition 910000/1403477\n",
      "partition 920000/1403477\n",
      "partition 930000/1403477\n",
      "partition 940000/1403477\n",
      "partition 950000/1403477\n",
      "partition 960000/1403477\n",
      "partition 970000/1403477\n",
      "partition 980000/1403477\n",
      "partition 990000/1403477\n",
      "partition 1000000/1403477\n",
      "partition 1010000/1403477\n",
      "partition 1020000/1403477\n",
      "partition 1030000/1403477\n",
      "partition 1040000/1403477\n",
      "partition 1050000/1403477\n",
      "partition 1060000/1403477\n",
      "partition 1070000/1403477\n",
      "partition 1080000/1403477\n",
      "partition 1090000/1403477\n",
      "partition 1100000/1403477\n",
      "partition 1110000/1403477\n",
      "partition 1120000/1403477\n",
      "partition 1130000/1403477\n",
      "partition 1140000/1403477\n",
      "partition 1150000/1403477\n",
      "partition 1160000/1403477\n",
      "partition 1170000/1403477\n",
      "partition 1180000/1403477\n",
      "partition 1190000/1403477\n",
      "partition 1200000/1403477\n",
      "partition 1210000/1403477\n",
      "partition 1220000/1403477\n",
      "partition 1230000/1403477\n",
      "partition 1240000/1403477\n",
      "partition 1250000/1403477\n",
      "partition 1260000/1403477\n",
      "partition 1270000/1403477\n",
      "partition 1280000/1403477\n",
      "partition 1290000/1403477\n",
      "partition 1300000/1403477\n",
      "partition 1310000/1403477\n",
      "partition 1320000/1403477\n",
      "partition 1330000/1403477\n",
      "partition 1340000/1403477\n",
      "partition 1350000/1403477\n",
      "partition 1360000/1403477\n",
      "partition 1370000/1403477\n",
      "partition 1380000/1403477\n",
      "partition 1390000/1403477\n",
      "partition 1400000/1403477\n",
      "partition 1403477/1403477\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1014 - test loss: 25.4610 - train acc: 0.0336 - test acc: 0.0567 - 0m 1s\n",
      "batch: 200/1563 - train loss: 23.6629 - test loss: 23.0207 - train acc: 0.0696 - test acc: 0.0769 - 0m 6s\n",
      "batch: 300/1563 - train loss: 22.9224 - test loss: 22.5953 - train acc: 0.0736 - test acc: 0.0975 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.2311 - test loss: 22.3966 - train acc: 0.1009 - test acc: 0.1030 - 0m 15s\n",
      "batch: 500/1563 - train loss: 21.7468 - test loss: 21.3950 - train acc: 0.1084 - test acc: 0.1240 - 0m 20s\n",
      "batch: 600/1563 - train loss: 21.0853 - test loss: 20.6897 - train acc: 0.1369 - test acc: 0.1353 - 0m 25s\n",
      "batch: 700/1563 - train loss: 20.6502 - test loss: 20.4982 - train acc: 0.1297 - test acc: 0.1429 - 0m 30s\n",
      "batch: 800/1563 - train loss: 20.4702 - test loss: 20.0753 - train acc: 0.1444 - test acc: 0.1514 - 0m 34s\n",
      "batch: 900/1563 - train loss: 20.1332 - test loss: 20.0867 - train acc: 0.1607 - test acc: 0.1606 - 0m 39s\n",
      "batch: 1000/1563 - train loss: 19.6242 - test loss: 19.1776 - train acc: 0.1719 - test acc: 0.1812 - 0m 43s\n",
      "batch: 1100/1563 - train loss: 19.3734 - test loss: 19.8940 - train acc: 0.1691 - test acc: 0.1733 - 0m 48s\n",
      "batch: 1200/1563 - train loss: 19.2998 - test loss: 18.7406 - train acc: 0.1869 - test acc: 0.1989 - 0m 53s\n",
      "batch: 1300/1563 - train loss: 18.6651 - test loss: 19.7169 - train acc: 0.2060 - test acc: 0.1697 - 0m 58s\n",
      "batch: 1400/1563 - train loss: 18.7167 - test loss: 18.6780 - train acc: 0.1975 - test acc: 0.1973 - 1m 2s\n",
      "batch: 1500/1563 - train loss: 18.4602 - test loss: 17.8622 - train acc: 0.2048 - test acc: 0.2311 - 1m 7s\n",
      "batch: 1563/1563 - train loss: 18.4252 - test loss: 18.7668 - train acc: 0.2045 - test acc: 0.1948 - 1m 11s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.4844 - test loss: 17.8417 - train acc: 0.2381 - test acc: 0.2324 - 1m 16s\n",
      "batch: 200/1563 - train loss: 17.3117 - test loss: 17.6567 - train acc: 0.2447 - test acc: 0.2400 - 1m 20s\n",
      "batch: 300/1563 - train loss: 17.5698 - test loss: 17.1806 - train acc: 0.2344 - test acc: 0.2585 - 1m 25s\n",
      "batch: 400/1563 - train loss: 16.9976 - test loss: 17.3618 - train acc: 0.2575 - test acc: 0.2441 - 1m 30s\n",
      "batch: 500/1563 - train loss: 17.4100 - test loss: 17.0338 - train acc: 0.2363 - test acc: 0.2552 - 1m 34s\n",
      "batch: 600/1563 - train loss: 16.8116 - test loss: 18.4892 - train acc: 0.2594 - test acc: 0.2209 - 1m 39s\n",
      "batch: 700/1563 - train loss: 17.0018 - test loss: 16.7567 - train acc: 0.2441 - test acc: 0.2672 - 1m 44s\n",
      "batch: 800/1563 - train loss: 16.8585 - test loss: 17.2126 - train acc: 0.2606 - test acc: 0.2506 - 1m 48s\n",
      "batch: 900/1563 - train loss: 16.6283 - test loss: 17.4120 - train acc: 0.2649 - test acc: 0.2361 - 1m 53s\n",
      "batch: 1000/1563 - train loss: 16.2572 - test loss: 17.5111 - train acc: 0.2859 - test acc: 0.2499 - 1m 58s\n",
      "batch: 1100/1563 - train loss: 16.5056 - test loss: 16.6783 - train acc: 0.2722 - test acc: 0.2734 - 2m 3s\n",
      "batch: 1200/1563 - train loss: 16.2653 - test loss: 16.2327 - train acc: 0.2728 - test acc: 0.2819 - 2m 7s\n",
      "batch: 1300/1563 - train loss: 16.2644 - test loss: 16.0320 - train acc: 0.2743 - test acc: 0.2869 - 2m 12s\n",
      "batch: 1400/1563 - train loss: 16.0074 - test loss: 16.6693 - train acc: 0.2847 - test acc: 0.2710 - 2m 17s\n",
      "batch: 1500/1563 - train loss: 15.9151 - test loss: 18.3388 - train acc: 0.2809 - test acc: 0.2330 - 2m 21s\n",
      "batch: 1563/1563 - train loss: 16.1460 - test loss: 15.9921 - train acc: 0.2834 - test acc: 0.2832 - 2m 25s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.5303 - test loss: 16.0031 - train acc: 0.3331 - test acc: 0.2979 - 2m 30s\n",
      "batch: 200/1563 - train loss: 14.7986 - test loss: 16.2266 - train acc: 0.3240 - test acc: 0.2872 - 2m 35s\n",
      "batch: 300/1563 - train loss: 14.9630 - test loss: 16.2001 - train acc: 0.3149 - test acc: 0.2912 - 2m 40s\n",
      "batch: 400/1563 - train loss: 14.5725 - test loss: 15.5516 - train acc: 0.3278 - test acc: 0.3073 - 2m 44s\n",
      "batch: 500/1563 - train loss: 14.9700 - test loss: 15.3249 - train acc: 0.3212 - test acc: 0.3182 - 2m 49s\n",
      "batch: 600/1563 - train loss: 14.5578 - test loss: 15.6686 - train acc: 0.3337 - test acc: 0.3126 - 2m 54s\n",
      "batch: 700/1563 - train loss: 14.5983 - test loss: 15.1570 - train acc: 0.3444 - test acc: 0.3209 - 2m 58s\n",
      "batch: 800/1563 - train loss: 14.7652 - test loss: 15.3016 - train acc: 0.3290 - test acc: 0.3207 - 3m 3s\n",
      "batch: 900/1563 - train loss: 14.4195 - test loss: 16.4556 - train acc: 0.3305 - test acc: 0.2805 - 3m 8s\n",
      "batch: 1000/1563 - train loss: 14.5699 - test loss: 15.1924 - train acc: 0.3394 - test acc: 0.3219 - 3m 13s\n",
      "batch: 1100/1563 - train loss: 14.3688 - test loss: 16.7124 - train acc: 0.3456 - test acc: 0.2784 - 3m 17s\n",
      "batch: 1200/1563 - train loss: 14.4725 - test loss: 14.6533 - train acc: 0.3393 - test acc: 0.3398 - 3m 22s\n",
      "batch: 1300/1563 - train loss: 14.5729 - test loss: 15.7597 - train acc: 0.3475 - test acc: 0.3096 - 3m 27s\n",
      "batch: 1400/1563 - train loss: 14.4531 - test loss: 15.8763 - train acc: 0.3409 - test acc: 0.3019 - 3m 31s\n",
      "batch: 1500/1563 - train loss: 14.1817 - test loss: 14.5212 - train acc: 0.3465 - test acc: 0.3456 - 3m 36s\n",
      "batch: 1563/1563 - train loss: 14.3663 - test loss: 14.4493 - train acc: 0.3384 - test acc: 0.3476 - 3m 40s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.7767 - test loss: 14.5175 - train acc: 0.4076 - test acc: 0.3491 - 3m 45s\n",
      "batch: 200/1563 - train loss: 13.0050 - test loss: 14.6011 - train acc: 0.3978 - test acc: 0.3494 - 3m 49s\n",
      "batch: 300/1563 - train loss: 13.0548 - test loss: 14.5413 - train acc: 0.3938 - test acc: 0.3516 - 3m 54s\n",
      "batch: 400/1563 - train loss: 12.9834 - test loss: 16.7989 - train acc: 0.3810 - test acc: 0.2913 - 3m 59s\n",
      "batch: 500/1563 - train loss: 13.3263 - test loss: 14.7633 - train acc: 0.3809 - test acc: 0.3475 - 4m 3s\n",
      "batch: 600/1563 - train loss: 13.0671 - test loss: 14.4985 - train acc: 0.3997 - test acc: 0.3517 - 4m 8s\n",
      "batch: 700/1563 - train loss: 13.3825 - test loss: 14.6813 - train acc: 0.3784 - test acc: 0.3354 - 4m 13s\n",
      "batch: 800/1563 - train loss: 13.2203 - test loss: 14.1382 - train acc: 0.3922 - test acc: 0.3667 - 4m 18s\n",
      "batch: 900/1563 - train loss: 13.0074 - test loss: 15.1377 - train acc: 0.3829 - test acc: 0.3287 - 4m 22s\n",
      "batch: 1000/1563 - train loss: 12.9201 - test loss: 14.1932 - train acc: 0.4047 - test acc: 0.3604 - 4m 27s\n",
      "batch: 1100/1563 - train loss: 13.3909 - test loss: 14.0059 - train acc: 0.3812 - test acc: 0.3648 - 4m 32s\n",
      "batch: 1200/1563 - train loss: 12.8565 - test loss: 14.3573 - train acc: 0.4041 - test acc: 0.3583 - 4m 36s\n",
      "batch: 1300/1563 - train loss: 12.7255 - test loss: 14.5775 - train acc: 0.4228 - test acc: 0.3542 - 4m 41s\n",
      "batch: 1400/1563 - train loss: 12.7994 - test loss: 14.0896 - train acc: 0.4057 - test acc: 0.3670 - 4m 46s\n",
      "batch: 1500/1563 - train loss: 12.9571 - test loss: 15.7642 - train acc: 0.3969 - test acc: 0.3221 - 4m 50s\n",
      "batch: 1563/1563 - train loss: 12.6229 - test loss: 15.0208 - train acc: 0.4062 - test acc: 0.3396 - 4m 54s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.1688 - test loss: 14.4032 - train acc: 0.4613 - test acc: 0.3597 - 4m 59s\n",
      "batch: 200/1563 - train loss: 11.2757 - test loss: 13.9652 - train acc: 0.4563 - test acc: 0.3687 - 5m 4s\n",
      "batch: 300/1563 - train loss: 11.6696 - test loss: 14.6508 - train acc: 0.4575 - test acc: 0.3545 - 5m 9s\n",
      "batch: 400/1563 - train loss: 11.7251 - test loss: 14.1379 - train acc: 0.4406 - test acc: 0.3710 - 5m 13s\n",
      "batch: 500/1563 - train loss: 11.6760 - test loss: 14.0275 - train acc: 0.4513 - test acc: 0.3733 - 5m 18s\n",
      "batch: 600/1563 - train loss: 11.7150 - test loss: 14.2040 - train acc: 0.4438 - test acc: 0.3740 - 5m 23s\n",
      "batch: 700/1563 - train loss: 11.5408 - test loss: 13.7245 - train acc: 0.4616 - test acc: 0.3842 - 5m 28s\n",
      "batch: 800/1563 - train loss: 11.6616 - test loss: 14.2568 - train acc: 0.4422 - test acc: 0.3673 - 5m 32s\n",
      "batch: 900/1563 - train loss: 11.8282 - test loss: 13.9712 - train acc: 0.4378 - test acc: 0.3713 - 5m 37s\n",
      "batch: 1000/1563 - train loss: 11.5905 - test loss: 14.1356 - train acc: 0.4582 - test acc: 0.3705 - 5m 42s\n",
      "batch: 1100/1563 - train loss: 11.8795 - test loss: 13.5851 - train acc: 0.4360 - test acc: 0.3905 - 5m 47s\n",
      "batch: 1200/1563 - train loss: 11.8183 - test loss: 14.5557 - train acc: 0.4388 - test acc: 0.3593 - 5m 51s\n",
      "batch: 1300/1563 - train loss: 12.0796 - test loss: 14.0473 - train acc: 0.4341 - test acc: 0.3657 - 5m 56s\n",
      "batch: 1400/1563 - train loss: 11.7684 - test loss: 13.8845 - train acc: 0.4472 - test acc: 0.3752 - 6m 1s\n",
      "batch: 1500/1563 - train loss: 11.8585 - test loss: 13.4093 - train acc: 0.4462 - test acc: 0.3916 - 6m 5s\n",
      "batch: 1563/1563 - train loss: 11.6912 - test loss: 13.3971 - train acc: 0.4466 - test acc: 0.3974 - 6m 9s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.8153 - test loss: 13.9300 - train acc: 0.5103 - test acc: 0.3867 - 6m 14s\n",
      "batch: 200/1563 - train loss: 9.8613 - test loss: 13.6931 - train acc: 0.5212 - test acc: 0.3946 - 6m 19s\n",
      "batch: 300/1563 - train loss: 10.3394 - test loss: 15.0847 - train acc: 0.4987 - test acc: 0.3571 - 6m 24s\n",
      "batch: 400/1563 - train loss: 10.0503 - test loss: 14.6876 - train acc: 0.5000 - test acc: 0.3636 - 6m 28s\n",
      "batch: 500/1563 - train loss: 10.4569 - test loss: 14.1296 - train acc: 0.4985 - test acc: 0.3803 - 6m 33s\n",
      "batch: 600/1563 - train loss: 10.5112 - test loss: 13.5755 - train acc: 0.4956 - test acc: 0.3984 - 6m 38s\n",
      "batch: 700/1563 - train loss: 10.5803 - test loss: 14.5849 - train acc: 0.4956 - test acc: 0.3767 - 6m 43s\n",
      "batch: 800/1563 - train loss: 10.4131 - test loss: 14.0742 - train acc: 0.5034 - test acc: 0.3807 - 6m 48s\n",
      "batch: 900/1563 - train loss: 10.7646 - test loss: 13.9518 - train acc: 0.4637 - test acc: 0.3889 - 6m 52s\n",
      "batch: 1000/1563 - train loss: 10.7175 - test loss: 14.0918 - train acc: 0.4894 - test acc: 0.3796 - 6m 57s\n",
      "batch: 1100/1563 - train loss: 10.9672 - test loss: 13.6527 - train acc: 0.4722 - test acc: 0.3988 - 7m 2s\n",
      "batch: 1200/1563 - train loss: 10.8938 - test loss: 14.8797 - train acc: 0.4800 - test acc: 0.3637 - 7m 7s\n",
      "batch: 1300/1563 - train loss: 11.1705 - test loss: 13.4581 - train acc: 0.4672 - test acc: 0.3916 - 7m 11s\n",
      "batch: 1400/1563 - train loss: 10.9850 - test loss: 13.3607 - train acc: 0.4800 - test acc: 0.4024 - 7m 16s\n",
      "batch: 1500/1563 - train loss: 10.8494 - test loss: 15.1481 - train acc: 0.4850 - test acc: 0.3452 - 7m 21s\n",
      "batch: 1563/1563 - train loss: 11.2129 - test loss: 14.5071 - train acc: 0.4697 - test acc: 0.3652 - 7m 25s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.6871 - test loss: 13.3784 - train acc: 0.5734 - test acc: 0.4031 - 7m 30s\n",
      "batch: 200/1563 - train loss: 8.7065 - test loss: 13.8300 - train acc: 0.5687 - test acc: 0.3994 - 7m 35s\n",
      "batch: 300/1563 - train loss: 9.0961 - test loss: 14.5597 - train acc: 0.5484 - test acc: 0.3781 - 7m 40s\n",
      "batch: 400/1563 - train loss: 9.1022 - test loss: 13.9970 - train acc: 0.5581 - test acc: 0.3920 - 7m 44s\n",
      "batch: 500/1563 - train loss: 9.1826 - test loss: 13.9087 - train acc: 0.5547 - test acc: 0.3919 - 7m 49s\n",
      "batch: 600/1563 - train loss: 9.6680 - test loss: 14.0680 - train acc: 0.5175 - test acc: 0.3919 - 7m 54s\n",
      "batch: 700/1563 - train loss: 9.5764 - test loss: 13.6846 - train acc: 0.5299 - test acc: 0.4063 - 7m 59s\n",
      "batch: 800/1563 - train loss: 9.3852 - test loss: 13.7715 - train acc: 0.5484 - test acc: 0.3911 - 8m 3s\n",
      "batch: 900/1563 - train loss: 9.5179 - test loss: 13.8525 - train acc: 0.5387 - test acc: 0.3988 - 8m 8s\n",
      "batch: 1000/1563 - train loss: 9.7077 - test loss: 13.8763 - train acc: 0.5263 - test acc: 0.3974 - 8m 13s\n",
      "batch: 1100/1563 - train loss: 9.6615 - test loss: 13.7615 - train acc: 0.5300 - test acc: 0.3926 - 8m 18s\n",
      "batch: 1200/1563 - train loss: 10.0780 - test loss: 13.4237 - train acc: 0.5146 - test acc: 0.4091 - 8m 23s\n",
      "batch: 1300/1563 - train loss: 9.9296 - test loss: 13.3377 - train acc: 0.5150 - test acc: 0.4077 - 8m 28s\n",
      "batch: 1400/1563 - train loss: 10.2434 - test loss: 13.2752 - train acc: 0.5156 - test acc: 0.4103 - 8m 32s\n",
      "batch: 1500/1563 - train loss: 9.7384 - test loss: 13.7238 - train acc: 0.5272 - test acc: 0.4009 - 8m 37s\n",
      "batch: 1563/1563 - train loss: 9.8691 - test loss: 14.3419 - train acc: 0.5228 - test acc: 0.3837 - 8m 41s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.9099 - test loss: 13.5380 - train acc: 0.6113 - test acc: 0.4115 - 8m 46s\n",
      "batch: 200/1563 - train loss: 7.4334 - test loss: 13.7335 - train acc: 0.6153 - test acc: 0.4148 - 8m 51s\n",
      "batch: 300/1563 - train loss: 7.8489 - test loss: 13.4972 - train acc: 0.5981 - test acc: 0.4259 - 8m 56s\n",
      "batch: 400/1563 - train loss: 8.0416 - test loss: 13.9293 - train acc: 0.5934 - test acc: 0.4114 - 9m 0s\n",
      "batch: 500/1563 - train loss: 8.1071 - test loss: 15.2066 - train acc: 0.5950 - test acc: 0.3732 - 9m 5s\n",
      "batch: 600/1563 - train loss: 8.4640 - test loss: 13.9743 - train acc: 0.5850 - test acc: 0.4047 - 9m 10s\n",
      "batch: 700/1563 - train loss: 8.3289 - test loss: 13.8639 - train acc: 0.5844 - test acc: 0.4089 - 9m 14s\n",
      "batch: 800/1563 - train loss: 8.8591 - test loss: 13.6591 - train acc: 0.5612 - test acc: 0.4132 - 9m 19s\n",
      "batch: 900/1563 - train loss: 8.7528 - test loss: 13.8336 - train acc: 0.5566 - test acc: 0.4019 - 9m 24s\n",
      "batch: 1000/1563 - train loss: 8.8526 - test loss: 14.0253 - train acc: 0.5791 - test acc: 0.4001 - 9m 29s\n",
      "batch: 1100/1563 - train loss: 8.7007 - test loss: 14.1014 - train acc: 0.5500 - test acc: 0.4001 - 9m 34s\n",
      "batch: 1200/1563 - train loss: 9.0468 - test loss: 14.6488 - train acc: 0.5506 - test acc: 0.3840 - 9m 38s\n",
      "batch: 1300/1563 - train loss: 8.8378 - test loss: 14.4911 - train acc: 0.5634 - test acc: 0.3928 - 9m 43s\n",
      "batch: 1400/1563 - train loss: 8.7890 - test loss: 13.6530 - train acc: 0.5674 - test acc: 0.4107 - 9m 48s\n",
      "batch: 1500/1563 - train loss: 9.2262 - test loss: 13.8105 - train acc: 0.5444 - test acc: 0.4029 - 9m 53s\n",
      "batch: 1563/1563 - train loss: 8.8308 - test loss: 14.1183 - train acc: 0.5606 - test acc: 0.4052 - 9m 57s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.7802 - test loss: 14.3768 - train acc: 0.6391 - test acc: 0.3969 - 10m 2s\n",
      "batch: 200/1563 - train loss: 6.5175 - test loss: 14.0734 - train acc: 0.6663 - test acc: 0.4078 - 10m 7s\n",
      "batch: 300/1563 - train loss: 6.7177 - test loss: 14.2370 - train acc: 0.6660 - test acc: 0.4132 - 10m 11s\n",
      "batch: 400/1563 - train loss: 6.7740 - test loss: 15.0717 - train acc: 0.6579 - test acc: 0.3938 - 10m 16s\n",
      "batch: 500/1563 - train loss: 7.1363 - test loss: 14.1085 - train acc: 0.6366 - test acc: 0.4173 - 10m 21s\n",
      "batch: 600/1563 - train loss: 7.4799 - test loss: 14.5722 - train acc: 0.6238 - test acc: 0.4024 - 10m 26s\n",
      "batch: 700/1563 - train loss: 7.3202 - test loss: 15.9784 - train acc: 0.6275 - test acc: 0.3756 - 10m 31s\n",
      "batch: 800/1563 - train loss: 7.9505 - test loss: 15.7008 - train acc: 0.5972 - test acc: 0.3727 - 10m 35s\n",
      "batch: 900/1563 - train loss: 7.6795 - test loss: 14.0816 - train acc: 0.6059 - test acc: 0.4144 - 10m 40s\n",
      "batch: 1000/1563 - train loss: 8.0023 - test loss: 13.9245 - train acc: 0.6015 - test acc: 0.4152 - 10m 45s\n",
      "batch: 1100/1563 - train loss: 7.9942 - test loss: 13.9627 - train acc: 0.6003 - test acc: 0.4199 - 10m 50s\n",
      "batch: 1200/1563 - train loss: 7.9471 - test loss: 16.0331 - train acc: 0.5909 - test acc: 0.3658 - 10m 54s\n",
      "batch: 1300/1563 - train loss: 7.9691 - test loss: 13.9095 - train acc: 0.5975 - test acc: 0.4214 - 10m 59s\n",
      "batch: 1400/1563 - train loss: 8.2461 - test loss: 14.9155 - train acc: 0.5894 - test acc: 0.3931 - 11m 4s\n",
      "batch: 1500/1563 - train loss: 8.2282 - test loss: 14.4815 - train acc: 0.5853 - test acc: 0.4025 - 11m 9s\n",
      "batch: 1563/1563 - train loss: 8.1619 - test loss: 13.7855 - train acc: 0.5800 - test acc: 0.4192 - 11m 13s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.4327 - test loss: 13.8484 - train acc: 0.7156 - test acc: 0.4346 - 11m 18s\n",
      "batch: 200/1563 - train loss: 5.6619 - test loss: 14.7652 - train acc: 0.7069 - test acc: 0.4037 - 11m 22s\n",
      "batch: 300/1563 - train loss: 5.6313 - test loss: 15.0447 - train acc: 0.7044 - test acc: 0.4053 - 11m 27s\n",
      "batch: 400/1563 - train loss: 6.0573 - test loss: 14.4239 - train acc: 0.6891 - test acc: 0.4279 - 11m 32s\n",
      "batch: 500/1563 - train loss: 6.3826 - test loss: 14.9005 - train acc: 0.6700 - test acc: 0.4086 - 11m 37s\n",
      "batch: 600/1563 - train loss: 6.5255 - test loss: 15.3547 - train acc: 0.6563 - test acc: 0.3986 - 11m 42s\n",
      "batch: 700/1563 - train loss: 6.5036 - test loss: 15.3857 - train acc: 0.6572 - test acc: 0.3893 - 11m 46s\n",
      "batch: 800/1563 - train loss: 6.7397 - test loss: 15.1213 - train acc: 0.6506 - test acc: 0.4046 - 11m 51s\n",
      "batch: 900/1563 - train loss: 6.7999 - test loss: 16.1729 - train acc: 0.6469 - test acc: 0.3855 - 11m 56s\n",
      "batch: 1000/1563 - train loss: 6.7642 - test loss: 14.8448 - train acc: 0.6557 - test acc: 0.4059 - 12m 1s\n",
      "batch: 1100/1563 - train loss: 7.3599 - test loss: 14.5287 - train acc: 0.6272 - test acc: 0.4124 - 12m 6s\n",
      "batch: 1200/1563 - train loss: 6.9538 - test loss: 14.2407 - train acc: 0.6441 - test acc: 0.4136 - 12m 11s\n",
      "batch: 1300/1563 - train loss: 7.2615 - test loss: 14.2903 - train acc: 0.6275 - test acc: 0.4214 - 12m 15s\n",
      "batch: 1400/1563 - train loss: 7.2431 - test loss: 13.9533 - train acc: 0.6206 - test acc: 0.4253 - 12m 20s\n",
      "batch: 1500/1563 - train loss: 7.1930 - test loss: 14.4981 - train acc: 0.6350 - test acc: 0.4090 - 12m 25s\n",
      "batch: 1563/1563 - train loss: 7.2346 - test loss: 14.2176 - train acc: 0.6347 - test acc: 0.4247 - 12m 29s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.4596 - test loss: 14.9479 - train acc: 0.7606 - test acc: 0.4163 - 12m 34s\n",
      "batch: 200/1563 - train loss: 4.6655 - test loss: 15.0403 - train acc: 0.7537 - test acc: 0.4166 - 12m 39s\n",
      "batch: 300/1563 - train loss: 4.6582 - test loss: 15.0538 - train acc: 0.7413 - test acc: 0.4163 - 12m 43s\n",
      "batch: 400/1563 - train loss: 5.0913 - test loss: 15.2039 - train acc: 0.7260 - test acc: 0.4051 - 12m 48s\n",
      "batch: 500/1563 - train loss: 5.2920 - test loss: 15.3193 - train acc: 0.7203 - test acc: 0.4122 - 12m 53s\n",
      "batch: 600/1563 - train loss: 5.7749 - test loss: 15.6734 - train acc: 0.6913 - test acc: 0.3958 - 12m 58s\n",
      "batch: 700/1563 - train loss: 5.6068 - test loss: 14.9847 - train acc: 0.7063 - test acc: 0.4157 - 13m 3s\n",
      "batch: 800/1563 - train loss: 5.9146 - test loss: 15.3458 - train acc: 0.6969 - test acc: 0.4055 - 13m 8s\n",
      "batch: 900/1563 - train loss: 5.9581 - test loss: 15.2652 - train acc: 0.6932 - test acc: 0.4105 - 13m 12s\n",
      "batch: 1000/1563 - train loss: 5.9721 - test loss: 15.1341 - train acc: 0.6822 - test acc: 0.4141 - 13m 17s\n",
      "batch: 1100/1563 - train loss: 6.1730 - test loss: 15.7916 - train acc: 0.6850 - test acc: 0.4001 - 13m 22s\n",
      "batch: 1200/1563 - train loss: 6.4376 - test loss: 14.8462 - train acc: 0.6660 - test acc: 0.4152 - 13m 27s\n",
      "batch: 1300/1563 - train loss: 6.2853 - test loss: 15.2783 - train acc: 0.6635 - test acc: 0.4133 - 13m 32s\n",
      "batch: 1400/1563 - train loss: 6.1926 - test loss: 16.1804 - train acc: 0.6759 - test acc: 0.3914 - 13m 37s\n",
      "batch: 1500/1563 - train loss: 6.5519 - test loss: 14.7250 - train acc: 0.6670 - test acc: 0.4229 - 13m 41s\n",
      "batch: 1563/1563 - train loss: 6.5564 - test loss: 15.1562 - train acc: 0.6569 - test acc: 0.4037 - 13m 46s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.1126 - test loss: 14.7689 - train acc: 0.7843 - test acc: 0.4243 - 13m 50s\n",
      "batch: 200/1563 - train loss: 4.0162 - test loss: 15.6383 - train acc: 0.7828 - test acc: 0.4133 - 13m 55s\n",
      "batch: 300/1563 - train loss: 4.0667 - test loss: 15.2785 - train acc: 0.7868 - test acc: 0.4263 - 14m 0s\n",
      "batch: 400/1563 - train loss: 4.0199 - test loss: 15.6680 - train acc: 0.7809 - test acc: 0.4195 - 14m 5s\n",
      "batch: 500/1563 - train loss: 4.6250 - test loss: 17.3129 - train acc: 0.7506 - test acc: 0.3905 - 14m 10s\n",
      "batch: 600/1563 - train loss: 4.8794 - test loss: 15.9125 - train acc: 0.7365 - test acc: 0.4045 - 14m 14s\n",
      "batch: 700/1563 - train loss: 4.6146 - test loss: 15.8767 - train acc: 0.7490 - test acc: 0.4138 - 14m 19s\n",
      "batch: 800/1563 - train loss: 4.8475 - test loss: 16.5551 - train acc: 0.7412 - test acc: 0.3954 - 14m 24s\n",
      "batch: 900/1563 - train loss: 5.3874 - test loss: 16.1392 - train acc: 0.7140 - test acc: 0.4015 - 14m 29s\n",
      "batch: 1000/1563 - train loss: 5.3231 - test loss: 15.3358 - train acc: 0.7191 - test acc: 0.4194 - 14m 33s\n",
      "batch: 1100/1563 - train loss: 5.2836 - test loss: 15.1709 - train acc: 0.7225 - test acc: 0.4329 - 14m 39s\n",
      "batch: 1200/1563 - train loss: 5.5367 - test loss: 16.9848 - train acc: 0.7088 - test acc: 0.3912 - 14m 43s\n",
      "batch: 1300/1563 - train loss: 5.7293 - test loss: 15.8083 - train acc: 0.6894 - test acc: 0.4156 - 14m 48s\n",
      "batch: 1400/1563 - train loss: 5.6370 - test loss: 15.7080 - train acc: 0.6863 - test acc: 0.4108 - 14m 53s\n",
      "batch: 1500/1563 - train loss: 5.7998 - test loss: 15.5167 - train acc: 0.6994 - test acc: 0.4187 - 14m 58s\n",
      "batch: 1563/1563 - train loss: 6.0361 - test loss: 15.5168 - train acc: 0.6825 - test acc: 0.4094 - 15m 2s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.5929 - test loss: 15.1493 - train acc: 0.8100 - test acc: 0.4316 - 15m 7s\n",
      "batch: 200/1563 - train loss: 3.2491 - test loss: 15.5096 - train acc: 0.8234 - test acc: 0.4266 - 15m 11s\n",
      "batch: 300/1563 - train loss: 3.4988 - test loss: 16.2637 - train acc: 0.8075 - test acc: 0.4242 - 15m 16s\n",
      "batch: 400/1563 - train loss: 3.5672 - test loss: 16.1629 - train acc: 0.8091 - test acc: 0.4171 - 15m 21s\n",
      "batch: 500/1563 - train loss: 3.7613 - test loss: 16.1360 - train acc: 0.7969 - test acc: 0.4246 - 15m 26s\n",
      "batch: 600/1563 - train loss: 3.8819 - test loss: 16.7288 - train acc: 0.7780 - test acc: 0.4094 - 15m 31s\n",
      "batch: 700/1563 - train loss: 4.2279 - test loss: 17.4329 - train acc: 0.7699 - test acc: 0.3988 - 15m 35s\n",
      "batch: 800/1563 - train loss: 4.2947 - test loss: 16.3629 - train acc: 0.7700 - test acc: 0.4197 - 15m 40s\n",
      "batch: 900/1563 - train loss: 4.6952 - test loss: 16.6792 - train acc: 0.7425 - test acc: 0.4150 - 15m 45s\n",
      "batch: 1000/1563 - train loss: 4.4798 - test loss: 16.4578 - train acc: 0.7459 - test acc: 0.4136 - 15m 50s\n",
      "batch: 1100/1563 - train loss: 4.7151 - test loss: 16.1533 - train acc: 0.7457 - test acc: 0.4122 - 15m 55s\n",
      "batch: 1200/1563 - train loss: 4.4880 - test loss: 16.1531 - train acc: 0.7506 - test acc: 0.4181 - 15m 59s\n",
      "batch: 1300/1563 - train loss: 4.8715 - test loss: 16.1521 - train acc: 0.7334 - test acc: 0.4228 - 16m 4s\n",
      "batch: 1400/1563 - train loss: 5.1429 - test loss: 15.9688 - train acc: 0.7250 - test acc: 0.4239 - 16m 9s\n",
      "batch: 1500/1563 - train loss: 5.3194 - test loss: 16.3081 - train acc: 0.7112 - test acc: 0.4160 - 16m 14s\n",
      "batch: 1563/1563 - train loss: 5.2712 - test loss: 16.4895 - train acc: 0.7225 - test acc: 0.4114 - 16m 18s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8330 - test loss: 16.0136 - train acc: 0.8512 - test acc: 0.4290 - 16m 23s\n",
      "batch: 200/1563 - train loss: 2.8177 - test loss: 16.7637 - train acc: 0.8440 - test acc: 0.4163 - 16m 28s\n",
      "batch: 300/1563 - train loss: 2.8992 - test loss: 16.9440 - train acc: 0.8371 - test acc: 0.4227 - 16m 32s\n",
      "batch: 400/1563 - train loss: 2.9481 - test loss: 16.9967 - train acc: 0.8412 - test acc: 0.4261 - 16m 37s\n",
      "batch: 500/1563 - train loss: 3.2251 - test loss: 16.9397 - train acc: 0.8240 - test acc: 0.4274 - 16m 42s\n",
      "batch: 600/1563 - train loss: 3.4349 - test loss: 17.8981 - train acc: 0.8106 - test acc: 0.4064 - 16m 47s\n",
      "batch: 700/1563 - train loss: 3.2779 - test loss: 16.7046 - train acc: 0.8093 - test acc: 0.4308 - 16m 52s\n",
      "batch: 800/1563 - train loss: 3.5561 - test loss: 18.2250 - train acc: 0.8030 - test acc: 0.4042 - 16m 57s\n",
      "batch: 900/1563 - train loss: 3.6565 - test loss: 16.7091 - train acc: 0.8009 - test acc: 0.4353 - 17m 2s\n",
      "batch: 1000/1563 - train loss: 4.2236 - test loss: 16.7169 - train acc: 0.7691 - test acc: 0.4188 - 17m 6s\n",
      "batch: 1100/1563 - train loss: 3.9948 - test loss: 17.2264 - train acc: 0.7765 - test acc: 0.4152 - 17m 11s\n",
      "batch: 1200/1563 - train loss: 3.9313 - test loss: 16.5711 - train acc: 0.7793 - test acc: 0.4193 - 17m 16s\n",
      "batch: 1300/1563 - train loss: 4.3016 - test loss: 16.8122 - train acc: 0.7619 - test acc: 0.4167 - 17m 21s\n",
      "batch: 1400/1563 - train loss: 4.3483 - test loss: 16.8373 - train acc: 0.7547 - test acc: 0.4198 - 17m 26s\n",
      "batch: 1500/1563 - train loss: 4.3794 - test loss: 16.7654 - train acc: 0.7590 - test acc: 0.4232 - 17m 30s\n",
      "batch: 1563/1563 - train loss: 4.3771 - test loss: 17.1142 - train acc: 0.7628 - test acc: 0.4157 - 17m 35s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.4922 - test loss: 16.7160 - train acc: 0.8544 - test acc: 0.4275 - 17m 39s\n",
      "batch: 200/1563 - train loss: 2.5254 - test loss: 16.6670 - train acc: 0.8650 - test acc: 0.4323 - 17m 45s\n",
      "batch: 300/1563 - train loss: 2.3309 - test loss: 17.0989 - train acc: 0.8666 - test acc: 0.4350 - 17m 49s\n",
      "batch: 400/1563 - train loss: 2.4132 - test loss: 18.3348 - train acc: 0.8691 - test acc: 0.4113 - 17m 54s\n",
      "batch: 500/1563 - train loss: 2.7746 - test loss: 18.0301 - train acc: 0.8405 - test acc: 0.4152 - 17m 59s\n",
      "batch: 600/1563 - train loss: 2.8684 - test loss: 17.4886 - train acc: 0.8425 - test acc: 0.4242 - 18m 4s\n",
      "batch: 700/1563 - train loss: 3.1496 - test loss: 17.5982 - train acc: 0.8253 - test acc: 0.4146 - 18m 9s\n",
      "batch: 800/1563 - train loss: 3.2344 - test loss: 17.5142 - train acc: 0.8243 - test acc: 0.4173 - 18m 13s\n",
      "batch: 900/1563 - train loss: 3.3735 - test loss: 17.5825 - train acc: 0.8140 - test acc: 0.4204 - 18m 18s\n",
      "batch: 1000/1563 - train loss: 3.3237 - test loss: 17.5134 - train acc: 0.8187 - test acc: 0.4193 - 18m 23s\n",
      "batch: 1100/1563 - train loss: 3.3815 - test loss: 18.6737 - train acc: 0.8156 - test acc: 0.3977 - 18m 28s\n",
      "batch: 1200/1563 - train loss: 3.5376 - test loss: 18.1505 - train acc: 0.7949 - test acc: 0.4053 - 18m 33s\n",
      "batch: 1300/1563 - train loss: 3.7207 - test loss: 17.8069 - train acc: 0.7925 - test acc: 0.4167 - 18m 38s\n",
      "batch: 1400/1563 - train loss: 3.4541 - test loss: 17.5757 - train acc: 0.8190 - test acc: 0.4157 - 18m 42s\n",
      "batch: 1500/1563 - train loss: 3.6645 - test loss: 18.1049 - train acc: 0.7983 - test acc: 0.4174 - 18m 48s\n",
      "batch: 1563/1563 - train loss: 3.9085 - test loss: 17.5423 - train acc: 0.7818 - test acc: 0.4202 - 18m 52s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.1681 - test loss: 17.9316 - train acc: 0.8769 - test acc: 0.4107 - 18m 56s\n",
      "batch: 200/1563 - train loss: 2.0504 - test loss: 18.0739 - train acc: 0.8869 - test acc: 0.4118 - 19m 1s\n",
      "batch: 300/1563 - train loss: 2.2557 - test loss: 18.0921 - train acc: 0.8713 - test acc: 0.4275 - 19m 6s\n",
      "batch: 400/1563 - train loss: 2.3924 - test loss: 18.8494 - train acc: 0.8634 - test acc: 0.4073 - 19m 11s\n",
      "batch: 500/1563 - train loss: 2.6083 - test loss: 18.3902 - train acc: 0.8587 - test acc: 0.4173 - 19m 16s\n",
      "batch: 600/1563 - train loss: 2.5910 - test loss: 18.7436 - train acc: 0.8465 - test acc: 0.4001 - 19m 21s\n",
      "batch: 700/1563 - train loss: 2.8118 - test loss: 18.2257 - train acc: 0.8349 - test acc: 0.4226 - 19m 26s\n",
      "batch: 800/1563 - train loss: 2.9009 - test loss: 20.2365 - train acc: 0.8400 - test acc: 0.3833 - 19m 30s\n",
      "batch: 900/1563 - train loss: 2.9664 - test loss: 18.6874 - train acc: 0.8306 - test acc: 0.4151 - 19m 35s\n",
      "batch: 1000/1563 - train loss: 2.9533 - test loss: 18.3981 - train acc: 0.8355 - test acc: 0.4171 - 19m 40s\n",
      "batch: 1100/1563 - train loss: 2.9358 - test loss: 18.6732 - train acc: 0.8343 - test acc: 0.4173 - 19m 45s\n",
      "batch: 1200/1563 - train loss: 3.1534 - test loss: 18.2120 - train acc: 0.8241 - test acc: 0.4193 - 19m 50s\n",
      "batch: 1300/1563 - train loss: 3.1023 - test loss: 18.6457 - train acc: 0.8184 - test acc: 0.4143 - 19m 55s\n",
      "batch: 1400/1563 - train loss: 3.3730 - test loss: 19.9656 - train acc: 0.8096 - test acc: 0.3961 - 19m 59s\n",
      "time is up! finishing training\n",
      "batch: 1401/1563 - train loss: 3.3864 - test loss: 19.9926 - train acc: 0.8100 - test acc: 0.3949 - 20m 2s\n",
      "GPU memory used: 0.46 GB - max: 0.52 GB - memory reserved: 0.57 GB - max: 0.57 GB\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "ci = 0\n",
    "\n",
    "beta = 0.9\n",
    "partition_size = 8\n",
    "block_updates = 64\n",
    "\n",
    "n_runs = 10\n",
    "\n",
    "for _ in range(n_runs):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'testing - beta: {beta} - part size: {partition_size} - block upd: {block_updates} - combination nº: {ci + 1}')\n",
    "    ci += 1\n",
    "\n",
    "    default_metrics, _ = train_network_fisher_optimization(apply_fisher = True,\n",
    "                                                           beta = beta,\n",
    "                                                           partition_size = partition_size,\n",
    "                                                           block_updates = block_updates,\n",
    "                                                           epochs = 100,\n",
    "                                                           time_limit_secs = 20 * 60)\n",
    "\n",
    "    results_list.append( (default_metrics, beta, partition_size, block_updates) )\n",
    "    results_list_to_json(results_list, step=ci)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12476.853514,
   "end_time": "2023-01-17T05:11:20.078477",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-17T01:43:23.224963",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "14c8014dced5450cba4c1fd71c805bc6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "29684b8645c4440e9694e3131a87c0ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3774e97a6993453688432306b67c39d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7ce28054f8d9415283f81d5fc32d0b37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c27b849daa7a497d97bb681cc24d6b02",
       "placeholder": "​",
       "style": "IPY_MODEL_29684b8645c4440e9694e3131a87c0ff",
       "value": ""
      }
     },
     "aa0dac7adb53480087b3dd8bda40580d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7ce28054f8d9415283f81d5fc32d0b37",
        "IPY_MODEL_e6861e0beab74ec89f98692e7ae4a4d5",
        "IPY_MODEL_e1163fb56c914cd29ce9e1c820116d4a"
       ],
       "layout": "IPY_MODEL_14c8014dced5450cba4c1fd71c805bc6"
      }
     },
     "c27b849daa7a497d97bb681cc24d6b02": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce3a40ddfb354ee9b638ad709e67bc5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3409dedcab74adc9788a92a66e6dd56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e1163fb56c914cd29ce9e1c820116d4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ce3a40ddfb354ee9b638ad709e67bc5f",
       "placeholder": "​",
       "style": "IPY_MODEL_d3409dedcab74adc9788a92a66e6dd56",
       "value": " 169001984/? [00:05&lt;00:00, 33785417.00it/s]"
      }
     },
     "e2e0d00740854d24a046f9f40a9bb3e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6861e0beab74ec89f98692e7ae4a4d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e2e0d00740854d24a046f9f40a9bb3e9",
       "max": 169001437,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3774e97a6993453688432306b67c39d9",
       "value": 169001437
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
