{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ac3960",
   "metadata": {
    "papermill": {
     "duration": 0.008724,
     "end_time": "2022-09-29T04:13:21.675441",
     "exception": false,
     "start_time": "2022-09-29T04:13:21.666717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fisher FIFO\n",
    "\n",
    "In the `fisher-fifo-v3` notebook, we set a code to use the Fisher information effectively in neural netwrks using the *first-in-first-out* strategy to store gradients.\n",
    "\n",
    "Here our objective is to assess the effect of the FIFO buffer in the quality of the model. Also, we implement the partitioning strategy to make our algorithm generalizable to larger networks, as well as datasets with larger instances (like images).\n",
    "\n",
    "To enhance the partition effectiveness, we proceed to use the \"maximum-block-update\", to make the algorithm faster.\n",
    "\n",
    "---\n",
    "\n",
    "in fisher `fisher-fifo-v4` notebook we are trying to implement the algorithm in a more efficient way by using the [torch.bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html). The main idea is to execute the matrix-multiplications of more than one block in an optimized way.\n",
    "\n",
    "---\n",
    "\n",
    "in `fisher-fifo-v4.2` notebook, we are trying to make the algorithm even faster using a single centralized object responsible for storing all the matrices (and their inverses) as well as all gradients and buffers. The main idea here is to make the most of vectorization using Pytorch utilities for GPU.\n",
    "\n",
    "---\n",
    "\n",
    "in `fisher-fifo-v4.3` notebook, we try to take the algorithm one step further in efficiency. We are implementing the strategy to retrieve the partitions in sets, instead of individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06cb2f7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:21.692133Z",
     "iopub.status.busy": "2022-09-29T04:13:21.691411Z",
     "iopub.status.idle": "2022-09-29T04:13:23.056513Z",
     "shell.execute_reply": "2022-09-29T04:13:23.055553Z"
    },
    "papermill": {
     "duration": 1.376247,
     "end_time": "2022-09-29T04:13:23.059055",
     "exception": false,
     "start_time": "2022-09-29T04:13:21.682808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import gp_minimize\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f60717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:23.075557Z",
     "iopub.status.busy": "2022-09-29T04:13:23.075268Z",
     "iopub.status.idle": "2022-09-29T04:13:23.101108Z",
     "shell.execute_reply": "2022-09-29T04:13:23.100186Z"
    },
    "papermill": {
     "duration": 0.037311,
     "end_time": "2022-09-29T04:13:23.103954",
     "exception": false,
     "start_time": "2022-09-29T04:13:23.066643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/bayes-opt-results.npz\n",
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/results_step_20.json\n",
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/__results__.html\n",
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/__notebook__.ipynb\n",
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/__output__.json\n",
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/custom.css\n",
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/data/cifar-100-python.tar.gz\n",
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/data/cifar-100-python/meta\n",
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/data/cifar-100-python/file.txt~\n",
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/data/cifar-100-python/test\n",
      "/kaggle/input/fisher-fifo-v4-3-comparing-cifar100-resnet18/data/cifar-100-python/train\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96566a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:23.120959Z",
     "iopub.status.busy": "2022-09-29T04:13:23.120651Z",
     "iopub.status.idle": "2022-09-29T04:13:24.977053Z",
     "shell.execute_reply": "2022-09-29T04:13:24.976077Z"
    },
    "papermill": {
     "duration": 1.866843,
     "end_time": "2022-09-29T04:13:24.979386",
     "exception": false,
     "start_time": "2022-09-29T04:13:23.112543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb55556f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:24.995378Z",
     "iopub.status.busy": "2022-09-29T04:13:24.994979Z",
     "iopub.status.idle": "2022-09-29T04:13:25.001146Z",
     "shell.execute_reply": "2022-09-29T04:13:25.000316Z"
    },
    "papermill": {
     "duration": 0.016226,
     "end_time": "2022-09-29T04:13:25.003129",
     "exception": false,
     "start_time": "2022-09-29T04:13:24.986903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_device():    \n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        device = torch.device('cuda')\n",
    "        print( torch.cuda.get_device_name(device) )\n",
    "        print( torch.cuda.get_device_properties(device) )\n",
    "\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(device)\n",
    "        \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5395f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:25.018908Z",
     "iopub.status.busy": "2022-09-29T04:13:25.018328Z",
     "iopub.status.idle": "2022-09-29T04:13:36.112567Z",
     "shell.execute_reply": "2022-09-29T04:13:36.111430Z"
    },
    "papermill": {
     "duration": 11.104686,
     "end_time": "2022-09-29T04:13:36.115013",
     "exception": false,
     "start_time": "2022-09-29T04:13:25.010327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd1feef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.132840Z",
     "iopub.status.busy": "2022-09-29T04:13:36.131709Z",
     "iopub.status.idle": "2022-09-29T04:13:36.195739Z",
     "shell.execute_reply": "2022-09-29T04:13:36.194734Z"
    },
    "papermill": {
     "duration": 0.075185,
     "end_time": "2022-09-29T04:13:36.198088",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.122903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla P100-PCIE-16GB\n",
      "_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n"
     ]
    }
   ],
   "source": [
    "class cfg:\n",
    "    # n_features = 28 * 28\n",
    "    img_size = (32, 32)\n",
    "    img_channels = 3\n",
    "    n_classes = 100  ## we have 100 classes in CIFAR100\n",
    "    \n",
    "    # device = torch.device('cpu')\n",
    "    device = get_device()\n",
    "    \n",
    "    max_loss = 20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8bb00",
   "metadata": {
    "papermill": {
     "duration": 0.007301,
     "end_time": "2022-09-29T04:13:36.213761",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.206460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33130e27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.229955Z",
     "iopub.status.busy": "2022-09-29T04:13:36.229633Z",
     "iopub.status.idle": "2022-09-29T04:13:36.235911Z",
     "shell.execute_reply": "2022-09-29T04:13:36.234999Z"
    },
    "papermill": {
     "duration": 0.016783,
     "end_time": "2022-09-29T04:13:36.237861",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.221078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset_mnist(batch_size):\n",
    "    print(f'generating MNIST data with {cfg.n_classes} classes')\n",
    "    \n",
    "    transf_ = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize(size=[14, 14]),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transf_)\n",
    "    mnist_test  = datasets.MNIST(root='./data', train=False, download=True, transform=transf_)\n",
    "    \n",
    "    mnist_train_dataloader = DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True)\n",
    "    mnist_test_dataloader  = DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return mnist_train_dataloader, mnist_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610a9c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.254207Z",
     "iopub.status.busy": "2022-09-29T04:13:36.253339Z",
     "iopub.status.idle": "2022-09-29T04:13:36.259847Z",
     "shell.execute_reply": "2022-09-29T04:13:36.259030Z"
    },
    "papermill": {
     "duration": 0.016678,
     "end_time": "2022-09-29T04:13:36.261827",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.245149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset_cifar10(batch_size):\n",
    "    print(f'generating CIFAR10 data with {cfg.n_classes} classes')\n",
    "    \n",
    "    transf_ = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize(size=[14, 14]),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transf_)\n",
    "    cifar10_test  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transf_)\n",
    "    \n",
    "    cifar10_train_dataloader = DataLoader(dataset=cifar10_train, batch_size=batch_size, shuffle=True)\n",
    "    cifar10_test_dataloader  = DataLoader(dataset=cifar10_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return cifar10_train_dataloader, cifar10_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08de04f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.278102Z",
     "iopub.status.busy": "2022-09-29T04:13:36.277350Z",
     "iopub.status.idle": "2022-09-29T04:13:36.283692Z",
     "shell.execute_reply": "2022-09-29T04:13:36.282835Z"
    },
    "papermill": {
     "duration": 0.016412,
     "end_time": "2022-09-29T04:13:36.285598",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.269186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset_cifar100(batch_size):\n",
    "    print(f'generating CIFAR100 data with {cfg.n_classes} classes')\n",
    "    \n",
    "    transf_ = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    cifar100_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transf_)\n",
    "    cifar100_test  = datasets.CIFAR100(root='./data', train=False, download=True, transform=transf_)\n",
    "    \n",
    "    cifar100_train_dataloader = DataLoader(dataset=cifar100_train, batch_size=batch_size, shuffle=True)\n",
    "    cifar100_test_dataloader  = DataLoader(dataset=cifar100_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return cifar100_train_dataloader, cifar100_test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03028790",
   "metadata": {
    "papermill": {
     "duration": 0.007565,
     "end_time": "2022-09-29T04:13:36.300580",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.293015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## declaring network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c096e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.317055Z",
     "iopub.status.busy": "2022-09-29T04:13:36.316289Z",
     "iopub.status.idle": "2022-09-29T04:13:36.322418Z",
     "shell.execute_reply": "2022-09-29T04:13:36.321590Z"
    },
    "papermill": {
     "duration": 0.016157,
     "end_time": "2022-09-29T04:13:36.324387",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.308230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_default_network(c=16, device=cfg.device):\n",
    "    net = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=cfg.n_features, out_features=c),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=c, out_features=c),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=c, out_features=c),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=c, out_features=cfg.n_classes)\n",
    "    )\n",
    "    \n",
    "    torchsummary.summary(net, input_size=[[cfg.n_features]], device='cpu')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4190f28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.340932Z",
     "iopub.status.busy": "2022-09-29T04:13:36.340313Z",
     "iopub.status.idle": "2022-09-29T04:13:36.349604Z",
     "shell.execute_reply": "2022-09-29T04:13:36.348611Z"
    },
    "papermill": {
     "duration": 0.019595,
     "end_time": "2022-09-29T04:13:36.351638",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.332043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cnn_network(in_channels=cfg.img_channels, c=16, p_drop=0.1, device=cfg.device):\n",
    "    \n",
    "    img_flat_size = (4 * c * (cfg.img_size[0] // 8) * (cfg.img_size[1] // 8) )\n",
    "    print(img_flat_size)\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=c, kernel_size=5, stride=2, padding=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(in_channels=c, out_channels=(2 * c), kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout2d(p=p_drop),\n",
    "        \n",
    "        nn.Conv2d(in_channels=(2 * c), out_channels=(4 * c), kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout2d(p=p_drop),\n",
    "        \n",
    "        nn.Flatten(),\n",
    "        \n",
    "        nn.Linear(in_features=img_flat_size, out_features=(8 * c) ),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=p_drop),\n",
    "        \n",
    "        nn.Linear(in_features=(8 * c), out_features=(4 * c) ),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=p_drop),\n",
    "        \n",
    "        nn.Linear(in_features=(4 * c), out_features=cfg.n_classes)\n",
    "    )\n",
    "    \n",
    "    torchsummary.summary(net, input_size=[[cfg.img_channels, *cfg.img_size]], device='cpu')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "208e2171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.367893Z",
     "iopub.status.busy": "2022-09-29T04:13:36.367196Z",
     "iopub.status.idle": "2022-09-29T04:13:36.375315Z",
     "shell.execute_reply": "2022-09-29T04:13:36.374420Z"
    },
    "papermill": {
     "duration": 0.018193,
     "end_time": "2022-09-29T04:13:36.377189",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.358996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cnn_network_v2(in_channels=cfg.img_channels, p_drop=0.1, device=cfg.device):\n",
    "    \n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=96, kernel_size=5, padding=2),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(in_channels=96, out_channels=80, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout2d(p=p_drop),\n",
    "        \n",
    "        nn.Conv2d(in_channels=80, out_channels=96, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p=p_drop),\n",
    "        \n",
    "        nn.Conv2d(in_channels=96, out_channels=64, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p=p_drop),\n",
    "        \n",
    "        nn.Flatten(),\n",
    "        \n",
    "        # nn.Linear(in_features=4096, out_features=256 ),\n",
    "        nn.Linear(in_features=(cfg.img_size[0] // 4) * (cfg.img_size[1] // 4) * 64, out_features=256 ),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=p_drop),\n",
    "        \n",
    "        nn.Linear(in_features=256, out_features=cfg.n_classes)\n",
    "    )\n",
    "    \n",
    "    torchsummary.summary(net, input_size=[[cfg.img_channels, *cfg.img_size]], device='cpu')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bcf25b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.393414Z",
     "iopub.status.busy": "2022-09-29T04:13:36.392558Z",
     "iopub.status.idle": "2022-09-29T04:13:36.397813Z",
     "shell.execute_reply": "2022-09-29T04:13:36.397021Z"
    },
    "papermill": {
     "duration": 0.015241,
     "end_time": "2022-09-29T04:13:36.399745",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.384504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_resnet18(device=cfg.device):\n",
    "    \n",
    "    net = torchvision.models.resnet18(num_classes=cfg.n_classes)\n",
    "    torchsummary.summary(net, input_size=[[cfg.img_channels, *cfg.img_size]], device='cpu')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cacdf1c",
   "metadata": {
    "papermill": {
     "duration": 0.007357,
     "end_time": "2022-09-29T04:13:36.414449",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.407092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# object for calculation of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c53948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.430801Z",
     "iopub.status.busy": "2022-09-29T04:13:36.430053Z",
     "iopub.status.idle": "2022-09-29T04:13:36.439510Z",
     "shell.execute_reply": "2022-09-29T04:13:36.438715Z"
    },
    "papermill": {
     "duration": 0.019645,
     "end_time": "2022-09-29T04:13:36.441499",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.421854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, value_round=None, time_round=None):\n",
    "        self.metrics_dict = {}\n",
    "        self.set_initial_time()\n",
    "        self.val_round = value_round\n",
    "        self.time_round = time_round\n",
    "        \n",
    "    def set_initial_time(self):\n",
    "        self.init_time = time.time()\n",
    "        \n",
    "    def get_time(self):\n",
    "        return time.time() - self.init_time\n",
    "    \n",
    "    def add(self, key, value, step=None):\n",
    "        \n",
    "        if step is None:\n",
    "            step = np.nan\n",
    "        \n",
    "        if key not in self.metrics_dict:\n",
    "            self.metrics_dict[key] = []\n",
    "        \n",
    "        t = self.get_time()\n",
    "        if self.time_round is not None:\n",
    "            t = round(t, ndigits=self.time_round)\n",
    "        \n",
    "        if self.val_round is not None:\n",
    "            value = round(value, ndigits=self.val_round)\n",
    "        \n",
    "        self.metrics_dict[key].append( (value, step, t) )\n",
    "    \n",
    "    def add_(self, dict_, step=None):\n",
    "        for key, value in dict_.items():\n",
    "            self.add(key, value, step)\n",
    "    \n",
    "    def get(self, key, get_step=False, get_time=False):\n",
    "        y, x, t = zip(*self.metrics_dict[key])\n",
    "        y, x, t = list(y), list(x), list(t)\n",
    "        \n",
    "        return x, y, t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784f97d",
   "metadata": {
    "papermill": {
     "duration": 0.007242,
     "end_time": "2022-09-29T04:13:36.456100",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.448858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fisher Information calculation objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9735bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.473537Z",
     "iopub.status.busy": "2022-09-29T04:13:36.472049Z",
     "iopub.status.idle": "2022-09-29T04:13:36.502374Z",
     "shell.execute_reply": "2022-09-29T04:13:36.501487Z"
    },
    "papermill": {
     "duration": 0.040819,
     "end_time": "2022-09-29T04:13:36.504390",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.463571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FisherFIFO():\n",
    "    def __init__(self,\n",
    "                 named_params,\n",
    "                 buffer_size,\n",
    "                 partition_size,\n",
    "                 block_updates):\n",
    "        \n",
    "        self.buffer_size = buffer_size\n",
    "        self.partition_size = partition_size\n",
    "        self.block_updates = block_updates\n",
    "        \n",
    "        named_params = list(named_params)\n",
    "        \n",
    "        self.partition_fisher_list = []\n",
    "        total_partitions, total_block_upd = 0, 0\n",
    "        for pi, (n, p) in enumerate( named_params ):\n",
    "            part_fisher = PartitionerFisherFIFO(param = p,\n",
    "                                                name = n,\n",
    "                                                buffer_size = buffer_size,\n",
    "                                                partition_size = partition_size,\n",
    "                                                block_updates = block_updates,\n",
    "                                                parent_fifo = None)\n",
    "            \n",
    "            self.partition_fisher_list.append( (p, part_fisher, total_partitions, total_block_upd) )\n",
    "            \n",
    "            total_partitions += part_fisher.num_part\n",
    "            total_block_upd += part_fisher.block_updates\n",
    "            \n",
    "        self.num_part = total_partitions\n",
    "        self.total_block_updates = total_block_upd\n",
    "        \n",
    "        print(f'total partitions: {self.num_part} - effective block updates: {self.total_block_updates}')\n",
    "        \n",
    "        ## pre-alocate the memory for the tensor that stores the selected gradients (changes every iteration)\n",
    "        self.g = torch.zeros(size=[self.total_block_updates, partition_size, 1], dtype=torch.float, device=cfg.device)\n",
    "        \n",
    "        ## pre-alocate the memory for the tensor that stores the buffer and the tensor for the inverse\n",
    "        self.buffer = torch.zeros(size=[self.num_part, partition_size, buffer_size], dtype=torch.float, device=cfg.device)\n",
    "        self.fisher_inv = torch.zeros(size=[self.num_part, partition_size, partition_size], dtype=torch.float, device=cfg.device)        \n",
    "    \n",
    "        print('initializing buffers and inverses...')\n",
    "        ## now we initialize the buffer and the inverse for all partitions\n",
    "        i = 0\n",
    "        for _, part_fisher, _, _ in self.partition_fisher_list:\n",
    "            for _, start, end in part_fisher.ind_fisher_list:\n",
    "\n",
    "                if i == 0 or ( (i + 1) % 10000 ) == 0 or i == (self.num_part - 1):\n",
    "                    print(f'partition {i+1}/{self.num_part}')\n",
    "\n",
    "                n = end - start\n",
    "                buffer, _, fisher_inv = self.initialize_fisher_partition(param_size=n, buffer_size=self.buffer_size)\n",
    "\n",
    "                self.buffer[i, :n, :] = buffer\n",
    "                self.fisher_inv[i, :n, :n] = fisher_inv\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    \n",
    "    def initialize_fisher_partition(self, param_size, buffer_size):\n",
    "        \n",
    "        buffer = self.get_initial_buffer_v2(param_size, buffer_size)\n",
    "            \n",
    "        ## shuffle buffer across columns\n",
    "        buffer = buffer[:, torch.randperm(buffer.shape[1]) ]\n",
    "        \n",
    "        ## the fisher matrix will be initialized as G @ G.T, in which G is our buffer. We built\n",
    "        ## our buffer in a smart way so the resulting Fisher info matrix is initialized close to identity\n",
    "        fisher = buffer @ buffer.T\n",
    "        \n",
    "        ## since our Fisher information is diagonal for now, its inverse is given just by the innverted\n",
    "        ## elements of the diagonal. \n",
    "        fisher_inv = torch.diag( 1 / torch.diag(fisher) )\n",
    "        \n",
    "        return buffer, fisher, fisher_inv\n",
    "    \n",
    "    \n",
    "    def get_initial_buffer_v2(self, param_size, buffer_size):\n",
    "        ## here we adopt a faster approach to initialize the buffer. We use n \n",
    "        ## identity matrices concatenated column-wise. n is determined by `param_size` and `buffer_size`\n",
    "        \n",
    "        n_eye = math.ceil(buffer_size / param_size)\n",
    "        I = torch.eye(n=param_size, dtype=torch.float, device=cfg.device)\n",
    "        \n",
    "        buffer = torch.cat(n_eye * [I], dim=1)[:, :buffer_size]\n",
    "        \n",
    "        assert buffer.shape == (param_size, buffer_size)\n",
    "        \n",
    "        return buffer\n",
    "\n",
    "\n",
    "    def get_idx_lists(self):\n",
    "        run_enc_list = []\n",
    "        default_idx_list = []\n",
    "        for p, part_fisher, num_part, block_upd in self.partition_fisher_list:\n",
    "            init_block, end_block, g_init_idx, g_end_idx = part_fisher.get_random_blocks()\n",
    "            \n",
    "            # print(f'param shape: {p.shape} - blocks: {init_block} to {end_block} - grad: {g_init_idx} to {g_end_idx}')\n",
    "            \n",
    "            run_enc_list.append( (num_part + init_block, num_part + end_block, g_init_idx, g_end_idx) )\n",
    "            default_idx_list.append( np.arange(start=num_part + init_block, stop=num_part + end_block + 1) )\n",
    "            \n",
    "            \n",
    "        return run_enc_list, np.concatenate(default_idx_list)\n",
    "    \n",
    "    \n",
    "    def read_gradients(self, idx):\n",
    "        self_g_start = 0\n",
    "        for i, (_, _, g_start, g_end) in enumerate(idx):\n",
    "            n_grad = g_end - g_start\n",
    "            # self_g_end = min( self_g_start + n_grad, torch.numel(self.g) )\n",
    "            self_g_end = self_g_start + n_grad\n",
    "            \n",
    "            p, _, _, _ = self.partition_fisher_list[i]\n",
    "            \n",
    "            self.g.view(-1)[self_g_start:self_g_end] = p.grad.view(-1)[g_start:g_end]\n",
    "            \n",
    "            if (n_grad % self.partition_size) > 0:\n",
    "                extra_zeros = self.partition_size - (n_grad % self.partition_size)\n",
    "                self.g.view(-1)[self_g_end:(self_g_end + extra_zeros)] = 0.0\n",
    "            else:\n",
    "                extra_zeros = 0\n",
    "            \n",
    "#             print(f'self_g_start: {self_g_start} - self_g_end: {self_g_end} - self.g.shape: {self.g.view(-1).shape}')\n",
    "#             print(f'g_start: {g_start} - g_end: {g_end} - p.grad.shape: {p.grad.view(-1).shape}')\n",
    "#             print(f'n_grad: {n_grad} - part-size: {self.partition_size} - extra-zeros: {extra_zeros}')\n",
    "#             print()\n",
    "            \n",
    "            self_g_start = self_g_end + extra_zeros\n",
    "            \n",
    "\n",
    "    def write_gradients(self, idx):\n",
    "        self_g_start = 0\n",
    "        for i, (_, _, g_start, g_end) in enumerate(idx):\n",
    "            n_grad = g_end - g_start\n",
    "            self_g_end = self_g_start + n_grad\n",
    "            \n",
    "            p, _, _, _ = self.partition_fisher_list[i]\n",
    "            p.grad.view(-1)[g_start:g_end] = self.g.view(-1)[self_g_start:self_g_end]\n",
    "\n",
    "            if (n_grad % self.partition_size) > 0:\n",
    "                extra_zeros = self.partition_size - (n_grad % self.partition_size)\n",
    "            else:\n",
    "                extra_zeros = 0\n",
    "            \n",
    "            self_g_start = self_g_end + extra_zeros\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        ## selects the blocks to be updated\n",
    "        run_enc_idx, default_idx = self.get_idx_lists()\n",
    "        \n",
    "        ## read the selected blocks gradients and stores them in self.g\n",
    "        self.read_gradients(run_enc_idx)\n",
    "        \n",
    "        ## get apart the inverses and buffers for the selected blocks\n",
    "        inv = self.fisher_inv[default_idx, ...]\n",
    "        buffer = self.buffer[default_idx, ...]\n",
    "        \n",
    "        # print(inv.shape, buffer.shape)\n",
    "        \n",
    "        ## update the buffer\n",
    "        \n",
    "        ## dimensions are: partitions, gradient-size, bufffer-size\n",
    "        g_old = buffer[:, :, 0:1] \n",
    "        \n",
    "        ## we  update in the third dimension: \"buffer-size\"\n",
    "        buffer = torch.cat([buffer[:, :, 1:], self.g], dim=2)\n",
    "        \n",
    "        ## update the inverses and modify current gradients\n",
    "        ## ...\n",
    "        sqrt_NB = math.sqrt(self.buffer_size)\n",
    "        \n",
    "        ## update inverse - phase 1: add new gradient ##\n",
    "        g_phase1 = self.upd_inverse( (1 / sqrt_NB) * self.g, inv, type_='sum')\n",
    "\n",
    "        ## update inverse - phase 2: remove old gradient ##\n",
    "        self.upd_inverse( (1 / sqrt_NB) * g_old, inv, type_='sub')\n",
    "\n",
    "        ## modify the current gradients\n",
    "        if False:\n",
    "            ## use the \"phase-1-trick\" to get the estimated new gradient\n",
    "            self.g = g_phase1 * sqrt_NB\n",
    "        else:\n",
    "            ## get the modified gradient using \"de facto\" the new inverses and the gradients\n",
    "            self.g = self.modify_grad(self.g, inv)\n",
    "        \n",
    "        ## return the inverses and buffers to the main tensor\n",
    "        self.fisher_inv[default_idx, ...] = inv\n",
    "        self.buffer[default_idx, ...] = buffer\n",
    "        \n",
    "        ## return the modified gradients to the parameters\n",
    "        self.write_gradients(run_enc_idx)\n",
    "\n",
    "\n",
    "    def upd_inverse(self, g, inverse, type_='sum'):\n",
    "        ## update the inverse based on the woodbury inversion\n",
    "        f_inv_g = torch.bmm(inverse, g)\n",
    "\n",
    "        if type_ == 'sum':\n",
    "            d = 1 + torch.sum(g * f_inv_g, dim=[1, 2], keepdim=True)\n",
    "            inverse[:] = inverse - (f_inv_g * torch.transpose(f_inv_g, 1, 2) / d)\n",
    "\n",
    "        elif type_ == 'sub':\n",
    "            d = 1 - torch.sum(g * f_inv_g, dim=[1, 2], keepdim=True)\n",
    "            inverse[:] = inverse + (f_inv_g * torch.transpose(f_inv_g, 1, 2) / d)\n",
    "\n",
    "        else:\n",
    "            ## incorrect type\n",
    "            print('incorrect rank-1 update type: ' + type_)\n",
    "        \n",
    "        return f_inv_g\n",
    "\n",
    "\n",
    "    def modify_grad(self, g, inverse):\n",
    "        return torch.bmm(inverse, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c5c9993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.520759Z",
     "iopub.status.busy": "2022-09-29T04:13:36.520197Z",
     "iopub.status.idle": "2022-09-29T04:13:36.530156Z",
     "shell.execute_reply": "2022-09-29T04:13:36.529306Z"
    },
    "papermill": {
     "duration": 0.02024,
     "end_time": "2022-09-29T04:13:36.532065",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.511825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PartitionerFisherFIFO():\n",
    "    def __init__(self,\n",
    "                 param,\n",
    "                 name,\n",
    "                 buffer_size,\n",
    "                 partition_size,\n",
    "                 block_updates,\n",
    "                 parent_fifo):\n",
    "        \n",
    "        self.param = param\n",
    "        self.name = name \n",
    "        \n",
    "        if partition_size is None:\n",
    "            self.partition_size = param.numel()\n",
    "        else:\n",
    "            self.partition_size = partition_size\n",
    "        \n",
    "        ## calculates the number of partitions required. It is calculated using the param size and\n",
    "        ## our partition maximum size. The gradient (the same size as param) is going to be partitioned in\n",
    "        ## equal pieces (except possibly the last one) to be processed individually by our \"IndividualFisherFIFO\"\n",
    "        self.param_size = param.numel()\n",
    "        self.num_part = math.ceil(self.param_size / self.partition_size)\n",
    "        \n",
    "        ## the number of blocks (partitions) to update at each iteration. This can be < num_part to make\n",
    "        ## the algorithm more efficient. (we dont update every partition at every iteration)\n",
    "        if block_updates is None:\n",
    "            self.block_updates = self.num_part\n",
    "        else:\n",
    "            self.block_updates = min(block_updates, self.num_part)\n",
    "        \n",
    "        print(f'FisherPartitioner: param: {self.param_size} - partition: {self.partition_size} - nº part: {self.num_part} - block updates: {self.block_updates}')\n",
    "                \n",
    "        ## the list stores the indexes used to partition the gradient\n",
    "        self.ind_fisher_list = []\n",
    "        for i in range(self.num_part):\n",
    "            start = i * self.partition_size\n",
    "            end = min(start + self.partition_size, self.param_size)\n",
    "            \n",
    "            self.ind_fisher_list.append( (i, start, end) )\n",
    "        \n",
    "    \n",
    "    def get_random_blocks(self, num_part=None, block_upd=None):\n",
    "        \n",
    "        if num_part is None:\n",
    "            num_part = self.num_part\n",
    "        \n",
    "        if block_upd is None:\n",
    "            block_upd = self.block_updates\n",
    "        \n",
    "        ## choose the initial block randomly\n",
    "        init_block = np.random.choice(num_part - block_upd + 1)\n",
    "        \n",
    "        ## the final block will be necessarily `block_upd` blocks further. This means we select\n",
    "        ## a contiguous sequence of blocks. This is going to be used for performance reasons\n",
    "        end_block = init_block + block_upd - 1\n",
    "        \n",
    "        ## therefore, the starting and ending index to be used to fetch the gradient positions for the\n",
    "        ## blocks will be the starting index for the first block and the ending positions for the last block\n",
    "        _, g_init_idx, _ = self.ind_fisher_list[init_block]\n",
    "        _, _, g_end_idx = self.ind_fisher_list[end_block]\n",
    "        \n",
    "        return init_block, end_block, g_init_idx, g_end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf72936",
   "metadata": {
    "papermill": {
     "duration": 0.007091,
     "end_time": "2022-09-29T04:13:36.546474",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.539383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d54fb",
   "metadata": {
    "papermill": {
     "duration": 0.007104,
     "end_time": "2022-09-29T04:13:36.560979",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.553875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# utils function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c13a4161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.576952Z",
     "iopub.status.busy": "2022-09-29T04:13:36.576695Z",
     "iopub.status.idle": "2022-09-29T04:13:36.580740Z",
     "shell.execute_reply": "2022-09-29T04:13:36.579810Z"
    },
    "papermill": {
     "duration": 0.014349,
     "end_time": "2022-09-29T04:13:36.582624",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.568275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_score_tns(y_true, y_pred):\n",
    "    return torch.mean( (y_true == y_pred).to(dtype=torch.float) ).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "111b4408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.598880Z",
     "iopub.status.busy": "2022-09-29T04:13:36.598095Z",
     "iopub.status.idle": "2022-09-29T04:13:36.603928Z",
     "shell.execute_reply": "2022-09-29T04:13:36.603127Z"
    },
    "papermill": {
     "duration": 0.015864,
     "end_time": "2022-09-29T04:13:36.605840",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.589976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_iteration(x, y, net, optim, loss, fisher=None):\n",
    "    net.train()\n",
    "    net.zero_grad()\n",
    "    \n",
    "    y_pred = net(x)\n",
    "    l = loss(y_pred, y)\n",
    "    \n",
    "    l.backward()\n",
    "    \n",
    "    if fisher is not None:\n",
    "        fisher.step()\n",
    "    \n",
    "    optim.step()\n",
    "    \n",
    "    return l.item(), accuracy_score_tns( y.view(-1), y_pred.argmax(dim=1).view(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27e6df45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.621857Z",
     "iopub.status.busy": "2022-09-29T04:13:36.621336Z",
     "iopub.status.idle": "2022-09-29T04:13:36.628101Z",
     "shell.execute_reply": "2022-09-29T04:13:36.627186Z"
    },
    "papermill": {
     "duration": 0.016838,
     "end_time": "2022-09-29T04:13:36.630029",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.613191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, loss):\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        loss_list = []\n",
    "        y_pred_list = []\n",
    "        y_label_list = []\n",
    "        for x, y in dataloader:\n",
    "            \n",
    "            x = x.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "\n",
    "            y_pred = net(x)\n",
    "            l = loss(y_pred, y)\n",
    "\n",
    "            loss_list.append( l.cpu().item() )\n",
    "            y_pred_list.append( y_pred.argmax(dim=1).view(-1) )\n",
    "            y_label_list.append( y.view(-1) )\n",
    "\n",
    "        y_pred_list = torch.cat(y_pred_list).view(-1)\n",
    "        y_label_list = torch.cat(y_label_list).view(-1)\n",
    "\n",
    "    return np.mean(loss_list), accuracy_score_tns(y_label_list, y_pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a2e36",
   "metadata": {
    "papermill": {
     "duration": 0.00716,
     "end_time": "2022-09-29T04:13:36.644479",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.637319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec6dbac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.660582Z",
     "iopub.status.busy": "2022-09-29T04:13:36.660331Z",
     "iopub.status.idle": "2022-09-29T04:13:36.675154Z",
     "shell.execute_reply": "2022-09-29T04:13:36.674285Z"
    },
    "papermill": {
     "duration": 0.025188,
     "end_time": "2022-09-29T04:13:36.677057",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.651869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_network_fisher_optimization(batch_size = 32,\n",
    "                                      lr = 1e-3,\n",
    "                                      momentum = 0.9,\n",
    "                                      epochs = 30,\n",
    "                                      buffer_size = 1000,\n",
    "                                      partition_size = 256,\n",
    "                                      block_updates = 4,\n",
    "                                      net_params = {'c':16, 'p':0.1},\n",
    "                                      apply_fisher = True,\n",
    "                                      # gpu_memory_check = 20,\n",
    "                                      time_limit_secs = 600,\n",
    "                                      interval_print = 100):\n",
    "\n",
    "    ## declare (instantiate) the dataset\n",
    "    # train_dataloader, test_dataloader = generate_dataset_cifar10(batch_size = batch_size)\n",
    "    # train_dataloader, test_dataloader = generate_dataset_mnist(batch_size = batch_size)\n",
    "    train_dataloader, test_dataloader = generate_dataset_cifar100(batch_size = batch_size)\n",
    "\n",
    "    ## instantiate the network\n",
    "    # net = get_cnn_network_v2(p_drop = net_params['p']).to(device=cfg.device)\n",
    "    net = get_resnet18().to(device=cfg.device)\n",
    "    \n",
    "    if apply_fisher:\n",
    "        ## instantiate FisherFIFO object to create and update the Fisher info matrix\n",
    "        fisher_fifo = FisherFIFO(named_params = net.named_parameters(),\n",
    "                                 buffer_size = buffer_size,\n",
    "                                 partition_size = partition_size,\n",
    "                                 block_updates = block_updates)\n",
    "    else:\n",
    "        fisher_fifo = None\n",
    "\n",
    "    ## create loss object: we multiply by our constant to stabilize norms\n",
    "    # cross_entropy = nn.CrossEntropyLoss(reduction='mean') # standard version\n",
    "    cross_entropy_standard = nn.CrossEntropyLoss(reduction='mean')\n",
    "    cross_entropy = lambda y_pred, y: math.sqrt(batch_size) * cross_entropy_standard(y_pred, y)\n",
    "    \n",
    "    ## create optimize objects\n",
    "    optim = torch.optim.SGD(params=net.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    default_metrics = Metrics(value_round=3, time_round=2)\n",
    "\n",
    "    ini_time = time.time()\n",
    "\n",
    "    step = 0\n",
    "    training_finished = False\n",
    "    for epc in range(1, epochs + 1):\n",
    "        \n",
    "        if training_finished:\n",
    "            break\n",
    "        \n",
    "        print(f'starting epoch: {epc}/{epochs}')\n",
    "\n",
    "        for nbt, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "            if training_finished:\n",
    "                break\n",
    "\n",
    "            x = x.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "\n",
    "            train_loss, train_acc = train_iteration(x, y, net, optim, cross_entropy, fisher_fifo)\n",
    "            default_metrics.add_({'train-loss': train_loss, 'train-acc': train_acc}, step=step)\n",
    "            \n",
    "            ## check time limit\n",
    "            t = int(time.time() - ini_time)\n",
    "            if t > time_limit_secs:\n",
    "                print('time is up! finishing training')\n",
    "                training_finished = True\n",
    "\n",
    "            if ( (nbt + 1) % interval_print ) == 0 or (nbt + 1) == len(train_dataloader) or training_finished:\n",
    "                avg_train_loss = np.mean( default_metrics.get('train-loss')[1][-interval_print:] )\n",
    "                avg_train_acc = np.mean( default_metrics.get('train-acc')[1][-interval_print:] )\n",
    "                \n",
    "                test_loss, test_acc = evaluate(net, test_dataloader, cross_entropy)\n",
    "                default_metrics.add_({'test-loss': test_loss, 'test-acc': test_acc}, step=step)\n",
    "\n",
    "                m, s = t // 60, t % 60\n",
    "\n",
    "                print(f'batch: {nbt + 1}/{len(train_dataloader)}', end='')\n",
    "                print(f' - train loss: {avg_train_loss:.4f} - test loss: {test_loss:.4f}', end='')\n",
    "                print(f' - train acc: {avg_train_acc:.4f} - test acc: {test_acc:.4f}', end='')\n",
    "                print(f' - {m}m {s}s')\n",
    "                \n",
    "            step += 1\n",
    "\n",
    "        ## check for GPU memory consumption\n",
    "        if torch.cuda.is_available():\n",
    "            mem_alloc_gb = torch.cuda.memory_allocated(cfg.device) / 1024**3\n",
    "            mem_res_gb = torch.cuda.memory_reserved(cfg.device) / 1024**3\n",
    "            max_mem_alloc_gb = torch.cuda.max_memory_allocated(cfg.device) / 1024**3\n",
    "            max_mem_res_gb = torch.cuda.max_memory_reserved(cfg.device) / 1024**3\n",
    "\n",
    "            print(f'GPU memory used: {mem_alloc_gb:.2f} GB - max: {max_mem_alloc_gb:.2f} GB - memory reserved: {mem_res_gb:.2f} GB - max: {max_mem_res_gb:.2f} GB')\n",
    "\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "    return default_metrics, fisher_fifo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23c84bd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.692766Z",
     "iopub.status.busy": "2022-09-29T04:13:36.692492Z",
     "iopub.status.idle": "2022-09-29T04:13:36.696045Z",
     "shell.execute_reply": "2022-09-29T04:13:36.695101Z"
    },
    "papermill": {
     "duration": 0.013623,
     "end_time": "2022-09-29T04:13:36.697983",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.684360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_network_fisher_optimization(apply_fisher = True,\n",
    "#                                    buffer_size = 32,\n",
    "#                                    partition_size = 16,\n",
    "#                                    block_updates = 100,\n",
    "#                                    net_params = {'p': 0.1},\n",
    "#                                    epochs = 10,\n",
    "#                                    time_limit_secs = 15 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46d87388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.714157Z",
     "iopub.status.busy": "2022-09-29T04:13:36.713324Z",
     "iopub.status.idle": "2022-09-29T04:13:36.717479Z",
     "shell.execute_reply": "2022-09-29T04:13:36.716652Z"
    },
    "papermill": {
     "duration": 0.014211,
     "end_time": "2022-09-29T04:13:36.719411",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.705200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# net = torchvision.models.resnet18()\n",
    "# torchsummary.summary(net, input_size=[[3, 64, 64]], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "063b324d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.735738Z",
     "iopub.status.busy": "2022-09-29T04:13:36.734889Z",
     "iopub.status.idle": "2022-09-29T04:13:36.741733Z",
     "shell.execute_reply": "2022-09-29T04:13:36.740948Z"
    },
    "papermill": {
     "duration": 0.016888,
     "end_time": "2022-09-29T04:13:36.743611",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.726723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_step_saved = None\n",
    "\n",
    "def results_list_to_json(results_list, out_dir='/kaggle/working', step=0):\n",
    "    global last_step_saved\n",
    "\n",
    "    json_results = []\n",
    "\n",
    "    for metrics, bs, ps, bu in results_list:\n",
    "        json_results.append({\n",
    "            'buffer-size': bs,\n",
    "            'partition-size': ps,\n",
    "            'blocks-updates': bu,\n",
    "            'metrics': metrics.metrics_dict\n",
    "        })\n",
    "\n",
    "    with open( os.path.join(out_dir, f'results_step_{step}.json'), 'w' ) as fp:\n",
    "        json.dump(json_results, fp)\n",
    "    \n",
    "    if last_step_saved is not None:\n",
    "        old_file = os.path.join(out_dir, f'results_step_{last_step_saved}.json')\n",
    "        if os.path.exists(old_file):\n",
    "            os.remove(old_file)\n",
    "    \n",
    "    last_step_saved = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "978eaa62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.760043Z",
     "iopub.status.busy": "2022-09-29T04:13:36.759210Z",
     "iopub.status.idle": "2022-09-29T04:13:36.764155Z",
     "shell.execute_reply": "2022-09-29T04:13:36.763357Z"
    },
    "papermill": {
     "duration": 0.0149,
     "end_time": "2022-09-29T04:13:36.766044",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.751144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_min_test_loss(metrics):\n",
    "    _, test_loss, _ = metrics.get('test-loss')\n",
    "    return min(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb0552",
   "metadata": {
    "papermill": {
     "duration": 0.007179,
     "end_time": "2022-09-29T04:13:36.780524",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.773345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd69c548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.796842Z",
     "iopub.status.busy": "2022-09-29T04:13:36.796059Z",
     "iopub.status.idle": "2022-09-29T04:13:36.800479Z",
     "shell.execute_reply": "2022-09-29T04:13:36.799696Z"
    },
    "papermill": {
     "duration": 0.014564,
     "end_time": "2022-09-29T04:13:36.802375",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.787811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nruns = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf5871d6",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-09-29T04:13:36.818216Z",
     "iopub.status.busy": "2022-09-29T04:13:36.817956Z",
     "iopub.status.idle": "2022-09-29T11:22:10.939032Z",
     "shell.execute_reply": "2022-09-29T11:22:10.937792Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 25714.131435,
     "end_time": "2022-09-29T11:22:10.941268",
     "exception": false,
     "start_time": "2022-09-29T04:13:36.809833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 1\n",
      "generating CIFAR100 data with 100 classes\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24add722f4354e5e9d6d5f7159a1fcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169001437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 25.9180 - test loss: 24.0397 - train acc: 0.0389 - test acc: 0.0523 - 0m 7s\n",
      "batch: 200/1563 - train loss: 24.0361 - test loss: 23.5225 - train acc: 0.0598 - test acc: 0.0731 - 0m 13s\n",
      "batch: 300/1563 - train loss: 23.1668 - test loss: 22.6302 - train acc: 0.0864 - test acc: 0.0948 - 0m 18s\n",
      "batch: 400/1563 - train loss: 22.3778 - test loss: 21.5632 - train acc: 0.0934 - test acc: 0.1110 - 0m 23s\n",
      "batch: 500/1563 - train loss: 22.0812 - test loss: 21.1219 - train acc: 0.1040 - test acc: 0.1293 - 0m 29s\n",
      "batch: 600/1563 - train loss: 21.5632 - test loss: 21.3522 - train acc: 0.1118 - test acc: 0.1274 - 0m 34s\n",
      "batch: 700/1563 - train loss: 20.8606 - test loss: 21.5246 - train acc: 0.1463 - test acc: 0.1233 - 0m 39s\n",
      "batch: 800/1563 - train loss: 20.5852 - test loss: 20.6460 - train acc: 0.1466 - test acc: 0.1482 - 0m 44s\n",
      "batch: 900/1563 - train loss: 20.2316 - test loss: 20.4937 - train acc: 0.1491 - test acc: 0.1497 - 0m 50s\n",
      "batch: 1000/1563 - train loss: 20.2295 - test loss: 19.4240 - train acc: 0.1522 - test acc: 0.1773 - 0m 55s\n",
      "batch: 1100/1563 - train loss: 20.1174 - test loss: 19.3859 - train acc: 0.1603 - test acc: 0.1783 - 1m 0s\n",
      "batch: 1200/1563 - train loss: 19.7861 - test loss: 20.8702 - train acc: 0.1708 - test acc: 0.1404 - 1m 5s\n",
      "batch: 1300/1563 - train loss: 19.4866 - test loss: 19.4445 - train acc: 0.1835 - test acc: 0.1843 - 1m 10s\n",
      "batch: 1400/1563 - train loss: 18.9810 - test loss: 18.9619 - train acc: 0.1926 - test acc: 0.1938 - 1m 16s\n",
      "batch: 1500/1563 - train loss: 18.7502 - test loss: 19.3679 - train acc: 0.1948 - test acc: 0.1808 - 1m 21s\n",
      "batch: 1563/1563 - train loss: 18.8041 - test loss: 18.1661 - train acc: 0.1882 - test acc: 0.2204 - 1m 25s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.8035 - test loss: 19.0401 - train acc: 0.2191 - test acc: 0.1954 - 1m 30s\n",
      "batch: 200/1563 - train loss: 18.1255 - test loss: 17.9307 - train acc: 0.2116 - test acc: 0.2302 - 1m 36s\n",
      "batch: 300/1563 - train loss: 17.7226 - test loss: 18.8403 - train acc: 0.2278 - test acc: 0.1962 - 1m 41s\n",
      "batch: 400/1563 - train loss: 17.4396 - test loss: 18.3334 - train acc: 0.2463 - test acc: 0.2180 - 1m 46s\n",
      "batch: 500/1563 - train loss: 17.6326 - test loss: 18.2752 - train acc: 0.2238 - test acc: 0.2107 - 1m 52s\n",
      "batch: 600/1563 - train loss: 17.2411 - test loss: 17.0868 - train acc: 0.2453 - test acc: 0.2585 - 1m 57s\n",
      "batch: 700/1563 - train loss: 17.1953 - test loss: 17.3424 - train acc: 0.2394 - test acc: 0.2447 - 2m 2s\n",
      "batch: 800/1563 - train loss: 17.1197 - test loss: 17.2192 - train acc: 0.2478 - test acc: 0.2587 - 2m 7s\n",
      "batch: 900/1563 - train loss: 16.8347 - test loss: 17.2525 - train acc: 0.2719 - test acc: 0.2447 - 2m 12s\n",
      "batch: 1000/1563 - train loss: 16.9701 - test loss: 16.6240 - train acc: 0.2588 - test acc: 0.2689 - 2m 17s\n",
      "batch: 1100/1563 - train loss: 16.8993 - test loss: 16.7459 - train acc: 0.2578 - test acc: 0.2719 - 2m 23s\n",
      "batch: 1200/1563 - train loss: 16.7719 - test loss: 16.1827 - train acc: 0.2572 - test acc: 0.2793 - 2m 28s\n",
      "batch: 1300/1563 - train loss: 16.5586 - test loss: 16.5573 - train acc: 0.2753 - test acc: 0.2749 - 2m 34s\n",
      "batch: 1400/1563 - train loss: 16.1263 - test loss: 16.3037 - train acc: 0.2813 - test acc: 0.2823 - 2m 39s\n",
      "batch: 1500/1563 - train loss: 16.4379 - test loss: 16.9105 - train acc: 0.2750 - test acc: 0.2674 - 2m 44s\n",
      "batch: 1563/1563 - train loss: 16.1938 - test loss: 17.4596 - train acc: 0.2737 - test acc: 0.2574 - 2m 48s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.1148 - test loss: 16.3549 - train acc: 0.3110 - test acc: 0.2821 - 2m 54s\n",
      "batch: 200/1563 - train loss: 15.0762 - test loss: 15.9279 - train acc: 0.3103 - test acc: 0.3006 - 2m 59s\n",
      "batch: 300/1563 - train loss: 14.8351 - test loss: 17.7103 - train acc: 0.3203 - test acc: 0.2515 - 3m 4s\n",
      "batch: 400/1563 - train loss: 14.8346 - test loss: 15.8847 - train acc: 0.3256 - test acc: 0.2992 - 3m 9s\n",
      "batch: 500/1563 - train loss: 15.3159 - test loss: 15.5333 - train acc: 0.3178 - test acc: 0.3099 - 3m 14s\n",
      "batch: 600/1563 - train loss: 14.8921 - test loss: 16.3271 - train acc: 0.3266 - test acc: 0.2939 - 3m 20s\n",
      "batch: 700/1563 - train loss: 14.9691 - test loss: 15.8227 - train acc: 0.3224 - test acc: 0.3039 - 3m 25s\n",
      "batch: 800/1563 - train loss: 15.1928 - test loss: 15.1586 - train acc: 0.3122 - test acc: 0.3192 - 3m 31s\n",
      "batch: 900/1563 - train loss: 14.8890 - test loss: 15.6590 - train acc: 0.3221 - test acc: 0.3050 - 3m 36s\n",
      "batch: 1000/1563 - train loss: 14.7451 - test loss: 15.8396 - train acc: 0.3291 - test acc: 0.3044 - 3m 41s\n",
      "batch: 1100/1563 - train loss: 14.7736 - test loss: 14.9690 - train acc: 0.3387 - test acc: 0.3270 - 3m 46s\n",
      "batch: 1200/1563 - train loss: 14.7720 - test loss: 16.2067 - train acc: 0.3265 - test acc: 0.2906 - 3m 51s\n",
      "batch: 1300/1563 - train loss: 14.4519 - test loss: 15.1658 - train acc: 0.3419 - test acc: 0.3270 - 3m 57s\n",
      "batch: 1400/1563 - train loss: 14.6997 - test loss: 15.4954 - train acc: 0.3384 - test acc: 0.3213 - 4m 2s\n",
      "batch: 1500/1563 - train loss: 14.4472 - test loss: 15.5100 - train acc: 0.3356 - test acc: 0.3097 - 4m 7s\n",
      "batch: 1563/1563 - train loss: 14.5345 - test loss: 15.5932 - train acc: 0.3431 - test acc: 0.3125 - 4m 12s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.4068 - test loss: 14.7943 - train acc: 0.3809 - test acc: 0.3394 - 4m 17s\n",
      "batch: 200/1563 - train loss: 12.9876 - test loss: 16.0688 - train acc: 0.3873 - test acc: 0.3101 - 4m 22s\n",
      "batch: 300/1563 - train loss: 13.3178 - test loss: 14.1945 - train acc: 0.3912 - test acc: 0.3697 - 4m 28s\n",
      "batch: 400/1563 - train loss: 13.4202 - test loss: 14.8938 - train acc: 0.3806 - test acc: 0.3405 - 4m 33s\n",
      "batch: 500/1563 - train loss: 13.1432 - test loss: 14.8070 - train acc: 0.3803 - test acc: 0.3461 - 4m 38s\n",
      "batch: 600/1563 - train loss: 13.4117 - test loss: 16.3737 - train acc: 0.3791 - test acc: 0.2994 - 4m 43s\n",
      "batch: 700/1563 - train loss: 13.4299 - test loss: 15.4959 - train acc: 0.3872 - test acc: 0.3154 - 4m 48s\n",
      "batch: 800/1563 - train loss: 13.2942 - test loss: 15.0428 - train acc: 0.3888 - test acc: 0.3332 - 4m 54s\n",
      "batch: 900/1563 - train loss: 13.5416 - test loss: 14.1581 - train acc: 0.3726 - test acc: 0.3592 - 4m 59s\n",
      "batch: 1000/1563 - train loss: 13.2164 - test loss: 15.1122 - train acc: 0.3903 - test acc: 0.3368 - 5m 5s\n",
      "batch: 1100/1563 - train loss: 13.7006 - test loss: 14.7846 - train acc: 0.3722 - test acc: 0.3388 - 5m 10s\n",
      "batch: 1200/1563 - train loss: 13.1943 - test loss: 15.6307 - train acc: 0.3872 - test acc: 0.3180 - 5m 15s\n",
      "batch: 1300/1563 - train loss: 13.0155 - test loss: 14.1035 - train acc: 0.3976 - test acc: 0.3644 - 5m 20s\n",
      "batch: 1400/1563 - train loss: 13.3953 - test loss: 14.5692 - train acc: 0.3810 - test acc: 0.3493 - 5m 25s\n",
      "batch: 1500/1563 - train loss: 13.2076 - test loss: 13.9905 - train acc: 0.3859 - test acc: 0.3713 - 5m 31s\n",
      "batch: 1563/1563 - train loss: 13.3004 - test loss: 14.9184 - train acc: 0.3891 - test acc: 0.3362 - 5m 35s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 11.5483 - test loss: 14.4987 - train acc: 0.4485 - test acc: 0.3596 - 5m 40s\n",
      "batch: 200/1563 - train loss: 11.8536 - test loss: 14.2795 - train acc: 0.4372 - test acc: 0.3688 - 5m 45s\n",
      "batch: 300/1563 - train loss: 11.9146 - test loss: 14.1461 - train acc: 0.4300 - test acc: 0.3701 - 5m 51s\n",
      "batch: 400/1563 - train loss: 11.8092 - test loss: 14.7023 - train acc: 0.4435 - test acc: 0.3617 - 5m 56s\n",
      "batch: 500/1563 - train loss: 12.1262 - test loss: 14.2434 - train acc: 0.4322 - test acc: 0.3691 - 6m 1s\n",
      "batch: 600/1563 - train loss: 11.9621 - test loss: 14.6337 - train acc: 0.4425 - test acc: 0.3552 - 6m 6s\n",
      "batch: 700/1563 - train loss: 12.0691 - test loss: 14.9162 - train acc: 0.4222 - test acc: 0.3575 - 6m 12s\n",
      "batch: 800/1563 - train loss: 11.9389 - test loss: 14.6133 - train acc: 0.4338 - test acc: 0.3497 - 6m 17s\n",
      "batch: 900/1563 - train loss: 11.9853 - test loss: 14.9499 - train acc: 0.4400 - test acc: 0.3540 - 6m 22s\n",
      "batch: 1000/1563 - train loss: 12.0297 - test loss: 14.2051 - train acc: 0.4416 - test acc: 0.3677 - 6m 27s\n",
      "batch: 1100/1563 - train loss: 12.0130 - test loss: 20.4302 - train acc: 0.4450 - test acc: 0.2554 - 6m 32s\n",
      "batch: 1200/1563 - train loss: 12.3090 - test loss: 13.7016 - train acc: 0.4225 - test acc: 0.3822 - 6m 38s\n",
      "batch: 1300/1563 - train loss: 12.0069 - test loss: 13.9409 - train acc: 0.4391 - test acc: 0.3792 - 6m 43s\n",
      "batch: 1400/1563 - train loss: 12.3003 - test loss: 13.6284 - train acc: 0.4316 - test acc: 0.3864 - 6m 48s\n",
      "batch: 1500/1563 - train loss: 12.2348 - test loss: 13.7215 - train acc: 0.4272 - test acc: 0.3831 - 6m 53s\n",
      "batch: 1563/1563 - train loss: 11.8342 - test loss: 14.1565 - train acc: 0.4453 - test acc: 0.3687 - 6m 58s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0882 - test loss: 13.7723 - train acc: 0.5162 - test acc: 0.3923 - 7m 3s\n",
      "batch: 200/1563 - train loss: 10.3033 - test loss: 14.1478 - train acc: 0.4941 - test acc: 0.3750 - 7m 8s\n",
      "batch: 300/1563 - train loss: 10.5668 - test loss: 13.6432 - train acc: 0.4875 - test acc: 0.3850 - 7m 14s\n",
      "batch: 400/1563 - train loss: 10.7113 - test loss: 14.1901 - train acc: 0.4859 - test acc: 0.3788 - 7m 19s\n",
      "batch: 500/1563 - train loss: 10.7297 - test loss: 16.4333 - train acc: 0.4847 - test acc: 0.3231 - 7m 24s\n",
      "batch: 600/1563 - train loss: 10.8713 - test loss: 16.6128 - train acc: 0.4662 - test acc: 0.3193 - 7m 29s\n",
      "batch: 700/1563 - train loss: 10.7348 - test loss: 14.0345 - train acc: 0.4772 - test acc: 0.3852 - 7m 34s\n",
      "batch: 800/1563 - train loss: 11.2445 - test loss: 13.4055 - train acc: 0.4750 - test acc: 0.3927 - 7m 40s\n",
      "batch: 900/1563 - train loss: 10.9577 - test loss: 13.4472 - train acc: 0.4784 - test acc: 0.3966 - 7m 45s\n",
      "batch: 1000/1563 - train loss: 11.2436 - test loss: 14.4427 - train acc: 0.4490 - test acc: 0.3696 - 7m 50s\n",
      "batch: 1100/1563 - train loss: 11.1263 - test loss: 14.1364 - train acc: 0.4769 - test acc: 0.3764 - 7m 55s\n",
      "batch: 1200/1563 - train loss: 11.1244 - test loss: 14.0437 - train acc: 0.4800 - test acc: 0.3852 - 8m 1s\n",
      "batch: 1300/1563 - train loss: 10.6926 - test loss: 13.6858 - train acc: 0.4838 - test acc: 0.3958 - 8m 6s\n",
      "batch: 1400/1563 - train loss: 10.9864 - test loss: 13.9485 - train acc: 0.4713 - test acc: 0.3874 - 8m 11s\n",
      "batch: 1500/1563 - train loss: 10.9091 - test loss: 13.1441 - train acc: 0.4825 - test acc: 0.4112 - 8m 17s\n",
      "batch: 1563/1563 - train loss: 11.0527 - test loss: 13.0944 - train acc: 0.4644 - test acc: 0.4116 - 8m 21s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.9122 - test loss: 14.5981 - train acc: 0.5593 - test acc: 0.3790 - 8m 26s\n",
      "batch: 200/1563 - train loss: 9.2586 - test loss: 14.2645 - train acc: 0.5472 - test acc: 0.3834 - 8m 31s\n",
      "batch: 300/1563 - train loss: 9.2521 - test loss: 13.8783 - train acc: 0.5359 - test acc: 0.3940 - 8m 36s\n",
      "batch: 400/1563 - train loss: 9.4841 - test loss: 13.9849 - train acc: 0.5390 - test acc: 0.3970 - 8m 42s\n",
      "batch: 500/1563 - train loss: 9.5130 - test loss: 14.3300 - train acc: 0.5290 - test acc: 0.3882 - 8m 47s\n",
      "batch: 600/1563 - train loss: 9.8876 - test loss: 13.3080 - train acc: 0.5203 - test acc: 0.4160 - 8m 52s\n",
      "batch: 700/1563 - train loss: 9.7773 - test loss: 14.1444 - train acc: 0.5269 - test acc: 0.3969 - 8m 57s\n",
      "batch: 800/1563 - train loss: 9.8844 - test loss: 13.7797 - train acc: 0.5110 - test acc: 0.4007 - 9m 3s\n",
      "batch: 900/1563 - train loss: 9.9545 - test loss: 14.6166 - train acc: 0.5225 - test acc: 0.3771 - 9m 8s\n",
      "batch: 1000/1563 - train loss: 10.1908 - test loss: 14.7079 - train acc: 0.5031 - test acc: 0.3671 - 9m 13s\n",
      "batch: 1100/1563 - train loss: 9.7156 - test loss: 13.8147 - train acc: 0.5231 - test acc: 0.3942 - 9m 19s\n",
      "batch: 1200/1563 - train loss: 9.7937 - test loss: 13.9657 - train acc: 0.5253 - test acc: 0.3939 - 9m 24s\n",
      "batch: 1300/1563 - train loss: 9.9948 - test loss: 15.0946 - train acc: 0.5153 - test acc: 0.3731 - 9m 29s\n",
      "batch: 1400/1563 - train loss: 10.0070 - test loss: 14.8514 - train acc: 0.5068 - test acc: 0.3669 - 9m 34s\n",
      "batch: 1500/1563 - train loss: 10.1482 - test loss: 13.6951 - train acc: 0.5106 - test acc: 0.4047 - 9m 39s\n",
      "batch: 1563/1563 - train loss: 10.1587 - test loss: 13.3228 - train acc: 0.5028 - test acc: 0.4145 - 9m 44s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.8465 - test loss: 13.7958 - train acc: 0.5959 - test acc: 0.4095 - 9m 49s\n",
      "batch: 200/1563 - train loss: 7.7398 - test loss: 14.3457 - train acc: 0.6147 - test acc: 0.3956 - 9m 54s\n",
      "batch: 300/1563 - train loss: 8.2102 - test loss: 13.7165 - train acc: 0.5841 - test acc: 0.4228 - 10m 0s\n",
      "batch: 400/1563 - train loss: 8.4661 - test loss: 13.3985 - train acc: 0.5753 - test acc: 0.4195 - 10m 5s\n",
      "batch: 500/1563 - train loss: 8.3713 - test loss: 14.2041 - train acc: 0.5919 - test acc: 0.4009 - 10m 10s\n",
      "batch: 600/1563 - train loss: 8.7206 - test loss: 14.1893 - train acc: 0.5634 - test acc: 0.3995 - 10m 15s\n",
      "batch: 700/1563 - train loss: 8.8416 - test loss: 13.7193 - train acc: 0.5656 - test acc: 0.4114 - 10m 21s\n",
      "batch: 800/1563 - train loss: 8.6644 - test loss: 14.1230 - train acc: 0.5628 - test acc: 0.4071 - 10m 26s\n",
      "batch: 900/1563 - train loss: 8.8016 - test loss: 14.0739 - train acc: 0.5665 - test acc: 0.4057 - 10m 31s\n",
      "batch: 1000/1563 - train loss: 8.9203 - test loss: 14.4795 - train acc: 0.5525 - test acc: 0.3948 - 10m 36s\n",
      "batch: 1100/1563 - train loss: 9.1781 - test loss: 13.8876 - train acc: 0.5428 - test acc: 0.4041 - 10m 41s\n",
      "batch: 1200/1563 - train loss: 9.2144 - test loss: 13.2749 - train acc: 0.5530 - test acc: 0.4239 - 10m 47s\n",
      "batch: 1300/1563 - train loss: 8.9290 - test loss: 14.1854 - train acc: 0.5578 - test acc: 0.3977 - 10m 52s\n",
      "batch: 1400/1563 - train loss: 9.3160 - test loss: 14.0433 - train acc: 0.5484 - test acc: 0.4035 - 10m 57s\n",
      "batch: 1500/1563 - train loss: 9.0793 - test loss: 13.3725 - train acc: 0.5453 - test acc: 0.4214 - 11m 2s\n",
      "batch: 1563/1563 - train loss: 9.3290 - test loss: 13.6180 - train acc: 0.5435 - test acc: 0.4109 - 11m 7s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.8584 - test loss: 14.2826 - train acc: 0.6475 - test acc: 0.4141 - 11m 12s\n",
      "batch: 200/1563 - train loss: 6.7628 - test loss: 14.6300 - train acc: 0.6447 - test acc: 0.4092 - 11m 17s\n",
      "batch: 300/1563 - train loss: 7.1329 - test loss: 14.4691 - train acc: 0.6347 - test acc: 0.4084 - 11m 23s\n",
      "batch: 400/1563 - train loss: 7.4656 - test loss: 14.7496 - train acc: 0.6140 - test acc: 0.4027 - 11m 28s\n",
      "batch: 500/1563 - train loss: 7.5961 - test loss: 14.0514 - train acc: 0.6144 - test acc: 0.4113 - 11m 33s\n",
      "batch: 600/1563 - train loss: 7.7853 - test loss: 14.2777 - train acc: 0.6010 - test acc: 0.4104 - 11m 38s\n",
      "batch: 700/1563 - train loss: 8.1080 - test loss: 13.9031 - train acc: 0.5953 - test acc: 0.4201 - 11m 44s\n",
      "batch: 800/1563 - train loss: 7.8089 - test loss: 14.0699 - train acc: 0.6041 - test acc: 0.4173 - 11m 49s\n",
      "batch: 900/1563 - train loss: 7.8507 - test loss: 14.3778 - train acc: 0.6012 - test acc: 0.4042 - 11m 54s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 7.6567 - test loss: 14.6821 - train acc: 0.6153 - test acc: 0.4044 - 12m 0s\n",
      "batch: 1100/1563 - train loss: 8.4627 - test loss: 14.2077 - train acc: 0.5637 - test acc: 0.4050 - 12m 5s\n",
      "batch: 1200/1563 - train loss: 8.0714 - test loss: 14.1906 - train acc: 0.5872 - test acc: 0.4076 - 12m 10s\n",
      "batch: 1300/1563 - train loss: 8.0824 - test loss: 13.7809 - train acc: 0.5875 - test acc: 0.4172 - 12m 15s\n",
      "batch: 1400/1563 - train loss: 8.1665 - test loss: 14.1929 - train acc: 0.5993 - test acc: 0.4037 - 12m 20s\n",
      "batch: 1500/1563 - train loss: 8.2131 - test loss: 13.8615 - train acc: 0.5878 - test acc: 0.4126 - 12m 26s\n",
      "batch: 1563/1563 - train loss: 8.3318 - test loss: 13.5141 - train acc: 0.5787 - test acc: 0.4226 - 12m 31s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.7367 - test loss: 14.0895 - train acc: 0.7066 - test acc: 0.4238 - 12m 36s\n",
      "batch: 200/1563 - train loss: 5.6130 - test loss: 14.8098 - train acc: 0.7038 - test acc: 0.4130 - 12m 41s\n",
      "batch: 300/1563 - train loss: 6.3439 - test loss: 14.0745 - train acc: 0.6684 - test acc: 0.4251 - 12m 46s\n",
      "batch: 400/1563 - train loss: 6.1886 - test loss: 14.4086 - train acc: 0.6816 - test acc: 0.4133 - 12m 51s\n",
      "batch: 500/1563 - train loss: 6.4562 - test loss: 14.8011 - train acc: 0.6475 - test acc: 0.4138 - 12m 57s\n",
      "batch: 600/1563 - train loss: 6.7054 - test loss: 14.6496 - train acc: 0.6550 - test acc: 0.4147 - 13m 2s\n",
      "batch: 700/1563 - train loss: 6.7199 - test loss: 15.3468 - train acc: 0.6516 - test acc: 0.3960 - 13m 7s\n",
      "batch: 800/1563 - train loss: 6.7895 - test loss: 14.7972 - train acc: 0.6428 - test acc: 0.4131 - 13m 13s\n",
      "batch: 900/1563 - train loss: 7.1761 - test loss: 14.9537 - train acc: 0.6319 - test acc: 0.4077 - 13m 18s\n",
      "batch: 1000/1563 - train loss: 7.1779 - test loss: 14.2952 - train acc: 0.6294 - test acc: 0.4225 - 13m 23s\n",
      "batch: 1100/1563 - train loss: 7.0142 - test loss: 14.3007 - train acc: 0.6441 - test acc: 0.4206 - 13m 29s\n",
      "batch: 1200/1563 - train loss: 6.9569 - test loss: 14.0955 - train acc: 0.6403 - test acc: 0.4262 - 13m 34s\n",
      "batch: 1300/1563 - train loss: 7.3255 - test loss: 14.9064 - train acc: 0.6304 - test acc: 0.4035 - 13m 39s\n",
      "batch: 1400/1563 - train loss: 7.0685 - test loss: 14.9721 - train acc: 0.6310 - test acc: 0.4135 - 13m 44s\n",
      "batch: 1500/1563 - train loss: 7.3160 - test loss: 14.8393 - train acc: 0.6341 - test acc: 0.4104 - 13m 49s\n",
      "batch: 1563/1563 - train loss: 6.9655 - test loss: 14.9764 - train acc: 0.6425 - test acc: 0.4026 - 13m 54s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.9259 - test loss: 15.3148 - train acc: 0.7456 - test acc: 0.4153 - 13m 59s\n",
      "batch: 200/1563 - train loss: 4.8928 - test loss: 14.5129 - train acc: 0.7407 - test acc: 0.4342 - 14m 4s\n",
      "batch: 300/1563 - train loss: 5.1003 - test loss: 15.2479 - train acc: 0.7310 - test acc: 0.4149 - 14m 10s\n",
      "batch: 400/1563 - train loss: 5.6852 - test loss: 15.2367 - train acc: 0.7016 - test acc: 0.4197 - 14m 15s\n",
      "batch: 500/1563 - train loss: 5.5948 - test loss: 14.9428 - train acc: 0.7091 - test acc: 0.4312 - 14m 20s\n",
      "batch: 600/1563 - train loss: 5.5451 - test loss: 15.4320 - train acc: 0.6944 - test acc: 0.4133 - 14m 25s\n",
      "batch: 700/1563 - train loss: 5.7660 - test loss: 15.2540 - train acc: 0.7041 - test acc: 0.4176 - 14m 31s\n",
      "batch: 800/1563 - train loss: 6.0487 - test loss: 15.2878 - train acc: 0.6766 - test acc: 0.4200 - 14m 36s\n",
      "batch: 900/1563 - train loss: 6.0596 - test loss: 15.2504 - train acc: 0.6725 - test acc: 0.4150 - 14m 41s\n",
      "batch: 1000/1563 - train loss: 6.2792 - test loss: 15.4261 - train acc: 0.6562 - test acc: 0.4137 - 14m 47s\n",
      "batch: 1100/1563 - train loss: 6.4229 - test loss: 15.1098 - train acc: 0.6581 - test acc: 0.4224 - 14m 52s\n",
      "batch: 1200/1563 - train loss: 6.3986 - test loss: 15.5257 - train acc: 0.6675 - test acc: 0.4089 - 14m 57s\n",
      "batch: 1300/1563 - train loss: 6.3973 - test loss: 15.1023 - train acc: 0.6607 - test acc: 0.4145 - 15m 3s\n",
      "batch: 1400/1563 - train loss: 6.5002 - test loss: 15.1147 - train acc: 0.6556 - test acc: 0.4168 - 15m 8s\n",
      "batch: 1500/1563 - train loss: 6.8189 - test loss: 15.0557 - train acc: 0.6413 - test acc: 0.4142 - 15m 13s\n",
      "batch: 1563/1563 - train loss: 6.4991 - test loss: 15.7753 - train acc: 0.6604 - test acc: 0.3971 - 15m 17s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.3314 - test loss: 15.9303 - train acc: 0.7650 - test acc: 0.4050 - 15m 23s\n",
      "batch: 200/1563 - train loss: 4.4459 - test loss: 15.9532 - train acc: 0.7538 - test acc: 0.4116 - 15m 28s\n",
      "batch: 300/1563 - train loss: 4.2771 - test loss: 15.8822 - train acc: 0.7584 - test acc: 0.4192 - 15m 33s\n",
      "batch: 400/1563 - train loss: 4.5803 - test loss: 16.7131 - train acc: 0.7521 - test acc: 0.3949 - 15m 39s\n",
      "batch: 500/1563 - train loss: 4.6424 - test loss: 16.0167 - train acc: 0.7484 - test acc: 0.4093 - 15m 44s\n",
      "batch: 600/1563 - train loss: 4.9785 - test loss: 15.6469 - train acc: 0.7400 - test acc: 0.4252 - 15m 49s\n",
      "batch: 700/1563 - train loss: 4.8854 - test loss: 15.7497 - train acc: 0.7337 - test acc: 0.4203 - 15m 54s\n",
      "batch: 800/1563 - train loss: 5.0311 - test loss: 15.8512 - train acc: 0.7331 - test acc: 0.4210 - 15m 59s\n",
      "batch: 900/1563 - train loss: 5.4503 - test loss: 16.3881 - train acc: 0.7097 - test acc: 0.3954 - 16m 4s\n",
      "batch: 1000/1563 - train loss: 5.1781 - test loss: 15.2640 - train acc: 0.7234 - test acc: 0.4286 - 16m 10s\n",
      "batch: 1100/1563 - train loss: 5.3138 - test loss: 16.1542 - train acc: 0.7075 - test acc: 0.4076 - 16m 15s\n",
      "batch: 1200/1563 - train loss: 5.5162 - test loss: 15.2825 - train acc: 0.7101 - test acc: 0.4239 - 16m 20s\n",
      "batch: 1300/1563 - train loss: 5.7366 - test loss: 15.1867 - train acc: 0.6938 - test acc: 0.4321 - 16m 25s\n",
      "batch: 1400/1563 - train loss: 5.4924 - test loss: 16.1356 - train acc: 0.7053 - test acc: 0.4102 - 16m 31s\n",
      "batch: 1500/1563 - train loss: 5.5913 - test loss: 15.2988 - train acc: 0.7143 - test acc: 0.4307 - 16m 36s\n",
      "batch: 1563/1563 - train loss: 5.8684 - test loss: 15.6915 - train acc: 0.7028 - test acc: 0.4155 - 16m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.7213 - test loss: 15.1588 - train acc: 0.7984 - test acc: 0.4413 - 16m 46s\n",
      "batch: 200/1563 - train loss: 3.5417 - test loss: 15.8625 - train acc: 0.7993 - test acc: 0.4259 - 16m 51s\n",
      "batch: 300/1563 - train loss: 3.8045 - test loss: 16.5684 - train acc: 0.7909 - test acc: 0.4100 - 16m 56s\n",
      "batch: 400/1563 - train loss: 3.8111 - test loss: 16.1972 - train acc: 0.7860 - test acc: 0.4228 - 17m 2s\n",
      "batch: 500/1563 - train loss: 4.0177 - test loss: 17.2300 - train acc: 0.7831 - test acc: 0.4132 - 17m 7s\n",
      "batch: 600/1563 - train loss: 3.9729 - test loss: 16.5814 - train acc: 0.7816 - test acc: 0.4270 - 17m 12s\n",
      "batch: 700/1563 - train loss: 4.2636 - test loss: 16.4171 - train acc: 0.7681 - test acc: 0.4136 - 17m 18s\n",
      "batch: 800/1563 - train loss: 4.5267 - test loss: 16.1904 - train acc: 0.7478 - test acc: 0.4192 - 17m 23s\n",
      "batch: 900/1563 - train loss: 4.6710 - test loss: 15.9749 - train acc: 0.7528 - test acc: 0.4218 - 17m 28s\n",
      "batch: 1000/1563 - train loss: 4.4603 - test loss: 16.2305 - train acc: 0.7581 - test acc: 0.4234 - 17m 33s\n",
      "batch: 1100/1563 - train loss: 4.9268 - test loss: 16.3489 - train acc: 0.7318 - test acc: 0.4179 - 17m 38s\n",
      "batch: 1200/1563 - train loss: 4.7818 - test loss: 16.1503 - train acc: 0.7399 - test acc: 0.4202 - 17m 44s\n",
      "batch: 1300/1563 - train loss: 4.7968 - test loss: 16.5842 - train acc: 0.7353 - test acc: 0.4069 - 17m 49s\n",
      "batch: 1400/1563 - train loss: 4.9634 - test loss: 15.8771 - train acc: 0.7234 - test acc: 0.4242 - 17m 54s\n",
      "batch: 1500/1563 - train loss: 5.2039 - test loss: 15.9965 - train acc: 0.7259 - test acc: 0.4253 - 17m 59s\n",
      "batch: 1563/1563 - train loss: 5.1378 - test loss: 16.1613 - train acc: 0.7353 - test acc: 0.4265 - 18m 4s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8844 - test loss: 16.6000 - train acc: 0.8402 - test acc: 0.4287 - 18m 9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 3.1088 - test loss: 16.4597 - train acc: 0.8219 - test acc: 0.4348 - 18m 14s\n",
      "batch: 300/1563 - train loss: 2.9350 - test loss: 17.0242 - train acc: 0.8403 - test acc: 0.4290 - 18m 20s\n",
      "batch: 400/1563 - train loss: 3.2403 - test loss: 17.1984 - train acc: 0.8141 - test acc: 0.4152 - 18m 25s\n",
      "batch: 500/1563 - train loss: 3.5648 - test loss: 17.1037 - train acc: 0.7939 - test acc: 0.4243 - 18m 30s\n",
      "batch: 600/1563 - train loss: 3.3808 - test loss: 16.9939 - train acc: 0.8071 - test acc: 0.4241 - 18m 35s\n",
      "batch: 700/1563 - train loss: 3.7905 - test loss: 18.0141 - train acc: 0.7850 - test acc: 0.4113 - 18m 40s\n",
      "batch: 800/1563 - train loss: 3.7821 - test loss: 17.2948 - train acc: 0.7828 - test acc: 0.4143 - 18m 46s\n",
      "batch: 900/1563 - train loss: 3.9447 - test loss: 16.6679 - train acc: 0.7796 - test acc: 0.4263 - 18m 51s\n",
      "batch: 1000/1563 - train loss: 4.0294 - test loss: 17.3205 - train acc: 0.7808 - test acc: 0.4121 - 18m 56s\n",
      "batch: 1100/1563 - train loss: 4.0381 - test loss: 16.7751 - train acc: 0.7787 - test acc: 0.4243 - 19m 1s\n",
      "batch: 1200/1563 - train loss: 4.2455 - test loss: 17.0115 - train acc: 0.7747 - test acc: 0.4201 - 19m 7s\n",
      "batch: 1300/1563 - train loss: 4.2534 - test loss: 16.3771 - train acc: 0.7647 - test acc: 0.4310 - 19m 12s\n",
      "batch: 1400/1563 - train loss: 4.2435 - test loss: 16.5266 - train acc: 0.7648 - test acc: 0.4268 - 19m 17s\n",
      "batch: 1500/1563 - train loss: 4.4595 - test loss: 16.9990 - train acc: 0.7509 - test acc: 0.4100 - 19m 22s\n",
      "batch: 1563/1563 - train loss: 4.4830 - test loss: 16.5638 - train acc: 0.7534 - test acc: 0.4215 - 19m 27s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.5512 - test loss: 17.5722 - train acc: 0.8615 - test acc: 0.4156 - 19m 32s\n",
      "batch: 200/1563 - train loss: 2.4824 - test loss: 16.8665 - train acc: 0.8562 - test acc: 0.4380 - 19m 37s\n",
      "batch: 300/1563 - train loss: 2.6079 - test loss: 17.9569 - train acc: 0.8478 - test acc: 0.4237 - 19m 42s\n",
      "batch: 400/1563 - train loss: 2.7947 - test loss: 18.0659 - train acc: 0.8412 - test acc: 0.4194 - 19m 48s\n",
      "batch: 500/1563 - train loss: 2.8430 - test loss: 17.7033 - train acc: 0.8290 - test acc: 0.4221 - 19m 53s\n",
      "batch: 600/1563 - train loss: 3.0036 - test loss: 17.2620 - train acc: 0.8343 - test acc: 0.4235 - 19m 59s\n",
      "time is up! finishing training\n",
      "batch: 601/1563 - train loss: 3.0111 - test loss: 17.0760 - train acc: 0.8347 - test acc: 0.4276 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 2\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0331 - test loss: 24.6751 - train acc: 0.0392 - test acc: 0.0592 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.6848 - test loss: 22.9696 - train acc: 0.0764 - test acc: 0.0875 - 0m 8s\n",
      "batch: 300/1563 - train loss: 22.8623 - test loss: 22.9488 - train acc: 0.0890 - test acc: 0.0802 - 0m 13s\n",
      "batch: 400/1563 - train loss: 22.2310 - test loss: 21.4540 - train acc: 0.1043 - test acc: 0.1192 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.4620 - test loss: 21.5784 - train acc: 0.1183 - test acc: 0.1269 - 0m 23s\n",
      "batch: 600/1563 - train loss: 21.0408 - test loss: 20.4219 - train acc: 0.1256 - test acc: 0.1459 - 0m 28s\n",
      "batch: 700/1563 - train loss: 20.5906 - test loss: 21.5501 - train acc: 0.1394 - test acc: 0.1231 - 0m 33s\n",
      "batch: 800/1563 - train loss: 20.0373 - test loss: 20.3300 - train acc: 0.1563 - test acc: 0.1620 - 0m 39s\n",
      "batch: 900/1563 - train loss: 19.9201 - test loss: 20.3755 - train acc: 0.1616 - test acc: 0.1572 - 0m 44s\n",
      "batch: 1000/1563 - train loss: 19.6044 - test loss: 19.4401 - train acc: 0.1703 - test acc: 0.1744 - 0m 49s\n",
      "batch: 1100/1563 - train loss: 19.2627 - test loss: 18.7824 - train acc: 0.1875 - test acc: 0.1924 - 0m 54s\n",
      "batch: 1200/1563 - train loss: 19.1415 - test loss: 19.1817 - train acc: 0.1857 - test acc: 0.1827 - 0m 59s\n",
      "batch: 1300/1563 - train loss: 18.9251 - test loss: 18.9819 - train acc: 0.1950 - test acc: 0.2006 - 1m 4s\n",
      "batch: 1400/1563 - train loss: 18.7543 - test loss: 19.4852 - train acc: 0.2076 - test acc: 0.1912 - 1m 10s\n",
      "batch: 1500/1563 - train loss: 18.5108 - test loss: 18.5358 - train acc: 0.2097 - test acc: 0.2140 - 1m 15s\n",
      "batch: 1563/1563 - train loss: 18.1832 - test loss: 18.0133 - train acc: 0.2163 - test acc: 0.2271 - 1m 19s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.6135 - test loss: 18.4288 - train acc: 0.2334 - test acc: 0.2152 - 1m 24s\n",
      "batch: 200/1563 - train loss: 17.4119 - test loss: 17.4666 - train acc: 0.2363 - test acc: 0.2394 - 1m 29s\n",
      "batch: 300/1563 - train loss: 17.2815 - test loss: 18.5794 - train acc: 0.2425 - test acc: 0.2166 - 1m 35s\n",
      "batch: 400/1563 - train loss: 17.2027 - test loss: 17.4588 - train acc: 0.2441 - test acc: 0.2445 - 1m 40s\n",
      "batch: 500/1563 - train loss: 16.7659 - test loss: 16.7599 - train acc: 0.2619 - test acc: 0.2626 - 1m 45s\n",
      "batch: 600/1563 - train loss: 17.0298 - test loss: 16.8293 - train acc: 0.2541 - test acc: 0.2668 - 1m 50s\n",
      "batch: 700/1563 - train loss: 16.5834 - test loss: 17.6180 - train acc: 0.2737 - test acc: 0.2482 - 1m 55s\n",
      "batch: 800/1563 - train loss: 16.9066 - test loss: 17.5436 - train acc: 0.2519 - test acc: 0.2458 - 2m 0s\n",
      "batch: 900/1563 - train loss: 16.4864 - test loss: 17.3878 - train acc: 0.2594 - test acc: 0.2462 - 2m 6s\n",
      "batch: 1000/1563 - train loss: 16.4881 - test loss: 16.2704 - train acc: 0.2684 - test acc: 0.2769 - 2m 11s\n",
      "batch: 1100/1563 - train loss: 16.1594 - test loss: 16.1368 - train acc: 0.2918 - test acc: 0.2869 - 2m 16s\n",
      "batch: 1200/1563 - train loss: 15.7877 - test loss: 16.6880 - train acc: 0.2978 - test acc: 0.2599 - 2m 22s\n",
      "batch: 1300/1563 - train loss: 16.2040 - test loss: 16.2293 - train acc: 0.2869 - test acc: 0.2750 - 2m 27s\n",
      "batch: 1400/1563 - train loss: 16.2925 - test loss: 16.2071 - train acc: 0.2775 - test acc: 0.2923 - 2m 32s\n",
      "batch: 1500/1563 - train loss: 16.0691 - test loss: 15.6507 - train acc: 0.2831 - test acc: 0.2947 - 2m 37s\n",
      "batch: 1563/1563 - train loss: 15.5710 - test loss: 15.8620 - train acc: 0.3050 - test acc: 0.2976 - 2m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.9344 - test loss: 17.7499 - train acc: 0.3272 - test acc: 0.2521 - 2m 47s\n",
      "batch: 200/1563 - train loss: 15.0277 - test loss: 15.6224 - train acc: 0.3268 - test acc: 0.3110 - 2m 52s\n",
      "batch: 300/1563 - train loss: 14.7790 - test loss: 18.7909 - train acc: 0.3362 - test acc: 0.2383 - 2m 57s\n",
      "batch: 400/1563 - train loss: 14.3803 - test loss: 16.9516 - train acc: 0.3391 - test acc: 0.2698 - 3m 2s\n",
      "batch: 500/1563 - train loss: 14.7793 - test loss: 15.2327 - train acc: 0.3337 - test acc: 0.3217 - 3m 7s\n",
      "batch: 600/1563 - train loss: 14.4472 - test loss: 18.7636 - train acc: 0.3446 - test acc: 0.2383 - 3m 13s\n",
      "batch: 700/1563 - train loss: 14.4045 - test loss: 15.3994 - train acc: 0.3418 - test acc: 0.3153 - 3m 18s\n",
      "batch: 800/1563 - train loss: 14.4337 - test loss: 15.3938 - train acc: 0.3393 - test acc: 0.3195 - 3m 23s\n",
      "batch: 900/1563 - train loss: 14.9138 - test loss: 15.9806 - train acc: 0.3268 - test acc: 0.2907 - 3m 28s\n",
      "batch: 1000/1563 - train loss: 14.6427 - test loss: 16.1173 - train acc: 0.3377 - test acc: 0.2958 - 3m 34s\n",
      "batch: 1100/1563 - train loss: 14.7925 - test loss: 14.9545 - train acc: 0.3290 - test acc: 0.3302 - 3m 39s\n",
      "batch: 1200/1563 - train loss: 14.3353 - test loss: 15.0640 - train acc: 0.3600 - test acc: 0.3241 - 3m 44s\n",
      "batch: 1300/1563 - train loss: 14.0734 - test loss: 16.4134 - train acc: 0.3534 - test acc: 0.2910 - 3m 49s\n",
      "batch: 1400/1563 - train loss: 14.5142 - test loss: 15.6059 - train acc: 0.3381 - test acc: 0.3134 - 3m 54s\n",
      "batch: 1500/1563 - train loss: 14.5147 - test loss: 15.3992 - train acc: 0.3434 - test acc: 0.3183 - 4m 0s\n",
      "batch: 1563/1563 - train loss: 14.3519 - test loss: 15.7017 - train acc: 0.3490 - test acc: 0.3120 - 4m 4s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.7498 - test loss: 14.5930 - train acc: 0.4066 - test acc: 0.3491 - 4m 9s\n",
      "batch: 200/1563 - train loss: 12.9548 - test loss: 14.9955 - train acc: 0.3985 - test acc: 0.3344 - 4m 14s\n",
      "batch: 300/1563 - train loss: 13.3419 - test loss: 14.8059 - train acc: 0.3779 - test acc: 0.3386 - 4m 20s\n",
      "batch: 400/1563 - train loss: 13.4444 - test loss: 14.7811 - train acc: 0.3835 - test acc: 0.3409 - 4m 25s\n",
      "batch: 500/1563 - train loss: 13.3788 - test loss: 14.9853 - train acc: 0.3812 - test acc: 0.3311 - 4m 30s\n",
      "batch: 600/1563 - train loss: 13.1458 - test loss: 15.5757 - train acc: 0.3866 - test acc: 0.3234 - 4m 35s\n",
      "batch: 700/1563 - train loss: 13.2200 - test loss: 15.0568 - train acc: 0.3900 - test acc: 0.3439 - 4m 40s\n",
      "batch: 800/1563 - train loss: 12.9699 - test loss: 14.8621 - train acc: 0.3937 - test acc: 0.3443 - 4m 45s\n",
      "batch: 900/1563 - train loss: 13.1074 - test loss: 15.2471 - train acc: 0.3947 - test acc: 0.3289 - 4m 51s\n",
      "batch: 1000/1563 - train loss: 13.0124 - test loss: 14.8028 - train acc: 0.3941 - test acc: 0.3396 - 4m 56s\n",
      "batch: 1100/1563 - train loss: 13.1323 - test loss: 14.0689 - train acc: 0.3997 - test acc: 0.3685 - 5m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1200/1563 - train loss: 12.8159 - test loss: 14.3271 - train acc: 0.4053 - test acc: 0.3576 - 5m 6s\n",
      "batch: 1300/1563 - train loss: 13.1249 - test loss: 14.9015 - train acc: 0.4031 - test acc: 0.3431 - 5m 11s\n",
      "batch: 1400/1563 - train loss: 13.1138 - test loss: 14.0316 - train acc: 0.3909 - test acc: 0.3696 - 5m 16s\n",
      "batch: 1500/1563 - train loss: 12.8874 - test loss: 15.3673 - train acc: 0.4015 - test acc: 0.3322 - 5m 22s\n",
      "batch: 1563/1563 - train loss: 12.8539 - test loss: 14.0765 - train acc: 0.4085 - test acc: 0.3677 - 5m 26s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.5686 - test loss: 14.4657 - train acc: 0.4519 - test acc: 0.3585 - 5m 31s\n",
      "batch: 200/1563 - train loss: 11.6238 - test loss: 14.4339 - train acc: 0.4416 - test acc: 0.3703 - 5m 37s\n",
      "batch: 300/1563 - train loss: 11.9325 - test loss: 14.7248 - train acc: 0.4335 - test acc: 0.3489 - 5m 42s\n",
      "batch: 400/1563 - train loss: 11.8285 - test loss: 13.8500 - train acc: 0.4397 - test acc: 0.3821 - 5m 47s\n",
      "batch: 500/1563 - train loss: 11.8620 - test loss: 13.7655 - train acc: 0.4338 - test acc: 0.3797 - 5m 52s\n",
      "batch: 600/1563 - train loss: 12.0015 - test loss: 14.7394 - train acc: 0.4285 - test acc: 0.3547 - 5m 58s\n",
      "batch: 700/1563 - train loss: 11.9889 - test loss: 14.6823 - train acc: 0.4256 - test acc: 0.3471 - 6m 3s\n",
      "batch: 800/1563 - train loss: 12.1636 - test loss: 14.1054 - train acc: 0.4213 - test acc: 0.3718 - 6m 8s\n",
      "batch: 900/1563 - train loss: 11.5556 - test loss: 14.5520 - train acc: 0.4491 - test acc: 0.3652 - 6m 13s\n",
      "batch: 1000/1563 - train loss: 12.1970 - test loss: 14.6605 - train acc: 0.4275 - test acc: 0.3583 - 6m 18s\n",
      "batch: 1100/1563 - train loss: 11.7552 - test loss: 14.3829 - train acc: 0.4360 - test acc: 0.3752 - 6m 23s\n",
      "batch: 1200/1563 - train loss: 11.8882 - test loss: 13.5448 - train acc: 0.4269 - test acc: 0.3871 - 6m 29s\n",
      "batch: 1300/1563 - train loss: 11.8919 - test loss: 13.9818 - train acc: 0.4390 - test acc: 0.3742 - 6m 34s\n",
      "batch: 1400/1563 - train loss: 11.8686 - test loss: 14.6354 - train acc: 0.4397 - test acc: 0.3613 - 6m 39s\n",
      "batch: 1500/1563 - train loss: 11.6780 - test loss: 14.5222 - train acc: 0.4466 - test acc: 0.3617 - 6m 44s\n",
      "batch: 1563/1563 - train loss: 12.0253 - test loss: 13.9489 - train acc: 0.4353 - test acc: 0.3825 - 6m 48s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.3021 - test loss: 19.1745 - train acc: 0.4969 - test acc: 0.2853 - 6m 54s\n",
      "batch: 200/1563 - train loss: 10.2536 - test loss: 13.6431 - train acc: 0.4985 - test acc: 0.4019 - 6m 59s\n",
      "batch: 300/1563 - train loss: 10.3369 - test loss: 14.0448 - train acc: 0.5044 - test acc: 0.3768 - 7m 4s\n",
      "batch: 400/1563 - train loss: 10.7109 - test loss: 13.5248 - train acc: 0.4809 - test acc: 0.4022 - 7m 9s\n",
      "batch: 500/1563 - train loss: 10.5342 - test loss: 14.0840 - train acc: 0.4885 - test acc: 0.3789 - 7m 15s\n",
      "batch: 600/1563 - train loss: 10.8879 - test loss: 14.4888 - train acc: 0.4768 - test acc: 0.3660 - 7m 20s\n",
      "batch: 700/1563 - train loss: 11.3914 - test loss: 14.0113 - train acc: 0.4566 - test acc: 0.3790 - 7m 25s\n",
      "batch: 800/1563 - train loss: 10.7651 - test loss: 13.8675 - train acc: 0.4906 - test acc: 0.3883 - 7m 30s\n",
      "batch: 900/1563 - train loss: 10.6454 - test loss: 13.5322 - train acc: 0.4881 - test acc: 0.4000 - 7m 36s\n",
      "batch: 1000/1563 - train loss: 10.6512 - test loss: 13.6070 - train acc: 0.4884 - test acc: 0.4012 - 7m 41s\n",
      "batch: 1100/1563 - train loss: 10.8570 - test loss: 13.1061 - train acc: 0.4791 - test acc: 0.4103 - 7m 46s\n",
      "batch: 1200/1563 - train loss: 10.5658 - test loss: 13.6391 - train acc: 0.4916 - test acc: 0.3985 - 7m 51s\n",
      "batch: 1300/1563 - train loss: 10.8504 - test loss: 15.2508 - train acc: 0.4809 - test acc: 0.3574 - 7m 56s\n",
      "batch: 1400/1563 - train loss: 10.8862 - test loss: 13.6740 - train acc: 0.4753 - test acc: 0.3942 - 8m 2s\n",
      "batch: 1500/1563 - train loss: 11.1403 - test loss: 13.3370 - train acc: 0.4678 - test acc: 0.4090 - 8m 7s\n",
      "batch: 1563/1563 - train loss: 10.8839 - test loss: 13.6147 - train acc: 0.4859 - test acc: 0.3981 - 8m 11s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 9.0160 - test loss: 14.2969 - train acc: 0.5527 - test acc: 0.3862 - 8m 16s\n",
      "batch: 200/1563 - train loss: 8.9627 - test loss: 13.7545 - train acc: 0.5537 - test acc: 0.4032 - 8m 21s\n",
      "batch: 300/1563 - train loss: 9.4013 - test loss: 13.5729 - train acc: 0.5362 - test acc: 0.4112 - 8m 27s\n",
      "batch: 400/1563 - train loss: 9.5917 - test loss: 13.6091 - train acc: 0.5284 - test acc: 0.4013 - 8m 32s\n",
      "batch: 500/1563 - train loss: 9.5226 - test loss: 14.2235 - train acc: 0.5403 - test acc: 0.3908 - 8m 37s\n",
      "batch: 600/1563 - train loss: 9.7546 - test loss: 14.4497 - train acc: 0.5347 - test acc: 0.3797 - 8m 42s\n",
      "batch: 700/1563 - train loss: 9.5484 - test loss: 13.5838 - train acc: 0.5378 - test acc: 0.4017 - 8m 48s\n",
      "batch: 800/1563 - train loss: 9.8343 - test loss: 13.7608 - train acc: 0.5153 - test acc: 0.4075 - 8m 53s\n",
      "batch: 900/1563 - train loss: 9.7804 - test loss: 14.4814 - train acc: 0.5231 - test acc: 0.3824 - 8m 58s\n",
      "batch: 1000/1563 - train loss: 9.8111 - test loss: 13.7389 - train acc: 0.5207 - test acc: 0.4085 - 9m 3s\n",
      "batch: 1100/1563 - train loss: 9.7758 - test loss: 13.9303 - train acc: 0.5122 - test acc: 0.4016 - 9m 9s\n",
      "batch: 1200/1563 - train loss: 9.9932 - test loss: 14.9362 - train acc: 0.5150 - test acc: 0.3663 - 9m 14s\n",
      "batch: 1300/1563 - train loss: 9.8197 - test loss: 14.1856 - train acc: 0.5259 - test acc: 0.3832 - 9m 19s\n",
      "batch: 1400/1563 - train loss: 10.1012 - test loss: 14.2867 - train acc: 0.5053 - test acc: 0.3899 - 9m 24s\n",
      "batch: 1500/1563 - train loss: 10.1266 - test loss: 13.5075 - train acc: 0.5091 - test acc: 0.4039 - 9m 29s\n",
      "batch: 1563/1563 - train loss: 10.2930 - test loss: 13.3415 - train acc: 0.5006 - test acc: 0.4087 - 9m 34s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.9646 - test loss: 13.5899 - train acc: 0.6131 - test acc: 0.4180 - 9m 39s\n",
      "batch: 200/1563 - train loss: 7.8307 - test loss: 14.1843 - train acc: 0.5934 - test acc: 0.4021 - 9m 44s\n",
      "batch: 300/1563 - train loss: 8.1479 - test loss: 14.2097 - train acc: 0.5884 - test acc: 0.4030 - 9m 50s\n",
      "batch: 400/1563 - train loss: 8.3687 - test loss: 13.9975 - train acc: 0.5834 - test acc: 0.4100 - 9m 55s\n",
      "batch: 500/1563 - train loss: 8.4105 - test loss: 13.8435 - train acc: 0.5762 - test acc: 0.4107 - 10m 0s\n",
      "batch: 600/1563 - train loss: 8.4191 - test loss: 14.3547 - train acc: 0.5769 - test acc: 0.3937 - 10m 5s\n",
      "batch: 700/1563 - train loss: 8.5890 - test loss: 13.8876 - train acc: 0.5687 - test acc: 0.4081 - 10m 11s\n",
      "batch: 800/1563 - train loss: 9.0060 - test loss: 14.1741 - train acc: 0.5440 - test acc: 0.3945 - 10m 16s\n",
      "batch: 900/1563 - train loss: 9.1695 - test loss: 14.9859 - train acc: 0.5478 - test acc: 0.3801 - 10m 21s\n",
      "batch: 1000/1563 - train loss: 8.7114 - test loss: 13.5391 - train acc: 0.5636 - test acc: 0.4234 - 10m 26s\n",
      "batch: 1100/1563 - train loss: 9.1288 - test loss: 13.6116 - train acc: 0.5494 - test acc: 0.4060 - 10m 31s\n",
      "batch: 1200/1563 - train loss: 9.0382 - test loss: 13.8288 - train acc: 0.5537 - test acc: 0.4076 - 10m 36s\n",
      "batch: 1300/1563 - train loss: 9.0945 - test loss: 13.4935 - train acc: 0.5578 - test acc: 0.4112 - 10m 42s\n",
      "batch: 1400/1563 - train loss: 9.2307 - test loss: 13.9748 - train acc: 0.5459 - test acc: 0.4024 - 10m 47s\n",
      "batch: 1500/1563 - train loss: 9.1822 - test loss: 13.6295 - train acc: 0.5409 - test acc: 0.4171 - 10m 52s\n",
      "batch: 1563/1563 - train loss: 9.1388 - test loss: 13.4786 - train acc: 0.5419 - test acc: 0.4239 - 10m 57s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.9953 - test loss: 13.5154 - train acc: 0.6500 - test acc: 0.4246 - 11m 2s\n",
      "batch: 200/1563 - train loss: 6.7495 - test loss: 13.8170 - train acc: 0.6462 - test acc: 0.4258 - 11m 7s\n",
      "batch: 300/1563 - train loss: 6.9368 - test loss: 14.5034 - train acc: 0.6466 - test acc: 0.4070 - 11m 12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 400/1563 - train loss: 7.4765 - test loss: 14.4263 - train acc: 0.6193 - test acc: 0.4071 - 11m 18s\n",
      "batch: 500/1563 - train loss: 7.6146 - test loss: 15.6537 - train acc: 0.6106 - test acc: 0.3793 - 11m 23s\n",
      "batch: 600/1563 - train loss: 7.6228 - test loss: 14.8979 - train acc: 0.6141 - test acc: 0.3871 - 11m 28s\n",
      "batch: 700/1563 - train loss: 7.6032 - test loss: 14.4621 - train acc: 0.6087 - test acc: 0.4059 - 11m 33s\n",
      "batch: 800/1563 - train loss: 7.7442 - test loss: 13.9600 - train acc: 0.6054 - test acc: 0.4188 - 11m 38s\n",
      "batch: 900/1563 - train loss: 8.1646 - test loss: 14.2549 - train acc: 0.5834 - test acc: 0.4128 - 11m 44s\n",
      "batch: 1000/1563 - train loss: 7.9430 - test loss: 14.2221 - train acc: 0.5991 - test acc: 0.4169 - 11m 49s\n",
      "batch: 1100/1563 - train loss: 7.8539 - test loss: 13.6851 - train acc: 0.6022 - test acc: 0.4365 - 11m 54s\n",
      "batch: 1200/1563 - train loss: 8.0178 - test loss: 14.1875 - train acc: 0.5941 - test acc: 0.4194 - 11m 59s\n",
      "batch: 1300/1563 - train loss: 8.2917 - test loss: 14.1455 - train acc: 0.5878 - test acc: 0.4173 - 12m 5s\n",
      "batch: 1400/1563 - train loss: 8.3087 - test loss: 14.8841 - train acc: 0.5828 - test acc: 0.3993 - 12m 10s\n",
      "batch: 1500/1563 - train loss: 8.0840 - test loss: 13.8016 - train acc: 0.5825 - test acc: 0.4154 - 12m 15s\n",
      "batch: 1563/1563 - train loss: 8.2859 - test loss: 13.9012 - train acc: 0.5875 - test acc: 0.4195 - 12m 19s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 6.0079 - test loss: 14.5243 - train acc: 0.6922 - test acc: 0.4190 - 12m 25s\n",
      "batch: 200/1563 - train loss: 5.9575 - test loss: 14.1790 - train acc: 0.6932 - test acc: 0.4263 - 12m 30s\n",
      "batch: 300/1563 - train loss: 6.2607 - test loss: 14.6359 - train acc: 0.6681 - test acc: 0.4124 - 12m 35s\n",
      "batch: 400/1563 - train loss: 6.3603 - test loss: 14.4715 - train acc: 0.6766 - test acc: 0.4211 - 12m 40s\n",
      "batch: 500/1563 - train loss: 6.4080 - test loss: 15.5843 - train acc: 0.6669 - test acc: 0.3942 - 12m 45s\n",
      "batch: 600/1563 - train loss: 6.7306 - test loss: 15.1816 - train acc: 0.6584 - test acc: 0.4011 - 12m 51s\n",
      "batch: 700/1563 - train loss: 6.7175 - test loss: 14.8510 - train acc: 0.6547 - test acc: 0.4117 - 12m 56s\n",
      "batch: 800/1563 - train loss: 6.6981 - test loss: 14.2324 - train acc: 0.6500 - test acc: 0.4178 - 13m 1s\n",
      "batch: 900/1563 - train loss: 6.8341 - test loss: 14.4638 - train acc: 0.6363 - test acc: 0.4195 - 13m 7s\n",
      "batch: 1000/1563 - train loss: 7.0741 - test loss: 15.3374 - train acc: 0.6281 - test acc: 0.3970 - 13m 12s\n",
      "batch: 1100/1563 - train loss: 7.0889 - test loss: 14.5255 - train acc: 0.6294 - test acc: 0.4124 - 13m 17s\n",
      "batch: 1200/1563 - train loss: 7.2967 - test loss: 14.2484 - train acc: 0.6368 - test acc: 0.4222 - 13m 22s\n",
      "batch: 1300/1563 - train loss: 7.2887 - test loss: 14.6434 - train acc: 0.6140 - test acc: 0.4168 - 13m 28s\n",
      "batch: 1400/1563 - train loss: 7.6509 - test loss: 14.3277 - train acc: 0.6125 - test acc: 0.4220 - 13m 33s\n",
      "batch: 1500/1563 - train loss: 7.5994 - test loss: 15.5092 - train acc: 0.6166 - test acc: 0.3961 - 13m 38s\n",
      "batch: 1563/1563 - train loss: 7.5025 - test loss: 14.1277 - train acc: 0.6078 - test acc: 0.4293 - 13m 42s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 5.0522 - test loss: 14.0367 - train acc: 0.7350 - test acc: 0.4331 - 13m 47s\n",
      "batch: 200/1563 - train loss: 5.1248 - test loss: 14.5687 - train acc: 0.7329 - test acc: 0.4341 - 13m 53s\n",
      "batch: 300/1563 - train loss: 5.0861 - test loss: 14.6856 - train acc: 0.7291 - test acc: 0.4234 - 13m 58s\n",
      "batch: 400/1563 - train loss: 5.2082 - test loss: 15.4151 - train acc: 0.7225 - test acc: 0.4097 - 14m 4s\n",
      "batch: 500/1563 - train loss: 5.6484 - test loss: 14.8242 - train acc: 0.7025 - test acc: 0.4246 - 14m 9s\n",
      "batch: 600/1563 - train loss: 5.6332 - test loss: 14.7913 - train acc: 0.6891 - test acc: 0.4286 - 14m 14s\n",
      "batch: 700/1563 - train loss: 5.8715 - test loss: 14.9863 - train acc: 0.6819 - test acc: 0.4187 - 14m 19s\n",
      "batch: 800/1563 - train loss: 5.8188 - test loss: 15.7821 - train acc: 0.6894 - test acc: 0.4089 - 14m 25s\n",
      "batch: 900/1563 - train loss: 6.2112 - test loss: 15.5752 - train acc: 0.6754 - test acc: 0.4057 - 14m 30s\n",
      "batch: 1000/1563 - train loss: 6.2059 - test loss: 14.8263 - train acc: 0.6829 - test acc: 0.4211 - 14m 35s\n",
      "batch: 1100/1563 - train loss: 6.5109 - test loss: 15.0080 - train acc: 0.6619 - test acc: 0.4175 - 14m 40s\n",
      "batch: 1200/1563 - train loss: 6.6764 - test loss: 15.1252 - train acc: 0.6534 - test acc: 0.4145 - 14m 45s\n",
      "batch: 1300/1563 - train loss: 6.5911 - test loss: 15.0922 - train acc: 0.6541 - test acc: 0.4119 - 14m 50s\n",
      "batch: 1400/1563 - train loss: 6.4901 - test loss: 14.5961 - train acc: 0.6603 - test acc: 0.4273 - 14m 56s\n",
      "batch: 1500/1563 - train loss: 6.7017 - test loss: 14.4932 - train acc: 0.6553 - test acc: 0.4268 - 15m 1s\n",
      "batch: 1563/1563 - train loss: 6.6525 - test loss: 15.6424 - train acc: 0.6597 - test acc: 0.3991 - 15m 5s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.2616 - test loss: 15.0143 - train acc: 0.7702 - test acc: 0.4332 - 15m 11s\n",
      "batch: 200/1563 - train loss: 4.0210 - test loss: 15.4103 - train acc: 0.7856 - test acc: 0.4245 - 15m 16s\n",
      "batch: 300/1563 - train loss: 4.7540 - test loss: 15.6587 - train acc: 0.7540 - test acc: 0.4188 - 15m 21s\n",
      "batch: 400/1563 - train loss: 4.5877 - test loss: 15.4873 - train acc: 0.7406 - test acc: 0.4310 - 15m 26s\n",
      "batch: 500/1563 - train loss: 4.6598 - test loss: 15.7576 - train acc: 0.7471 - test acc: 0.4246 - 15m 32s\n",
      "batch: 600/1563 - train loss: 5.1391 - test loss: 15.5366 - train acc: 0.7201 - test acc: 0.4191 - 15m 37s\n",
      "batch: 700/1563 - train loss: 5.0332 - test loss: 15.3959 - train acc: 0.7257 - test acc: 0.4175 - 15m 42s\n",
      "batch: 800/1563 - train loss: 5.2665 - test loss: 15.5655 - train acc: 0.7125 - test acc: 0.4184 - 15m 47s\n",
      "batch: 900/1563 - train loss: 5.2302 - test loss: 15.5422 - train acc: 0.7188 - test acc: 0.4292 - 15m 52s\n",
      "batch: 1000/1563 - train loss: 5.3621 - test loss: 15.8581 - train acc: 0.7172 - test acc: 0.4138 - 15m 58s\n",
      "batch: 1100/1563 - train loss: 5.4835 - test loss: 15.3009 - train acc: 0.7072 - test acc: 0.4201 - 16m 3s\n",
      "batch: 1200/1563 - train loss: 5.3246 - test loss: 15.8877 - train acc: 0.7132 - test acc: 0.4165 - 16m 8s\n",
      "batch: 1300/1563 - train loss: 5.5308 - test loss: 15.8384 - train acc: 0.7147 - test acc: 0.4198 - 16m 14s\n",
      "batch: 1400/1563 - train loss: 5.8455 - test loss: 15.7198 - train acc: 0.6760 - test acc: 0.4143 - 16m 19s\n",
      "batch: 1500/1563 - train loss: 5.7962 - test loss: 16.1310 - train acc: 0.6947 - test acc: 0.4065 - 16m 24s\n",
      "batch: 1563/1563 - train loss: 5.7757 - test loss: 15.6964 - train acc: 0.6984 - test acc: 0.4281 - 16m 29s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.4873 - test loss: 15.5641 - train acc: 0.8021 - test acc: 0.4314 - 16m 34s\n",
      "batch: 200/1563 - train loss: 3.5312 - test loss: 15.4728 - train acc: 0.8103 - test acc: 0.4356 - 16m 39s\n",
      "batch: 300/1563 - train loss: 3.6808 - test loss: 15.5736 - train acc: 0.7984 - test acc: 0.4333 - 16m 44s\n",
      "batch: 400/1563 - train loss: 4.0171 - test loss: 16.1696 - train acc: 0.7825 - test acc: 0.4251 - 16m 49s\n",
      "batch: 500/1563 - train loss: 4.0781 - test loss: 15.9566 - train acc: 0.7746 - test acc: 0.4340 - 16m 55s\n",
      "batch: 600/1563 - train loss: 4.3488 - test loss: 15.5431 - train acc: 0.7634 - test acc: 0.4375 - 17m 0s\n",
      "batch: 700/1563 - train loss: 4.4217 - test loss: 15.8554 - train acc: 0.7660 - test acc: 0.4265 - 17m 5s\n",
      "batch: 800/1563 - train loss: 4.5581 - test loss: 16.9237 - train acc: 0.7519 - test acc: 0.4098 - 17m 11s\n",
      "batch: 900/1563 - train loss: 4.5879 - test loss: 16.2083 - train acc: 0.7515 - test acc: 0.4286 - 17m 16s\n",
      "batch: 1000/1563 - train loss: 4.5220 - test loss: 16.7424 - train acc: 0.7506 - test acc: 0.4110 - 17m 21s\n",
      "batch: 1100/1563 - train loss: 4.8092 - test loss: 15.6936 - train acc: 0.7366 - test acc: 0.4289 - 17m 26s\n",
      "batch: 1200/1563 - train loss: 4.9271 - test loss: 16.2985 - train acc: 0.7391 - test acc: 0.4171 - 17m 32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1300/1563 - train loss: 4.7614 - test loss: 15.6227 - train acc: 0.7428 - test acc: 0.4354 - 17m 37s\n",
      "batch: 1400/1563 - train loss: 4.9992 - test loss: 16.2024 - train acc: 0.7328 - test acc: 0.4238 - 17m 42s\n",
      "batch: 1500/1563 - train loss: 5.5672 - test loss: 15.5072 - train acc: 0.7022 - test acc: 0.4236 - 17m 47s\n",
      "batch: 1563/1563 - train loss: 5.3881 - test loss: 15.8968 - train acc: 0.7066 - test acc: 0.4230 - 17m 52s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 3.1511 - test loss: 15.6233 - train acc: 0.8269 - test acc: 0.4356 - 17m 57s\n",
      "batch: 200/1563 - train loss: 3.1632 - test loss: 15.9125 - train acc: 0.8274 - test acc: 0.4404 - 18m 2s\n",
      "batch: 300/1563 - train loss: 3.1765 - test loss: 15.9972 - train acc: 0.8178 - test acc: 0.4422 - 18m 8s\n",
      "batch: 400/1563 - train loss: 3.3163 - test loss: 16.6209 - train acc: 0.8143 - test acc: 0.4276 - 18m 13s\n",
      "batch: 500/1563 - train loss: 3.4742 - test loss: 16.3126 - train acc: 0.8093 - test acc: 0.4326 - 18m 18s\n",
      "batch: 600/1563 - train loss: 3.3280 - test loss: 16.8777 - train acc: 0.8153 - test acc: 0.4206 - 18m 23s\n",
      "batch: 700/1563 - train loss: 3.5076 - test loss: 16.5410 - train acc: 0.7997 - test acc: 0.4269 - 18m 28s\n",
      "batch: 800/1563 - train loss: 3.7193 - test loss: 17.5196 - train acc: 0.7900 - test acc: 0.4124 - 18m 34s\n",
      "batch: 900/1563 - train loss: 4.0086 - test loss: 16.6446 - train acc: 0.7803 - test acc: 0.4293 - 18m 40s\n",
      "batch: 1000/1563 - train loss: 3.7613 - test loss: 16.9486 - train acc: 0.7969 - test acc: 0.4258 - 18m 45s\n",
      "batch: 1100/1563 - train loss: 3.8561 - test loss: 17.3105 - train acc: 0.7806 - test acc: 0.4225 - 18m 50s\n",
      "batch: 1200/1563 - train loss: 4.2991 - test loss: 16.6366 - train acc: 0.7549 - test acc: 0.4289 - 18m 55s\n",
      "batch: 1300/1563 - train loss: 4.2987 - test loss: 16.4957 - train acc: 0.7612 - test acc: 0.4297 - 19m 0s\n",
      "batch: 1400/1563 - train loss: 4.5820 - test loss: 16.5650 - train acc: 0.7512 - test acc: 0.4248 - 19m 5s\n",
      "batch: 1500/1563 - train loss: 4.4150 - test loss: 16.9776 - train acc: 0.7578 - test acc: 0.4228 - 19m 11s\n",
      "batch: 1563/1563 - train loss: 4.5874 - test loss: 16.3356 - train acc: 0.7523 - test acc: 0.4289 - 19m 15s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.5940 - test loss: 16.1738 - train acc: 0.8578 - test acc: 0.4347 - 19m 21s\n",
      "batch: 200/1563 - train loss: 2.6358 - test loss: 16.4382 - train acc: 0.8513 - test acc: 0.4390 - 19m 26s\n",
      "batch: 300/1563 - train loss: 2.6853 - test loss: 16.8795 - train acc: 0.8491 - test acc: 0.4357 - 19m 31s\n",
      "batch: 400/1563 - train loss: 2.7778 - test loss: 16.9408 - train acc: 0.8431 - test acc: 0.4300 - 19m 36s\n",
      "batch: 500/1563 - train loss: 2.8850 - test loss: 17.0259 - train acc: 0.8346 - test acc: 0.4385 - 19m 42s\n",
      "batch: 600/1563 - train loss: 3.0606 - test loss: 17.0986 - train acc: 0.8237 - test acc: 0.4344 - 19m 47s\n",
      "batch: 700/1563 - train loss: 2.9268 - test loss: 17.4043 - train acc: 0.8381 - test acc: 0.4300 - 19m 52s\n",
      "batch: 800/1563 - train loss: 2.9926 - test loss: 16.9604 - train acc: 0.8300 - test acc: 0.4291 - 19m 57s\n",
      "time is up! finishing training\n",
      "batch: 825/1563 - train loss: 3.0987 - test loss: 16.9166 - train acc: 0.8249 - test acc: 0.4306 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 3\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0262 - test loss: 24.8430 - train acc: 0.0370 - test acc: 0.0567 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.9040 - test loss: 24.7459 - train acc: 0.0698 - test acc: 0.0697 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.7857 - test loss: 22.4476 - train acc: 0.0911 - test acc: 0.1093 - 0m 12s\n",
      "batch: 400/1563 - train loss: 22.0827 - test loss: 21.6428 - train acc: 0.0999 - test acc: 0.1142 - 0m 17s\n",
      "batch: 500/1563 - train loss: 21.5513 - test loss: 21.3691 - train acc: 0.1260 - test acc: 0.1228 - 0m 23s\n",
      "batch: 600/1563 - train loss: 21.1502 - test loss: 20.7436 - train acc: 0.1316 - test acc: 0.1435 - 0m 28s\n",
      "batch: 700/1563 - train loss: 20.9134 - test loss: 20.8594 - train acc: 0.1363 - test acc: 0.1360 - 0m 33s\n",
      "batch: 800/1563 - train loss: 20.6115 - test loss: 19.9233 - train acc: 0.1394 - test acc: 0.1633 - 0m 38s\n",
      "batch: 900/1563 - train loss: 19.5410 - test loss: 19.4921 - train acc: 0.1772 - test acc: 0.1799 - 0m 43s\n",
      "batch: 1000/1563 - train loss: 19.7587 - test loss: 19.8189 - train acc: 0.1694 - test acc: 0.1763 - 0m 49s\n",
      "batch: 1100/1563 - train loss: 19.5418 - test loss: 18.9568 - train acc: 0.1735 - test acc: 0.1917 - 0m 53s\n",
      "batch: 1200/1563 - train loss: 19.2677 - test loss: 19.0880 - train acc: 0.1863 - test acc: 0.1960 - 0m 59s\n",
      "batch: 1300/1563 - train loss: 18.6999 - test loss: 18.7379 - train acc: 0.2042 - test acc: 0.2038 - 1m 4s\n",
      "batch: 1400/1563 - train loss: 18.9056 - test loss: 19.7924 - train acc: 0.1935 - test acc: 0.1739 - 1m 9s\n",
      "batch: 1500/1563 - train loss: 18.6980 - test loss: 19.6688 - train acc: 0.1954 - test acc: 0.1788 - 1m 14s\n",
      "batch: 1563/1563 - train loss: 18.5269 - test loss: 18.2662 - train acc: 0.1932 - test acc: 0.2208 - 1m 19s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.3995 - test loss: 17.8810 - train acc: 0.2422 - test acc: 0.2373 - 1m 24s\n",
      "batch: 200/1563 - train loss: 17.3796 - test loss: 18.1328 - train acc: 0.2482 - test acc: 0.2219 - 1m 30s\n",
      "batch: 300/1563 - train loss: 17.3038 - test loss: 17.6425 - train acc: 0.2454 - test acc: 0.2333 - 1m 35s\n",
      "batch: 400/1563 - train loss: 17.1493 - test loss: 18.3031 - train acc: 0.2422 - test acc: 0.2195 - 1m 40s\n",
      "batch: 500/1563 - train loss: 17.3842 - test loss: 16.8771 - train acc: 0.2356 - test acc: 0.2660 - 1m 45s\n",
      "batch: 600/1563 - train loss: 16.5708 - test loss: 17.4635 - train acc: 0.2603 - test acc: 0.2400 - 1m 50s\n",
      "batch: 700/1563 - train loss: 16.6390 - test loss: 17.2315 - train acc: 0.2634 - test acc: 0.2472 - 1m 55s\n",
      "batch: 800/1563 - train loss: 16.6468 - test loss: 16.8660 - train acc: 0.2700 - test acc: 0.2605 - 2m 1s\n",
      "batch: 900/1563 - train loss: 16.6531 - test loss: 16.5448 - train acc: 0.2597 - test acc: 0.2711 - 2m 6s\n",
      "batch: 1000/1563 - train loss: 16.3848 - test loss: 16.2944 - train acc: 0.2762 - test acc: 0.2840 - 2m 11s\n",
      "batch: 1100/1563 - train loss: 16.5097 - test loss: 17.3433 - train acc: 0.2697 - test acc: 0.2593 - 2m 16s\n",
      "batch: 1200/1563 - train loss: 16.5966 - test loss: 16.7367 - train acc: 0.2659 - test acc: 0.2610 - 2m 22s\n",
      "batch: 1300/1563 - train loss: 15.9967 - test loss: 16.2890 - train acc: 0.2894 - test acc: 0.2773 - 2m 27s\n",
      "batch: 1400/1563 - train loss: 16.1324 - test loss: 16.0138 - train acc: 0.2853 - test acc: 0.2828 - 2m 33s\n",
      "batch: 1500/1563 - train loss: 15.9791 - test loss: 15.8976 - train acc: 0.2947 - test acc: 0.2876 - 2m 38s\n",
      "batch: 1563/1563 - train loss: 15.7084 - test loss: 17.0281 - train acc: 0.3028 - test acc: 0.2592 - 2m 42s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.5351 - test loss: 16.8402 - train acc: 0.3350 - test acc: 0.2741 - 2m 47s\n",
      "batch: 200/1563 - train loss: 14.6604 - test loss: 16.6355 - train acc: 0.3312 - test acc: 0.2804 - 2m 52s\n",
      "batch: 300/1563 - train loss: 14.7936 - test loss: 15.8938 - train acc: 0.3299 - test acc: 0.2897 - 2m 58s\n",
      "batch: 400/1563 - train loss: 14.7304 - test loss: 15.6333 - train acc: 0.3324 - test acc: 0.3132 - 3m 3s\n",
      "batch: 500/1563 - train loss: 14.4649 - test loss: 16.1630 - train acc: 0.3343 - test acc: 0.2892 - 3m 8s\n",
      "batch: 600/1563 - train loss: 14.8305 - test loss: 15.2450 - train acc: 0.3293 - test acc: 0.3254 - 3m 13s\n",
      "batch: 700/1563 - train loss: 14.4815 - test loss: 15.1208 - train acc: 0.3394 - test acc: 0.3278 - 3m 18s\n",
      "batch: 800/1563 - train loss: 14.7233 - test loss: 15.9603 - train acc: 0.3341 - test acc: 0.3098 - 3m 23s\n",
      "batch: 900/1563 - train loss: 14.5486 - test loss: 15.1807 - train acc: 0.3487 - test acc: 0.3200 - 3m 29s\n",
      "batch: 1000/1563 - train loss: 14.4806 - test loss: 15.7539 - train acc: 0.3377 - test acc: 0.3039 - 3m 34s\n",
      "batch: 1100/1563 - train loss: 14.6676 - test loss: 15.6620 - train acc: 0.3359 - test acc: 0.3135 - 3m 39s\n",
      "batch: 1200/1563 - train loss: 14.5669 - test loss: 15.7480 - train acc: 0.3428 - test acc: 0.3032 - 3m 44s\n",
      "batch: 1300/1563 - train loss: 14.1530 - test loss: 15.7133 - train acc: 0.3465 - test acc: 0.3044 - 3m 49s\n",
      "batch: 1400/1563 - train loss: 14.4118 - test loss: 16.1074 - train acc: 0.3372 - test acc: 0.2982 - 3m 55s\n",
      "batch: 1500/1563 - train loss: 14.1364 - test loss: 14.8762 - train acc: 0.3569 - test acc: 0.3335 - 4m 0s\n",
      "batch: 1563/1563 - train loss: 14.2978 - test loss: 14.9718 - train acc: 0.3466 - test acc: 0.3312 - 4m 4s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.6842 - test loss: 15.6636 - train acc: 0.4038 - test acc: 0.3224 - 4m 10s\n",
      "batch: 200/1563 - train loss: 12.9069 - test loss: 14.9415 - train acc: 0.4019 - test acc: 0.3397 - 4m 15s\n",
      "batch: 300/1563 - train loss: 13.0356 - test loss: 14.7446 - train acc: 0.3869 - test acc: 0.3465 - 4m 20s\n",
      "batch: 400/1563 - train loss: 12.8463 - test loss: 14.5481 - train acc: 0.4069 - test acc: 0.3483 - 4m 25s\n",
      "batch: 500/1563 - train loss: 13.1393 - test loss: 14.4977 - train acc: 0.3925 - test acc: 0.3494 - 4m 30s\n",
      "batch: 600/1563 - train loss: 13.1182 - test loss: 13.9926 - train acc: 0.3878 - test acc: 0.3742 - 4m 35s\n",
      "batch: 700/1563 - train loss: 12.9253 - test loss: 15.1234 - train acc: 0.3885 - test acc: 0.3303 - 4m 41s\n",
      "batch: 800/1563 - train loss: 13.1911 - test loss: 15.2815 - train acc: 0.3947 - test acc: 0.3237 - 4m 46s\n",
      "batch: 900/1563 - train loss: 12.8596 - test loss: 14.4611 - train acc: 0.4034 - test acc: 0.3493 - 4m 51s\n",
      "batch: 1000/1563 - train loss: 12.8348 - test loss: 15.0091 - train acc: 0.3962 - test acc: 0.3342 - 4m 56s\n",
      "batch: 1100/1563 - train loss: 13.0090 - test loss: 14.8348 - train acc: 0.3966 - test acc: 0.3470 - 5m 1s\n",
      "batch: 1200/1563 - train loss: 13.1191 - test loss: 14.6430 - train acc: 0.3797 - test acc: 0.3479 - 5m 6s\n",
      "batch: 1300/1563 - train loss: 12.7603 - test loss: 14.0840 - train acc: 0.4094 - test acc: 0.3699 - 5m 12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 12.9198 - test loss: 14.8790 - train acc: 0.3978 - test acc: 0.3379 - 5m 17s\n",
      "batch: 1500/1563 - train loss: 13.1352 - test loss: 13.8284 - train acc: 0.3916 - test acc: 0.3778 - 5m 22s\n",
      "batch: 1563/1563 - train loss: 13.1290 - test loss: 13.9783 - train acc: 0.3947 - test acc: 0.3762 - 5m 26s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.2693 - test loss: 14.3162 - train acc: 0.4578 - test acc: 0.3651 - 5m 32s\n",
      "batch: 200/1563 - train loss: 11.4261 - test loss: 15.7709 - train acc: 0.4472 - test acc: 0.3290 - 5m 37s\n",
      "batch: 300/1563 - train loss: 11.3631 - test loss: 14.2277 - train acc: 0.4557 - test acc: 0.3694 - 5m 42s\n",
      "batch: 400/1563 - train loss: 11.5982 - test loss: 13.7966 - train acc: 0.4478 - test acc: 0.3858 - 5m 47s\n",
      "batch: 500/1563 - train loss: 11.5016 - test loss: 13.8420 - train acc: 0.4462 - test acc: 0.3856 - 5m 52s\n",
      "batch: 600/1563 - train loss: 11.7196 - test loss: 14.0354 - train acc: 0.4456 - test acc: 0.3791 - 5m 57s\n",
      "batch: 700/1563 - train loss: 11.7662 - test loss: 13.9724 - train acc: 0.4416 - test acc: 0.3797 - 6m 2s\n",
      "batch: 800/1563 - train loss: 11.6377 - test loss: 13.8994 - train acc: 0.4466 - test acc: 0.3802 - 6m 8s\n",
      "batch: 900/1563 - train loss: 11.6889 - test loss: 14.4786 - train acc: 0.4410 - test acc: 0.3658 - 6m 13s\n",
      "batch: 1000/1563 - train loss: 11.8541 - test loss: 17.6215 - train acc: 0.4441 - test acc: 0.3039 - 6m 19s\n",
      "batch: 1100/1563 - train loss: 11.8921 - test loss: 13.9148 - train acc: 0.4347 - test acc: 0.3741 - 6m 24s\n",
      "batch: 1200/1563 - train loss: 11.8653 - test loss: 13.3633 - train acc: 0.4352 - test acc: 0.3972 - 6m 29s\n",
      "batch: 1300/1563 - train loss: 11.9349 - test loss: 14.5745 - train acc: 0.4360 - test acc: 0.3660 - 6m 34s\n",
      "batch: 1400/1563 - train loss: 12.0366 - test loss: 14.4621 - train acc: 0.4345 - test acc: 0.3635 - 6m 39s\n",
      "batch: 1500/1563 - train loss: 11.9292 - test loss: 14.7730 - train acc: 0.4472 - test acc: 0.3503 - 6m 44s\n",
      "batch: 1563/1563 - train loss: 12.1041 - test loss: 13.7531 - train acc: 0.4456 - test acc: 0.3764 - 6m 49s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.6799 - test loss: 14.1562 - train acc: 0.5231 - test acc: 0.3784 - 6m 54s\n",
      "batch: 200/1563 - train loss: 10.0488 - test loss: 13.9978 - train acc: 0.5166 - test acc: 0.3797 - 6m 59s\n",
      "batch: 300/1563 - train loss: 10.2437 - test loss: 14.0672 - train acc: 0.4988 - test acc: 0.3796 - 7m 4s\n",
      "batch: 400/1563 - train loss: 10.5309 - test loss: 14.1268 - train acc: 0.4897 - test acc: 0.3853 - 7m 10s\n",
      "batch: 500/1563 - train loss: 10.6202 - test loss: 15.8176 - train acc: 0.4784 - test acc: 0.3282 - 7m 15s\n",
      "batch: 600/1563 - train loss: 10.7135 - test loss: 14.5303 - train acc: 0.4750 - test acc: 0.3731 - 7m 20s\n",
      "batch: 700/1563 - train loss: 10.6671 - test loss: 15.2682 - train acc: 0.4903 - test acc: 0.3561 - 7m 26s\n",
      "batch: 800/1563 - train loss: 10.6205 - test loss: 13.5959 - train acc: 0.4843 - test acc: 0.3983 - 7m 31s\n",
      "batch: 900/1563 - train loss: 10.5953 - test loss: 14.0192 - train acc: 0.4941 - test acc: 0.3800 - 7m 36s\n",
      "batch: 1000/1563 - train loss: 10.6321 - test loss: 14.5970 - train acc: 0.4822 - test acc: 0.3683 - 7m 41s\n",
      "batch: 1100/1563 - train loss: 10.9329 - test loss: 13.3600 - train acc: 0.4804 - test acc: 0.3990 - 7m 46s\n",
      "batch: 1200/1563 - train loss: 10.8079 - test loss: 13.8018 - train acc: 0.4868 - test acc: 0.3875 - 7m 52s\n",
      "batch: 1300/1563 - train loss: 10.6961 - test loss: 13.5100 - train acc: 0.4809 - test acc: 0.3970 - 7m 57s\n",
      "batch: 1400/1563 - train loss: 10.8885 - test loss: 14.1540 - train acc: 0.4750 - test acc: 0.3817 - 8m 2s\n",
      "batch: 1500/1563 - train loss: 10.5512 - test loss: 13.5273 - train acc: 0.4928 - test acc: 0.3990 - 8m 7s\n",
      "batch: 1563/1563 - train loss: 10.8233 - test loss: 13.5395 - train acc: 0.4841 - test acc: 0.4016 - 8m 11s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.6900 - test loss: 13.6136 - train acc: 0.5618 - test acc: 0.4072 - 8m 17s\n",
      "batch: 200/1563 - train loss: 8.8437 - test loss: 13.6602 - train acc: 0.5590 - test acc: 0.4006 - 8m 22s\n",
      "batch: 300/1563 - train loss: 9.0106 - test loss: 14.1897 - train acc: 0.5434 - test acc: 0.3942 - 8m 27s\n",
      "batch: 400/1563 - train loss: 9.1879 - test loss: 13.4625 - train acc: 0.5440 - test acc: 0.4129 - 8m 32s\n",
      "batch: 500/1563 - train loss: 9.5796 - test loss: 13.4459 - train acc: 0.5328 - test acc: 0.4040 - 8m 37s\n",
      "batch: 600/1563 - train loss: 9.7387 - test loss: 13.9486 - train acc: 0.5210 - test acc: 0.3916 - 8m 43s\n",
      "batch: 700/1563 - train loss: 9.4647 - test loss: 13.8482 - train acc: 0.5234 - test acc: 0.3896 - 8m 48s\n",
      "batch: 800/1563 - train loss: 9.6237 - test loss: 14.1231 - train acc: 0.5203 - test acc: 0.3933 - 8m 53s\n",
      "batch: 900/1563 - train loss: 9.7837 - test loss: 13.5118 - train acc: 0.5244 - test acc: 0.4121 - 8m 58s\n",
      "batch: 1000/1563 - train loss: 9.9002 - test loss: 13.2383 - train acc: 0.5193 - test acc: 0.4193 - 9m 4s\n",
      "batch: 1100/1563 - train loss: 9.5216 - test loss: 13.6664 - train acc: 0.5262 - test acc: 0.3950 - 9m 9s\n",
      "batch: 1200/1563 - train loss: 9.5725 - test loss: 13.5593 - train acc: 0.5234 - test acc: 0.4116 - 9m 14s\n",
      "batch: 1300/1563 - train loss: 9.8085 - test loss: 13.3654 - train acc: 0.5159 - test acc: 0.4142 - 9m 19s\n",
      "batch: 1400/1563 - train loss: 9.9870 - test loss: 13.6817 - train acc: 0.5257 - test acc: 0.3930 - 9m 24s\n",
      "batch: 1500/1563 - train loss: 9.6801 - test loss: 13.5567 - train acc: 0.5271 - test acc: 0.4076 - 9m 29s\n",
      "batch: 1563/1563 - train loss: 9.9036 - test loss: 15.0472 - train acc: 0.5203 - test acc: 0.3708 - 9m 34s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.9039 - test loss: 13.5438 - train acc: 0.5981 - test acc: 0.4144 - 9m 39s\n",
      "batch: 200/1563 - train loss: 7.6344 - test loss: 13.8826 - train acc: 0.6121 - test acc: 0.4116 - 9m 44s\n",
      "batch: 300/1563 - train loss: 7.8039 - test loss: 14.1808 - train acc: 0.5984 - test acc: 0.3968 - 9m 49s\n",
      "batch: 400/1563 - train loss: 8.1768 - test loss: 14.7131 - train acc: 0.5881 - test acc: 0.3942 - 9m 54s\n",
      "batch: 500/1563 - train loss: 8.3509 - test loss: 13.6893 - train acc: 0.5794 - test acc: 0.4144 - 10m 0s\n",
      "batch: 600/1563 - train loss: 8.3445 - test loss: 14.2280 - train acc: 0.5722 - test acc: 0.3968 - 10m 5s\n",
      "batch: 700/1563 - train loss: 8.6329 - test loss: 13.9062 - train acc: 0.5690 - test acc: 0.4121 - 10m 10s\n",
      "batch: 800/1563 - train loss: 8.5856 - test loss: 14.1274 - train acc: 0.5646 - test acc: 0.4148 - 10m 15s\n",
      "batch: 900/1563 - train loss: 8.4737 - test loss: 13.9053 - train acc: 0.5797 - test acc: 0.4128 - 10m 20s\n",
      "batch: 1000/1563 - train loss: 8.7733 - test loss: 13.6694 - train acc: 0.5625 - test acc: 0.4149 - 10m 25s\n",
      "batch: 1100/1563 - train loss: 8.8768 - test loss: 13.3813 - train acc: 0.5550 - test acc: 0.4181 - 10m 31s\n",
      "batch: 1200/1563 - train loss: 9.0032 - test loss: 14.2557 - train acc: 0.5581 - test acc: 0.3985 - 10m 36s\n",
      "batch: 1300/1563 - train loss: 8.6878 - test loss: 13.4466 - train acc: 0.5644 - test acc: 0.4180 - 10m 41s\n",
      "batch: 1400/1563 - train loss: 9.0120 - test loss: 13.5439 - train acc: 0.5516 - test acc: 0.4209 - 10m 46s\n",
      "batch: 1500/1563 - train loss: 8.9129 - test loss: 13.5560 - train acc: 0.5546 - test acc: 0.4166 - 10m 52s\n",
      "batch: 1563/1563 - train loss: 9.0092 - test loss: 13.1756 - train acc: 0.5575 - test acc: 0.4194 - 10m 56s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.3638 - test loss: 15.0817 - train acc: 0.6663 - test acc: 0.3955 - 11m 1s\n",
      "batch: 200/1563 - train loss: 6.4182 - test loss: 14.5148 - train acc: 0.6691 - test acc: 0.4141 - 11m 6s\n",
      "batch: 300/1563 - train loss: 6.9885 - test loss: 14.4083 - train acc: 0.6332 - test acc: 0.4158 - 11m 12s\n",
      "batch: 400/1563 - train loss: 7.0922 - test loss: 14.5035 - train acc: 0.6278 - test acc: 0.4062 - 11m 17s\n",
      "batch: 500/1563 - train loss: 7.3134 - test loss: 14.1095 - train acc: 0.6291 - test acc: 0.4197 - 11m 22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.5932 - test loss: 15.7091 - train acc: 0.6106 - test acc: 0.3848 - 11m 27s\n",
      "batch: 700/1563 - train loss: 7.3360 - test loss: 14.6131 - train acc: 0.6260 - test acc: 0.4059 - 11m 32s\n",
      "batch: 800/1563 - train loss: 7.5507 - test loss: 14.1568 - train acc: 0.6066 - test acc: 0.4146 - 11m 38s\n",
      "batch: 900/1563 - train loss: 7.9049 - test loss: 14.9308 - train acc: 0.6041 - test acc: 0.3929 - 11m 43s\n",
      "batch: 1000/1563 - train loss: 7.7675 - test loss: 13.6657 - train acc: 0.6040 - test acc: 0.4226 - 11m 48s\n",
      "batch: 1100/1563 - train loss: 7.8597 - test loss: 13.7885 - train acc: 0.6150 - test acc: 0.4188 - 11m 53s\n",
      "batch: 1200/1563 - train loss: 7.9882 - test loss: 14.3661 - train acc: 0.6028 - test acc: 0.4079 - 11m 58s\n",
      "batch: 1300/1563 - train loss: 7.9614 - test loss: 13.7124 - train acc: 0.6078 - test acc: 0.4207 - 12m 4s\n",
      "batch: 1400/1563 - train loss: 7.7877 - test loss: 14.1831 - train acc: 0.6066 - test acc: 0.4138 - 12m 9s\n",
      "batch: 1500/1563 - train loss: 7.9136 - test loss: 14.1698 - train acc: 0.6044 - test acc: 0.4116 - 12m 14s\n",
      "batch: 1563/1563 - train loss: 8.3160 - test loss: 13.8331 - train acc: 0.5806 - test acc: 0.4171 - 12m 18s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.4036 - test loss: 14.3112 - train acc: 0.7197 - test acc: 0.4304 - 12m 23s\n",
      "batch: 200/1563 - train loss: 5.5971 - test loss: 14.4363 - train acc: 0.7037 - test acc: 0.4268 - 12m 29s\n",
      "batch: 300/1563 - train loss: 5.8339 - test loss: 15.5118 - train acc: 0.7021 - test acc: 0.4058 - 12m 34s\n",
      "batch: 400/1563 - train loss: 6.3393 - test loss: 16.4900 - train acc: 0.6750 - test acc: 0.3865 - 12m 39s\n",
      "batch: 500/1563 - train loss: 6.1781 - test loss: 14.5574 - train acc: 0.6807 - test acc: 0.4188 - 12m 44s\n",
      "batch: 600/1563 - train loss: 6.4553 - test loss: 14.8720 - train acc: 0.6703 - test acc: 0.4136 - 12m 49s\n",
      "batch: 700/1563 - train loss: 6.2223 - test loss: 14.7164 - train acc: 0.6684 - test acc: 0.4160 - 12m 55s\n",
      "batch: 800/1563 - train loss: 6.2160 - test loss: 15.2884 - train acc: 0.6691 - test acc: 0.4008 - 13m 0s\n",
      "batch: 900/1563 - train loss: 7.0771 - test loss: 14.8896 - train acc: 0.6334 - test acc: 0.4178 - 13m 5s\n",
      "batch: 1000/1563 - train loss: 7.1455 - test loss: 15.2835 - train acc: 0.6315 - test acc: 0.3989 - 13m 11s\n",
      "batch: 1100/1563 - train loss: 7.1453 - test loss: 14.6441 - train acc: 0.6288 - test acc: 0.4126 - 13m 16s\n",
      "batch: 1200/1563 - train loss: 7.2341 - test loss: 14.6092 - train acc: 0.6272 - test acc: 0.4158 - 13m 21s\n",
      "batch: 1300/1563 - train loss: 6.9097 - test loss: 14.1536 - train acc: 0.6412 - test acc: 0.4279 - 13m 26s\n",
      "batch: 1400/1563 - train loss: 7.2369 - test loss: 14.4700 - train acc: 0.6307 - test acc: 0.4222 - 13m 31s\n",
      "batch: 1500/1563 - train loss: 7.1017 - test loss: 14.7096 - train acc: 0.6354 - test acc: 0.4155 - 13m 36s\n",
      "batch: 1563/1563 - train loss: 7.1991 - test loss: 13.8118 - train acc: 0.6331 - test acc: 0.4381 - 13m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.7336 - test loss: 14.2922 - train acc: 0.7448 - test acc: 0.4292 - 13m 46s\n",
      "batch: 200/1563 - train loss: 4.9046 - test loss: 14.6349 - train acc: 0.7306 - test acc: 0.4330 - 13m 51s\n",
      "batch: 300/1563 - train loss: 5.0517 - test loss: 14.8593 - train acc: 0.7344 - test acc: 0.4271 - 13m 57s\n",
      "batch: 400/1563 - train loss: 5.0376 - test loss: 17.5892 - train acc: 0.7284 - test acc: 0.3760 - 14m 2s\n",
      "batch: 500/1563 - train loss: 4.9391 - test loss: 15.4557 - train acc: 0.7422 - test acc: 0.4169 - 14m 7s\n",
      "batch: 600/1563 - train loss: 5.3080 - test loss: 15.4073 - train acc: 0.7063 - test acc: 0.4175 - 14m 13s\n",
      "batch: 700/1563 - train loss: 5.6363 - test loss: 15.7616 - train acc: 0.7016 - test acc: 0.4066 - 14m 18s\n",
      "batch: 800/1563 - train loss: 5.8206 - test loss: 15.4642 - train acc: 0.6910 - test acc: 0.4129 - 14m 23s\n",
      "batch: 900/1563 - train loss: 5.9607 - test loss: 15.6199 - train acc: 0.6901 - test acc: 0.4070 - 14m 28s\n",
      "batch: 1000/1563 - train loss: 6.1113 - test loss: 15.0336 - train acc: 0.6744 - test acc: 0.4159 - 14m 33s\n",
      "batch: 1100/1563 - train loss: 6.0113 - test loss: 15.0275 - train acc: 0.6847 - test acc: 0.4141 - 14m 38s\n",
      "batch: 1200/1563 - train loss: 6.2454 - test loss: 14.8995 - train acc: 0.6544 - test acc: 0.4225 - 14m 44s\n",
      "batch: 1300/1563 - train loss: 6.2795 - test loss: 15.1725 - train acc: 0.6672 - test acc: 0.4115 - 14m 49s\n",
      "batch: 1400/1563 - train loss: 6.3689 - test loss: 15.1816 - train acc: 0.6653 - test acc: 0.4142 - 14m 54s\n",
      "batch: 1500/1563 - train loss: 6.5347 - test loss: 14.8242 - train acc: 0.6560 - test acc: 0.4204 - 14m 59s\n",
      "batch: 1563/1563 - train loss: 6.6835 - test loss: 16.0931 - train acc: 0.6597 - test acc: 0.3961 - 15m 4s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.0503 - test loss: 14.9143 - train acc: 0.7800 - test acc: 0.4284 - 15m 9s\n",
      "batch: 200/1563 - train loss: 3.9760 - test loss: 15.8523 - train acc: 0.7875 - test acc: 0.4091 - 15m 15s\n",
      "batch: 300/1563 - train loss: 4.4038 - test loss: 15.2449 - train acc: 0.7597 - test acc: 0.4247 - 15m 20s\n",
      "batch: 400/1563 - train loss: 4.2719 - test loss: 15.6153 - train acc: 0.7625 - test acc: 0.4281 - 15m 25s\n",
      "batch: 500/1563 - train loss: 4.7996 - test loss: 16.7704 - train acc: 0.7394 - test acc: 0.4051 - 15m 30s\n",
      "batch: 600/1563 - train loss: 4.6964 - test loss: 15.4681 - train acc: 0.7419 - test acc: 0.4259 - 15m 35s\n",
      "batch: 700/1563 - train loss: 5.0014 - test loss: 15.7429 - train acc: 0.7254 - test acc: 0.4203 - 15m 40s\n",
      "batch: 800/1563 - train loss: 5.0286 - test loss: 15.7555 - train acc: 0.7322 - test acc: 0.4230 - 15m 46s\n",
      "batch: 900/1563 - train loss: 5.2358 - test loss: 15.2922 - train acc: 0.7328 - test acc: 0.4289 - 15m 51s\n",
      "batch: 1000/1563 - train loss: 5.0810 - test loss: 15.2169 - train acc: 0.7197 - test acc: 0.4355 - 15m 56s\n",
      "batch: 1100/1563 - train loss: 5.2567 - test loss: 15.7297 - train acc: 0.7191 - test acc: 0.4108 - 16m 1s\n",
      "batch: 1200/1563 - train loss: 5.2233 - test loss: 16.0391 - train acc: 0.7216 - test acc: 0.4211 - 16m 6s\n",
      "batch: 1300/1563 - train loss: 5.2984 - test loss: 15.5658 - train acc: 0.7082 - test acc: 0.4174 - 16m 12s\n",
      "batch: 1400/1563 - train loss: 5.7410 - test loss: 15.0052 - train acc: 0.6963 - test acc: 0.4302 - 16m 17s\n",
      "batch: 1500/1563 - train loss: 5.7089 - test loss: 15.2687 - train acc: 0.6985 - test acc: 0.4276 - 16m 23s\n",
      "batch: 1563/1563 - train loss: 5.7502 - test loss: 17.4340 - train acc: 0.6919 - test acc: 0.3852 - 16m 27s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.5960 - test loss: 16.0019 - train acc: 0.8034 - test acc: 0.4235 - 16m 32s\n",
      "batch: 200/1563 - train loss: 3.4903 - test loss: 15.8571 - train acc: 0.8050 - test acc: 0.4284 - 16m 37s\n",
      "batch: 300/1563 - train loss: 3.5378 - test loss: 15.9421 - train acc: 0.8112 - test acc: 0.4282 - 16m 42s\n",
      "batch: 400/1563 - train loss: 3.7403 - test loss: 16.1495 - train acc: 0.7962 - test acc: 0.4214 - 16m 48s\n",
      "batch: 500/1563 - train loss: 3.6773 - test loss: 16.8962 - train acc: 0.7978 - test acc: 0.4160 - 16m 53s\n",
      "batch: 600/1563 - train loss: 3.9182 - test loss: 16.5409 - train acc: 0.7871 - test acc: 0.4208 - 16m 58s\n",
      "batch: 700/1563 - train loss: 4.0520 - test loss: 16.1433 - train acc: 0.7806 - test acc: 0.4211 - 17m 3s\n",
      "batch: 800/1563 - train loss: 4.2374 - test loss: 17.0645 - train acc: 0.7697 - test acc: 0.4020 - 17m 8s\n",
      "batch: 900/1563 - train loss: 4.4047 - test loss: 16.6839 - train acc: 0.7534 - test acc: 0.4088 - 17m 14s\n",
      "batch: 1000/1563 - train loss: 4.2943 - test loss: 16.5981 - train acc: 0.7609 - test acc: 0.4190 - 17m 19s\n",
      "batch: 1100/1563 - train loss: 4.5862 - test loss: 16.8929 - train acc: 0.7513 - test acc: 0.4164 - 17m 24s\n",
      "batch: 1200/1563 - train loss: 4.4777 - test loss: 15.9369 - train acc: 0.7537 - test acc: 0.4296 - 17m 30s\n",
      "batch: 1300/1563 - train loss: 4.5366 - test loss: 16.2304 - train acc: 0.7497 - test acc: 0.4228 - 17m 35s\n",
      "batch: 1400/1563 - train loss: 4.7588 - test loss: 17.7522 - train acc: 0.7422 - test acc: 0.3971 - 17m 40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 5.1090 - test loss: 16.1710 - train acc: 0.7216 - test acc: 0.4145 - 17m 45s\n",
      "batch: 1563/1563 - train loss: 5.0075 - test loss: 16.6111 - train acc: 0.7329 - test acc: 0.4196 - 17m 50s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 3.0732 - test loss: 16.7309 - train acc: 0.8237 - test acc: 0.4255 - 17m 55s\n",
      "batch: 200/1563 - train loss: 3.0215 - test loss: 16.8017 - train acc: 0.8328 - test acc: 0.4258 - 18m 0s\n",
      "batch: 300/1563 - train loss: 3.0839 - test loss: 16.3775 - train acc: 0.8309 - test acc: 0.4308 - 18m 5s\n",
      "batch: 400/1563 - train loss: 3.2378 - test loss: 17.8164 - train acc: 0.8155 - test acc: 0.4005 - 18m 11s\n",
      "batch: 500/1563 - train loss: 3.1539 - test loss: 16.5045 - train acc: 0.8240 - test acc: 0.4301 - 18m 16s\n",
      "batch: 600/1563 - train loss: 3.3603 - test loss: 16.8174 - train acc: 0.8137 - test acc: 0.4235 - 18m 21s\n",
      "batch: 700/1563 - train loss: 3.4051 - test loss: 17.2223 - train acc: 0.8049 - test acc: 0.4240 - 18m 27s\n",
      "batch: 800/1563 - train loss: 3.5353 - test loss: 19.0689 - train acc: 0.8059 - test acc: 0.3975 - 18m 32s\n",
      "batch: 900/1563 - train loss: 3.8313 - test loss: 17.2331 - train acc: 0.7943 - test acc: 0.4127 - 18m 37s\n",
      "batch: 1000/1563 - train loss: 3.7397 - test loss: 17.5087 - train acc: 0.7956 - test acc: 0.4108 - 18m 43s\n",
      "batch: 1100/1563 - train loss: 3.8128 - test loss: 17.3434 - train acc: 0.7891 - test acc: 0.4246 - 18m 48s\n",
      "batch: 1200/1563 - train loss: 4.2249 - test loss: 16.6385 - train acc: 0.7762 - test acc: 0.4192 - 18m 53s\n",
      "batch: 1300/1563 - train loss: 3.9845 - test loss: 16.3071 - train acc: 0.7849 - test acc: 0.4316 - 18m 58s\n",
      "batch: 1400/1563 - train loss: 4.2709 - test loss: 16.2841 - train acc: 0.7603 - test acc: 0.4333 - 19m 3s\n",
      "batch: 1500/1563 - train loss: 4.5274 - test loss: 17.6822 - train acc: 0.7510 - test acc: 0.4048 - 19m 9s\n",
      "batch: 1563/1563 - train loss: 4.7411 - test loss: 16.6013 - train acc: 0.7409 - test acc: 0.4262 - 19m 13s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.4540 - test loss: 16.7138 - train acc: 0.8591 - test acc: 0.4342 - 19m 18s\n",
      "batch: 200/1563 - train loss: 2.4579 - test loss: 16.6248 - train acc: 0.8593 - test acc: 0.4388 - 19m 23s\n",
      "batch: 300/1563 - train loss: 2.4625 - test loss: 17.7997 - train acc: 0.8656 - test acc: 0.4132 - 19m 29s\n",
      "batch: 400/1563 - train loss: 2.6262 - test loss: 17.7353 - train acc: 0.8506 - test acc: 0.4164 - 19m 34s\n",
      "batch: 500/1563 - train loss: 2.7155 - test loss: 17.1943 - train acc: 0.8434 - test acc: 0.4298 - 19m 39s\n",
      "batch: 600/1563 - train loss: 2.9449 - test loss: 17.6389 - train acc: 0.8415 - test acc: 0.4284 - 19m 44s\n",
      "batch: 700/1563 - train loss: 2.8985 - test loss: 17.7472 - train acc: 0.8337 - test acc: 0.4229 - 19m 50s\n",
      "batch: 800/1563 - train loss: 3.2120 - test loss: 17.2133 - train acc: 0.8196 - test acc: 0.4337 - 19m 55s\n",
      "time is up! finishing training\n",
      "batch: 898/1563 - train loss: 2.9895 - test loss: 17.8590 - train acc: 0.8375 - test acc: 0.4218 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 4\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 25.9785 - test loss: 24.6546 - train acc: 0.0361 - test acc: 0.0542 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.8597 - test loss: 23.3483 - train acc: 0.0702 - test acc: 0.0811 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.8659 - test loss: 22.3966 - train acc: 0.0886 - test acc: 0.1038 - 0m 12s\n",
      "batch: 400/1563 - train loss: 21.8450 - test loss: 21.2250 - train acc: 0.1143 - test acc: 0.1167 - 0m 17s\n",
      "batch: 500/1563 - train loss: 21.4901 - test loss: 21.1376 - train acc: 0.1290 - test acc: 0.1353 - 0m 22s\n",
      "batch: 600/1563 - train loss: 21.0045 - test loss: 20.4752 - train acc: 0.1328 - test acc: 0.1499 - 0m 27s\n",
      "batch: 700/1563 - train loss: 20.6070 - test loss: 21.4179 - train acc: 0.1475 - test acc: 0.1354 - 0m 33s\n",
      "batch: 800/1563 - train loss: 20.4205 - test loss: 22.3871 - train acc: 0.1425 - test acc: 0.1416 - 0m 38s\n",
      "batch: 900/1563 - train loss: 20.0406 - test loss: 20.1558 - train acc: 0.1598 - test acc: 0.1530 - 0m 43s\n",
      "batch: 1000/1563 - train loss: 19.9754 - test loss: 19.6894 - train acc: 0.1597 - test acc: 0.1736 - 0m 48s\n",
      "batch: 1100/1563 - train loss: 19.4499 - test loss: 19.1749 - train acc: 0.1772 - test acc: 0.1835 - 0m 53s\n",
      "batch: 1200/1563 - train loss: 19.2932 - test loss: 20.6893 - train acc: 0.1804 - test acc: 0.1642 - 0m 58s\n",
      "batch: 1300/1563 - train loss: 19.1736 - test loss: 18.7477 - train acc: 0.1863 - test acc: 0.2026 - 1m 3s\n",
      "batch: 1400/1563 - train loss: 18.7019 - test loss: 18.3790 - train acc: 0.1944 - test acc: 0.2141 - 1m 8s\n",
      "batch: 1500/1563 - train loss: 18.7298 - test loss: 18.6564 - train acc: 0.2013 - test acc: 0.1987 - 1m 13s\n",
      "batch: 1563/1563 - train loss: 18.6837 - test loss: 18.9524 - train acc: 0.2007 - test acc: 0.2073 - 1m 18s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.8212 - test loss: 18.6765 - train acc: 0.2366 - test acc: 0.2023 - 1m 23s\n",
      "batch: 200/1563 - train loss: 17.7544 - test loss: 18.9527 - train acc: 0.2288 - test acc: 0.1926 - 1m 28s\n",
      "batch: 300/1563 - train loss: 17.6306 - test loss: 17.4772 - train acc: 0.2390 - test acc: 0.2369 - 1m 33s\n",
      "batch: 400/1563 - train loss: 17.1010 - test loss: 18.2502 - train acc: 0.2622 - test acc: 0.2258 - 1m 39s\n",
      "batch: 500/1563 - train loss: 17.3335 - test loss: 18.1641 - train acc: 0.2397 - test acc: 0.2288 - 1m 44s\n",
      "batch: 600/1563 - train loss: 17.1100 - test loss: 17.6011 - train acc: 0.2482 - test acc: 0.2392 - 1m 49s\n",
      "batch: 700/1563 - train loss: 17.0162 - test loss: 17.3617 - train acc: 0.2478 - test acc: 0.2456 - 1m 54s\n",
      "batch: 800/1563 - train loss: 16.5471 - test loss: 16.5745 - train acc: 0.2625 - test acc: 0.2672 - 1m 59s\n",
      "batch: 900/1563 - train loss: 16.4641 - test loss: 17.0279 - train acc: 0.2650 - test acc: 0.2576 - 2m 4s\n",
      "batch: 1000/1563 - train loss: 16.4756 - test loss: 16.5703 - train acc: 0.2672 - test acc: 0.2747 - 2m 10s\n",
      "batch: 1100/1563 - train loss: 16.4770 - test loss: 17.7858 - train acc: 0.2682 - test acc: 0.2344 - 2m 15s\n",
      "batch: 1200/1563 - train loss: 16.5585 - test loss: 17.1544 - train acc: 0.2725 - test acc: 0.2504 - 2m 20s\n",
      "batch: 1300/1563 - train loss: 16.3429 - test loss: 16.3447 - train acc: 0.2704 - test acc: 0.2814 - 2m 25s\n",
      "batch: 1400/1563 - train loss: 16.0522 - test loss: 16.1202 - train acc: 0.2765 - test acc: 0.2844 - 2m 30s\n",
      "batch: 1500/1563 - train loss: 15.8556 - test loss: 16.6704 - train acc: 0.2893 - test acc: 0.2719 - 2m 35s\n",
      "batch: 1563/1563 - train loss: 15.8820 - test loss: 15.8907 - train acc: 0.2972 - test acc: 0.2967 - 2m 40s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.8066 - test loss: 15.9428 - train acc: 0.3237 - test acc: 0.2914 - 2m 45s\n",
      "batch: 200/1563 - train loss: 14.7163 - test loss: 15.5202 - train acc: 0.3316 - test acc: 0.2998 - 2m 50s\n",
      "batch: 300/1563 - train loss: 14.9403 - test loss: 16.0336 - train acc: 0.3246 - test acc: 0.2978 - 2m 55s\n",
      "batch: 400/1563 - train loss: 14.7805 - test loss: 16.1708 - train acc: 0.3172 - test acc: 0.2960 - 3m 1s\n",
      "batch: 500/1563 - train loss: 15.1011 - test loss: 20.1759 - train acc: 0.3212 - test acc: 0.2132 - 3m 6s\n",
      "batch: 600/1563 - train loss: 14.9084 - test loss: 15.9197 - train acc: 0.3206 - test acc: 0.2965 - 3m 11s\n",
      "batch: 700/1563 - train loss: 14.7614 - test loss: 19.1715 - train acc: 0.3412 - test acc: 0.2522 - 3m 17s\n",
      "batch: 800/1563 - train loss: 14.8149 - test loss: 15.3786 - train acc: 0.3206 - test acc: 0.3159 - 3m 22s\n",
      "batch: 900/1563 - train loss: 14.5794 - test loss: 16.8826 - train acc: 0.3435 - test acc: 0.2764 - 3m 28s\n",
      "batch: 1000/1563 - train loss: 14.7035 - test loss: 15.1862 - train acc: 0.3366 - test acc: 0.3255 - 3m 33s\n",
      "batch: 1100/1563 - train loss: 14.5599 - test loss: 17.2369 - train acc: 0.3384 - test acc: 0.2662 - 3m 39s\n",
      "batch: 1200/1563 - train loss: 14.8097 - test loss: 15.9763 - train acc: 0.3322 - test acc: 0.3062 - 3m 44s\n",
      "batch: 1300/1563 - train loss: 14.4714 - test loss: 14.6930 - train acc: 0.3450 - test acc: 0.3394 - 3m 50s\n",
      "batch: 1400/1563 - train loss: 14.4179 - test loss: 14.8109 - train acc: 0.3471 - test acc: 0.3327 - 3m 55s\n",
      "batch: 1500/1563 - train loss: 14.2780 - test loss: 15.3370 - train acc: 0.3481 - test acc: 0.3136 - 4m 0s\n",
      "batch: 1563/1563 - train loss: 14.3444 - test loss: 15.1651 - train acc: 0.3462 - test acc: 0.3231 - 4m 4s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.0598 - test loss: 15.1230 - train acc: 0.3903 - test acc: 0.3281 - 4m 10s\n",
      "batch: 200/1563 - train loss: 13.4607 - test loss: 14.6669 - train acc: 0.3669 - test acc: 0.3432 - 4m 15s\n",
      "batch: 300/1563 - train loss: 13.1127 - test loss: 14.5887 - train acc: 0.3963 - test acc: 0.3456 - 4m 20s\n",
      "batch: 400/1563 - train loss: 13.4872 - test loss: 14.8292 - train acc: 0.3800 - test acc: 0.3386 - 4m 25s\n",
      "batch: 500/1563 - train loss: 13.4214 - test loss: 14.8360 - train acc: 0.3709 - test acc: 0.3447 - 4m 31s\n",
      "batch: 600/1563 - train loss: 13.5041 - test loss: 14.4711 - train acc: 0.3741 - test acc: 0.3536 - 4m 36s\n",
      "batch: 700/1563 - train loss: 13.2487 - test loss: 14.4662 - train acc: 0.3925 - test acc: 0.3437 - 4m 41s\n",
      "batch: 800/1563 - train loss: 13.1682 - test loss: 14.4220 - train acc: 0.3900 - test acc: 0.3454 - 4m 46s\n",
      "batch: 900/1563 - train loss: 13.0066 - test loss: 14.4104 - train acc: 0.4025 - test acc: 0.3607 - 4m 51s\n",
      "batch: 1000/1563 - train loss: 13.3798 - test loss: 14.8107 - train acc: 0.3825 - test acc: 0.3330 - 4m 56s\n",
      "batch: 1100/1563 - train loss: 13.3625 - test loss: 14.4934 - train acc: 0.3778 - test acc: 0.3488 - 5m 2s\n",
      "batch: 1200/1563 - train loss: 13.0112 - test loss: 14.6760 - train acc: 0.3787 - test acc: 0.3465 - 5m 7s\n",
      "batch: 1300/1563 - train loss: 13.1901 - test loss: 14.8176 - train acc: 0.3838 - test acc: 0.3452 - 5m 12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.0808 - test loss: 13.8530 - train acc: 0.3863 - test acc: 0.3754 - 5m 17s\n",
      "batch: 1500/1563 - train loss: 13.1445 - test loss: 14.7994 - train acc: 0.3925 - test acc: 0.3438 - 5m 22s\n",
      "batch: 1563/1563 - train loss: 13.0196 - test loss: 14.4316 - train acc: 0.4010 - test acc: 0.3610 - 5m 27s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3106 - test loss: 13.9998 - train acc: 0.4500 - test acc: 0.3816 - 5m 32s\n",
      "batch: 200/1563 - train loss: 11.5535 - test loss: 13.9805 - train acc: 0.4481 - test acc: 0.3761 - 5m 37s\n",
      "batch: 300/1563 - train loss: 11.9335 - test loss: 14.4794 - train acc: 0.4488 - test acc: 0.3610 - 5m 42s\n",
      "batch: 400/1563 - train loss: 12.1250 - test loss: 15.6124 - train acc: 0.4303 - test acc: 0.3337 - 5m 48s\n",
      "batch: 500/1563 - train loss: 11.8908 - test loss: 14.4657 - train acc: 0.4476 - test acc: 0.3641 - 5m 53s\n",
      "batch: 600/1563 - train loss: 11.7068 - test loss: 14.1590 - train acc: 0.4394 - test acc: 0.3686 - 5m 58s\n",
      "batch: 700/1563 - train loss: 12.1481 - test loss: 14.4631 - train acc: 0.4294 - test acc: 0.3598 - 6m 3s\n",
      "batch: 800/1563 - train loss: 12.1865 - test loss: 13.6479 - train acc: 0.4238 - test acc: 0.3905 - 6m 8s\n",
      "batch: 900/1563 - train loss: 12.1640 - test loss: 14.0789 - train acc: 0.4201 - test acc: 0.3693 - 6m 14s\n",
      "batch: 1000/1563 - train loss: 12.1770 - test loss: 13.9707 - train acc: 0.4400 - test acc: 0.3714 - 6m 19s\n",
      "batch: 1100/1563 - train loss: 12.0332 - test loss: 13.8335 - train acc: 0.4381 - test acc: 0.3776 - 6m 24s\n",
      "batch: 1200/1563 - train loss: 11.8629 - test loss: 13.7055 - train acc: 0.4422 - test acc: 0.3830 - 6m 29s\n",
      "batch: 1300/1563 - train loss: 12.2582 - test loss: 13.7278 - train acc: 0.4266 - test acc: 0.3881 - 6m 34s\n",
      "batch: 1400/1563 - train loss: 12.0074 - test loss: 13.8748 - train acc: 0.4429 - test acc: 0.3662 - 6m 39s\n",
      "batch: 1500/1563 - train loss: 11.9484 - test loss: 14.1873 - train acc: 0.4360 - test acc: 0.3720 - 6m 45s\n",
      "batch: 1563/1563 - train loss: 11.9969 - test loss: 13.3681 - train acc: 0.4344 - test acc: 0.3898 - 6m 49s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.8656 - test loss: 13.6292 - train acc: 0.5082 - test acc: 0.3937 - 6m 54s\n",
      "batch: 200/1563 - train loss: 10.5341 - test loss: 14.6161 - train acc: 0.4928 - test acc: 0.3670 - 6m 59s\n",
      "batch: 300/1563 - train loss: 10.6443 - test loss: 13.8189 - train acc: 0.4831 - test acc: 0.3883 - 7m 4s\n",
      "batch: 400/1563 - train loss: 10.4807 - test loss: 15.1606 - train acc: 0.4994 - test acc: 0.3532 - 7m 10s\n",
      "batch: 500/1563 - train loss: 10.8299 - test loss: 13.8421 - train acc: 0.4735 - test acc: 0.3858 - 7m 15s\n",
      "batch: 600/1563 - train loss: 11.0642 - test loss: 15.4347 - train acc: 0.4691 - test acc: 0.3454 - 7m 20s\n",
      "batch: 700/1563 - train loss: 10.2072 - test loss: 13.4492 - train acc: 0.5022 - test acc: 0.3998 - 7m 25s\n",
      "batch: 800/1563 - train loss: 10.9878 - test loss: 14.0241 - train acc: 0.4756 - test acc: 0.3844 - 7m 30s\n",
      "batch: 900/1563 - train loss: 10.8871 - test loss: 14.4178 - train acc: 0.4835 - test acc: 0.3661 - 7m 35s\n",
      "batch: 1000/1563 - train loss: 11.1807 - test loss: 13.2553 - train acc: 0.4791 - test acc: 0.4069 - 7m 41s\n",
      "batch: 1100/1563 - train loss: 10.9939 - test loss: 13.4995 - train acc: 0.4666 - test acc: 0.4018 - 7m 46s\n",
      "batch: 1200/1563 - train loss: 11.0353 - test loss: 14.3481 - train acc: 0.4650 - test acc: 0.3791 - 7m 51s\n",
      "batch: 1300/1563 - train loss: 10.7930 - test loss: 13.5851 - train acc: 0.4785 - test acc: 0.3966 - 7m 56s\n",
      "batch: 1400/1563 - train loss: 10.9014 - test loss: 13.3410 - train acc: 0.4850 - test acc: 0.4032 - 8m 2s\n",
      "batch: 1500/1563 - train loss: 11.0871 - test loss: 13.3384 - train acc: 0.4884 - test acc: 0.4046 - 8m 7s\n",
      "batch: 1563/1563 - train loss: 11.1276 - test loss: 13.3269 - train acc: 0.4799 - test acc: 0.4071 - 8m 12s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.9599 - test loss: 13.9749 - train acc: 0.5550 - test acc: 0.3967 - 8m 17s\n",
      "batch: 200/1563 - train loss: 9.0822 - test loss: 14.2622 - train acc: 0.5466 - test acc: 0.3896 - 8m 22s\n",
      "batch: 300/1563 - train loss: 9.3059 - test loss: 13.4760 - train acc: 0.5456 - test acc: 0.4036 - 8m 27s\n",
      "batch: 400/1563 - train loss: 9.1155 - test loss: 13.6699 - train acc: 0.5453 - test acc: 0.4060 - 8m 32s\n",
      "batch: 500/1563 - train loss: 9.7827 - test loss: 14.7194 - train acc: 0.5259 - test acc: 0.3641 - 8m 37s\n",
      "batch: 600/1563 - train loss: 9.5068 - test loss: 13.7036 - train acc: 0.5359 - test acc: 0.4071 - 8m 43s\n",
      "batch: 700/1563 - train loss: 9.7545 - test loss: 13.9077 - train acc: 0.5232 - test acc: 0.3988 - 8m 48s\n",
      "batch: 800/1563 - train loss: 10.1058 - test loss: 13.4467 - train acc: 0.5088 - test acc: 0.3973 - 8m 53s\n",
      "batch: 900/1563 - train loss: 9.8374 - test loss: 14.2420 - train acc: 0.5168 - test acc: 0.3896 - 8m 58s\n",
      "batch: 1000/1563 - train loss: 10.3098 - test loss: 14.0236 - train acc: 0.4996 - test acc: 0.3930 - 9m 3s\n",
      "batch: 1100/1563 - train loss: 10.0443 - test loss: 13.7504 - train acc: 0.5096 - test acc: 0.3976 - 9m 9s\n",
      "batch: 1200/1563 - train loss: 9.7591 - test loss: 13.2059 - train acc: 0.5219 - test acc: 0.4204 - 9m 14s\n",
      "batch: 1300/1563 - train loss: 10.3470 - test loss: 13.4087 - train acc: 0.4900 - test acc: 0.4107 - 9m 19s\n",
      "batch: 1400/1563 - train loss: 10.2497 - test loss: 14.5399 - train acc: 0.5066 - test acc: 0.3783 - 9m 24s\n",
      "batch: 1500/1563 - train loss: 10.2366 - test loss: 13.1322 - train acc: 0.4937 - test acc: 0.4199 - 9m 29s\n",
      "batch: 1563/1563 - train loss: 10.3928 - test loss: 12.9609 - train acc: 0.4856 - test acc: 0.4238 - 9m 34s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 8.1288 - test loss: 13.6425 - train acc: 0.5947 - test acc: 0.4111 - 9m 39s\n",
      "batch: 200/1563 - train loss: 8.2378 - test loss: 13.9760 - train acc: 0.5862 - test acc: 0.4104 - 9m 44s\n",
      "batch: 300/1563 - train loss: 7.8825 - test loss: 14.4887 - train acc: 0.5950 - test acc: 0.3927 - 9m 49s\n",
      "batch: 400/1563 - train loss: 8.1989 - test loss: 13.8818 - train acc: 0.5956 - test acc: 0.4125 - 9m 54s\n",
      "batch: 500/1563 - train loss: 8.6008 - test loss: 13.4266 - train acc: 0.5806 - test acc: 0.4185 - 10m 0s\n",
      "batch: 600/1563 - train loss: 8.5708 - test loss: 14.4592 - train acc: 0.5594 - test acc: 0.3964 - 10m 5s\n",
      "batch: 700/1563 - train loss: 8.5872 - test loss: 14.2859 - train acc: 0.5650 - test acc: 0.4050 - 10m 10s\n",
      "batch: 800/1563 - train loss: 8.7094 - test loss: 13.6158 - train acc: 0.5666 - test acc: 0.4185 - 10m 15s\n",
      "batch: 900/1563 - train loss: 8.9474 - test loss: 14.0260 - train acc: 0.5637 - test acc: 0.4085 - 10m 21s\n",
      "batch: 1000/1563 - train loss: 9.0801 - test loss: 13.5727 - train acc: 0.5459 - test acc: 0.4160 - 10m 26s\n",
      "batch: 1100/1563 - train loss: 8.9776 - test loss: 14.2393 - train acc: 0.5597 - test acc: 0.3971 - 10m 31s\n",
      "batch: 1200/1563 - train loss: 8.9791 - test loss: 13.4452 - train acc: 0.5478 - test acc: 0.4196 - 10m 36s\n",
      "batch: 1300/1563 - train loss: 9.2469 - test loss: 13.5190 - train acc: 0.5396 - test acc: 0.4230 - 10m 41s\n",
      "batch: 1400/1563 - train loss: 9.4904 - test loss: 15.1835 - train acc: 0.5372 - test acc: 0.3810 - 10m 46s\n",
      "batch: 1500/1563 - train loss: 9.0131 - test loss: 13.6245 - train acc: 0.5575 - test acc: 0.4189 - 10m 52s\n",
      "batch: 1563/1563 - train loss: 8.9804 - test loss: 13.7622 - train acc: 0.5613 - test acc: 0.4189 - 10m 56s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.7668 - test loss: 14.5293 - train acc: 0.6516 - test acc: 0.4072 - 11m 1s\n",
      "batch: 200/1563 - train loss: 6.9347 - test loss: 14.5927 - train acc: 0.6435 - test acc: 0.4002 - 11m 7s\n",
      "batch: 300/1563 - train loss: 7.2995 - test loss: 15.0396 - train acc: 0.6270 - test acc: 0.3907 - 11m 12s\n",
      "batch: 400/1563 - train loss: 6.9189 - test loss: 14.7678 - train acc: 0.6300 - test acc: 0.4084 - 11m 17s\n",
      "batch: 500/1563 - train loss: 7.2959 - test loss: 14.3238 - train acc: 0.6225 - test acc: 0.4107 - 11m 22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.6065 - test loss: 14.1388 - train acc: 0.6190 - test acc: 0.4104 - 11m 28s\n",
      "batch: 700/1563 - train loss: 7.9952 - test loss: 13.8166 - train acc: 0.5900 - test acc: 0.4166 - 11m 33s\n",
      "batch: 800/1563 - train loss: 7.9359 - test loss: 15.3795 - train acc: 0.5982 - test acc: 0.3845 - 11m 38s\n",
      "batch: 900/1563 - train loss: 7.9068 - test loss: 14.6748 - train acc: 0.6044 - test acc: 0.3976 - 11m 43s\n",
      "batch: 1000/1563 - train loss: 7.8859 - test loss: 13.7755 - train acc: 0.6043 - test acc: 0.4178 - 11m 48s\n",
      "batch: 1100/1563 - train loss: 8.1556 - test loss: 13.5809 - train acc: 0.5938 - test acc: 0.4218 - 11m 54s\n",
      "batch: 1200/1563 - train loss: 8.3275 - test loss: 14.8515 - train acc: 0.5787 - test acc: 0.3812 - 11m 59s\n",
      "batch: 1300/1563 - train loss: 8.2885 - test loss: 14.0795 - train acc: 0.5785 - test acc: 0.4095 - 12m 4s\n",
      "batch: 1400/1563 - train loss: 8.3034 - test loss: 13.9393 - train acc: 0.5781 - test acc: 0.4200 - 12m 9s\n",
      "batch: 1500/1563 - train loss: 8.3588 - test loss: 14.2488 - train acc: 0.5868 - test acc: 0.4068 - 12m 14s\n",
      "batch: 1563/1563 - train loss: 8.4459 - test loss: 14.0185 - train acc: 0.5768 - test acc: 0.4153 - 12m 19s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.8919 - test loss: 14.9860 - train acc: 0.6860 - test acc: 0.4071 - 12m 24s\n",
      "batch: 200/1563 - train loss: 5.8679 - test loss: 14.6224 - train acc: 0.6916 - test acc: 0.4125 - 12m 29s\n",
      "batch: 300/1563 - train loss: 6.4535 - test loss: 15.2673 - train acc: 0.6672 - test acc: 0.4002 - 12m 35s\n",
      "batch: 400/1563 - train loss: 6.1614 - test loss: 15.2279 - train acc: 0.6809 - test acc: 0.4096 - 12m 40s\n",
      "batch: 500/1563 - train loss: 6.4167 - test loss: 14.3713 - train acc: 0.6610 - test acc: 0.4197 - 12m 45s\n",
      "batch: 600/1563 - train loss: 6.9399 - test loss: 14.6615 - train acc: 0.6444 - test acc: 0.4197 - 12m 50s\n",
      "batch: 700/1563 - train loss: 7.1180 - test loss: 14.8559 - train acc: 0.6413 - test acc: 0.4119 - 12m 56s\n",
      "batch: 800/1563 - train loss: 6.9229 - test loss: 14.9489 - train acc: 0.6425 - test acc: 0.4072 - 13m 1s\n",
      "batch: 900/1563 - train loss: 6.9497 - test loss: 14.3553 - train acc: 0.6313 - test acc: 0.4246 - 13m 6s\n",
      "batch: 1000/1563 - train loss: 7.1384 - test loss: 14.0139 - train acc: 0.6259 - test acc: 0.4281 - 13m 11s\n",
      "batch: 1100/1563 - train loss: 7.2877 - test loss: 14.2015 - train acc: 0.6284 - test acc: 0.4297 - 13m 16s\n",
      "batch: 1200/1563 - train loss: 7.0553 - test loss: 14.8056 - train acc: 0.6287 - test acc: 0.4117 - 13m 22s\n",
      "batch: 1300/1563 - train loss: 7.3223 - test loss: 14.1916 - train acc: 0.6166 - test acc: 0.4165 - 13m 27s\n",
      "batch: 1400/1563 - train loss: 7.3794 - test loss: 14.3711 - train acc: 0.6194 - test acc: 0.4259 - 13m 32s\n",
      "batch: 1500/1563 - train loss: 7.5598 - test loss: 14.6271 - train acc: 0.6084 - test acc: 0.4103 - 13m 37s\n",
      "batch: 1563/1563 - train loss: 7.5470 - test loss: 14.3698 - train acc: 0.6084 - test acc: 0.4192 - 13m 42s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 5.2331 - test loss: 14.5880 - train acc: 0.7203 - test acc: 0.4222 - 13m 47s\n",
      "batch: 200/1563 - train loss: 5.2492 - test loss: 15.5226 - train acc: 0.7209 - test acc: 0.4074 - 13m 52s\n",
      "batch: 300/1563 - train loss: 5.0242 - test loss: 15.4557 - train acc: 0.7322 - test acc: 0.4191 - 13m 57s\n",
      "batch: 400/1563 - train loss: 5.4013 - test loss: 14.7219 - train acc: 0.7137 - test acc: 0.4243 - 14m 3s\n",
      "batch: 500/1563 - train loss: 5.4170 - test loss: 14.8860 - train acc: 0.7128 - test acc: 0.4246 - 14m 8s\n",
      "batch: 600/1563 - train loss: 5.8053 - test loss: 15.0759 - train acc: 0.6898 - test acc: 0.4158 - 14m 13s\n",
      "batch: 700/1563 - train loss: 5.8209 - test loss: 14.9751 - train acc: 0.7004 - test acc: 0.4231 - 14m 18s\n",
      "batch: 800/1563 - train loss: 6.3444 - test loss: 14.8429 - train acc: 0.6710 - test acc: 0.4251 - 14m 23s\n",
      "batch: 900/1563 - train loss: 6.3800 - test loss: 14.7381 - train acc: 0.6713 - test acc: 0.4196 - 14m 28s\n",
      "batch: 1000/1563 - train loss: 6.1871 - test loss: 15.0479 - train acc: 0.6775 - test acc: 0.4223 - 14m 34s\n",
      "batch: 1100/1563 - train loss: 6.6087 - test loss: 15.3376 - train acc: 0.6541 - test acc: 0.4135 - 14m 39s\n",
      "batch: 1200/1563 - train loss: 6.1470 - test loss: 15.0428 - train acc: 0.6785 - test acc: 0.4220 - 14m 44s\n",
      "batch: 1300/1563 - train loss: 6.4049 - test loss: 15.2178 - train acc: 0.6781 - test acc: 0.4129 - 14m 49s\n",
      "batch: 1400/1563 - train loss: 6.5421 - test loss: 15.5850 - train acc: 0.6609 - test acc: 0.4069 - 14m 54s\n",
      "batch: 1500/1563 - train loss: 6.5972 - test loss: 15.9317 - train acc: 0.6637 - test acc: 0.3974 - 15m 0s\n",
      "batch: 1563/1563 - train loss: 6.4615 - test loss: 14.5356 - train acc: 0.6644 - test acc: 0.4164 - 15m 4s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.2928 - test loss: 14.9473 - train acc: 0.7653 - test acc: 0.4272 - 15m 10s\n",
      "batch: 200/1563 - train loss: 4.4257 - test loss: 16.5466 - train acc: 0.7628 - test acc: 0.4014 - 15m 15s\n",
      "batch: 300/1563 - train loss: 4.5142 - test loss: 15.5073 - train acc: 0.7472 - test acc: 0.4278 - 15m 20s\n",
      "batch: 400/1563 - train loss: 4.7413 - test loss: 15.7657 - train acc: 0.7426 - test acc: 0.4082 - 15m 25s\n",
      "batch: 500/1563 - train loss: 4.5896 - test loss: 15.8770 - train acc: 0.7522 - test acc: 0.4109 - 15m 30s\n",
      "batch: 600/1563 - train loss: 4.9508 - test loss: 15.7437 - train acc: 0.7362 - test acc: 0.4152 - 15m 36s\n",
      "batch: 700/1563 - train loss: 5.2415 - test loss: 15.7367 - train acc: 0.7128 - test acc: 0.4211 - 15m 41s\n",
      "batch: 800/1563 - train loss: 5.0652 - test loss: 15.7234 - train acc: 0.7228 - test acc: 0.4193 - 15m 46s\n",
      "batch: 900/1563 - train loss: 5.5669 - test loss: 15.5943 - train acc: 0.7100 - test acc: 0.4226 - 15m 51s\n",
      "batch: 1000/1563 - train loss: 5.2163 - test loss: 15.3640 - train acc: 0.7172 - test acc: 0.4257 - 15m 56s\n",
      "batch: 1100/1563 - train loss: 5.6801 - test loss: 15.4269 - train acc: 0.6997 - test acc: 0.4228 - 16m 1s\n",
      "batch: 1200/1563 - train loss: 5.5823 - test loss: 15.1062 - train acc: 0.7066 - test acc: 0.4276 - 16m 7s\n",
      "batch: 1300/1563 - train loss: 5.8153 - test loss: 15.3530 - train acc: 0.6869 - test acc: 0.4130 - 16m 12s\n",
      "batch: 1400/1563 - train loss: 5.7392 - test loss: 15.0196 - train acc: 0.6894 - test acc: 0.4265 - 16m 18s\n",
      "batch: 1500/1563 - train loss: 5.5233 - test loss: 15.2040 - train acc: 0.7040 - test acc: 0.4285 - 16m 23s\n",
      "batch: 1563/1563 - train loss: 5.7418 - test loss: 15.4672 - train acc: 0.6953 - test acc: 0.4170 - 16m 27s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.8164 - test loss: 15.7914 - train acc: 0.7915 - test acc: 0.4187 - 16m 32s\n",
      "batch: 200/1563 - train loss: 3.7052 - test loss: 15.6007 - train acc: 0.7934 - test acc: 0.4333 - 16m 38s\n",
      "batch: 300/1563 - train loss: 3.6775 - test loss: 15.9107 - train acc: 0.7916 - test acc: 0.4220 - 16m 43s\n",
      "batch: 400/1563 - train loss: 3.8269 - test loss: 16.4239 - train acc: 0.7843 - test acc: 0.4179 - 16m 48s\n",
      "batch: 500/1563 - train loss: 3.8891 - test loss: 16.4351 - train acc: 0.7831 - test acc: 0.4244 - 16m 53s\n",
      "batch: 600/1563 - train loss: 4.3085 - test loss: 16.6473 - train acc: 0.7697 - test acc: 0.4177 - 16m 59s\n",
      "batch: 700/1563 - train loss: 4.1232 - test loss: 16.9093 - train acc: 0.7750 - test acc: 0.4143 - 17m 4s\n",
      "batch: 800/1563 - train loss: 4.5175 - test loss: 16.8896 - train acc: 0.7491 - test acc: 0.4090 - 17m 9s\n",
      "batch: 900/1563 - train loss: 4.4862 - test loss: 15.9714 - train acc: 0.7531 - test acc: 0.4286 - 17m 15s\n",
      "batch: 1000/1563 - train loss: 4.8073 - test loss: 16.0919 - train acc: 0.7438 - test acc: 0.4229 - 17m 20s\n",
      "batch: 1100/1563 - train loss: 4.6317 - test loss: 16.2124 - train acc: 0.7469 - test acc: 0.4201 - 17m 25s\n",
      "batch: 1200/1563 - train loss: 4.9376 - test loss: 16.2654 - train acc: 0.7256 - test acc: 0.4238 - 17m 30s\n",
      "batch: 1300/1563 - train loss: 5.0150 - test loss: 15.8454 - train acc: 0.7306 - test acc: 0.4157 - 17m 35s\n",
      "batch: 1400/1563 - train loss: 5.0364 - test loss: 16.4892 - train acc: 0.7325 - test acc: 0.4101 - 17m 40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 4.7647 - test loss: 15.9613 - train acc: 0.7457 - test acc: 0.4196 - 17m 46s\n",
      "batch: 1563/1563 - train loss: 5.0505 - test loss: 16.3047 - train acc: 0.7322 - test acc: 0.4164 - 17m 50s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 3.2857 - test loss: 15.9788 - train acc: 0.8171 - test acc: 0.4292 - 17m 55s\n",
      "batch: 200/1563 - train loss: 2.8709 - test loss: 16.0936 - train acc: 0.8343 - test acc: 0.4343 - 18m 0s\n",
      "batch: 300/1563 - train loss: 3.1815 - test loss: 16.9319 - train acc: 0.8274 - test acc: 0.4182 - 18m 6s\n",
      "batch: 400/1563 - train loss: 3.4779 - test loss: 16.7319 - train acc: 0.8103 - test acc: 0.4254 - 18m 11s\n",
      "batch: 500/1563 - train loss: 3.5223 - test loss: 16.5564 - train acc: 0.8022 - test acc: 0.4313 - 18m 16s\n",
      "batch: 600/1563 - train loss: 3.7369 - test loss: 17.6124 - train acc: 0.7859 - test acc: 0.4075 - 18m 21s\n",
      "batch: 700/1563 - train loss: 3.9966 - test loss: 16.4776 - train acc: 0.7831 - test acc: 0.4180 - 18m 27s\n",
      "batch: 800/1563 - train loss: 3.7120 - test loss: 16.7320 - train acc: 0.8012 - test acc: 0.4265 - 18m 32s\n",
      "batch: 900/1563 - train loss: 3.8699 - test loss: 17.3913 - train acc: 0.7853 - test acc: 0.4085 - 18m 37s\n",
      "batch: 1000/1563 - train loss: 3.9836 - test loss: 16.7751 - train acc: 0.7756 - test acc: 0.4196 - 18m 42s\n",
      "batch: 1100/1563 - train loss: 4.1380 - test loss: 17.0910 - train acc: 0.7659 - test acc: 0.4167 - 18m 48s\n",
      "batch: 1200/1563 - train loss: 4.2725 - test loss: 15.9240 - train acc: 0.7681 - test acc: 0.4303 - 18m 53s\n",
      "batch: 1300/1563 - train loss: 4.4257 - test loss: 16.7149 - train acc: 0.7503 - test acc: 0.4193 - 18m 58s\n",
      "batch: 1400/1563 - train loss: 4.5030 - test loss: 16.2813 - train acc: 0.7450 - test acc: 0.4326 - 19m 3s\n",
      "batch: 1500/1563 - train loss: 4.5659 - test loss: 16.3025 - train acc: 0.7559 - test acc: 0.4182 - 19m 8s\n",
      "batch: 1563/1563 - train loss: 4.4413 - test loss: 16.4567 - train acc: 0.7572 - test acc: 0.4257 - 19m 13s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.7556 - test loss: 16.5311 - train acc: 0.8409 - test acc: 0.4314 - 19m 18s\n",
      "batch: 200/1563 - train loss: 2.5136 - test loss: 17.1264 - train acc: 0.8575 - test acc: 0.4283 - 19m 23s\n",
      "batch: 300/1563 - train loss: 2.4376 - test loss: 17.0562 - train acc: 0.8634 - test acc: 0.4305 - 19m 29s\n",
      "batch: 400/1563 - train loss: 2.7810 - test loss: 17.3936 - train acc: 0.8487 - test acc: 0.4254 - 19m 34s\n",
      "batch: 500/1563 - train loss: 3.0266 - test loss: 17.3068 - train acc: 0.8311 - test acc: 0.4247 - 19m 39s\n",
      "batch: 600/1563 - train loss: 3.2123 - test loss: 17.4008 - train acc: 0.8215 - test acc: 0.4242 - 19m 44s\n",
      "batch: 700/1563 - train loss: 3.3627 - test loss: 17.0540 - train acc: 0.8086 - test acc: 0.4299 - 19m 50s\n",
      "batch: 800/1563 - train loss: 3.3498 - test loss: 17.5491 - train acc: 0.8093 - test acc: 0.4271 - 19m 55s\n",
      "time is up! finishing training\n",
      "batch: 879/1563 - train loss: 3.4444 - test loss: 17.3839 - train acc: 0.8047 - test acc: 0.4224 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 5\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 25.9909 - test loss: 24.9052 - train acc: 0.0417 - test acc: 0.0592 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.9464 - test loss: 22.8502 - train acc: 0.0724 - test acc: 0.0849 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.8454 - test loss: 22.8618 - train acc: 0.0958 - test acc: 0.1022 - 0m 12s\n",
      "batch: 400/1563 - train loss: 22.0760 - test loss: 21.6101 - train acc: 0.1035 - test acc: 0.1276 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.2669 - test loss: 21.2349 - train acc: 0.1281 - test acc: 0.1274 - 0m 23s\n",
      "batch: 600/1563 - train loss: 20.9635 - test loss: 21.4208 - train acc: 0.1378 - test acc: 0.1359 - 0m 28s\n",
      "batch: 700/1563 - train loss: 20.6769 - test loss: 20.8950 - train acc: 0.1479 - test acc: 0.1430 - 0m 34s\n",
      "batch: 800/1563 - train loss: 20.1113 - test loss: 20.0586 - train acc: 0.1538 - test acc: 0.1636 - 0m 39s\n",
      "batch: 900/1563 - train loss: 19.7185 - test loss: 19.4511 - train acc: 0.1735 - test acc: 0.1794 - 0m 44s\n",
      "batch: 1000/1563 - train loss: 19.8196 - test loss: 19.6847 - train acc: 0.1698 - test acc: 0.1795 - 0m 49s\n",
      "batch: 1100/1563 - train loss: 19.3552 - test loss: 19.3990 - train acc: 0.1767 - test acc: 0.1821 - 0m 55s\n",
      "batch: 1200/1563 - train loss: 18.9628 - test loss: 19.6667 - train acc: 0.1913 - test acc: 0.1726 - 1m 0s\n",
      "batch: 1300/1563 - train loss: 18.9550 - test loss: 18.5185 - train acc: 0.1945 - test acc: 0.2086 - 1m 5s\n",
      "batch: 1400/1563 - train loss: 18.6532 - test loss: 18.2352 - train acc: 0.2060 - test acc: 0.2184 - 1m 10s\n",
      "batch: 1500/1563 - train loss: 18.6032 - test loss: 19.9863 - train acc: 0.2069 - test acc: 0.1619 - 1m 16s\n",
      "batch: 1563/1563 - train loss: 18.3983 - test loss: 18.1365 - train acc: 0.2063 - test acc: 0.2232 - 1m 20s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.4773 - test loss: 17.7812 - train acc: 0.2344 - test acc: 0.2251 - 1m 25s\n",
      "batch: 200/1563 - train loss: 17.3626 - test loss: 17.8230 - train acc: 0.2462 - test acc: 0.2286 - 1m 30s\n",
      "batch: 300/1563 - train loss: 17.2855 - test loss: 18.0587 - train acc: 0.2497 - test acc: 0.2291 - 1m 36s\n",
      "batch: 400/1563 - train loss: 17.1410 - test loss: 18.0136 - train acc: 0.2398 - test acc: 0.2308 - 1m 41s\n",
      "batch: 500/1563 - train loss: 17.5285 - test loss: 17.4955 - train acc: 0.2350 - test acc: 0.2401 - 1m 46s\n",
      "batch: 600/1563 - train loss: 17.0206 - test loss: 17.0140 - train acc: 0.2606 - test acc: 0.2543 - 1m 52s\n",
      "batch: 700/1563 - train loss: 17.1122 - test loss: 16.7411 - train acc: 0.2509 - test acc: 0.2581 - 1m 57s\n",
      "batch: 800/1563 - train loss: 16.6859 - test loss: 16.9432 - train acc: 0.2647 - test acc: 0.2563 - 2m 2s\n",
      "batch: 900/1563 - train loss: 16.7544 - test loss: 16.5014 - train acc: 0.2628 - test acc: 0.2722 - 2m 7s\n",
      "batch: 1000/1563 - train loss: 16.6227 - test loss: 16.8079 - train acc: 0.2637 - test acc: 0.2549 - 2m 13s\n",
      "batch: 1100/1563 - train loss: 16.3948 - test loss: 16.2397 - train acc: 0.2672 - test acc: 0.2825 - 2m 18s\n",
      "batch: 1200/1563 - train loss: 16.0906 - test loss: 16.5717 - train acc: 0.2781 - test acc: 0.2745 - 2m 23s\n",
      "batch: 1300/1563 - train loss: 16.1007 - test loss: 16.1755 - train acc: 0.2762 - test acc: 0.2916 - 2m 28s\n",
      "batch: 1400/1563 - train loss: 15.7026 - test loss: 16.3329 - train acc: 0.3016 - test acc: 0.2795 - 2m 34s\n",
      "batch: 1500/1563 - train loss: 16.0121 - test loss: 16.3967 - train acc: 0.2959 - test acc: 0.2832 - 2m 39s\n",
      "batch: 1563/1563 - train loss: 16.2961 - test loss: 16.5589 - train acc: 0.2846 - test acc: 0.2855 - 2m 43s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.0907 - test loss: 16.0343 - train acc: 0.3284 - test acc: 0.2890 - 2m 49s\n",
      "batch: 200/1563 - train loss: 14.8183 - test loss: 15.8495 - train acc: 0.3391 - test acc: 0.3107 - 2m 54s\n",
      "batch: 300/1563 - train loss: 14.7092 - test loss: 16.8952 - train acc: 0.3256 - test acc: 0.2875 - 2m 59s\n",
      "batch: 400/1563 - train loss: 14.8291 - test loss: 15.5615 - train acc: 0.3219 - test acc: 0.3135 - 3m 5s\n",
      "batch: 500/1563 - train loss: 14.7509 - test loss: 15.3895 - train acc: 0.3356 - test acc: 0.3144 - 3m 10s\n",
      "batch: 600/1563 - train loss: 14.7437 - test loss: 15.3937 - train acc: 0.3331 - test acc: 0.3198 - 3m 15s\n",
      "batch: 700/1563 - train loss: 14.7040 - test loss: 15.5490 - train acc: 0.3316 - test acc: 0.3090 - 3m 21s\n",
      "batch: 800/1563 - train loss: 14.5267 - test loss: 15.4651 - train acc: 0.3397 - test acc: 0.3151 - 3m 26s\n",
      "batch: 900/1563 - train loss: 14.5913 - test loss: 15.2549 - train acc: 0.3321 - test acc: 0.3228 - 3m 31s\n",
      "batch: 1000/1563 - train loss: 14.3589 - test loss: 15.0340 - train acc: 0.3466 - test acc: 0.3306 - 3m 36s\n",
      "batch: 1100/1563 - train loss: 14.5039 - test loss: 14.7445 - train acc: 0.3391 - test acc: 0.3341 - 3m 42s\n",
      "batch: 1200/1563 - train loss: 14.4370 - test loss: 15.5524 - train acc: 0.3431 - test acc: 0.3129 - 3m 47s\n",
      "batch: 1300/1563 - train loss: 13.9601 - test loss: 14.4969 - train acc: 0.3619 - test acc: 0.3449 - 3m 52s\n",
      "batch: 1400/1563 - train loss: 14.7197 - test loss: 14.7850 - train acc: 0.3256 - test acc: 0.3380 - 3m 58s\n",
      "batch: 1500/1563 - train loss: 14.0144 - test loss: 15.7655 - train acc: 0.3691 - test acc: 0.3161 - 4m 3s\n",
      "batch: 1563/1563 - train loss: 14.0259 - test loss: 15.0868 - train acc: 0.3635 - test acc: 0.3300 - 4m 7s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.7994 - test loss: 14.3682 - train acc: 0.4000 - test acc: 0.3520 - 4m 13s\n",
      "batch: 200/1563 - train loss: 12.6542 - test loss: 14.8392 - train acc: 0.4153 - test acc: 0.3391 - 4m 18s\n",
      "batch: 300/1563 - train loss: 13.1543 - test loss: 14.7147 - train acc: 0.3932 - test acc: 0.3456 - 4m 23s\n",
      "batch: 400/1563 - train loss: 13.0079 - test loss: 14.5922 - train acc: 0.3950 - test acc: 0.3500 - 4m 28s\n",
      "batch: 500/1563 - train loss: 12.9113 - test loss: 15.3475 - train acc: 0.3991 - test acc: 0.3300 - 4m 34s\n",
      "batch: 600/1563 - train loss: 13.2358 - test loss: 14.4062 - train acc: 0.3825 - test acc: 0.3566 - 4m 39s\n",
      "batch: 700/1563 - train loss: 13.4224 - test loss: 14.2429 - train acc: 0.3872 - test acc: 0.3578 - 4m 44s\n",
      "batch: 800/1563 - train loss: 12.9906 - test loss: 14.0051 - train acc: 0.3891 - test acc: 0.3652 - 4m 50s\n",
      "batch: 900/1563 - train loss: 12.9664 - test loss: 16.6660 - train acc: 0.3997 - test acc: 0.3015 - 4m 55s\n",
      "batch: 1000/1563 - train loss: 12.7625 - test loss: 14.3444 - train acc: 0.4100 - test acc: 0.3601 - 5m 0s\n",
      "batch: 1100/1563 - train loss: 13.0853 - test loss: 15.0749 - train acc: 0.3945 - test acc: 0.3341 - 5m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1200/1563 - train loss: 12.9872 - test loss: 14.3991 - train acc: 0.3957 - test acc: 0.3509 - 5m 11s\n",
      "batch: 1300/1563 - train loss: 12.9784 - test loss: 14.5948 - train acc: 0.3897 - test acc: 0.3469 - 5m 16s\n",
      "batch: 1400/1563 - train loss: 13.2389 - test loss: 13.9166 - train acc: 0.3922 - test acc: 0.3715 - 5m 22s\n",
      "batch: 1500/1563 - train loss: 12.9260 - test loss: 14.7977 - train acc: 0.3950 - test acc: 0.3486 - 5m 27s\n",
      "batch: 1563/1563 - train loss: 12.8267 - test loss: 14.0835 - train acc: 0.3953 - test acc: 0.3665 - 5m 31s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.1011 - test loss: 13.4306 - train acc: 0.4713 - test acc: 0.3963 - 5m 36s\n",
      "batch: 200/1563 - train loss: 11.5652 - test loss: 13.7528 - train acc: 0.4503 - test acc: 0.3847 - 5m 42s\n",
      "batch: 300/1563 - train loss: 11.7894 - test loss: 15.2816 - train acc: 0.4391 - test acc: 0.3456 - 5m 47s\n",
      "batch: 400/1563 - train loss: 11.3288 - test loss: 14.2680 - train acc: 0.4660 - test acc: 0.3690 - 5m 53s\n",
      "batch: 500/1563 - train loss: 11.6143 - test loss: 13.8038 - train acc: 0.4526 - test acc: 0.3910 - 5m 58s\n",
      "batch: 600/1563 - train loss: 11.5543 - test loss: 13.6800 - train acc: 0.4456 - test acc: 0.3903 - 6m 3s\n",
      "batch: 700/1563 - train loss: 11.8084 - test loss: 14.0592 - train acc: 0.4503 - test acc: 0.3721 - 6m 8s\n",
      "batch: 800/1563 - train loss: 11.6859 - test loss: 13.6350 - train acc: 0.4553 - test acc: 0.3845 - 6m 14s\n",
      "batch: 900/1563 - train loss: 11.8831 - test loss: 13.8613 - train acc: 0.4294 - test acc: 0.3836 - 6m 19s\n",
      "batch: 1000/1563 - train loss: 11.4956 - test loss: 14.2009 - train acc: 0.4500 - test acc: 0.3740 - 6m 24s\n",
      "batch: 1100/1563 - train loss: 11.9022 - test loss: 13.9235 - train acc: 0.4397 - test acc: 0.3743 - 6m 30s\n",
      "batch: 1200/1563 - train loss: 11.8304 - test loss: 15.5751 - train acc: 0.4437 - test acc: 0.3371 - 6m 35s\n",
      "batch: 1300/1563 - train loss: 11.9661 - test loss: 13.7591 - train acc: 0.4297 - test acc: 0.3787 - 6m 40s\n",
      "batch: 1400/1563 - train loss: 11.8769 - test loss: 13.9416 - train acc: 0.4378 - test acc: 0.3718 - 6m 46s\n",
      "batch: 1500/1563 - train loss: 11.6611 - test loss: 13.4254 - train acc: 0.4472 - test acc: 0.3972 - 6m 51s\n",
      "batch: 1563/1563 - train loss: 11.6914 - test loss: 13.4077 - train acc: 0.4497 - test acc: 0.4013 - 6m 56s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.6573 - test loss: 14.1839 - train acc: 0.5309 - test acc: 0.3765 - 7m 1s\n",
      "batch: 200/1563 - train loss: 9.9166 - test loss: 13.7799 - train acc: 0.5125 - test acc: 0.3941 - 7m 6s\n",
      "batch: 300/1563 - train loss: 10.2978 - test loss: 14.3048 - train acc: 0.5047 - test acc: 0.3784 - 7m 11s\n",
      "batch: 400/1563 - train loss: 10.2964 - test loss: 13.7555 - train acc: 0.5012 - test acc: 0.3884 - 7m 17s\n",
      "batch: 500/1563 - train loss: 10.2162 - test loss: 13.6807 - train acc: 0.5038 - test acc: 0.3951 - 7m 22s\n",
      "batch: 600/1563 - train loss: 10.6412 - test loss: 14.1689 - train acc: 0.4835 - test acc: 0.3818 - 7m 27s\n",
      "batch: 700/1563 - train loss: 10.5643 - test loss: 14.2656 - train acc: 0.4953 - test acc: 0.3768 - 7m 33s\n",
      "batch: 800/1563 - train loss: 10.8449 - test loss: 13.7237 - train acc: 0.4746 - test acc: 0.3876 - 7m 38s\n",
      "batch: 900/1563 - train loss: 10.5636 - test loss: 15.5352 - train acc: 0.4915 - test acc: 0.3414 - 7m 44s\n",
      "batch: 1000/1563 - train loss: 10.8504 - test loss: 13.3652 - train acc: 0.4787 - test acc: 0.4014 - 7m 49s\n",
      "batch: 1100/1563 - train loss: 10.9860 - test loss: 14.0697 - train acc: 0.4788 - test acc: 0.3807 - 7m 54s\n",
      "batch: 1200/1563 - train loss: 10.7319 - test loss: 14.8282 - train acc: 0.4797 - test acc: 0.3631 - 8m 0s\n",
      "batch: 1300/1563 - train loss: 10.6581 - test loss: 13.1658 - train acc: 0.4906 - test acc: 0.4135 - 8m 5s\n",
      "batch: 1400/1563 - train loss: 10.6957 - test loss: 18.3321 - train acc: 0.4885 - test acc: 0.3028 - 8m 10s\n",
      "batch: 1500/1563 - train loss: 10.6690 - test loss: 13.3574 - train acc: 0.4968 - test acc: 0.4017 - 8m 15s\n",
      "batch: 1563/1563 - train loss: 11.1092 - test loss: 13.5677 - train acc: 0.4813 - test acc: 0.3995 - 8m 20s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.6484 - test loss: 13.4769 - train acc: 0.5778 - test acc: 0.4073 - 8m 25s\n",
      "batch: 200/1563 - train loss: 8.6817 - test loss: 13.8858 - train acc: 0.5681 - test acc: 0.4005 - 8m 31s\n",
      "batch: 300/1563 - train loss: 8.8394 - test loss: 13.8826 - train acc: 0.5643 - test acc: 0.4070 - 8m 36s\n",
      "batch: 400/1563 - train loss: 9.0813 - test loss: 16.0084 - train acc: 0.5490 - test acc: 0.3594 - 8m 41s\n",
      "batch: 500/1563 - train loss: 9.1943 - test loss: 13.5957 - train acc: 0.5478 - test acc: 0.4010 - 8m 46s\n",
      "batch: 600/1563 - train loss: 9.3489 - test loss: 14.1017 - train acc: 0.5487 - test acc: 0.3895 - 8m 51s\n",
      "batch: 700/1563 - train loss: 9.0726 - test loss: 13.7379 - train acc: 0.5524 - test acc: 0.4084 - 8m 57s\n",
      "batch: 800/1563 - train loss: 9.9156 - test loss: 13.5215 - train acc: 0.5203 - test acc: 0.4006 - 9m 2s\n",
      "batch: 900/1563 - train loss: 9.7942 - test loss: 13.7512 - train acc: 0.5172 - test acc: 0.4024 - 9m 8s\n",
      "batch: 1000/1563 - train loss: 9.7299 - test loss: 13.9465 - train acc: 0.5175 - test acc: 0.3910 - 9m 13s\n",
      "batch: 1100/1563 - train loss: 9.9181 - test loss: 14.4073 - train acc: 0.5128 - test acc: 0.3801 - 9m 18s\n",
      "batch: 1200/1563 - train loss: 9.7747 - test loss: 13.9036 - train acc: 0.5196 - test acc: 0.3961 - 9m 23s\n",
      "batch: 1300/1563 - train loss: 9.7145 - test loss: 13.8744 - train acc: 0.5296 - test acc: 0.3964 - 9m 29s\n",
      "batch: 1400/1563 - train loss: 9.9653 - test loss: 13.3351 - train acc: 0.5119 - test acc: 0.4111 - 9m 35s\n",
      "batch: 1500/1563 - train loss: 9.9763 - test loss: 13.5402 - train acc: 0.5087 - test acc: 0.4035 - 9m 40s\n",
      "batch: 1563/1563 - train loss: 10.2097 - test loss: 13.5197 - train acc: 0.4938 - test acc: 0.4099 - 9m 45s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.5694 - test loss: 14.0255 - train acc: 0.6076 - test acc: 0.4047 - 9m 50s\n",
      "batch: 200/1563 - train loss: 7.6098 - test loss: 14.9863 - train acc: 0.6072 - test acc: 0.3900 - 9m 56s\n",
      "batch: 300/1563 - train loss: 7.7892 - test loss: 13.8997 - train acc: 0.6012 - test acc: 0.4122 - 10m 1s\n",
      "batch: 400/1563 - train loss: 7.8880 - test loss: 14.7660 - train acc: 0.6028 - test acc: 0.3950 - 10m 7s\n",
      "batch: 500/1563 - train loss: 7.8693 - test loss: 13.7916 - train acc: 0.6056 - test acc: 0.4128 - 10m 12s\n",
      "batch: 600/1563 - train loss: 8.3230 - test loss: 14.2497 - train acc: 0.5928 - test acc: 0.4105 - 10m 17s\n",
      "batch: 700/1563 - train loss: 8.6112 - test loss: 14.3461 - train acc: 0.5722 - test acc: 0.4063 - 10m 22s\n",
      "batch: 800/1563 - train loss: 8.3745 - test loss: 14.1834 - train acc: 0.5756 - test acc: 0.4032 - 10m 27s\n",
      "batch: 900/1563 - train loss: 8.6623 - test loss: 13.7259 - train acc: 0.5706 - test acc: 0.4135 - 10m 33s\n",
      "batch: 1000/1563 - train loss: 8.4979 - test loss: 14.0692 - train acc: 0.5853 - test acc: 0.4054 - 10m 39s\n",
      "batch: 1100/1563 - train loss: 8.8070 - test loss: 14.2388 - train acc: 0.5587 - test acc: 0.4075 - 10m 44s\n",
      "batch: 1200/1563 - train loss: 9.0271 - test loss: 13.5904 - train acc: 0.5568 - test acc: 0.4159 - 10m 49s\n",
      "batch: 1300/1563 - train loss: 8.7729 - test loss: 13.8524 - train acc: 0.5669 - test acc: 0.4104 - 10m 54s\n",
      "batch: 1400/1563 - train loss: 9.1207 - test loss: 13.8491 - train acc: 0.5444 - test acc: 0.4050 - 10m 59s\n",
      "batch: 1500/1563 - train loss: 9.0231 - test loss: 13.1145 - train acc: 0.5503 - test acc: 0.4270 - 11m 5s\n",
      "batch: 1563/1563 - train loss: 8.9602 - test loss: 13.7182 - train acc: 0.5582 - test acc: 0.4118 - 11m 9s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4292 - test loss: 14.4203 - train acc: 0.6659 - test acc: 0.4069 - 11m 15s\n",
      "batch: 200/1563 - train loss: 6.5846 - test loss: 14.8895 - train acc: 0.6653 - test acc: 0.4055 - 11m 20s\n",
      "batch: 300/1563 - train loss: 6.8781 - test loss: 14.1477 - train acc: 0.6491 - test acc: 0.4144 - 11m 25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 400/1563 - train loss: 7.0789 - test loss: 14.8660 - train acc: 0.6266 - test acc: 0.4028 - 11m 30s\n",
      "batch: 500/1563 - train loss: 7.0235 - test loss: 14.3760 - train acc: 0.6460 - test acc: 0.4084 - 11m 36s\n",
      "batch: 600/1563 - train loss: 7.0557 - test loss: 14.7094 - train acc: 0.6329 - test acc: 0.4136 - 11m 41s\n",
      "batch: 700/1563 - train loss: 7.3614 - test loss: 14.2556 - train acc: 0.6235 - test acc: 0.4153 - 11m 46s\n",
      "batch: 800/1563 - train loss: 7.6283 - test loss: 14.0398 - train acc: 0.6141 - test acc: 0.4197 - 11m 52s\n",
      "batch: 900/1563 - train loss: 7.7456 - test loss: 14.9997 - train acc: 0.6050 - test acc: 0.3893 - 11m 57s\n",
      "batch: 1000/1563 - train loss: 7.6848 - test loss: 14.2100 - train acc: 0.6153 - test acc: 0.4216 - 12m 2s\n",
      "batch: 1100/1563 - train loss: 8.0996 - test loss: 14.2839 - train acc: 0.5871 - test acc: 0.4076 - 12m 8s\n",
      "batch: 1200/1563 - train loss: 7.6084 - test loss: 14.6899 - train acc: 0.6131 - test acc: 0.4019 - 12m 13s\n",
      "batch: 1300/1563 - train loss: 7.8999 - test loss: 14.3869 - train acc: 0.6113 - test acc: 0.4027 - 12m 18s\n",
      "batch: 1400/1563 - train loss: 8.0778 - test loss: 14.0831 - train acc: 0.5916 - test acc: 0.4179 - 12m 23s\n",
      "batch: 1500/1563 - train loss: 7.8701 - test loss: 14.1747 - train acc: 0.5984 - test acc: 0.4206 - 12m 29s\n",
      "batch: 1563/1563 - train loss: 8.0031 - test loss: 14.1847 - train acc: 0.5981 - test acc: 0.4177 - 12m 33s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.5285 - test loss: 14.4638 - train acc: 0.7066 - test acc: 0.4192 - 12m 38s\n",
      "batch: 200/1563 - train loss: 5.4893 - test loss: 14.5676 - train acc: 0.7044 - test acc: 0.4226 - 12m 44s\n",
      "batch: 300/1563 - train loss: 5.8661 - test loss: 14.8046 - train acc: 0.6913 - test acc: 0.4226 - 12m 49s\n",
      "batch: 400/1563 - train loss: 6.1804 - test loss: 14.6736 - train acc: 0.6869 - test acc: 0.4178 - 12m 54s\n",
      "batch: 500/1563 - train loss: 6.0220 - test loss: 16.0578 - train acc: 0.6819 - test acc: 0.3875 - 13m 0s\n",
      "batch: 600/1563 - train loss: 6.6440 - test loss: 14.4418 - train acc: 0.6463 - test acc: 0.4229 - 13m 5s\n",
      "batch: 700/1563 - train loss: 6.6230 - test loss: 14.4778 - train acc: 0.6578 - test acc: 0.4223 - 13m 10s\n",
      "batch: 800/1563 - train loss: 6.5316 - test loss: 14.8423 - train acc: 0.6541 - test acc: 0.4085 - 13m 16s\n",
      "batch: 900/1563 - train loss: 6.6381 - test loss: 14.6040 - train acc: 0.6494 - test acc: 0.4253 - 13m 21s\n",
      "batch: 1000/1563 - train loss: 6.7949 - test loss: 14.4902 - train acc: 0.6425 - test acc: 0.4178 - 13m 26s\n",
      "batch: 1100/1563 - train loss: 6.7676 - test loss: 14.6231 - train acc: 0.6490 - test acc: 0.4198 - 13m 31s\n",
      "batch: 1200/1563 - train loss: 6.9689 - test loss: 14.9260 - train acc: 0.6504 - test acc: 0.4119 - 13m 37s\n",
      "batch: 1300/1563 - train loss: 6.9726 - test loss: 14.8140 - train acc: 0.6372 - test acc: 0.4096 - 13m 42s\n",
      "batch: 1400/1563 - train loss: 6.7965 - test loss: 14.6050 - train acc: 0.6519 - test acc: 0.4140 - 13m 48s\n",
      "batch: 1500/1563 - train loss: 7.1749 - test loss: 14.1524 - train acc: 0.6269 - test acc: 0.4261 - 13m 53s\n",
      "batch: 1563/1563 - train loss: 7.0398 - test loss: 14.1595 - train acc: 0.6406 - test acc: 0.4215 - 13m 57s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.9982 - test loss: 14.5033 - train acc: 0.7378 - test acc: 0.4289 - 14m 2s\n",
      "batch: 200/1563 - train loss: 4.6896 - test loss: 15.3591 - train acc: 0.7535 - test acc: 0.4233 - 14m 8s\n",
      "batch: 300/1563 - train loss: 5.0364 - test loss: 16.7673 - train acc: 0.7322 - test acc: 0.3874 - 14m 13s\n",
      "batch: 400/1563 - train loss: 4.8289 - test loss: 15.0510 - train acc: 0.7477 - test acc: 0.4244 - 14m 19s\n",
      "batch: 500/1563 - train loss: 5.1661 - test loss: 15.5643 - train acc: 0.7206 - test acc: 0.4202 - 14m 24s\n",
      "batch: 600/1563 - train loss: 5.4280 - test loss: 14.9813 - train acc: 0.7175 - test acc: 0.4249 - 14m 29s\n",
      "batch: 700/1563 - train loss: 5.6313 - test loss: 15.3589 - train acc: 0.6957 - test acc: 0.4166 - 14m 34s\n",
      "batch: 800/1563 - train loss: 5.7832 - test loss: 15.5185 - train acc: 0.7000 - test acc: 0.4120 - 14m 40s\n",
      "batch: 900/1563 - train loss: 5.6989 - test loss: 15.6893 - train acc: 0.6934 - test acc: 0.4127 - 14m 45s\n",
      "batch: 1000/1563 - train loss: 5.9870 - test loss: 14.7538 - train acc: 0.6803 - test acc: 0.4270 - 14m 51s\n",
      "batch: 1100/1563 - train loss: 6.3732 - test loss: 14.3697 - train acc: 0.6694 - test acc: 0.4305 - 14m 56s\n",
      "batch: 1200/1563 - train loss: 6.0712 - test loss: 14.6540 - train acc: 0.6859 - test acc: 0.4269 - 15m 1s\n",
      "batch: 1300/1563 - train loss: 6.2120 - test loss: 14.6740 - train acc: 0.6672 - test acc: 0.4259 - 15m 6s\n",
      "batch: 1400/1563 - train loss: 6.2416 - test loss: 15.8034 - train acc: 0.6644 - test acc: 0.3973 - 15m 12s\n",
      "batch: 1500/1563 - train loss: 6.5607 - test loss: 14.8677 - train acc: 0.6538 - test acc: 0.4181 - 15m 17s\n",
      "batch: 1563/1563 - train loss: 6.5886 - test loss: 14.3076 - train acc: 0.6519 - test acc: 0.4361 - 15m 21s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.8465 - test loss: 14.9925 - train acc: 0.7971 - test acc: 0.4312 - 15m 27s\n",
      "batch: 200/1563 - train loss: 3.8082 - test loss: 15.3703 - train acc: 0.7919 - test acc: 0.4266 - 15m 32s\n",
      "batch: 300/1563 - train loss: 4.0759 - test loss: 15.3173 - train acc: 0.7765 - test acc: 0.4256 - 15m 37s\n",
      "batch: 400/1563 - train loss: 4.0795 - test loss: 16.2346 - train acc: 0.7728 - test acc: 0.4175 - 15m 43s\n",
      "batch: 500/1563 - train loss: 4.3248 - test loss: 15.1535 - train acc: 0.7662 - test acc: 0.4372 - 15m 48s\n",
      "batch: 600/1563 - train loss: 4.5883 - test loss: 15.9394 - train acc: 0.7444 - test acc: 0.4138 - 15m 53s\n",
      "batch: 700/1563 - train loss: 5.0733 - test loss: 15.4923 - train acc: 0.7294 - test acc: 0.4208 - 15m 59s\n",
      "batch: 800/1563 - train loss: 4.7766 - test loss: 15.5479 - train acc: 0.7429 - test acc: 0.4285 - 16m 4s\n",
      "batch: 900/1563 - train loss: 5.2188 - test loss: 15.1601 - train acc: 0.7290 - test acc: 0.4346 - 16m 9s\n",
      "batch: 1000/1563 - train loss: 4.9630 - test loss: 15.2042 - train acc: 0.7278 - test acc: 0.4377 - 16m 14s\n",
      "batch: 1100/1563 - train loss: 5.1540 - test loss: 16.1239 - train acc: 0.7234 - test acc: 0.4198 - 16m 20s\n",
      "batch: 1200/1563 - train loss: 5.3555 - test loss: 15.0422 - train acc: 0.7213 - test acc: 0.4319 - 16m 26s\n",
      "batch: 1300/1563 - train loss: 5.6042 - test loss: 16.0137 - train acc: 0.6991 - test acc: 0.4088 - 16m 31s\n",
      "batch: 1400/1563 - train loss: 5.8114 - test loss: 15.2197 - train acc: 0.6831 - test acc: 0.4290 - 16m 36s\n",
      "batch: 1500/1563 - train loss: 5.8713 - test loss: 15.3194 - train acc: 0.6897 - test acc: 0.4308 - 16m 42s\n",
      "batch: 1563/1563 - train loss: 5.6142 - test loss: 15.1436 - train acc: 0.7003 - test acc: 0.4331 - 16m 46s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.3077 - test loss: 15.4497 - train acc: 0.8087 - test acc: 0.4369 - 16m 51s\n",
      "batch: 200/1563 - train loss: 3.2796 - test loss: 16.0413 - train acc: 0.8206 - test acc: 0.4240 - 16m 57s\n",
      "batch: 300/1563 - train loss: 3.3664 - test loss: 15.9719 - train acc: 0.8139 - test acc: 0.4316 - 17m 2s\n",
      "batch: 400/1563 - train loss: 3.6393 - test loss: 16.4075 - train acc: 0.8012 - test acc: 0.4260 - 17m 7s\n",
      "batch: 500/1563 - train loss: 3.7965 - test loss: 16.7998 - train acc: 0.7931 - test acc: 0.4157 - 17m 12s\n",
      "batch: 600/1563 - train loss: 3.8747 - test loss: 16.3422 - train acc: 0.7800 - test acc: 0.4214 - 17m 18s\n",
      "batch: 700/1563 - train loss: 4.1345 - test loss: 16.6616 - train acc: 0.7790 - test acc: 0.4246 - 17m 23s\n",
      "batch: 800/1563 - train loss: 4.2918 - test loss: 15.7790 - train acc: 0.7587 - test acc: 0.4261 - 17m 28s\n",
      "batch: 900/1563 - train loss: 4.0584 - test loss: 16.1648 - train acc: 0.7777 - test acc: 0.4259 - 17m 34s\n",
      "batch: 1000/1563 - train loss: 4.3520 - test loss: 16.2896 - train acc: 0.7693 - test acc: 0.4200 - 17m 39s\n",
      "batch: 1100/1563 - train loss: 4.7866 - test loss: 16.3947 - train acc: 0.7460 - test acc: 0.4235 - 17m 44s\n",
      "batch: 1200/1563 - train loss: 4.6985 - test loss: 16.3685 - train acc: 0.7444 - test acc: 0.4172 - 17m 50s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1300/1563 - train loss: 4.5941 - test loss: 15.9329 - train acc: 0.7453 - test acc: 0.4262 - 17m 55s\n",
      "batch: 1400/1563 - train loss: 4.5010 - test loss: 16.5374 - train acc: 0.7543 - test acc: 0.4185 - 18m 0s\n",
      "batch: 1500/1563 - train loss: 5.0126 - test loss: 15.6656 - train acc: 0.7310 - test acc: 0.4233 - 18m 6s\n",
      "batch: 1563/1563 - train loss: 4.8839 - test loss: 16.2688 - train acc: 0.7331 - test acc: 0.4199 - 18m 10s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.9823 - test loss: 15.9853 - train acc: 0.8346 - test acc: 0.4333 - 18m 15s\n",
      "batch: 200/1563 - train loss: 2.8312 - test loss: 16.6880 - train acc: 0.8437 - test acc: 0.4188 - 18m 21s\n",
      "batch: 300/1563 - train loss: 2.8277 - test loss: 16.7625 - train acc: 0.8440 - test acc: 0.4305 - 18m 26s\n",
      "batch: 400/1563 - train loss: 3.0835 - test loss: 17.3267 - train acc: 0.8325 - test acc: 0.4173 - 18m 31s\n",
      "batch: 500/1563 - train loss: 3.3255 - test loss: 16.6338 - train acc: 0.8168 - test acc: 0.4279 - 18m 37s\n",
      "batch: 600/1563 - train loss: 3.1353 - test loss: 17.2933 - train acc: 0.8218 - test acc: 0.4262 - 18m 42s\n",
      "batch: 700/1563 - train loss: 3.5110 - test loss: 17.2406 - train acc: 0.8100 - test acc: 0.4185 - 18m 47s\n",
      "batch: 800/1563 - train loss: 3.6880 - test loss: 16.5946 - train acc: 0.7914 - test acc: 0.4266 - 18m 53s\n",
      "batch: 900/1563 - train loss: 3.7653 - test loss: 17.6630 - train acc: 0.7862 - test acc: 0.4112 - 18m 58s\n",
      "batch: 1000/1563 - train loss: 3.8317 - test loss: 16.5375 - train acc: 0.7959 - test acc: 0.4255 - 19m 4s\n",
      "batch: 1100/1563 - train loss: 3.8629 - test loss: 16.6754 - train acc: 0.7768 - test acc: 0.4239 - 19m 9s\n",
      "batch: 1200/1563 - train loss: 3.9292 - test loss: 16.7311 - train acc: 0.7881 - test acc: 0.4302 - 19m 14s\n",
      "batch: 1300/1563 - train loss: 3.8452 - test loss: 16.9855 - train acc: 0.7872 - test acc: 0.4289 - 19m 19s\n",
      "batch: 1400/1563 - train loss: 4.1135 - test loss: 16.8397 - train acc: 0.7684 - test acc: 0.4255 - 19m 24s\n",
      "batch: 1500/1563 - train loss: 4.0255 - test loss: 17.0949 - train acc: 0.7787 - test acc: 0.4119 - 19m 30s\n",
      "batch: 1563/1563 - train loss: 4.2505 - test loss: 17.1462 - train acc: 0.7637 - test acc: 0.4242 - 19m 35s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.3145 - test loss: 16.6931 - train acc: 0.8679 - test acc: 0.4402 - 19m 40s\n",
      "batch: 200/1563 - train loss: 2.2183 - test loss: 16.5668 - train acc: 0.8778 - test acc: 0.4465 - 19m 46s\n",
      "batch: 300/1563 - train loss: 2.3936 - test loss: 16.8679 - train acc: 0.8597 - test acc: 0.4418 - 19m 51s\n",
      "batch: 400/1563 - train loss: 2.4707 - test loss: 17.6903 - train acc: 0.8575 - test acc: 0.4266 - 19m 56s\n",
      "time is up! finishing training\n",
      "batch: 452/1563 - train loss: 2.6066 - test loss: 17.4888 - train acc: 0.8490 - test acc: 0.4268 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 6\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.2120 - test loss: 24.3706 - train acc: 0.0395 - test acc: 0.0597 - 0m 2s\n",
      "batch: 200/1563 - train loss: 24.0365 - test loss: 23.0258 - train acc: 0.0651 - test acc: 0.0836 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.8261 - test loss: 22.4957 - train acc: 0.0862 - test acc: 0.0972 - 0m 13s\n",
      "batch: 400/1563 - train loss: 22.2656 - test loss: 21.7725 - train acc: 0.1046 - test acc: 0.1109 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.9355 - test loss: 21.2528 - train acc: 0.1099 - test acc: 0.1242 - 0m 24s\n",
      "batch: 600/1563 - train loss: 21.0199 - test loss: 21.8663 - train acc: 0.1294 - test acc: 0.1243 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.9294 - test loss: 20.9590 - train acc: 0.1429 - test acc: 0.1301 - 0m 34s\n",
      "batch: 800/1563 - train loss: 20.6728 - test loss: 21.5897 - train acc: 0.1516 - test acc: 0.1386 - 0m 39s\n",
      "batch: 900/1563 - train loss: 20.1344 - test loss: 20.7547 - train acc: 0.1570 - test acc: 0.1548 - 0m 44s\n",
      "batch: 1000/1563 - train loss: 20.0443 - test loss: 19.3575 - train acc: 0.1585 - test acc: 0.1814 - 0m 50s\n",
      "batch: 1100/1563 - train loss: 19.6345 - test loss: 19.8368 - train acc: 0.1744 - test acc: 0.1769 - 0m 56s\n",
      "batch: 1200/1563 - train loss: 19.7311 - test loss: 18.7022 - train acc: 0.1691 - test acc: 0.1973 - 1m 1s\n",
      "batch: 1300/1563 - train loss: 19.1072 - test loss: 20.1567 - train acc: 0.1900 - test acc: 0.1663 - 1m 6s\n",
      "batch: 1400/1563 - train loss: 18.7667 - test loss: 19.4728 - train acc: 0.1963 - test acc: 0.1883 - 1m 12s\n",
      "batch: 1500/1563 - train loss: 18.4989 - test loss: 19.7358 - train acc: 0.2119 - test acc: 0.1883 - 1m 17s\n",
      "batch: 1563/1563 - train loss: 18.6171 - test loss: 18.6308 - train acc: 0.2020 - test acc: 0.2092 - 1m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.7299 - test loss: 18.4791 - train acc: 0.2256 - test acc: 0.2112 - 1m 27s\n",
      "batch: 200/1563 - train loss: 17.4837 - test loss: 17.8862 - train acc: 0.2363 - test acc: 0.2327 - 1m 32s\n",
      "batch: 300/1563 - train loss: 17.8874 - test loss: 17.9394 - train acc: 0.2263 - test acc: 0.2286 - 1m 37s\n",
      "batch: 400/1563 - train loss: 17.2167 - test loss: 17.6401 - train acc: 0.2507 - test acc: 0.2332 - 1m 43s\n",
      "batch: 500/1563 - train loss: 17.4905 - test loss: 20.2156 - train acc: 0.2406 - test acc: 0.1778 - 1m 48s\n",
      "batch: 600/1563 - train loss: 17.4920 - test loss: 17.5271 - train acc: 0.2457 - test acc: 0.2439 - 1m 53s\n",
      "batch: 700/1563 - train loss: 16.9912 - test loss: 17.9436 - train acc: 0.2550 - test acc: 0.2310 - 1m 59s\n",
      "batch: 800/1563 - train loss: 17.0932 - test loss: 17.3890 - train acc: 0.2528 - test acc: 0.2547 - 2m 4s\n",
      "batch: 900/1563 - train loss: 16.9735 - test loss: 16.7982 - train acc: 0.2616 - test acc: 0.2631 - 2m 9s\n",
      "batch: 1000/1563 - train loss: 16.5498 - test loss: 16.5995 - train acc: 0.2635 - test acc: 0.2662 - 2m 14s\n",
      "batch: 1100/1563 - train loss: 16.7585 - test loss: 17.0598 - train acc: 0.2637 - test acc: 0.2598 - 2m 19s\n",
      "batch: 1200/1563 - train loss: 16.2581 - test loss: 16.1474 - train acc: 0.2769 - test acc: 0.2899 - 2m 25s\n",
      "batch: 1300/1563 - train loss: 15.9209 - test loss: 16.6778 - train acc: 0.2918 - test acc: 0.2735 - 2m 30s\n",
      "batch: 1400/1563 - train loss: 16.1951 - test loss: 16.4006 - train acc: 0.2853 - test acc: 0.2776 - 2m 36s\n",
      "batch: 1500/1563 - train loss: 16.2281 - test loss: 15.9852 - train acc: 0.2779 - test acc: 0.2891 - 2m 41s\n",
      "batch: 1563/1563 - train loss: 15.9553 - test loss: 17.0630 - train acc: 0.2972 - test acc: 0.2601 - 2m 45s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.1301 - test loss: 15.7796 - train acc: 0.3125 - test acc: 0.2984 - 2m 50s\n",
      "batch: 200/1563 - train loss: 14.8061 - test loss: 16.3886 - train acc: 0.3337 - test acc: 0.2804 - 2m 55s\n",
      "batch: 300/1563 - train loss: 14.7359 - test loss: 16.1787 - train acc: 0.3322 - test acc: 0.2902 - 3m 1s\n",
      "batch: 400/1563 - train loss: 14.9221 - test loss: 15.7388 - train acc: 0.3262 - test acc: 0.3052 - 3m 6s\n",
      "batch: 500/1563 - train loss: 14.9809 - test loss: 15.3542 - train acc: 0.3203 - test acc: 0.3130 - 3m 11s\n",
      "batch: 600/1563 - train loss: 14.7147 - test loss: 16.6610 - train acc: 0.3272 - test acc: 0.2707 - 3m 17s\n",
      "batch: 700/1563 - train loss: 14.7682 - test loss: 16.2215 - train acc: 0.3256 - test acc: 0.2907 - 3m 22s\n",
      "batch: 800/1563 - train loss: 14.6791 - test loss: 15.7786 - train acc: 0.3315 - test acc: 0.2956 - 3m 27s\n",
      "batch: 900/1563 - train loss: 15.0430 - test loss: 15.8250 - train acc: 0.3322 - test acc: 0.3031 - 3m 33s\n",
      "batch: 1000/1563 - train loss: 14.8087 - test loss: 14.8179 - train acc: 0.3335 - test acc: 0.3311 - 3m 38s\n",
      "batch: 1100/1563 - train loss: 14.5126 - test loss: 16.2702 - train acc: 0.3484 - test acc: 0.3059 - 3m 43s\n",
      "batch: 1200/1563 - train loss: 14.2402 - test loss: 15.2094 - train acc: 0.3519 - test acc: 0.3221 - 3m 48s\n",
      "batch: 1300/1563 - train loss: 13.9985 - test loss: 14.9181 - train acc: 0.3731 - test acc: 0.3383 - 3m 53s\n",
      "batch: 1400/1563 - train loss: 14.4057 - test loss: 15.0289 - train acc: 0.3481 - test acc: 0.3280 - 3m 59s\n",
      "batch: 1500/1563 - train loss: 14.0626 - test loss: 14.8413 - train acc: 0.3500 - test acc: 0.3358 - 4m 4s\n",
      "batch: 1563/1563 - train loss: 14.5613 - test loss: 14.8348 - train acc: 0.3366 - test acc: 0.3364 - 4m 9s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.8537 - test loss: 14.4579 - train acc: 0.4122 - test acc: 0.3525 - 4m 14s\n",
      "batch: 200/1563 - train loss: 12.9011 - test loss: 14.6209 - train acc: 0.4031 - test acc: 0.3488 - 4m 19s\n",
      "batch: 300/1563 - train loss: 13.0723 - test loss: 14.6013 - train acc: 0.3906 - test acc: 0.3427 - 4m 24s\n",
      "batch: 400/1563 - train loss: 13.2484 - test loss: 15.4759 - train acc: 0.3897 - test acc: 0.3155 - 4m 29s\n",
      "batch: 500/1563 - train loss: 13.3824 - test loss: 14.2550 - train acc: 0.3809 - test acc: 0.3574 - 4m 35s\n",
      "batch: 600/1563 - train loss: 13.3439 - test loss: 14.3226 - train acc: 0.3891 - test acc: 0.3602 - 4m 40s\n",
      "batch: 700/1563 - train loss: 13.2353 - test loss: 14.8916 - train acc: 0.3828 - test acc: 0.3425 - 4m 45s\n",
      "batch: 800/1563 - train loss: 13.2534 - test loss: 14.5344 - train acc: 0.3771 - test acc: 0.3483 - 4m 51s\n",
      "batch: 900/1563 - train loss: 13.0589 - test loss: 15.3431 - train acc: 0.3975 - test acc: 0.3268 - 4m 56s\n",
      "batch: 1000/1563 - train loss: 13.2416 - test loss: 14.0770 - train acc: 0.3891 - test acc: 0.3605 - 5m 1s\n",
      "batch: 1100/1563 - train loss: 13.1306 - test loss: 14.3032 - train acc: 0.3888 - test acc: 0.3512 - 5m 6s\n",
      "batch: 1200/1563 - train loss: 12.8156 - test loss: 14.2557 - train acc: 0.4066 - test acc: 0.3594 - 5m 12s\n",
      "batch: 1300/1563 - train loss: 12.9128 - test loss: 14.8451 - train acc: 0.3966 - test acc: 0.3451 - 5m 17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.1349 - test loss: 13.9475 - train acc: 0.3940 - test acc: 0.3704 - 5m 22s\n",
      "batch: 1500/1563 - train loss: 13.0985 - test loss: 14.0940 - train acc: 0.4019 - test acc: 0.3579 - 5m 27s\n",
      "batch: 1563/1563 - train loss: 13.0341 - test loss: 15.8518 - train acc: 0.4044 - test acc: 0.3179 - 5m 32s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.2815 - test loss: 14.2026 - train acc: 0.4682 - test acc: 0.3726 - 5m 37s\n",
      "batch: 200/1563 - train loss: 11.3720 - test loss: 14.2265 - train acc: 0.4531 - test acc: 0.3653 - 5m 42s\n",
      "batch: 300/1563 - train loss: 11.2506 - test loss: 14.4869 - train acc: 0.4585 - test acc: 0.3637 - 5m 48s\n",
      "batch: 400/1563 - train loss: 11.6914 - test loss: 14.7373 - train acc: 0.4453 - test acc: 0.3617 - 5m 53s\n",
      "batch: 500/1563 - train loss: 11.7910 - test loss: 14.2378 - train acc: 0.4425 - test acc: 0.3657 - 5m 58s\n",
      "batch: 600/1563 - train loss: 12.2354 - test loss: 14.6713 - train acc: 0.4257 - test acc: 0.3500 - 6m 3s\n",
      "batch: 700/1563 - train loss: 11.8000 - test loss: 13.9169 - train acc: 0.4322 - test acc: 0.3773 - 6m 9s\n",
      "batch: 800/1563 - train loss: 11.9807 - test loss: 14.0949 - train acc: 0.4381 - test acc: 0.3756 - 6m 14s\n",
      "batch: 900/1563 - train loss: 11.6444 - test loss: 14.6507 - train acc: 0.4447 - test acc: 0.3645 - 6m 19s\n",
      "batch: 1000/1563 - train loss: 12.2304 - test loss: 14.2589 - train acc: 0.4385 - test acc: 0.3668 - 6m 24s\n",
      "batch: 1100/1563 - train loss: 11.8485 - test loss: 14.8070 - train acc: 0.4359 - test acc: 0.3491 - 6m 29s\n",
      "batch: 1200/1563 - train loss: 12.0987 - test loss: 13.8428 - train acc: 0.4182 - test acc: 0.3780 - 6m 35s\n",
      "batch: 1300/1563 - train loss: 12.1114 - test loss: 14.2088 - train acc: 0.4247 - test acc: 0.3637 - 6m 40s\n",
      "batch: 1400/1563 - train loss: 12.0572 - test loss: 14.1366 - train acc: 0.4400 - test acc: 0.3775 - 6m 45s\n",
      "batch: 1500/1563 - train loss: 11.9027 - test loss: 13.9778 - train acc: 0.4337 - test acc: 0.3804 - 6m 51s\n",
      "batch: 1563/1563 - train loss: 11.7997 - test loss: 13.8109 - train acc: 0.4444 - test acc: 0.3741 - 6m 55s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0318 - test loss: 14.0777 - train acc: 0.5131 - test acc: 0.3834 - 7m 0s\n",
      "batch: 200/1563 - train loss: 10.2740 - test loss: 15.5852 - train acc: 0.5091 - test acc: 0.3574 - 7m 5s\n",
      "batch: 300/1563 - train loss: 10.2945 - test loss: 13.9052 - train acc: 0.5019 - test acc: 0.3927 - 7m 11s\n",
      "batch: 400/1563 - train loss: 10.0265 - test loss: 14.1316 - train acc: 0.5162 - test acc: 0.3792 - 7m 16s\n",
      "batch: 500/1563 - train loss: 11.1274 - test loss: 14.1716 - train acc: 0.4662 - test acc: 0.3741 - 7m 21s\n",
      "batch: 600/1563 - train loss: 10.5542 - test loss: 14.3815 - train acc: 0.4872 - test acc: 0.3841 - 7m 27s\n",
      "batch: 700/1563 - train loss: 10.8160 - test loss: 13.7622 - train acc: 0.4747 - test acc: 0.3914 - 7m 32s\n",
      "batch: 800/1563 - train loss: 10.5544 - test loss: 14.5433 - train acc: 0.4881 - test acc: 0.3709 - 7m 37s\n",
      "batch: 900/1563 - train loss: 10.8309 - test loss: 13.2840 - train acc: 0.4810 - test acc: 0.4083 - 7m 42s\n",
      "batch: 1000/1563 - train loss: 10.8875 - test loss: 13.6550 - train acc: 0.4778 - test acc: 0.3918 - 7m 48s\n",
      "batch: 1100/1563 - train loss: 11.1167 - test loss: 14.2988 - train acc: 0.4632 - test acc: 0.3810 - 7m 53s\n",
      "batch: 1200/1563 - train loss: 10.8694 - test loss: 14.3598 - train acc: 0.4828 - test acc: 0.3791 - 7m 58s\n",
      "batch: 1300/1563 - train loss: 10.8781 - test loss: 14.1219 - train acc: 0.4878 - test acc: 0.3776 - 8m 3s\n",
      "batch: 1400/1563 - train loss: 10.8518 - test loss: 13.0969 - train acc: 0.4825 - test acc: 0.4108 - 8m 9s\n",
      "batch: 1500/1563 - train loss: 10.8330 - test loss: 13.5024 - train acc: 0.4753 - test acc: 0.4000 - 8m 14s\n",
      "batch: 1563/1563 - train loss: 10.7245 - test loss: 14.6692 - train acc: 0.4835 - test acc: 0.3741 - 8m 19s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.5700 - test loss: 14.0167 - train acc: 0.5681 - test acc: 0.4036 - 8m 24s\n",
      "batch: 200/1563 - train loss: 8.8441 - test loss: 13.6304 - train acc: 0.5596 - test acc: 0.4137 - 8m 29s\n",
      "batch: 300/1563 - train loss: 8.9399 - test loss: 14.2772 - train acc: 0.5513 - test acc: 0.3887 - 8m 34s\n",
      "batch: 400/1563 - train loss: 9.6701 - test loss: 14.7869 - train acc: 0.5112 - test acc: 0.3744 - 8m 40s\n",
      "batch: 500/1563 - train loss: 9.4932 - test loss: 13.9673 - train acc: 0.5369 - test acc: 0.3966 - 8m 45s\n",
      "batch: 600/1563 - train loss: 9.5921 - test loss: 14.1513 - train acc: 0.5356 - test acc: 0.3956 - 8m 50s\n",
      "batch: 700/1563 - train loss: 9.4936 - test loss: 13.4631 - train acc: 0.5291 - test acc: 0.4153 - 8m 56s\n",
      "batch: 800/1563 - train loss: 9.5380 - test loss: 14.1339 - train acc: 0.5362 - test acc: 0.3872 - 9m 1s\n",
      "batch: 900/1563 - train loss: 9.8574 - test loss: 13.8530 - train acc: 0.5166 - test acc: 0.3996 - 9m 6s\n",
      "batch: 1000/1563 - train loss: 10.0802 - test loss: 14.1859 - train acc: 0.5115 - test acc: 0.3892 - 9m 11s\n",
      "batch: 1100/1563 - train loss: 9.6956 - test loss: 13.4954 - train acc: 0.5197 - test acc: 0.4090 - 9m 17s\n",
      "batch: 1200/1563 - train loss: 10.0888 - test loss: 13.6062 - train acc: 0.5184 - test acc: 0.4079 - 9m 22s\n",
      "batch: 1300/1563 - train loss: 9.9708 - test loss: 13.6631 - train acc: 0.5212 - test acc: 0.4013 - 9m 27s\n",
      "batch: 1400/1563 - train loss: 10.1831 - test loss: 14.5076 - train acc: 0.5031 - test acc: 0.3754 - 9m 32s\n",
      "batch: 1500/1563 - train loss: 10.0111 - test loss: 13.2578 - train acc: 0.5163 - test acc: 0.4081 - 9m 38s\n",
      "batch: 1563/1563 - train loss: 9.8803 - test loss: 13.8575 - train acc: 0.5166 - test acc: 0.4044 - 9m 42s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.6846 - test loss: 14.2763 - train acc: 0.6143 - test acc: 0.4030 - 9m 47s\n",
      "batch: 200/1563 - train loss: 7.7083 - test loss: 14.6715 - train acc: 0.6210 - test acc: 0.3933 - 9m 53s\n",
      "batch: 300/1563 - train loss: 8.1197 - test loss: 14.3205 - train acc: 0.5906 - test acc: 0.4062 - 9m 58s\n",
      "batch: 400/1563 - train loss: 7.9932 - test loss: 14.4020 - train acc: 0.5916 - test acc: 0.4041 - 10m 3s\n",
      "batch: 500/1563 - train loss: 8.2523 - test loss: 13.8111 - train acc: 0.5853 - test acc: 0.4182 - 10m 9s\n",
      "batch: 600/1563 - train loss: 8.3655 - test loss: 14.2980 - train acc: 0.5803 - test acc: 0.4007 - 10m 14s\n",
      "batch: 700/1563 - train loss: 8.5346 - test loss: 14.4343 - train acc: 0.5835 - test acc: 0.3987 - 10m 19s\n",
      "batch: 800/1563 - train loss: 8.5138 - test loss: 13.7731 - train acc: 0.5718 - test acc: 0.4138 - 10m 25s\n",
      "batch: 900/1563 - train loss: 8.5323 - test loss: 14.0432 - train acc: 0.5737 - test acc: 0.4097 - 10m 30s\n",
      "batch: 1000/1563 - train loss: 8.8100 - test loss: 13.9278 - train acc: 0.5662 - test acc: 0.4145 - 10m 35s\n",
      "batch: 1100/1563 - train loss: 9.0087 - test loss: 14.1391 - train acc: 0.5578 - test acc: 0.4066 - 10m 40s\n",
      "batch: 1200/1563 - train loss: 8.8190 - test loss: 14.0136 - train acc: 0.5569 - test acc: 0.4029 - 10m 46s\n",
      "batch: 1300/1563 - train loss: 9.2361 - test loss: 13.9646 - train acc: 0.5387 - test acc: 0.4057 - 10m 51s\n",
      "batch: 1400/1563 - train loss: 9.3510 - test loss: 13.1399 - train acc: 0.5372 - test acc: 0.4267 - 10m 56s\n",
      "batch: 1500/1563 - train loss: 9.4533 - test loss: 13.3836 - train acc: 0.5378 - test acc: 0.4210 - 11m 2s\n",
      "batch: 1563/1563 - train loss: 9.0375 - test loss: 14.1228 - train acc: 0.5522 - test acc: 0.4059 - 11m 6s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.8153 - test loss: 13.7092 - train acc: 0.6419 - test acc: 0.4253 - 11m 11s\n",
      "batch: 200/1563 - train loss: 6.5973 - test loss: 14.3550 - train acc: 0.6632 - test acc: 0.4121 - 11m 17s\n",
      "batch: 300/1563 - train loss: 6.8410 - test loss: 14.2169 - train acc: 0.6475 - test acc: 0.4122 - 11m 22s\n",
      "batch: 400/1563 - train loss: 7.0636 - test loss: 15.1869 - train acc: 0.6334 - test acc: 0.3945 - 11m 28s\n",
      "batch: 500/1563 - train loss: 7.3552 - test loss: 15.0247 - train acc: 0.6260 - test acc: 0.3969 - 11m 33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.3289 - test loss: 14.4075 - train acc: 0.6203 - test acc: 0.4153 - 11m 38s\n",
      "batch: 700/1563 - train loss: 7.6328 - test loss: 15.8237 - train acc: 0.6056 - test acc: 0.3826 - 11m 43s\n",
      "batch: 800/1563 - train loss: 8.1826 - test loss: 14.2303 - train acc: 0.5881 - test acc: 0.4047 - 11m 48s\n",
      "batch: 900/1563 - train loss: 7.8477 - test loss: 13.7089 - train acc: 0.5915 - test acc: 0.4228 - 11m 54s\n",
      "batch: 1000/1563 - train loss: 7.9381 - test loss: 13.7439 - train acc: 0.6000 - test acc: 0.4244 - 11m 59s\n",
      "batch: 1100/1563 - train loss: 8.0959 - test loss: 13.8564 - train acc: 0.5965 - test acc: 0.4236 - 12m 4s\n",
      "batch: 1200/1563 - train loss: 8.0258 - test loss: 14.2787 - train acc: 0.5938 - test acc: 0.4144 - 12m 10s\n",
      "batch: 1300/1563 - train loss: 7.9547 - test loss: 14.1773 - train acc: 0.5975 - test acc: 0.4084 - 12m 15s\n",
      "batch: 1400/1563 - train loss: 7.9030 - test loss: 13.8526 - train acc: 0.5981 - test acc: 0.4168 - 12m 20s\n",
      "batch: 1500/1563 - train loss: 8.3838 - test loss: 13.4833 - train acc: 0.5778 - test acc: 0.4272 - 12m 25s\n",
      "batch: 1563/1563 - train loss: 8.3988 - test loss: 13.6161 - train acc: 0.5859 - test acc: 0.4243 - 12m 30s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.7244 - test loss: 13.9339 - train acc: 0.7019 - test acc: 0.4245 - 12m 35s\n",
      "batch: 200/1563 - train loss: 5.6260 - test loss: 15.6345 - train acc: 0.7072 - test acc: 0.3847 - 12m 41s\n",
      "batch: 300/1563 - train loss: 5.9398 - test loss: 14.6543 - train acc: 0.6953 - test acc: 0.4187 - 12m 46s\n",
      "batch: 400/1563 - train loss: 6.0551 - test loss: 15.4578 - train acc: 0.6804 - test acc: 0.3998 - 12m 51s\n",
      "batch: 500/1563 - train loss: 6.3416 - test loss: 14.6334 - train acc: 0.6744 - test acc: 0.4210 - 12m 56s\n",
      "batch: 600/1563 - train loss: 6.4415 - test loss: 14.7315 - train acc: 0.6603 - test acc: 0.4117 - 13m 2s\n",
      "batch: 700/1563 - train loss: 6.7111 - test loss: 14.3596 - train acc: 0.6500 - test acc: 0.4200 - 13m 7s\n",
      "batch: 800/1563 - train loss: 6.7675 - test loss: 14.4939 - train acc: 0.6450 - test acc: 0.4243 - 13m 13s\n",
      "batch: 900/1563 - train loss: 6.6487 - test loss: 14.6235 - train acc: 0.6478 - test acc: 0.4200 - 13m 18s\n",
      "batch: 1000/1563 - train loss: 6.9513 - test loss: 14.6337 - train acc: 0.6447 - test acc: 0.4141 - 13m 23s\n",
      "batch: 1100/1563 - train loss: 6.9166 - test loss: 14.4184 - train acc: 0.6475 - test acc: 0.4250 - 13m 28s\n",
      "batch: 1200/1563 - train loss: 7.1550 - test loss: 14.8183 - train acc: 0.6269 - test acc: 0.4082 - 13m 34s\n",
      "batch: 1300/1563 - train loss: 7.2795 - test loss: 14.1533 - train acc: 0.6362 - test acc: 0.4133 - 13m 40s\n",
      "batch: 1400/1563 - train loss: 7.2914 - test loss: 14.1557 - train acc: 0.6216 - test acc: 0.4254 - 13m 45s\n",
      "batch: 1500/1563 - train loss: 7.5499 - test loss: 14.2286 - train acc: 0.6144 - test acc: 0.4162 - 13m 50s\n",
      "batch: 1563/1563 - train loss: 7.6991 - test loss: 14.7738 - train acc: 0.6015 - test acc: 0.4153 - 13m 54s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.7091 - test loss: 14.4437 - train acc: 0.7472 - test acc: 0.4299 - 14m 0s\n",
      "batch: 200/1563 - train loss: 4.8244 - test loss: 15.0920 - train acc: 0.7341 - test acc: 0.4171 - 14m 5s\n",
      "batch: 300/1563 - train loss: 5.0435 - test loss: 15.4800 - train acc: 0.7313 - test acc: 0.4189 - 14m 11s\n",
      "batch: 400/1563 - train loss: 5.4139 - test loss: 15.6453 - train acc: 0.7065 - test acc: 0.4144 - 14m 16s\n",
      "batch: 500/1563 - train loss: 5.6485 - test loss: 15.0691 - train acc: 0.7066 - test acc: 0.4152 - 14m 22s\n",
      "batch: 600/1563 - train loss: 5.7675 - test loss: 15.0948 - train acc: 0.7037 - test acc: 0.4190 - 14m 27s\n",
      "batch: 700/1563 - train loss: 5.7186 - test loss: 14.9569 - train acc: 0.6853 - test acc: 0.4109 - 14m 32s\n",
      "batch: 800/1563 - train loss: 5.9863 - test loss: 15.0020 - train acc: 0.6878 - test acc: 0.4151 - 14m 38s\n",
      "batch: 900/1563 - train loss: 6.1429 - test loss: 14.8585 - train acc: 0.6725 - test acc: 0.4207 - 14m 43s\n",
      "batch: 1000/1563 - train loss: 5.9390 - test loss: 14.6882 - train acc: 0.6881 - test acc: 0.4252 - 14m 48s\n",
      "batch: 1100/1563 - train loss: 6.2358 - test loss: 14.8852 - train acc: 0.6769 - test acc: 0.4254 - 14m 54s\n",
      "batch: 1200/1563 - train loss: 6.1018 - test loss: 14.9845 - train acc: 0.6841 - test acc: 0.4110 - 14m 59s\n",
      "batch: 1300/1563 - train loss: 6.3898 - test loss: 14.7665 - train acc: 0.6653 - test acc: 0.4198 - 15m 4s\n",
      "batch: 1400/1563 - train loss: 6.4089 - test loss: 14.8731 - train acc: 0.6706 - test acc: 0.4126 - 15m 10s\n",
      "batch: 1500/1563 - train loss: 6.5980 - test loss: 14.7830 - train acc: 0.6601 - test acc: 0.4180 - 15m 15s\n",
      "batch: 1563/1563 - train loss: 6.6562 - test loss: 14.6783 - train acc: 0.6553 - test acc: 0.4278 - 15m 20s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.2980 - test loss: 15.7156 - train acc: 0.7566 - test acc: 0.4203 - 15m 25s\n",
      "batch: 200/1563 - train loss: 4.1313 - test loss: 15.2843 - train acc: 0.7750 - test acc: 0.4304 - 15m 31s\n",
      "batch: 300/1563 - train loss: 4.3951 - test loss: 15.3739 - train acc: 0.7550 - test acc: 0.4331 - 15m 36s\n",
      "batch: 400/1563 - train loss: 4.7105 - test loss: 16.2294 - train acc: 0.7444 - test acc: 0.4115 - 15m 41s\n",
      "batch: 500/1563 - train loss: 4.6315 - test loss: 15.7728 - train acc: 0.7518 - test acc: 0.4197 - 15m 47s\n",
      "batch: 600/1563 - train loss: 4.7143 - test loss: 15.8219 - train acc: 0.7422 - test acc: 0.4176 - 15m 52s\n",
      "batch: 700/1563 - train loss: 5.0496 - test loss: 16.2319 - train acc: 0.7210 - test acc: 0.4056 - 15m 57s\n",
      "batch: 800/1563 - train loss: 4.8785 - test loss: 15.3880 - train acc: 0.7240 - test acc: 0.4285 - 16m 2s\n",
      "batch: 900/1563 - train loss: 5.3974 - test loss: 15.2118 - train acc: 0.7126 - test acc: 0.4271 - 16m 8s\n",
      "batch: 1000/1563 - train loss: 5.5121 - test loss: 15.4924 - train acc: 0.6950 - test acc: 0.4282 - 16m 14s\n",
      "batch: 1100/1563 - train loss: 5.2285 - test loss: 15.6148 - train acc: 0.7122 - test acc: 0.4180 - 16m 19s\n",
      "batch: 1200/1563 - train loss: 5.3818 - test loss: 15.2689 - train acc: 0.7140 - test acc: 0.4244 - 16m 24s\n",
      "batch: 1300/1563 - train loss: 5.8807 - test loss: 16.2435 - train acc: 0.6953 - test acc: 0.3979 - 16m 29s\n",
      "batch: 1400/1563 - train loss: 5.5537 - test loss: 15.3110 - train acc: 0.7078 - test acc: 0.4232 - 16m 35s\n",
      "batch: 1500/1563 - train loss: 5.5704 - test loss: 15.4430 - train acc: 0.7066 - test acc: 0.4141 - 16m 40s\n",
      "batch: 1563/1563 - train loss: 5.6081 - test loss: 15.7477 - train acc: 0.7081 - test acc: 0.4196 - 16m 45s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.3704 - test loss: 15.6937 - train acc: 0.8140 - test acc: 0.4300 - 16m 50s\n",
      "batch: 200/1563 - train loss: 3.2309 - test loss: 15.6585 - train acc: 0.8218 - test acc: 0.4367 - 16m 55s\n",
      "batch: 300/1563 - train loss: 3.4694 - test loss: 15.4398 - train acc: 0.8062 - test acc: 0.4371 - 17m 0s\n",
      "batch: 400/1563 - train loss: 3.7784 - test loss: 16.4252 - train acc: 0.7859 - test acc: 0.4193 - 17m 6s\n",
      "batch: 500/1563 - train loss: 4.1161 - test loss: 15.7577 - train acc: 0.7734 - test acc: 0.4268 - 17m 11s\n",
      "batch: 600/1563 - train loss: 3.9027 - test loss: 16.3840 - train acc: 0.7800 - test acc: 0.4120 - 17m 16s\n",
      "batch: 700/1563 - train loss: 4.4287 - test loss: 16.0603 - train acc: 0.7715 - test acc: 0.4225 - 17m 22s\n",
      "batch: 800/1563 - train loss: 4.3241 - test loss: 16.9179 - train acc: 0.7762 - test acc: 0.4123 - 17m 27s\n",
      "batch: 900/1563 - train loss: 4.3180 - test loss: 16.6220 - train acc: 0.7656 - test acc: 0.4193 - 17m 32s\n",
      "batch: 1000/1563 - train loss: 4.3632 - test loss: 16.5885 - train acc: 0.7600 - test acc: 0.4219 - 17m 38s\n",
      "batch: 1100/1563 - train loss: 4.6845 - test loss: 15.6067 - train acc: 0.7506 - test acc: 0.4379 - 17m 43s\n",
      "batch: 1200/1563 - train loss: 4.6057 - test loss: 15.6674 - train acc: 0.7525 - test acc: 0.4274 - 17m 48s\n",
      "batch: 1300/1563 - train loss: 4.5229 - test loss: 16.1590 - train acc: 0.7500 - test acc: 0.4211 - 17m 54s\n",
      "batch: 1400/1563 - train loss: 5.0298 - test loss: 16.3994 - train acc: 0.7272 - test acc: 0.4137 - 17m 59s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 5.0082 - test loss: 15.9222 - train acc: 0.7263 - test acc: 0.4232 - 18m 5s\n",
      "batch: 1563/1563 - train loss: 4.8071 - test loss: 16.8495 - train acc: 0.7391 - test acc: 0.4150 - 18m 9s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.9895 - test loss: 15.8495 - train acc: 0.8371 - test acc: 0.4273 - 18m 14s\n",
      "batch: 200/1563 - train loss: 2.6440 - test loss: 16.8652 - train acc: 0.8559 - test acc: 0.4233 - 18m 20s\n",
      "batch: 300/1563 - train loss: 3.0685 - test loss: 17.0773 - train acc: 0.8368 - test acc: 0.4202 - 18m 25s\n",
      "batch: 400/1563 - train loss: 3.3474 - test loss: 16.6706 - train acc: 0.8181 - test acc: 0.4240 - 18m 30s\n",
      "batch: 500/1563 - train loss: 3.1991 - test loss: 16.8054 - train acc: 0.8249 - test acc: 0.4203 - 18m 36s\n",
      "batch: 600/1563 - train loss: 3.3246 - test loss: 16.6253 - train acc: 0.8093 - test acc: 0.4316 - 18m 41s\n",
      "batch: 700/1563 - train loss: 3.5654 - test loss: 16.5528 - train acc: 0.8050 - test acc: 0.4236 - 18m 47s\n",
      "batch: 800/1563 - train loss: 3.6928 - test loss: 17.8846 - train acc: 0.7910 - test acc: 0.4144 - 18m 52s\n",
      "batch: 900/1563 - train loss: 3.8722 - test loss: 16.5589 - train acc: 0.7859 - test acc: 0.4210 - 18m 58s\n",
      "batch: 1000/1563 - train loss: 3.7899 - test loss: 17.1907 - train acc: 0.7891 - test acc: 0.4181 - 19m 3s\n",
      "batch: 1100/1563 - train loss: 3.9887 - test loss: 17.1294 - train acc: 0.7772 - test acc: 0.4197 - 19m 8s\n",
      "batch: 1200/1563 - train loss: 4.1001 - test loss: 16.9294 - train acc: 0.7716 - test acc: 0.4204 - 19m 13s\n",
      "batch: 1300/1563 - train loss: 4.2216 - test loss: 17.1868 - train acc: 0.7662 - test acc: 0.4133 - 19m 19s\n",
      "batch: 1400/1563 - train loss: 4.1533 - test loss: 16.8122 - train acc: 0.7762 - test acc: 0.4205 - 19m 25s\n",
      "batch: 1500/1563 - train loss: 4.3773 - test loss: 16.3074 - train acc: 0.7612 - test acc: 0.4247 - 19m 30s\n",
      "batch: 1563/1563 - train loss: 4.5860 - test loss: 16.3742 - train acc: 0.7513 - test acc: 0.4282 - 19m 34s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.4878 - test loss: 16.6449 - train acc: 0.8622 - test acc: 0.4302 - 19m 40s\n",
      "batch: 200/1563 - train loss: 2.3359 - test loss: 16.8306 - train acc: 0.8682 - test acc: 0.4389 - 19m 45s\n",
      "batch: 300/1563 - train loss: 2.4721 - test loss: 16.6214 - train acc: 0.8606 - test acc: 0.4398 - 19m 50s\n",
      "batch: 400/1563 - train loss: 2.4806 - test loss: 16.8379 - train acc: 0.8581 - test acc: 0.4291 - 19m 56s\n",
      "time is up! finishing training\n",
      "batch: 462/1563 - train loss: 2.6124 - test loss: 17.2528 - train acc: 0.8515 - test acc: 0.4283 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 7\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.2079 - test loss: 24.6439 - train acc: 0.0448 - test acc: 0.0530 - 0m 2s\n",
      "batch: 200/1563 - train loss: 24.0206 - test loss: 23.1578 - train acc: 0.0668 - test acc: 0.0893 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.8459 - test loss: 22.3750 - train acc: 0.0902 - test acc: 0.0950 - 0m 13s\n",
      "batch: 400/1563 - train loss: 21.9938 - test loss: 22.4618 - train acc: 0.1106 - test acc: 0.1049 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.4602 - test loss: 21.5828 - train acc: 0.1218 - test acc: 0.1209 - 0m 23s\n",
      "batch: 600/1563 - train loss: 21.0143 - test loss: 21.0090 - train acc: 0.1378 - test acc: 0.1385 - 0m 28s\n",
      "batch: 700/1563 - train loss: 20.4515 - test loss: 20.4395 - train acc: 0.1507 - test acc: 0.1463 - 0m 34s\n",
      "batch: 800/1563 - train loss: 20.3137 - test loss: 20.7414 - train acc: 0.1613 - test acc: 0.1521 - 0m 39s\n",
      "batch: 900/1563 - train loss: 20.0213 - test loss: 19.9011 - train acc: 0.1478 - test acc: 0.1614 - 0m 44s\n",
      "batch: 1000/1563 - train loss: 19.8364 - test loss: 20.2214 - train acc: 0.1638 - test acc: 0.1638 - 0m 50s\n",
      "batch: 1100/1563 - train loss: 19.2264 - test loss: 19.0975 - train acc: 0.1801 - test acc: 0.1864 - 0m 55s\n",
      "batch: 1200/1563 - train loss: 19.3695 - test loss: 19.4516 - train acc: 0.1841 - test acc: 0.1779 - 1m 0s\n",
      "batch: 1300/1563 - train loss: 19.1447 - test loss: 18.5663 - train acc: 0.1748 - test acc: 0.2103 - 1m 5s\n",
      "batch: 1400/1563 - train loss: 18.7438 - test loss: 19.1960 - train acc: 0.1941 - test acc: 0.1973 - 1m 11s\n",
      "batch: 1500/1563 - train loss: 18.6549 - test loss: 18.6091 - train acc: 0.2132 - test acc: 0.2041 - 1m 16s\n",
      "batch: 1563/1563 - train loss: 18.5922 - test loss: 18.6796 - train acc: 0.2060 - test acc: 0.2067 - 1m 20s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.8154 - test loss: 17.8921 - train acc: 0.2232 - test acc: 0.2291 - 1m 26s\n",
      "batch: 200/1563 - train loss: 17.3300 - test loss: 18.2795 - train acc: 0.2360 - test acc: 0.2247 - 1m 31s\n",
      "batch: 300/1563 - train loss: 17.3458 - test loss: 18.1622 - train acc: 0.2413 - test acc: 0.2295 - 1m 36s\n",
      "batch: 400/1563 - train loss: 17.1827 - test loss: 18.1381 - train acc: 0.2478 - test acc: 0.2190 - 1m 41s\n",
      "batch: 500/1563 - train loss: 17.2104 - test loss: 19.8341 - train acc: 0.2500 - test acc: 0.1854 - 1m 47s\n",
      "batch: 600/1563 - train loss: 17.0840 - test loss: 16.8638 - train acc: 0.2459 - test acc: 0.2631 - 1m 52s\n",
      "batch: 700/1563 - train loss: 16.8353 - test loss: 17.3319 - train acc: 0.2612 - test acc: 0.2530 - 1m 57s\n",
      "batch: 800/1563 - train loss: 16.7834 - test loss: 17.0742 - train acc: 0.2625 - test acc: 0.2555 - 2m 3s\n",
      "batch: 900/1563 - train loss: 16.6657 - test loss: 17.4523 - train acc: 0.2532 - test acc: 0.2475 - 2m 8s\n",
      "batch: 1000/1563 - train loss: 16.5050 - test loss: 16.6741 - train acc: 0.2728 - test acc: 0.2734 - 2m 13s\n",
      "batch: 1100/1563 - train loss: 16.4373 - test loss: 16.2402 - train acc: 0.2622 - test acc: 0.2930 - 2m 18s\n",
      "batch: 1200/1563 - train loss: 16.1895 - test loss: 17.9248 - train acc: 0.2815 - test acc: 0.2516 - 2m 24s\n",
      "batch: 1300/1563 - train loss: 16.4271 - test loss: 17.4155 - train acc: 0.2678 - test acc: 0.2524 - 2m 29s\n",
      "batch: 1400/1563 - train loss: 16.0189 - test loss: 16.3566 - train acc: 0.2731 - test acc: 0.2777 - 2m 34s\n",
      "batch: 1500/1563 - train loss: 15.9796 - test loss: 17.4006 - train acc: 0.2965 - test acc: 0.2505 - 2m 40s\n",
      "batch: 1563/1563 - train loss: 16.1732 - test loss: 17.4932 - train acc: 0.2834 - test acc: 0.2505 - 2m 44s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.7792 - test loss: 16.1458 - train acc: 0.3278 - test acc: 0.2853 - 2m 50s\n",
      "batch: 200/1563 - train loss: 14.7529 - test loss: 15.9222 - train acc: 0.3296 - test acc: 0.2967 - 2m 55s\n",
      "batch: 300/1563 - train loss: 14.9882 - test loss: 15.7166 - train acc: 0.3128 - test acc: 0.3038 - 3m 0s\n",
      "batch: 400/1563 - train loss: 14.8962 - test loss: 15.5146 - train acc: 0.3249 - test acc: 0.3049 - 3m 6s\n",
      "batch: 500/1563 - train loss: 14.9120 - test loss: 15.8804 - train acc: 0.3218 - test acc: 0.3049 - 3m 11s\n",
      "batch: 600/1563 - train loss: 15.0628 - test loss: 16.5071 - train acc: 0.3147 - test acc: 0.2910 - 3m 17s\n",
      "batch: 700/1563 - train loss: 14.8055 - test loss: 16.0274 - train acc: 0.3315 - test acc: 0.2957 - 3m 22s\n",
      "batch: 800/1563 - train loss: 14.4934 - test loss: 16.8725 - train acc: 0.3403 - test acc: 0.2793 - 3m 28s\n",
      "batch: 900/1563 - train loss: 14.2723 - test loss: 14.9634 - train acc: 0.3541 - test acc: 0.3309 - 3m 33s\n",
      "batch: 1000/1563 - train loss: 14.4123 - test loss: 15.7617 - train acc: 0.3393 - test acc: 0.3090 - 3m 38s\n",
      "batch: 1100/1563 - train loss: 14.3553 - test loss: 15.0008 - train acc: 0.3456 - test acc: 0.3256 - 3m 44s\n",
      "batch: 1200/1563 - train loss: 14.2636 - test loss: 15.6758 - train acc: 0.3422 - test acc: 0.3116 - 3m 49s\n",
      "batch: 1300/1563 - train loss: 14.3594 - test loss: 15.4912 - train acc: 0.3446 - test acc: 0.3120 - 3m 55s\n",
      "batch: 1400/1563 - train loss: 14.6321 - test loss: 14.9118 - train acc: 0.3396 - test acc: 0.3303 - 4m 0s\n",
      "batch: 1500/1563 - train loss: 14.4147 - test loss: 15.8510 - train acc: 0.3481 - test acc: 0.3149 - 4m 5s\n",
      "batch: 1563/1563 - train loss: 14.1526 - test loss: 14.9573 - train acc: 0.3525 - test acc: 0.3225 - 4m 10s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.1583 - test loss: 15.4704 - train acc: 0.3900 - test acc: 0.3116 - 4m 15s\n",
      "batch: 200/1563 - train loss: 13.1713 - test loss: 14.5886 - train acc: 0.3853 - test acc: 0.3485 - 4m 20s\n",
      "batch: 300/1563 - train loss: 12.9581 - test loss: 15.2422 - train acc: 0.3940 - test acc: 0.3217 - 4m 26s\n",
      "batch: 400/1563 - train loss: 13.0109 - test loss: 15.9996 - train acc: 0.3937 - test acc: 0.3067 - 4m 31s\n",
      "batch: 500/1563 - train loss: 13.0628 - test loss: 15.0479 - train acc: 0.3957 - test acc: 0.3386 - 4m 36s\n",
      "batch: 600/1563 - train loss: 13.0562 - test loss: 14.6801 - train acc: 0.3897 - test acc: 0.3442 - 4m 42s\n",
      "batch: 700/1563 - train loss: 13.1411 - test loss: 15.0169 - train acc: 0.3913 - test acc: 0.3447 - 4m 47s\n",
      "batch: 800/1563 - train loss: 13.3486 - test loss: 14.3406 - train acc: 0.3828 - test acc: 0.3581 - 4m 52s\n",
      "batch: 900/1563 - train loss: 12.8178 - test loss: 15.0453 - train acc: 0.3937 - test acc: 0.3339 - 4m 58s\n",
      "batch: 1000/1563 - train loss: 12.9435 - test loss: 16.0612 - train acc: 0.3978 - test acc: 0.3162 - 5m 3s\n",
      "batch: 1100/1563 - train loss: 12.9408 - test loss: 14.3271 - train acc: 0.3969 - test acc: 0.3590 - 5m 9s\n",
      "batch: 1200/1563 - train loss: 13.3396 - test loss: 14.3754 - train acc: 0.3894 - test acc: 0.3549 - 5m 14s\n",
      "batch: 1300/1563 - train loss: 12.7824 - test loss: 14.1408 - train acc: 0.3947 - test acc: 0.3619 - 5m 19s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.4664 - test loss: 14.2317 - train acc: 0.3756 - test acc: 0.3648 - 5m 24s\n",
      "batch: 1500/1563 - train loss: 12.8867 - test loss: 13.9075 - train acc: 0.4075 - test acc: 0.3686 - 5m 30s\n",
      "batch: 1563/1563 - train loss: 12.8083 - test loss: 15.0019 - train acc: 0.4072 - test acc: 0.3435 - 5m 34s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3976 - test loss: 13.9575 - train acc: 0.4647 - test acc: 0.3771 - 5m 39s\n",
      "batch: 200/1563 - train loss: 11.5530 - test loss: 14.7152 - train acc: 0.4591 - test acc: 0.3539 - 5m 44s\n",
      "batch: 300/1563 - train loss: 11.7578 - test loss: 15.4427 - train acc: 0.4410 - test acc: 0.3397 - 5m 50s\n",
      "batch: 400/1563 - train loss: 11.4060 - test loss: 13.8760 - train acc: 0.4597 - test acc: 0.3776 - 5m 55s\n",
      "batch: 500/1563 - train loss: 11.7574 - test loss: 14.1373 - train acc: 0.4409 - test acc: 0.3747 - 6m 1s\n",
      "batch: 600/1563 - train loss: 11.6542 - test loss: 15.1217 - train acc: 0.4534 - test acc: 0.3366 - 6m 6s\n",
      "batch: 700/1563 - train loss: 12.2502 - test loss: 14.3567 - train acc: 0.4304 - test acc: 0.3645 - 6m 11s\n",
      "batch: 800/1563 - train loss: 11.9455 - test loss: 13.9436 - train acc: 0.4322 - test acc: 0.3683 - 6m 16s\n",
      "batch: 900/1563 - train loss: 12.0485 - test loss: 13.8817 - train acc: 0.4432 - test acc: 0.3735 - 6m 21s\n",
      "batch: 1000/1563 - train loss: 11.9187 - test loss: 14.0566 - train acc: 0.4394 - test acc: 0.3783 - 6m 26s\n",
      "batch: 1100/1563 - train loss: 12.1075 - test loss: 15.9062 - train acc: 0.4363 - test acc: 0.3253 - 6m 32s\n",
      "batch: 1200/1563 - train loss: 11.7214 - test loss: 15.8538 - train acc: 0.4522 - test acc: 0.3419 - 6m 37s\n",
      "batch: 1300/1563 - train loss: 11.6897 - test loss: 14.2794 - train acc: 0.4509 - test acc: 0.3690 - 6m 42s\n",
      "batch: 1400/1563 - train loss: 11.6434 - test loss: 13.4999 - train acc: 0.4384 - test acc: 0.3903 - 6m 48s\n",
      "batch: 1500/1563 - train loss: 11.8580 - test loss: 14.3194 - train acc: 0.4400 - test acc: 0.3608 - 6m 53s\n",
      "batch: 1563/1563 - train loss: 11.9749 - test loss: 13.6809 - train acc: 0.4410 - test acc: 0.3860 - 6m 58s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.8152 - test loss: 13.6006 - train acc: 0.5203 - test acc: 0.3946 - 7m 3s\n",
      "batch: 200/1563 - train loss: 10.1017 - test loss: 13.7194 - train acc: 0.5171 - test acc: 0.3921 - 7m 8s\n",
      "batch: 300/1563 - train loss: 10.2034 - test loss: 14.6871 - train acc: 0.5010 - test acc: 0.3705 - 7m 13s\n",
      "batch: 400/1563 - train loss: 10.4118 - test loss: 14.0610 - train acc: 0.5019 - test acc: 0.3879 - 7m 19s\n",
      "batch: 500/1563 - train loss: 10.3771 - test loss: 13.7432 - train acc: 0.5038 - test acc: 0.3979 - 7m 24s\n",
      "batch: 600/1563 - train loss: 10.9025 - test loss: 14.5714 - train acc: 0.4784 - test acc: 0.3706 - 7m 29s\n",
      "batch: 700/1563 - train loss: 11.1189 - test loss: 13.5464 - train acc: 0.4725 - test acc: 0.4011 - 7m 35s\n",
      "batch: 800/1563 - train loss: 10.9608 - test loss: 13.7270 - train acc: 0.4741 - test acc: 0.3979 - 7m 40s\n",
      "batch: 900/1563 - train loss: 10.6673 - test loss: 13.5559 - train acc: 0.5009 - test acc: 0.3951 - 7m 45s\n",
      "batch: 1000/1563 - train loss: 10.9083 - test loss: 13.2806 - train acc: 0.4647 - test acc: 0.4019 - 7m 50s\n",
      "batch: 1100/1563 - train loss: 10.5239 - test loss: 18.4049 - train acc: 0.4916 - test acc: 0.3043 - 7m 56s\n",
      "batch: 1200/1563 - train loss: 11.0392 - test loss: 13.6057 - train acc: 0.4716 - test acc: 0.3975 - 8m 1s\n",
      "batch: 1300/1563 - train loss: 11.1133 - test loss: 13.3506 - train acc: 0.4550 - test acc: 0.4001 - 8m 7s\n",
      "batch: 1400/1563 - train loss: 10.9021 - test loss: 14.9761 - train acc: 0.4843 - test acc: 0.3570 - 8m 12s\n",
      "batch: 1500/1563 - train loss: 11.3750 - test loss: 13.5214 - train acc: 0.4550 - test acc: 0.3971 - 8m 17s\n",
      "batch: 1563/1563 - train loss: 10.6932 - test loss: 14.1027 - train acc: 0.4878 - test acc: 0.3849 - 8m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.5470 - test loss: 14.2801 - train acc: 0.5716 - test acc: 0.3899 - 8m 27s\n",
      "batch: 200/1563 - train loss: 9.1138 - test loss: 14.1902 - train acc: 0.5371 - test acc: 0.3934 - 8m 32s\n",
      "batch: 300/1563 - train loss: 9.1479 - test loss: 14.4308 - train acc: 0.5413 - test acc: 0.3884 - 8m 38s\n",
      "batch: 400/1563 - train loss: 9.7473 - test loss: 13.9982 - train acc: 0.5262 - test acc: 0.3906 - 8m 43s\n",
      "batch: 500/1563 - train loss: 9.3923 - test loss: 15.9395 - train acc: 0.5334 - test acc: 0.3424 - 8m 49s\n",
      "batch: 600/1563 - train loss: 9.4643 - test loss: 13.8557 - train acc: 0.5312 - test acc: 0.3965 - 8m 54s\n",
      "batch: 700/1563 - train loss: 9.4935 - test loss: 13.6340 - train acc: 0.5278 - test acc: 0.4041 - 8m 59s\n",
      "batch: 800/1563 - train loss: 9.7206 - test loss: 14.2013 - train acc: 0.5178 - test acc: 0.3924 - 9m 5s\n",
      "batch: 900/1563 - train loss: 9.6602 - test loss: 13.6565 - train acc: 0.5310 - test acc: 0.4024 - 9m 10s\n",
      "batch: 1000/1563 - train loss: 9.6876 - test loss: 13.5262 - train acc: 0.5278 - test acc: 0.4112 - 9m 15s\n",
      "batch: 1100/1563 - train loss: 9.9832 - test loss: 13.6533 - train acc: 0.5191 - test acc: 0.4055 - 9m 21s\n",
      "batch: 1200/1563 - train loss: 9.9892 - test loss: 13.5967 - train acc: 0.5203 - test acc: 0.4016 - 9m 26s\n",
      "batch: 1300/1563 - train loss: 9.8900 - test loss: 13.3422 - train acc: 0.5225 - test acc: 0.4158 - 9m 32s\n",
      "batch: 1400/1563 - train loss: 9.6677 - test loss: 13.7322 - train acc: 0.5265 - test acc: 0.4025 - 9m 37s\n",
      "batch: 1500/1563 - train loss: 9.9892 - test loss: 13.0993 - train acc: 0.5156 - test acc: 0.4172 - 9m 43s\n",
      "batch: 1563/1563 - train loss: 9.8721 - test loss: 13.3537 - train acc: 0.5209 - test acc: 0.4145 - 9m 47s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.8254 - test loss: 15.7161 - train acc: 0.6163 - test acc: 0.3673 - 9m 52s\n",
      "batch: 200/1563 - train loss: 8.2986 - test loss: 13.9531 - train acc: 0.5756 - test acc: 0.4000 - 9m 58s\n",
      "batch: 300/1563 - train loss: 8.3120 - test loss: 14.2655 - train acc: 0.5749 - test acc: 0.3966 - 10m 3s\n",
      "batch: 400/1563 - train loss: 7.8706 - test loss: 18.6900 - train acc: 0.6069 - test acc: 0.3271 - 10m 8s\n",
      "batch: 500/1563 - train loss: 8.3065 - test loss: 14.5807 - train acc: 0.5777 - test acc: 0.3999 - 10m 14s\n",
      "batch: 600/1563 - train loss: 8.2548 - test loss: 13.6608 - train acc: 0.5884 - test acc: 0.4118 - 10m 19s\n",
      "batch: 700/1563 - train loss: 9.0864 - test loss: 13.5372 - train acc: 0.5522 - test acc: 0.4187 - 10m 24s\n",
      "batch: 800/1563 - train loss: 8.5720 - test loss: 15.0426 - train acc: 0.5706 - test acc: 0.3795 - 10m 30s\n",
      "batch: 900/1563 - train loss: 8.8132 - test loss: 13.9579 - train acc: 0.5650 - test acc: 0.4014 - 10m 35s\n",
      "batch: 1000/1563 - train loss: 8.8348 - test loss: 14.4997 - train acc: 0.5497 - test acc: 0.3887 - 10m 40s\n",
      "batch: 1100/1563 - train loss: 8.8337 - test loss: 13.4834 - train acc: 0.5575 - test acc: 0.4121 - 10m 46s\n",
      "batch: 1200/1563 - train loss: 8.6910 - test loss: 13.7030 - train acc: 0.5700 - test acc: 0.4178 - 10m 51s\n",
      "batch: 1300/1563 - train loss: 8.7526 - test loss: 13.9754 - train acc: 0.5641 - test acc: 0.4013 - 10m 57s\n",
      "batch: 1400/1563 - train loss: 9.2923 - test loss: 14.2912 - train acc: 0.5494 - test acc: 0.3925 - 11m 2s\n",
      "batch: 1500/1563 - train loss: 8.9559 - test loss: 14.3224 - train acc: 0.5500 - test acc: 0.3969 - 11m 7s\n",
      "batch: 1563/1563 - train loss: 8.9841 - test loss: 14.2830 - train acc: 0.5531 - test acc: 0.3919 - 11m 12s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.8955 - test loss: 13.7196 - train acc: 0.6369 - test acc: 0.4246 - 11m 17s\n",
      "batch: 200/1563 - train loss: 6.6397 - test loss: 13.6984 - train acc: 0.6591 - test acc: 0.4301 - 11m 23s\n",
      "batch: 300/1563 - train loss: 6.9055 - test loss: 14.5001 - train acc: 0.6503 - test acc: 0.4068 - 11m 28s\n",
      "batch: 400/1563 - train loss: 7.0565 - test loss: 14.6960 - train acc: 0.6469 - test acc: 0.4008 - 11m 33s\n",
      "batch: 500/1563 - train loss: 7.6197 - test loss: 14.7219 - train acc: 0.6197 - test acc: 0.4049 - 11m 38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 8.0000 - test loss: 15.2597 - train acc: 0.5913 - test acc: 0.3907 - 11m 44s\n",
      "batch: 700/1563 - train loss: 7.8999 - test loss: 14.0368 - train acc: 0.5912 - test acc: 0.4172 - 11m 49s\n",
      "batch: 800/1563 - train loss: 7.8849 - test loss: 13.9254 - train acc: 0.5996 - test acc: 0.4198 - 11m 55s\n",
      "batch: 900/1563 - train loss: 7.6828 - test loss: 14.5191 - train acc: 0.6172 - test acc: 0.4120 - 12m 0s\n",
      "batch: 1000/1563 - train loss: 7.5917 - test loss: 14.5859 - train acc: 0.6160 - test acc: 0.3986 - 12m 5s\n",
      "batch: 1100/1563 - train loss: 8.0198 - test loss: 13.7074 - train acc: 0.5950 - test acc: 0.4199 - 12m 10s\n",
      "batch: 1200/1563 - train loss: 8.0900 - test loss: 14.0067 - train acc: 0.5881 - test acc: 0.4140 - 12m 16s\n",
      "batch: 1300/1563 - train loss: 7.9687 - test loss: 13.8218 - train acc: 0.5959 - test acc: 0.4203 - 12m 21s\n",
      "batch: 1400/1563 - train loss: 8.1734 - test loss: 13.4749 - train acc: 0.5875 - test acc: 0.4239 - 12m 27s\n",
      "batch: 1500/1563 - train loss: 7.8832 - test loss: 13.8978 - train acc: 0.6029 - test acc: 0.4161 - 12m 32s\n",
      "batch: 1563/1563 - train loss: 7.9681 - test loss: 13.5456 - train acc: 0.6004 - test acc: 0.4224 - 12m 36s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.7365 - test loss: 14.1775 - train acc: 0.7000 - test acc: 0.4181 - 12m 42s\n",
      "batch: 200/1563 - train loss: 5.7434 - test loss: 14.8633 - train acc: 0.7090 - test acc: 0.4182 - 12m 47s\n",
      "batch: 300/1563 - train loss: 6.1822 - test loss: 14.5206 - train acc: 0.6753 - test acc: 0.4153 - 12m 53s\n",
      "batch: 400/1563 - train loss: 6.2486 - test loss: 14.8610 - train acc: 0.6834 - test acc: 0.4097 - 12m 58s\n",
      "batch: 500/1563 - train loss: 6.4048 - test loss: 14.5992 - train acc: 0.6698 - test acc: 0.4251 - 13m 3s\n",
      "batch: 600/1563 - train loss: 6.4378 - test loss: 14.5850 - train acc: 0.6656 - test acc: 0.4134 - 13m 9s\n",
      "batch: 700/1563 - train loss: 6.6680 - test loss: 14.1313 - train acc: 0.6526 - test acc: 0.4228 - 13m 14s\n",
      "batch: 800/1563 - train loss: 6.5717 - test loss: 14.2444 - train acc: 0.6538 - test acc: 0.4229 - 13m 19s\n",
      "batch: 900/1563 - train loss: 6.9705 - test loss: 14.8061 - train acc: 0.6466 - test acc: 0.4152 - 13m 25s\n",
      "batch: 1000/1563 - train loss: 7.0451 - test loss: 14.4694 - train acc: 0.6335 - test acc: 0.4185 - 13m 31s\n",
      "batch: 1100/1563 - train loss: 7.2436 - test loss: 14.1190 - train acc: 0.6243 - test acc: 0.4232 - 13m 36s\n",
      "batch: 1200/1563 - train loss: 6.8058 - test loss: 14.8953 - train acc: 0.6447 - test acc: 0.4036 - 13m 41s\n",
      "batch: 1300/1563 - train loss: 7.3168 - test loss: 14.1263 - train acc: 0.6156 - test acc: 0.4247 - 13m 47s\n",
      "batch: 1400/1563 - train loss: 7.1494 - test loss: 14.3773 - train acc: 0.6422 - test acc: 0.4143 - 13m 52s\n",
      "batch: 1500/1563 - train loss: 7.3539 - test loss: 14.5842 - train acc: 0.6200 - test acc: 0.4167 - 13m 58s\n",
      "batch: 1563/1563 - train loss: 7.6383 - test loss: 13.8816 - train acc: 0.6044 - test acc: 0.4288 - 14m 3s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.9156 - test loss: 14.9668 - train acc: 0.7312 - test acc: 0.4160 - 14m 8s\n",
      "batch: 200/1563 - train loss: 5.1684 - test loss: 15.6534 - train acc: 0.7266 - test acc: 0.4051 - 14m 14s\n",
      "batch: 300/1563 - train loss: 5.1404 - test loss: 14.9284 - train acc: 0.7250 - test acc: 0.4203 - 14m 19s\n",
      "batch: 400/1563 - train loss: 5.2948 - test loss: 15.6160 - train acc: 0.7231 - test acc: 0.4042 - 14m 25s\n",
      "batch: 500/1563 - train loss: 5.3565 - test loss: 15.5522 - train acc: 0.7150 - test acc: 0.4119 - 14m 31s\n",
      "batch: 600/1563 - train loss: 5.8366 - test loss: 15.3011 - train acc: 0.6953 - test acc: 0.4127 - 14m 36s\n",
      "batch: 700/1563 - train loss: 5.7565 - test loss: 14.7894 - train acc: 0.6965 - test acc: 0.4248 - 14m 41s\n",
      "batch: 800/1563 - train loss: 5.9962 - test loss: 15.2080 - train acc: 0.6854 - test acc: 0.4099 - 14m 47s\n",
      "batch: 900/1563 - train loss: 5.7293 - test loss: 14.8380 - train acc: 0.6937 - test acc: 0.4218 - 14m 53s\n",
      "batch: 1000/1563 - train loss: 5.9107 - test loss: 15.3154 - train acc: 0.6878 - test acc: 0.4187 - 14m 59s\n",
      "batch: 1100/1563 - train loss: 6.0264 - test loss: 15.3110 - train acc: 0.6806 - test acc: 0.4087 - 15m 4s\n",
      "batch: 1200/1563 - train loss: 6.3396 - test loss: 15.1120 - train acc: 0.6616 - test acc: 0.4148 - 15m 10s\n",
      "batch: 1300/1563 - train loss: 6.3068 - test loss: 15.0616 - train acc: 0.6688 - test acc: 0.4119 - 15m 15s\n",
      "batch: 1400/1563 - train loss: 6.5483 - test loss: 14.6203 - train acc: 0.6494 - test acc: 0.4215 - 15m 21s\n",
      "batch: 1500/1563 - train loss: 6.4922 - test loss: 15.4333 - train acc: 0.6613 - test acc: 0.3986 - 15m 26s\n",
      "batch: 1563/1563 - train loss: 6.5107 - test loss: 15.3731 - train acc: 0.6644 - test acc: 0.4059 - 15m 31s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.1184 - test loss: 15.0313 - train acc: 0.7865 - test acc: 0.4301 - 15m 36s\n",
      "batch: 200/1563 - train loss: 4.3702 - test loss: 15.5573 - train acc: 0.7681 - test acc: 0.4184 - 15m 42s\n",
      "batch: 300/1563 - train loss: 4.5798 - test loss: 16.5634 - train acc: 0.7497 - test acc: 0.4013 - 15m 47s\n",
      "batch: 400/1563 - train loss: 4.5648 - test loss: 15.5189 - train acc: 0.7518 - test acc: 0.4232 - 15m 53s\n",
      "batch: 500/1563 - train loss: 4.6433 - test loss: 15.8436 - train acc: 0.7390 - test acc: 0.4217 - 15m 58s\n",
      "batch: 600/1563 - train loss: 4.8336 - test loss: 15.4365 - train acc: 0.7462 - test acc: 0.4241 - 16m 4s\n",
      "batch: 700/1563 - train loss: 4.9982 - test loss: 15.9857 - train acc: 0.7375 - test acc: 0.4108 - 16m 9s\n",
      "batch: 800/1563 - train loss: 5.1664 - test loss: 15.6820 - train acc: 0.7253 - test acc: 0.4145 - 16m 14s\n",
      "batch: 900/1563 - train loss: 5.2220 - test loss: 15.9588 - train acc: 0.7144 - test acc: 0.4107 - 16m 20s\n",
      "batch: 1000/1563 - train loss: 5.1577 - test loss: 16.2684 - train acc: 0.7266 - test acc: 0.4080 - 16m 25s\n",
      "batch: 1100/1563 - train loss: 5.5831 - test loss: 15.3266 - train acc: 0.7053 - test acc: 0.4276 - 16m 30s\n",
      "batch: 1200/1563 - train loss: 5.6537 - test loss: 15.9171 - train acc: 0.6978 - test acc: 0.4125 - 16m 36s\n",
      "batch: 1300/1563 - train loss: 5.5606 - test loss: 15.3532 - train acc: 0.7041 - test acc: 0.4276 - 16m 41s\n",
      "batch: 1400/1563 - train loss: 5.6525 - test loss: 15.6104 - train acc: 0.7060 - test acc: 0.4142 - 16m 47s\n",
      "batch: 1500/1563 - train loss: 5.9605 - test loss: 15.1891 - train acc: 0.6797 - test acc: 0.4164 - 16m 52s\n",
      "batch: 1563/1563 - train loss: 5.8152 - test loss: 15.9997 - train acc: 0.6938 - test acc: 0.4103 - 16m 57s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.3879 - test loss: 15.6955 - train acc: 0.8141 - test acc: 0.4236 - 17m 2s\n",
      "batch: 200/1563 - train loss: 3.4937 - test loss: 15.5694 - train acc: 0.8105 - test acc: 0.4289 - 17m 8s\n",
      "batch: 300/1563 - train loss: 3.5353 - test loss: 16.2319 - train acc: 0.8056 - test acc: 0.4237 - 17m 13s\n",
      "batch: 400/1563 - train loss: 3.8068 - test loss: 15.8138 - train acc: 0.7880 - test acc: 0.4268 - 17m 18s\n",
      "batch: 500/1563 - train loss: 3.9324 - test loss: 16.5534 - train acc: 0.7831 - test acc: 0.4189 - 17m 24s\n",
      "batch: 600/1563 - train loss: 4.0629 - test loss: 16.7967 - train acc: 0.7765 - test acc: 0.4119 - 17m 29s\n",
      "batch: 700/1563 - train loss: 4.2787 - test loss: 16.2474 - train acc: 0.7690 - test acc: 0.4165 - 17m 35s\n",
      "batch: 800/1563 - train loss: 4.3216 - test loss: 16.0031 - train acc: 0.7641 - test acc: 0.4234 - 17m 40s\n",
      "batch: 900/1563 - train loss: 4.8261 - test loss: 16.0270 - train acc: 0.7347 - test acc: 0.4202 - 17m 46s\n",
      "batch: 1000/1563 - train loss: 4.5794 - test loss: 15.9337 - train acc: 0.7482 - test acc: 0.4235 - 17m 51s\n",
      "batch: 1100/1563 - train loss: 4.6840 - test loss: 16.8316 - train acc: 0.7403 - test acc: 0.3996 - 17m 56s\n",
      "batch: 1200/1563 - train loss: 4.9014 - test loss: 16.2085 - train acc: 0.7428 - test acc: 0.4197 - 18m 2s\n",
      "batch: 1300/1563 - train loss: 4.7029 - test loss: 15.8436 - train acc: 0.7435 - test acc: 0.4269 - 18m 7s\n",
      "batch: 1400/1563 - train loss: 4.9330 - test loss: 16.0240 - train acc: 0.7244 - test acc: 0.4191 - 18m 12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 4.9595 - test loss: 16.0163 - train acc: 0.7244 - test acc: 0.4264 - 18m 18s\n",
      "batch: 1563/1563 - train loss: 5.1058 - test loss: 15.6579 - train acc: 0.7266 - test acc: 0.4376 - 18m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 3.0027 - test loss: 16.7264 - train acc: 0.8366 - test acc: 0.4225 - 18m 27s\n",
      "batch: 200/1563 - train loss: 3.1289 - test loss: 16.6018 - train acc: 0.8350 - test acc: 0.4291 - 18m 33s\n",
      "batch: 300/1563 - train loss: 3.0106 - test loss: 16.7048 - train acc: 0.8287 - test acc: 0.4205 - 18m 38s\n",
      "batch: 400/1563 - train loss: 3.1006 - test loss: 17.0023 - train acc: 0.8250 - test acc: 0.4273 - 18m 44s\n",
      "batch: 500/1563 - train loss: 3.4105 - test loss: 17.2829 - train acc: 0.8190 - test acc: 0.4210 - 18m 49s\n",
      "batch: 600/1563 - train loss: 3.3956 - test loss: 16.8602 - train acc: 0.8040 - test acc: 0.4283 - 18m 55s\n",
      "batch: 700/1563 - train loss: 3.4733 - test loss: 16.7819 - train acc: 0.8037 - test acc: 0.4233 - 19m 1s\n",
      "batch: 800/1563 - train loss: 3.6538 - test loss: 16.7536 - train acc: 0.7999 - test acc: 0.4292 - 19m 6s\n",
      "batch: 900/1563 - train loss: 3.7950 - test loss: 17.0380 - train acc: 0.7900 - test acc: 0.4244 - 19m 12s\n",
      "batch: 1000/1563 - train loss: 3.9616 - test loss: 17.3702 - train acc: 0.7721 - test acc: 0.4185 - 19m 17s\n",
      "batch: 1100/1563 - train loss: 4.1993 - test loss: 16.6266 - train acc: 0.7625 - test acc: 0.4254 - 19m 23s\n",
      "batch: 1200/1563 - train loss: 3.9392 - test loss: 17.3326 - train acc: 0.7728 - test acc: 0.4158 - 19m 28s\n",
      "batch: 1300/1563 - train loss: 4.2400 - test loss: 16.8535 - train acc: 0.7656 - test acc: 0.4205 - 19m 34s\n",
      "batch: 1400/1563 - train loss: 4.5624 - test loss: 16.8329 - train acc: 0.7562 - test acc: 0.4174 - 19m 39s\n",
      "batch: 1500/1563 - train loss: 4.6262 - test loss: 16.2785 - train acc: 0.7515 - test acc: 0.4338 - 19m 46s\n",
      "batch: 1563/1563 - train loss: 4.5254 - test loss: 16.2992 - train acc: 0.7546 - test acc: 0.4312 - 19m 50s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.6903 - test loss: 16.3602 - train acc: 0.8515 - test acc: 0.4359 - 19m 56s\n",
      "time is up! finishing training\n",
      "batch: 169/1563 - train loss: 2.5727 - test loss: 16.5662 - train acc: 0.8579 - test acc: 0.4281 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 8\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1949 - test loss: 24.3004 - train acc: 0.0367 - test acc: 0.0606 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.6466 - test loss: 24.0844 - train acc: 0.0758 - test acc: 0.0758 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.9037 - test loss: 21.8658 - train acc: 0.0823 - test acc: 0.1012 - 0m 13s\n",
      "batch: 400/1563 - train loss: 21.9645 - test loss: 21.5962 - train acc: 0.1087 - test acc: 0.1204 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.6062 - test loss: 21.1714 - train acc: 0.1196 - test acc: 0.1312 - 0m 24s\n",
      "batch: 600/1563 - train loss: 20.9959 - test loss: 20.8599 - train acc: 0.1272 - test acc: 0.1358 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.8562 - test loss: 20.6855 - train acc: 0.1413 - test acc: 0.1366 - 0m 35s\n",
      "batch: 800/1563 - train loss: 20.1660 - test loss: 19.9099 - train acc: 0.1513 - test acc: 0.1628 - 0m 40s\n",
      "batch: 900/1563 - train loss: 19.9133 - test loss: 19.9234 - train acc: 0.1647 - test acc: 0.1623 - 0m 45s\n",
      "batch: 1000/1563 - train loss: 19.9335 - test loss: 19.8061 - train acc: 0.1526 - test acc: 0.1651 - 0m 51s\n",
      "batch: 1100/1563 - train loss: 19.3318 - test loss: 19.3476 - train acc: 0.1891 - test acc: 0.1822 - 0m 56s\n",
      "batch: 1200/1563 - train loss: 19.5461 - test loss: 19.8051 - train acc: 0.1714 - test acc: 0.1648 - 1m 2s\n",
      "batch: 1300/1563 - train loss: 19.1034 - test loss: 19.0469 - train acc: 0.1872 - test acc: 0.1796 - 1m 7s\n",
      "batch: 1400/1563 - train loss: 18.3575 - test loss: 19.6263 - train acc: 0.2210 - test acc: 0.1869 - 1m 13s\n",
      "batch: 1500/1563 - train loss: 18.3772 - test loss: 18.4920 - train acc: 0.2038 - test acc: 0.2053 - 1m 18s\n",
      "batch: 1563/1563 - train loss: 18.4967 - test loss: 18.1957 - train acc: 0.2094 - test acc: 0.2175 - 1m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.2116 - test loss: 18.2331 - train acc: 0.2395 - test acc: 0.2182 - 1m 28s\n",
      "batch: 200/1563 - train loss: 17.5832 - test loss: 18.3953 - train acc: 0.2291 - test acc: 0.2067 - 1m 34s\n",
      "batch: 300/1563 - train loss: 17.4096 - test loss: 18.0403 - train acc: 0.2446 - test acc: 0.2270 - 1m 39s\n",
      "batch: 400/1563 - train loss: 17.3030 - test loss: 17.5716 - train acc: 0.2503 - test acc: 0.2379 - 1m 44s\n",
      "batch: 500/1563 - train loss: 17.1924 - test loss: 17.4709 - train acc: 0.2338 - test acc: 0.2374 - 1m 50s\n",
      "batch: 600/1563 - train loss: 16.7147 - test loss: 16.7034 - train acc: 0.2728 - test acc: 0.2657 - 1m 55s\n",
      "batch: 700/1563 - train loss: 16.8004 - test loss: 17.0332 - train acc: 0.2678 - test acc: 0.2590 - 2m 0s\n",
      "batch: 800/1563 - train loss: 16.6491 - test loss: 17.0651 - train acc: 0.2638 - test acc: 0.2580 - 2m 6s\n",
      "batch: 900/1563 - train loss: 16.5511 - test loss: 18.7273 - train acc: 0.2716 - test acc: 0.2145 - 2m 11s\n",
      "batch: 1000/1563 - train loss: 16.3894 - test loss: 17.7054 - train acc: 0.2812 - test acc: 0.2449 - 2m 16s\n",
      "batch: 1100/1563 - train loss: 16.3718 - test loss: 16.0490 - train acc: 0.2790 - test acc: 0.2922 - 2m 22s\n",
      "batch: 1200/1563 - train loss: 16.1145 - test loss: 16.7765 - train acc: 0.2931 - test acc: 0.2686 - 2m 27s\n",
      "batch: 1300/1563 - train loss: 16.0337 - test loss: 16.3254 - train acc: 0.2828 - test acc: 0.2764 - 2m 32s\n",
      "batch: 1400/1563 - train loss: 16.1285 - test loss: 16.1407 - train acc: 0.2781 - test acc: 0.2863 - 2m 38s\n",
      "batch: 1500/1563 - train loss: 16.0392 - test loss: 15.6020 - train acc: 0.2775 - test acc: 0.3013 - 2m 43s\n",
      "batch: 1563/1563 - train loss: 16.0101 - test loss: 15.7280 - train acc: 0.2909 - test acc: 0.2994 - 2m 47s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.3969 - test loss: 16.3405 - train acc: 0.3441 - test acc: 0.2918 - 2m 52s\n",
      "batch: 200/1563 - train loss: 14.7794 - test loss: 16.8029 - train acc: 0.3172 - test acc: 0.2768 - 2m 58s\n",
      "batch: 300/1563 - train loss: 14.7392 - test loss: 15.4879 - train acc: 0.3281 - test acc: 0.3151 - 3m 3s\n",
      "batch: 400/1563 - train loss: 14.6271 - test loss: 16.7275 - train acc: 0.3456 - test acc: 0.2779 - 3m 9s\n",
      "batch: 500/1563 - train loss: 14.7139 - test loss: 17.7706 - train acc: 0.3253 - test acc: 0.2544 - 3m 14s\n",
      "batch: 600/1563 - train loss: 14.4968 - test loss: 15.1849 - train acc: 0.3319 - test acc: 0.3234 - 3m 19s\n",
      "batch: 700/1563 - train loss: 14.6612 - test loss: 15.0276 - train acc: 0.3359 - test acc: 0.3296 - 3m 24s\n",
      "batch: 800/1563 - train loss: 14.5913 - test loss: 15.6604 - train acc: 0.3409 - test acc: 0.3134 - 3m 30s\n",
      "batch: 900/1563 - train loss: 14.4308 - test loss: 15.7498 - train acc: 0.3419 - test acc: 0.2999 - 3m 35s\n",
      "batch: 1000/1563 - train loss: 14.4825 - test loss: 15.7559 - train acc: 0.3366 - test acc: 0.3104 - 3m 41s\n",
      "batch: 1100/1563 - train loss: 14.4089 - test loss: 16.1419 - train acc: 0.3428 - test acc: 0.2904 - 3m 46s\n",
      "batch: 1200/1563 - train loss: 14.2464 - test loss: 15.0997 - train acc: 0.3450 - test acc: 0.3233 - 3m 51s\n",
      "batch: 1300/1563 - train loss: 14.4499 - test loss: 15.0035 - train acc: 0.3453 - test acc: 0.3287 - 3m 56s\n",
      "batch: 1400/1563 - train loss: 14.5492 - test loss: 14.8682 - train acc: 0.3265 - test acc: 0.3350 - 4m 2s\n",
      "batch: 1500/1563 - train loss: 14.2991 - test loss: 14.9479 - train acc: 0.3550 - test acc: 0.3313 - 4m 7s\n",
      "batch: 1563/1563 - train loss: 14.4685 - test loss: 16.6019 - train acc: 0.3484 - test acc: 0.2765 - 4m 12s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.7044 - test loss: 15.1191 - train acc: 0.4022 - test acc: 0.3284 - 4m 17s\n",
      "batch: 200/1563 - train loss: 12.8269 - test loss: 15.6260 - train acc: 0.4109 - test acc: 0.3132 - 4m 22s\n",
      "batch: 300/1563 - train loss: 12.6917 - test loss: 14.3368 - train acc: 0.3916 - test acc: 0.3538 - 4m 28s\n",
      "batch: 400/1563 - train loss: 12.8735 - test loss: 14.8471 - train acc: 0.3991 - test acc: 0.3405 - 4m 33s\n",
      "batch: 500/1563 - train loss: 13.1683 - test loss: 15.2741 - train acc: 0.3859 - test acc: 0.3342 - 4m 38s\n",
      "batch: 600/1563 - train loss: 13.5182 - test loss: 14.6762 - train acc: 0.3700 - test acc: 0.3518 - 4m 44s\n",
      "batch: 700/1563 - train loss: 13.0462 - test loss: 14.3812 - train acc: 0.3900 - test acc: 0.3538 - 4m 49s\n",
      "batch: 800/1563 - train loss: 12.9310 - test loss: 14.8373 - train acc: 0.4016 - test acc: 0.3404 - 4m 54s\n",
      "batch: 900/1563 - train loss: 12.5251 - test loss: 13.9321 - train acc: 0.4103 - test acc: 0.3713 - 5m 0s\n",
      "batch: 1000/1563 - train loss: 12.9423 - test loss: 14.9957 - train acc: 0.4003 - test acc: 0.3379 - 5m 5s\n",
      "batch: 1100/1563 - train loss: 12.9347 - test loss: 15.4343 - train acc: 0.3956 - test acc: 0.3301 - 5m 11s\n",
      "batch: 1200/1563 - train loss: 13.5448 - test loss: 15.0607 - train acc: 0.3803 - test acc: 0.3390 - 5m 17s\n",
      "batch: 1300/1563 - train loss: 13.1694 - test loss: 14.2082 - train acc: 0.3853 - test acc: 0.3567 - 5m 22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.2739 - test loss: 13.6927 - train acc: 0.3935 - test acc: 0.3771 - 5m 27s\n",
      "batch: 1500/1563 - train loss: 12.8351 - test loss: 14.3659 - train acc: 0.4035 - test acc: 0.3544 - 5m 33s\n",
      "batch: 1563/1563 - train loss: 12.8172 - test loss: 14.1071 - train acc: 0.4069 - test acc: 0.3682 - 5m 37s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3812 - test loss: 13.9392 - train acc: 0.4541 - test acc: 0.3750 - 5m 43s\n",
      "batch: 200/1563 - train loss: 11.5934 - test loss: 13.6788 - train acc: 0.4547 - test acc: 0.3865 - 5m 48s\n",
      "batch: 300/1563 - train loss: 11.5090 - test loss: 13.6610 - train acc: 0.4600 - test acc: 0.3867 - 5m 54s\n",
      "batch: 400/1563 - train loss: 11.6678 - test loss: 14.1254 - train acc: 0.4491 - test acc: 0.3715 - 5m 59s\n",
      "batch: 500/1563 - train loss: 11.5978 - test loss: 14.1210 - train acc: 0.4488 - test acc: 0.3729 - 6m 4s\n",
      "batch: 600/1563 - train loss: 12.0328 - test loss: 14.0524 - train acc: 0.4267 - test acc: 0.3770 - 6m 10s\n",
      "batch: 700/1563 - train loss: 11.9440 - test loss: 14.3183 - train acc: 0.4331 - test acc: 0.3659 - 6m 15s\n",
      "batch: 800/1563 - train loss: 11.5858 - test loss: 14.6319 - train acc: 0.4513 - test acc: 0.3614 - 6m 21s\n",
      "batch: 900/1563 - train loss: 12.0359 - test loss: 14.1399 - train acc: 0.4297 - test acc: 0.3692 - 6m 26s\n",
      "batch: 1000/1563 - train loss: 11.7113 - test loss: 13.6074 - train acc: 0.4506 - test acc: 0.3915 - 6m 31s\n",
      "batch: 1100/1563 - train loss: 11.9421 - test loss: 13.4475 - train acc: 0.4298 - test acc: 0.3884 - 6m 37s\n",
      "batch: 1200/1563 - train loss: 11.7318 - test loss: 13.6371 - train acc: 0.4469 - test acc: 0.3875 - 6m 42s\n",
      "batch: 1300/1563 - train loss: 11.8668 - test loss: 18.1217 - train acc: 0.4419 - test acc: 0.2847 - 6m 47s\n",
      "batch: 1400/1563 - train loss: 11.6851 - test loss: 13.3790 - train acc: 0.4456 - test acc: 0.3984 - 6m 53s\n",
      "batch: 1500/1563 - train loss: 11.5385 - test loss: 13.4793 - train acc: 0.4544 - test acc: 0.3912 - 6m 58s\n",
      "batch: 1563/1563 - train loss: 11.6390 - test loss: 13.8342 - train acc: 0.4491 - test acc: 0.3795 - 7m 3s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.1512 - test loss: 13.9504 - train acc: 0.5156 - test acc: 0.3877 - 7m 8s\n",
      "batch: 200/1563 - train loss: 10.2599 - test loss: 14.5504 - train acc: 0.5084 - test acc: 0.3687 - 7m 13s\n",
      "batch: 300/1563 - train loss: 10.3280 - test loss: 15.0011 - train acc: 0.5057 - test acc: 0.3545 - 7m 19s\n",
      "batch: 400/1563 - train loss: 10.4929 - test loss: 13.8898 - train acc: 0.4903 - test acc: 0.3864 - 7m 24s\n",
      "batch: 500/1563 - train loss: 10.4868 - test loss: 14.1301 - train acc: 0.4913 - test acc: 0.3871 - 7m 30s\n",
      "batch: 600/1563 - train loss: 10.4212 - test loss: 14.3619 - train acc: 0.4972 - test acc: 0.3801 - 7m 35s\n",
      "batch: 700/1563 - train loss: 10.3496 - test loss: 14.1422 - train acc: 0.5076 - test acc: 0.3812 - 7m 40s\n",
      "batch: 800/1563 - train loss: 10.6053 - test loss: 13.5533 - train acc: 0.4903 - test acc: 0.4007 - 7m 46s\n",
      "batch: 900/1563 - train loss: 10.8983 - test loss: 13.8109 - train acc: 0.4719 - test acc: 0.3918 - 7m 51s\n",
      "batch: 1000/1563 - train loss: 10.4688 - test loss: 13.6480 - train acc: 0.4959 - test acc: 0.3987 - 7m 57s\n",
      "batch: 1100/1563 - train loss: 10.8692 - test loss: 13.3569 - train acc: 0.4819 - test acc: 0.4043 - 8m 2s\n",
      "batch: 1200/1563 - train loss: 10.9994 - test loss: 13.5284 - train acc: 0.4759 - test acc: 0.3971 - 8m 7s\n",
      "batch: 1300/1563 - train loss: 10.9027 - test loss: 14.1281 - train acc: 0.4743 - test acc: 0.3809 - 8m 12s\n",
      "batch: 1400/1563 - train loss: 11.1529 - test loss: 13.2950 - train acc: 0.4662 - test acc: 0.4031 - 8m 18s\n",
      "batch: 1500/1563 - train loss: 10.7987 - test loss: 13.6002 - train acc: 0.4897 - test acc: 0.3989 - 8m 23s\n",
      "batch: 1563/1563 - train loss: 11.0037 - test loss: 13.1592 - train acc: 0.4887 - test acc: 0.4097 - 8m 28s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.5462 - test loss: 13.3644 - train acc: 0.5803 - test acc: 0.4137 - 8m 33s\n",
      "batch: 200/1563 - train loss: 8.8444 - test loss: 13.6974 - train acc: 0.5609 - test acc: 0.4041 - 8m 39s\n",
      "batch: 300/1563 - train loss: 9.2133 - test loss: 14.1459 - train acc: 0.5515 - test acc: 0.3922 - 8m 44s\n",
      "batch: 400/1563 - train loss: 9.3241 - test loss: 14.0916 - train acc: 0.5381 - test acc: 0.3935 - 8m 50s\n",
      "batch: 500/1563 - train loss: 9.5123 - test loss: 13.8131 - train acc: 0.5419 - test acc: 0.4053 - 8m 55s\n",
      "batch: 600/1563 - train loss: 9.5491 - test loss: 14.0489 - train acc: 0.5425 - test acc: 0.3973 - 9m 1s\n",
      "batch: 700/1563 - train loss: 9.3848 - test loss: 14.4520 - train acc: 0.5362 - test acc: 0.3725 - 9m 6s\n",
      "batch: 800/1563 - train loss: 9.5242 - test loss: 13.7494 - train acc: 0.5316 - test acc: 0.4146 - 9m 11s\n",
      "batch: 900/1563 - train loss: 9.2699 - test loss: 13.5758 - train acc: 0.5503 - test acc: 0.4103 - 9m 16s\n",
      "batch: 1000/1563 - train loss: 9.9099 - test loss: 13.3083 - train acc: 0.5147 - test acc: 0.4181 - 9m 22s\n",
      "batch: 1100/1563 - train loss: 9.8879 - test loss: 13.9120 - train acc: 0.5196 - test acc: 0.3944 - 9m 28s\n",
      "batch: 1200/1563 - train loss: 9.9602 - test loss: 13.7760 - train acc: 0.5203 - test acc: 0.3967 - 9m 33s\n",
      "batch: 1300/1563 - train loss: 9.8707 - test loss: 13.0544 - train acc: 0.5156 - test acc: 0.4238 - 9m 38s\n",
      "batch: 1400/1563 - train loss: 9.6995 - test loss: 13.2012 - train acc: 0.5141 - test acc: 0.4218 - 9m 44s\n",
      "batch: 1500/1563 - train loss: 10.0110 - test loss: 13.3477 - train acc: 0.5059 - test acc: 0.4165 - 9m 49s\n",
      "batch: 1563/1563 - train loss: 9.7405 - test loss: 13.1119 - train acc: 0.5187 - test acc: 0.4239 - 9m 54s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.6606 - test loss: 13.4302 - train acc: 0.6169 - test acc: 0.4252 - 9m 59s\n",
      "batch: 200/1563 - train loss: 7.4938 - test loss: 13.4491 - train acc: 0.6328 - test acc: 0.4193 - 10m 5s\n",
      "batch: 300/1563 - train loss: 7.9459 - test loss: 13.7023 - train acc: 0.5994 - test acc: 0.4104 - 10m 10s\n",
      "batch: 400/1563 - train loss: 8.2453 - test loss: 14.9956 - train acc: 0.5910 - test acc: 0.3924 - 10m 16s\n",
      "batch: 500/1563 - train loss: 8.4605 - test loss: 14.1265 - train acc: 0.5771 - test acc: 0.3990 - 10m 21s\n",
      "batch: 600/1563 - train loss: 8.2947 - test loss: 14.0288 - train acc: 0.5784 - test acc: 0.4139 - 10m 27s\n",
      "batch: 700/1563 - train loss: 8.3708 - test loss: 15.0449 - train acc: 0.5859 - test acc: 0.3859 - 10m 33s\n",
      "batch: 800/1563 - train loss: 8.6556 - test loss: 13.5913 - train acc: 0.5731 - test acc: 0.4097 - 10m 38s\n",
      "batch: 900/1563 - train loss: 8.9569 - test loss: 13.6411 - train acc: 0.5578 - test acc: 0.4143 - 10m 44s\n",
      "batch: 1000/1563 - train loss: 8.8338 - test loss: 13.8848 - train acc: 0.5512 - test acc: 0.4050 - 10m 49s\n",
      "batch: 1100/1563 - train loss: 8.5662 - test loss: 14.4841 - train acc: 0.5693 - test acc: 0.3934 - 10m 54s\n",
      "batch: 1200/1563 - train loss: 8.6411 - test loss: 13.3726 - train acc: 0.5662 - test acc: 0.4253 - 11m 0s\n",
      "batch: 1300/1563 - train loss: 8.9125 - test loss: 13.7929 - train acc: 0.5568 - test acc: 0.4095 - 11m 5s\n",
      "batch: 1400/1563 - train loss: 8.9630 - test loss: 13.1276 - train acc: 0.5503 - test acc: 0.4328 - 11m 11s\n",
      "batch: 1500/1563 - train loss: 8.9004 - test loss: 13.2384 - train acc: 0.5506 - test acc: 0.4210 - 11m 16s\n",
      "batch: 1563/1563 - train loss: 9.2272 - test loss: 13.4295 - train acc: 0.5384 - test acc: 0.4213 - 11m 21s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.6067 - test loss: 16.6620 - train acc: 0.6615 - test acc: 0.3594 - 11m 26s\n",
      "batch: 200/1563 - train loss: 6.8173 - test loss: 13.9225 - train acc: 0.6437 - test acc: 0.4218 - 11m 32s\n",
      "batch: 300/1563 - train loss: 6.9402 - test loss: 13.8850 - train acc: 0.6522 - test acc: 0.4212 - 11m 37s\n",
      "batch: 400/1563 - train loss: 7.1520 - test loss: 13.9949 - train acc: 0.6250 - test acc: 0.4145 - 11m 43s\n",
      "batch: 500/1563 - train loss: 6.8927 - test loss: 14.4050 - train acc: 0.6444 - test acc: 0.4077 - 11m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.4060 - test loss: 14.0304 - train acc: 0.6087 - test acc: 0.4168 - 11m 53s\n",
      "batch: 700/1563 - train loss: 7.6184 - test loss: 13.6967 - train acc: 0.6094 - test acc: 0.4265 - 11m 59s\n",
      "batch: 800/1563 - train loss: 7.7042 - test loss: 14.0013 - train acc: 0.6131 - test acc: 0.4181 - 12m 4s\n",
      "batch: 900/1563 - train loss: 7.8908 - test loss: 14.0954 - train acc: 0.5962 - test acc: 0.4153 - 12m 10s\n",
      "batch: 1000/1563 - train loss: 7.8546 - test loss: 13.9713 - train acc: 0.6044 - test acc: 0.4230 - 12m 15s\n",
      "batch: 1100/1563 - train loss: 8.0165 - test loss: 13.9359 - train acc: 0.5941 - test acc: 0.4187 - 12m 20s\n",
      "batch: 1200/1563 - train loss: 8.1277 - test loss: 13.5190 - train acc: 0.5834 - test acc: 0.4247 - 12m 26s\n",
      "batch: 1300/1563 - train loss: 8.1867 - test loss: 14.7538 - train acc: 0.5837 - test acc: 0.3911 - 12m 31s\n",
      "batch: 1400/1563 - train loss: 7.9880 - test loss: 13.7705 - train acc: 0.5952 - test acc: 0.4255 - 12m 36s\n",
      "batch: 1500/1563 - train loss: 8.0843 - test loss: 13.2579 - train acc: 0.5897 - test acc: 0.4328 - 12m 42s\n",
      "batch: 1563/1563 - train loss: 8.0038 - test loss: 14.0611 - train acc: 0.5912 - test acc: 0.4219 - 12m 47s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.8017 - test loss: 13.7593 - train acc: 0.7025 - test acc: 0.4383 - 12m 52s\n",
      "batch: 200/1563 - train loss: 5.7004 - test loss: 14.1653 - train acc: 0.7057 - test acc: 0.4212 - 12m 57s\n",
      "batch: 300/1563 - train loss: 6.0158 - test loss: 14.3112 - train acc: 0.6782 - test acc: 0.4259 - 13m 3s\n",
      "batch: 400/1563 - train loss: 6.0161 - test loss: 14.6698 - train acc: 0.6900 - test acc: 0.4193 - 13m 8s\n",
      "batch: 500/1563 - train loss: 6.3753 - test loss: 15.2167 - train acc: 0.6631 - test acc: 0.4009 - 13m 14s\n",
      "batch: 600/1563 - train loss: 6.5332 - test loss: 15.2441 - train acc: 0.6522 - test acc: 0.4115 - 13m 19s\n",
      "batch: 700/1563 - train loss: 6.6050 - test loss: 14.6558 - train acc: 0.6535 - test acc: 0.4125 - 13m 24s\n",
      "batch: 800/1563 - train loss: 6.6631 - test loss: 14.0325 - train acc: 0.6609 - test acc: 0.4313 - 13m 30s\n",
      "batch: 900/1563 - train loss: 6.5726 - test loss: 14.7787 - train acc: 0.6613 - test acc: 0.4118 - 13m 35s\n",
      "batch: 1000/1563 - train loss: 6.8163 - test loss: 15.0287 - train acc: 0.6528 - test acc: 0.4051 - 13m 41s\n",
      "batch: 1100/1563 - train loss: 7.2456 - test loss: 14.5034 - train acc: 0.6262 - test acc: 0.4166 - 13m 46s\n",
      "batch: 1200/1563 - train loss: 6.7598 - test loss: 14.2305 - train acc: 0.6472 - test acc: 0.4244 - 13m 52s\n",
      "batch: 1300/1563 - train loss: 7.1977 - test loss: 14.8277 - train acc: 0.6231 - test acc: 0.4148 - 13m 57s\n",
      "batch: 1400/1563 - train loss: 7.2132 - test loss: 13.8601 - train acc: 0.6235 - test acc: 0.4276 - 14m 2s\n",
      "batch: 1500/1563 - train loss: 7.4696 - test loss: 14.1328 - train acc: 0.6100 - test acc: 0.4299 - 14m 8s\n",
      "batch: 1563/1563 - train loss: 7.4425 - test loss: 14.1034 - train acc: 0.6106 - test acc: 0.4135 - 14m 12s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.7400 - test loss: 15.4496 - train acc: 0.7597 - test acc: 0.4020 - 14m 18s\n",
      "batch: 200/1563 - train loss: 5.0229 - test loss: 14.9530 - train acc: 0.7434 - test acc: 0.4235 - 14m 23s\n",
      "batch: 300/1563 - train loss: 4.8410 - test loss: 15.1783 - train acc: 0.7419 - test acc: 0.4157 - 14m 29s\n",
      "batch: 400/1563 - train loss: 5.4247 - test loss: 14.9981 - train acc: 0.7069 - test acc: 0.4233 - 14m 34s\n",
      "batch: 500/1563 - train loss: 5.2493 - test loss: 15.1844 - train acc: 0.7179 - test acc: 0.4194 - 14m 40s\n",
      "batch: 600/1563 - train loss: 5.4893 - test loss: 15.2911 - train acc: 0.7047 - test acc: 0.4239 - 14m 45s\n",
      "batch: 700/1563 - train loss: 5.9414 - test loss: 14.6219 - train acc: 0.6853 - test acc: 0.4251 - 14m 51s\n",
      "batch: 800/1563 - train loss: 5.8489 - test loss: 15.0770 - train acc: 0.6897 - test acc: 0.4263 - 14m 56s\n",
      "batch: 900/1563 - train loss: 6.0676 - test loss: 14.5914 - train acc: 0.6809 - test acc: 0.4337 - 15m 2s\n",
      "batch: 1000/1563 - train loss: 6.0863 - test loss: 15.0934 - train acc: 0.6865 - test acc: 0.4176 - 15m 7s\n",
      "batch: 1100/1563 - train loss: 6.0637 - test loss: 15.9339 - train acc: 0.6740 - test acc: 0.4051 - 15m 13s\n",
      "batch: 1200/1563 - train loss: 6.3701 - test loss: 14.2688 - train acc: 0.6606 - test acc: 0.4249 - 15m 19s\n",
      "batch: 1300/1563 - train loss: 6.1654 - test loss: 14.8069 - train acc: 0.6747 - test acc: 0.4149 - 15m 24s\n",
      "batch: 1400/1563 - train loss: 6.5046 - test loss: 14.4525 - train acc: 0.6635 - test acc: 0.4276 - 15m 30s\n",
      "batch: 1500/1563 - train loss: 6.4620 - test loss: 14.8094 - train acc: 0.6672 - test acc: 0.4212 - 15m 36s\n",
      "batch: 1563/1563 - train loss: 6.4357 - test loss: 14.9196 - train acc: 0.6763 - test acc: 0.4231 - 15m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.1351 - test loss: 15.0810 - train acc: 0.7787 - test acc: 0.4315 - 15m 46s\n",
      "batch: 200/1563 - train loss: 4.3052 - test loss: 15.7260 - train acc: 0.7606 - test acc: 0.4265 - 15m 52s\n",
      "batch: 300/1563 - train loss: 4.4094 - test loss: 15.8663 - train acc: 0.7577 - test acc: 0.4229 - 15m 58s\n",
      "batch: 400/1563 - train loss: 4.3931 - test loss: 15.1609 - train acc: 0.7556 - test acc: 0.4363 - 16m 4s\n",
      "batch: 500/1563 - train loss: 4.5950 - test loss: 15.2294 - train acc: 0.7465 - test acc: 0.4284 - 16m 9s\n",
      "batch: 600/1563 - train loss: 4.7649 - test loss: 16.1765 - train acc: 0.7365 - test acc: 0.4052 - 16m 15s\n",
      "batch: 700/1563 - train loss: 4.8795 - test loss: 15.5865 - train acc: 0.7353 - test acc: 0.4222 - 16m 21s\n",
      "batch: 800/1563 - train loss: 5.2033 - test loss: 15.9786 - train acc: 0.7209 - test acc: 0.4141 - 16m 26s\n",
      "batch: 900/1563 - train loss: 5.0995 - test loss: 15.4808 - train acc: 0.7210 - test acc: 0.4186 - 16m 32s\n",
      "batch: 1000/1563 - train loss: 5.4181 - test loss: 15.5382 - train acc: 0.7163 - test acc: 0.4201 - 16m 38s\n",
      "batch: 1100/1563 - train loss: 5.3342 - test loss: 15.1014 - train acc: 0.7059 - test acc: 0.4343 - 16m 44s\n",
      "batch: 1200/1563 - train loss: 5.6643 - test loss: 15.0458 - train acc: 0.7035 - test acc: 0.4371 - 16m 49s\n",
      "batch: 1300/1563 - train loss: 5.6084 - test loss: 15.5234 - train acc: 0.6966 - test acc: 0.4232 - 16m 56s\n",
      "batch: 1400/1563 - train loss: 5.3731 - test loss: 16.2912 - train acc: 0.7131 - test acc: 0.4103 - 17m 1s\n",
      "batch: 1500/1563 - train loss: 5.5655 - test loss: 16.0697 - train acc: 0.7088 - test acc: 0.4186 - 17m 7s\n",
      "batch: 1563/1563 - train loss: 5.9369 - test loss: 14.6412 - train acc: 0.6801 - test acc: 0.4377 - 17m 11s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.4913 - test loss: 15.1866 - train acc: 0.8196 - test acc: 0.4413 - 17m 17s\n",
      "batch: 200/1563 - train loss: 3.4014 - test loss: 15.5527 - train acc: 0.8149 - test acc: 0.4371 - 17m 23s\n",
      "batch: 300/1563 - train loss: 3.4925 - test loss: 15.6012 - train acc: 0.8118 - test acc: 0.4385 - 17m 29s\n",
      "batch: 400/1563 - train loss: 3.4697 - test loss: 16.1983 - train acc: 0.8103 - test acc: 0.4293 - 17m 34s\n",
      "batch: 500/1563 - train loss: 4.0685 - test loss: 16.0399 - train acc: 0.7694 - test acc: 0.4355 - 17m 40s\n",
      "batch: 600/1563 - train loss: 3.9177 - test loss: 16.0316 - train acc: 0.7849 - test acc: 0.4324 - 17m 45s\n",
      "batch: 700/1563 - train loss: 4.2059 - test loss: 15.7097 - train acc: 0.7694 - test acc: 0.4422 - 17m 51s\n",
      "batch: 800/1563 - train loss: 4.5177 - test loss: 15.6784 - train acc: 0.7562 - test acc: 0.4340 - 17m 56s\n",
      "batch: 900/1563 - train loss: 4.4357 - test loss: 15.6873 - train acc: 0.7563 - test acc: 0.4315 - 18m 2s\n",
      "batch: 1000/1563 - train loss: 4.3708 - test loss: 16.2046 - train acc: 0.7559 - test acc: 0.4250 - 18m 8s\n",
      "batch: 1100/1563 - train loss: 4.5727 - test loss: 16.3484 - train acc: 0.7550 - test acc: 0.4167 - 18m 13s\n",
      "batch: 1200/1563 - train loss: 4.5555 - test loss: 15.7322 - train acc: 0.7525 - test acc: 0.4302 - 18m 19s\n",
      "batch: 1300/1563 - train loss: 4.6883 - test loss: 15.8324 - train acc: 0.7394 - test acc: 0.4328 - 18m 25s\n",
      "batch: 1400/1563 - train loss: 4.8636 - test loss: 15.8998 - train acc: 0.7253 - test acc: 0.4255 - 18m 31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 4.8592 - test loss: 16.1157 - train acc: 0.7341 - test acc: 0.4223 - 18m 36s\n",
      "batch: 1563/1563 - train loss: 4.9547 - test loss: 15.7235 - train acc: 0.7363 - test acc: 0.4196 - 18m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.9473 - test loss: 15.7749 - train acc: 0.8321 - test acc: 0.4379 - 18m 46s\n",
      "batch: 200/1563 - train loss: 2.7986 - test loss: 16.5090 - train acc: 0.8412 - test acc: 0.4260 - 18m 52s\n",
      "batch: 300/1563 - train loss: 2.9360 - test loss: 16.7750 - train acc: 0.8384 - test acc: 0.4276 - 18m 57s\n",
      "batch: 400/1563 - train loss: 3.1506 - test loss: 16.3970 - train acc: 0.8190 - test acc: 0.4361 - 19m 3s\n",
      "batch: 500/1563 - train loss: 2.9904 - test loss: 17.3924 - train acc: 0.8300 - test acc: 0.4135 - 19m 8s\n",
      "batch: 600/1563 - train loss: 3.3192 - test loss: 17.4158 - train acc: 0.8156 - test acc: 0.4217 - 19m 14s\n",
      "batch: 700/1563 - train loss: 3.4509 - test loss: 16.3660 - train acc: 0.8078 - test acc: 0.4290 - 19m 19s\n",
      "batch: 800/1563 - train loss: 3.4625 - test loss: 17.4802 - train acc: 0.8077 - test acc: 0.4162 - 19m 25s\n",
      "batch: 900/1563 - train loss: 3.8724 - test loss: 16.1094 - train acc: 0.7893 - test acc: 0.4396 - 19m 30s\n",
      "batch: 1000/1563 - train loss: 3.8528 - test loss: 16.7241 - train acc: 0.7909 - test acc: 0.4314 - 19m 36s\n",
      "batch: 1100/1563 - train loss: 3.9079 - test loss: 16.6997 - train acc: 0.7906 - test acc: 0.4270 - 19m 41s\n",
      "batch: 1200/1563 - train loss: 3.9124 - test loss: 19.4426 - train acc: 0.7778 - test acc: 0.3802 - 19m 47s\n",
      "batch: 1300/1563 - train loss: 4.1522 - test loss: 16.5827 - train acc: 0.7678 - test acc: 0.4289 - 19m 52s\n",
      "batch: 1400/1563 - train loss: 4.3239 - test loss: 15.9450 - train acc: 0.7697 - test acc: 0.4283 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 1401/1563 - train loss: 4.3289 - test loss: 15.9545 - train acc: 0.7691 - test acc: 0.4274 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 9\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 25.7913 - test loss: 24.4823 - train acc: 0.0471 - test acc: 0.0593 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.9507 - test loss: 23.0725 - train acc: 0.0683 - test acc: 0.0861 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.5492 - test loss: 23.4507 - train acc: 0.0937 - test acc: 0.0938 - 0m 13s\n",
      "batch: 400/1563 - train loss: 22.0808 - test loss: 21.6525 - train acc: 0.1043 - test acc: 0.1159 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.5748 - test loss: 21.0117 - train acc: 0.1186 - test acc: 0.1296 - 0m 24s\n",
      "batch: 600/1563 - train loss: 20.9532 - test loss: 20.6981 - train acc: 0.1334 - test acc: 0.1413 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.4927 - test loss: 20.8785 - train acc: 0.1538 - test acc: 0.1351 - 0m 34s\n",
      "batch: 800/1563 - train loss: 20.3484 - test loss: 20.4838 - train acc: 0.1557 - test acc: 0.1477 - 0m 40s\n",
      "batch: 900/1563 - train loss: 19.8882 - test loss: 19.7332 - train acc: 0.1794 - test acc: 0.1746 - 0m 45s\n",
      "batch: 1000/1563 - train loss: 19.8194 - test loss: 19.6734 - train acc: 0.1660 - test acc: 0.1661 - 0m 51s\n",
      "batch: 1100/1563 - train loss: 19.7240 - test loss: 19.7145 - train acc: 0.1739 - test acc: 0.1641 - 0m 56s\n",
      "batch: 1200/1563 - train loss: 19.0128 - test loss: 18.5359 - train acc: 0.1879 - test acc: 0.2001 - 1m 1s\n",
      "batch: 1300/1563 - train loss: 18.8378 - test loss: 19.1116 - train acc: 0.1904 - test acc: 0.1975 - 1m 6s\n",
      "batch: 1400/1563 - train loss: 18.8341 - test loss: 19.9924 - train acc: 0.1913 - test acc: 0.1833 - 1m 12s\n",
      "batch: 1500/1563 - train loss: 18.7281 - test loss: 18.0428 - train acc: 0.2045 - test acc: 0.2259 - 1m 17s\n",
      "batch: 1563/1563 - train loss: 18.2504 - test loss: 19.2364 - train acc: 0.2126 - test acc: 0.1889 - 1m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.6054 - test loss: 18.9409 - train acc: 0.2382 - test acc: 0.2041 - 1m 27s\n",
      "batch: 200/1563 - train loss: 17.5222 - test loss: 18.5280 - train acc: 0.2419 - test acc: 0.2170 - 1m 33s\n",
      "batch: 300/1563 - train loss: 17.2073 - test loss: 18.0977 - train acc: 0.2416 - test acc: 0.2187 - 1m 38s\n",
      "batch: 400/1563 - train loss: 17.3200 - test loss: 17.8208 - train acc: 0.2500 - test acc: 0.2281 - 1m 44s\n",
      "batch: 500/1563 - train loss: 17.4193 - test loss: 17.2628 - train acc: 0.2326 - test acc: 0.2485 - 1m 50s\n",
      "batch: 600/1563 - train loss: 17.1547 - test loss: 17.6361 - train acc: 0.2459 - test acc: 0.2468 - 1m 56s\n",
      "batch: 700/1563 - train loss: 17.1179 - test loss: 17.0940 - train acc: 0.2478 - test acc: 0.2562 - 2m 1s\n",
      "batch: 800/1563 - train loss: 17.1272 - test loss: 17.4915 - train acc: 0.2563 - test acc: 0.2457 - 2m 7s\n",
      "batch: 900/1563 - train loss: 16.7063 - test loss: 17.9593 - train acc: 0.2591 - test acc: 0.2355 - 2m 12s\n",
      "batch: 1000/1563 - train loss: 16.6790 - test loss: 16.4965 - train acc: 0.2740 - test acc: 0.2835 - 2m 17s\n",
      "batch: 1100/1563 - train loss: 16.3417 - test loss: 17.2368 - train acc: 0.2737 - test acc: 0.2560 - 2m 23s\n",
      "batch: 1200/1563 - train loss: 16.1058 - test loss: 16.0034 - train acc: 0.2884 - test acc: 0.2909 - 2m 29s\n",
      "batch: 1300/1563 - train loss: 16.2001 - test loss: 15.8951 - train acc: 0.2831 - test acc: 0.2890 - 2m 34s\n",
      "batch: 1400/1563 - train loss: 16.1170 - test loss: 16.1262 - train acc: 0.2826 - test acc: 0.2889 - 2m 39s\n",
      "batch: 1500/1563 - train loss: 16.0280 - test loss: 15.8858 - train acc: 0.2846 - test acc: 0.2909 - 2m 45s\n",
      "batch: 1563/1563 - train loss: 16.1749 - test loss: 15.5866 - train acc: 0.2815 - test acc: 0.3052 - 2m 49s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.7464 - test loss: 17.1852 - train acc: 0.3284 - test acc: 0.2591 - 2m 55s\n",
      "batch: 200/1563 - train loss: 14.8797 - test loss: 15.9523 - train acc: 0.3212 - test acc: 0.3070 - 3m 0s\n",
      "batch: 300/1563 - train loss: 14.9402 - test loss: 15.8174 - train acc: 0.3193 - test acc: 0.3014 - 3m 6s\n",
      "batch: 400/1563 - train loss: 14.8915 - test loss: 16.3462 - train acc: 0.3212 - test acc: 0.2873 - 3m 11s\n",
      "batch: 500/1563 - train loss: 14.7747 - test loss: 16.4260 - train acc: 0.3312 - test acc: 0.2893 - 3m 16s\n",
      "batch: 600/1563 - train loss: 14.9524 - test loss: 15.1919 - train acc: 0.3300 - test acc: 0.3243 - 3m 22s\n",
      "batch: 700/1563 - train loss: 14.8163 - test loss: 16.8077 - train acc: 0.3256 - test acc: 0.2632 - 3m 27s\n",
      "batch: 800/1563 - train loss: 14.5108 - test loss: 15.5038 - train acc: 0.3519 - test acc: 0.3180 - 3m 33s\n",
      "batch: 900/1563 - train loss: 14.9605 - test loss: 15.3282 - train acc: 0.3265 - test acc: 0.3180 - 3m 38s\n",
      "batch: 1000/1563 - train loss: 14.3221 - test loss: 15.1135 - train acc: 0.3513 - test acc: 0.3259 - 3m 44s\n",
      "batch: 1100/1563 - train loss: 14.3627 - test loss: 16.7641 - train acc: 0.3372 - test acc: 0.2872 - 3m 49s\n",
      "batch: 1200/1563 - train loss: 14.2118 - test loss: 15.7041 - train acc: 0.3516 - test acc: 0.3106 - 3m 54s\n",
      "batch: 1300/1563 - train loss: 14.7257 - test loss: 14.9998 - train acc: 0.3312 - test acc: 0.3307 - 4m 0s\n",
      "batch: 1400/1563 - train loss: 14.2158 - test loss: 14.5053 - train acc: 0.3481 - test acc: 0.3481 - 4m 6s\n",
      "batch: 1500/1563 - train loss: 14.3537 - test loss: 14.9882 - train acc: 0.3612 - test acc: 0.3297 - 4m 11s\n",
      "batch: 1563/1563 - train loss: 14.5216 - test loss: 14.7224 - train acc: 0.3509 - test acc: 0.3379 - 4m 16s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.2058 - test loss: 15.5741 - train acc: 0.3844 - test acc: 0.3155 - 4m 21s\n",
      "batch: 200/1563 - train loss: 12.7831 - test loss: 14.4848 - train acc: 0.4085 - test acc: 0.3463 - 4m 27s\n",
      "batch: 300/1563 - train loss: 13.1677 - test loss: 15.5537 - train acc: 0.3910 - test acc: 0.3259 - 4m 32s\n",
      "batch: 400/1563 - train loss: 13.2868 - test loss: 14.7498 - train acc: 0.3859 - test acc: 0.3384 - 4m 38s\n",
      "batch: 500/1563 - train loss: 13.1994 - test loss: 14.6105 - train acc: 0.3859 - test acc: 0.3402 - 4m 44s\n",
      "batch: 600/1563 - train loss: 13.2259 - test loss: 14.4331 - train acc: 0.4007 - test acc: 0.3461 - 4m 49s\n",
      "batch: 700/1563 - train loss: 13.1686 - test loss: 15.5467 - train acc: 0.3966 - test acc: 0.3216 - 4m 55s\n",
      "batch: 800/1563 - train loss: 12.8215 - test loss: 14.1966 - train acc: 0.4041 - test acc: 0.3639 - 5m 0s\n",
      "batch: 900/1563 - train loss: 12.8442 - test loss: 15.8879 - train acc: 0.3909 - test acc: 0.3117 - 5m 6s\n",
      "batch: 1000/1563 - train loss: 12.9333 - test loss: 14.7846 - train acc: 0.4028 - test acc: 0.3440 - 5m 11s\n",
      "batch: 1100/1563 - train loss: 13.0177 - test loss: 14.1665 - train acc: 0.3904 - test acc: 0.3621 - 5m 17s\n",
      "batch: 1200/1563 - train loss: 12.9208 - test loss: 14.1004 - train acc: 0.4031 - test acc: 0.3676 - 5m 22s\n",
      "batch: 1300/1563 - train loss: 12.7039 - test loss: 14.6723 - train acc: 0.4050 - test acc: 0.3540 - 5m 28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 12.9952 - test loss: 13.8588 - train acc: 0.4034 - test acc: 0.3720 - 5m 34s\n",
      "batch: 1500/1563 - train loss: 12.9545 - test loss: 14.2874 - train acc: 0.3960 - test acc: 0.3541 - 5m 39s\n",
      "batch: 1563/1563 - train loss: 12.7545 - test loss: 14.0042 - train acc: 0.4069 - test acc: 0.3704 - 5m 44s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3788 - test loss: 20.2351 - train acc: 0.4585 - test acc: 0.2609 - 5m 50s\n",
      "batch: 200/1563 - train loss: 11.0401 - test loss: 14.0059 - train acc: 0.4716 - test acc: 0.3807 - 5m 56s\n",
      "batch: 300/1563 - train loss: 11.1619 - test loss: 14.6793 - train acc: 0.4638 - test acc: 0.3523 - 6m 1s\n",
      "batch: 400/1563 - train loss: 11.5825 - test loss: 14.9588 - train acc: 0.4456 - test acc: 0.3493 - 6m 7s\n",
      "batch: 500/1563 - train loss: 11.8624 - test loss: 14.0303 - train acc: 0.4440 - test acc: 0.3694 - 6m 13s\n",
      "batch: 600/1563 - train loss: 11.8247 - test loss: 13.6034 - train acc: 0.4466 - test acc: 0.3895 - 6m 18s\n",
      "batch: 700/1563 - train loss: 11.7548 - test loss: 14.5362 - train acc: 0.4366 - test acc: 0.3601 - 6m 24s\n",
      "batch: 800/1563 - train loss: 12.0198 - test loss: 13.7134 - train acc: 0.4354 - test acc: 0.3835 - 6m 30s\n",
      "batch: 900/1563 - train loss: 12.3408 - test loss: 13.9468 - train acc: 0.4241 - test acc: 0.3715 - 6m 35s\n",
      "batch: 1000/1563 - train loss: 11.9472 - test loss: 14.0111 - train acc: 0.4325 - test acc: 0.3738 - 6m 42s\n",
      "batch: 1100/1563 - train loss: 11.7251 - test loss: 13.8600 - train acc: 0.4500 - test acc: 0.3813 - 6m 48s\n",
      "batch: 1200/1563 - train loss: 11.7964 - test loss: 14.6416 - train acc: 0.4347 - test acc: 0.3606 - 6m 53s\n",
      "batch: 1300/1563 - train loss: 11.9131 - test loss: 13.5295 - train acc: 0.4435 - test acc: 0.3920 - 6m 59s\n",
      "batch: 1400/1563 - train loss: 11.9223 - test loss: 13.7734 - train acc: 0.4360 - test acc: 0.3793 - 7m 5s\n",
      "batch: 1500/1563 - train loss: 11.9500 - test loss: 17.0071 - train acc: 0.4488 - test acc: 0.2960 - 7m 11s\n",
      "batch: 1563/1563 - train loss: 12.1372 - test loss: 14.5149 - train acc: 0.4335 - test acc: 0.3542 - 7m 15s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.8522 - test loss: 13.7094 - train acc: 0.5188 - test acc: 0.3920 - 7m 21s\n",
      "batch: 200/1563 - train loss: 10.0537 - test loss: 13.5713 - train acc: 0.5131 - test acc: 0.3990 - 7m 26s\n",
      "batch: 300/1563 - train loss: 10.4126 - test loss: 13.4560 - train acc: 0.4931 - test acc: 0.4038 - 7m 32s\n",
      "batch: 400/1563 - train loss: 10.8228 - test loss: 13.5300 - train acc: 0.4819 - test acc: 0.4002 - 7m 37s\n",
      "batch: 500/1563 - train loss: 10.2872 - test loss: 13.4457 - train acc: 0.4965 - test acc: 0.4036 - 7m 43s\n",
      "batch: 600/1563 - train loss: 10.5705 - test loss: 14.4333 - train acc: 0.4850 - test acc: 0.3865 - 7m 48s\n",
      "batch: 700/1563 - train loss: 10.3887 - test loss: 15.0042 - train acc: 0.5038 - test acc: 0.3611 - 7m 53s\n",
      "batch: 800/1563 - train loss: 10.7020 - test loss: 13.5992 - train acc: 0.4897 - test acc: 0.3949 - 7m 59s\n",
      "batch: 900/1563 - train loss: 10.5715 - test loss: 13.6314 - train acc: 0.4943 - test acc: 0.3967 - 8m 4s\n",
      "batch: 1000/1563 - train loss: 10.7593 - test loss: 14.1508 - train acc: 0.4741 - test acc: 0.3783 - 8m 10s\n",
      "batch: 1100/1563 - train loss: 10.5408 - test loss: 14.3190 - train acc: 0.4906 - test acc: 0.3790 - 8m 16s\n",
      "batch: 1200/1563 - train loss: 10.6675 - test loss: 13.4528 - train acc: 0.4834 - test acc: 0.3992 - 8m 21s\n",
      "batch: 1300/1563 - train loss: 10.7320 - test loss: 13.6675 - train acc: 0.4762 - test acc: 0.3927 - 8m 26s\n",
      "batch: 1400/1563 - train loss: 10.5815 - test loss: 13.2833 - train acc: 0.4819 - test acc: 0.4043 - 8m 32s\n",
      "batch: 1500/1563 - train loss: 11.0390 - test loss: 13.7714 - train acc: 0.4722 - test acc: 0.3884 - 8m 37s\n",
      "batch: 1563/1563 - train loss: 10.8027 - test loss: 13.9253 - train acc: 0.4835 - test acc: 0.3859 - 8m 42s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.2781 - test loss: 13.8570 - train acc: 0.5931 - test acc: 0.4006 - 8m 48s\n",
      "batch: 200/1563 - train loss: 9.0363 - test loss: 13.5045 - train acc: 0.5565 - test acc: 0.4066 - 8m 53s\n",
      "batch: 300/1563 - train loss: 8.7675 - test loss: 13.7899 - train acc: 0.5672 - test acc: 0.4088 - 8m 58s\n",
      "batch: 400/1563 - train loss: 9.2128 - test loss: 14.0433 - train acc: 0.5409 - test acc: 0.3942 - 9m 4s\n",
      "batch: 500/1563 - train loss: 9.4829 - test loss: 13.8773 - train acc: 0.5296 - test acc: 0.3983 - 9m 9s\n",
      "batch: 600/1563 - train loss: 9.5419 - test loss: 13.9581 - train acc: 0.5347 - test acc: 0.3971 - 9m 15s\n",
      "batch: 700/1563 - train loss: 9.3434 - test loss: 14.4177 - train acc: 0.5419 - test acc: 0.3851 - 9m 21s\n",
      "batch: 800/1563 - train loss: 9.5057 - test loss: 13.9231 - train acc: 0.5303 - test acc: 0.4043 - 9m 26s\n",
      "batch: 900/1563 - train loss: 9.7769 - test loss: 13.7889 - train acc: 0.5172 - test acc: 0.3982 - 9m 31s\n",
      "batch: 1000/1563 - train loss: 9.7506 - test loss: 14.0242 - train acc: 0.5219 - test acc: 0.4011 - 9m 37s\n",
      "batch: 1100/1563 - train loss: 9.6343 - test loss: 13.6345 - train acc: 0.5275 - test acc: 0.4041 - 9m 42s\n",
      "batch: 1200/1563 - train loss: 9.7566 - test loss: 13.3181 - train acc: 0.5209 - test acc: 0.4206 - 9m 48s\n",
      "batch: 1300/1563 - train loss: 9.4935 - test loss: 13.1766 - train acc: 0.5256 - test acc: 0.4242 - 9m 54s\n",
      "batch: 1400/1563 - train loss: 9.6331 - test loss: 14.0778 - train acc: 0.5219 - test acc: 0.3976 - 9m 59s\n",
      "batch: 1500/1563 - train loss: 9.8148 - test loss: 14.6881 - train acc: 0.5110 - test acc: 0.3771 - 10m 4s\n",
      "batch: 1563/1563 - train loss: 9.8942 - test loss: 13.5014 - train acc: 0.5056 - test acc: 0.4118 - 10m 9s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.7475 - test loss: 13.7149 - train acc: 0.6000 - test acc: 0.4174 - 10m 14s\n",
      "batch: 200/1563 - train loss: 7.7133 - test loss: 13.4181 - train acc: 0.6047 - test acc: 0.4184 - 10m 20s\n",
      "batch: 300/1563 - train loss: 7.8994 - test loss: 14.8678 - train acc: 0.6046 - test acc: 0.3896 - 10m 26s\n",
      "batch: 400/1563 - train loss: 7.9826 - test loss: 13.8017 - train acc: 0.5997 - test acc: 0.4200 - 10m 31s\n",
      "batch: 500/1563 - train loss: 8.2415 - test loss: 14.7650 - train acc: 0.5925 - test acc: 0.3910 - 10m 36s\n",
      "batch: 600/1563 - train loss: 8.4184 - test loss: 15.1376 - train acc: 0.5815 - test acc: 0.3926 - 10m 42s\n",
      "batch: 700/1563 - train loss: 8.3348 - test loss: 14.0461 - train acc: 0.5834 - test acc: 0.4126 - 10m 47s\n",
      "batch: 800/1563 - train loss: 8.5263 - test loss: 13.7996 - train acc: 0.5703 - test acc: 0.4121 - 10m 53s\n",
      "batch: 900/1563 - train loss: 8.5157 - test loss: 13.3714 - train acc: 0.5638 - test acc: 0.4251 - 10m 58s\n",
      "batch: 1000/1563 - train loss: 8.5050 - test loss: 13.4598 - train acc: 0.5690 - test acc: 0.4221 - 11m 4s\n",
      "batch: 1100/1563 - train loss: 9.0020 - test loss: 14.3768 - train acc: 0.5462 - test acc: 0.3976 - 11m 9s\n",
      "batch: 1200/1563 - train loss: 8.4491 - test loss: 13.2629 - train acc: 0.5737 - test acc: 0.4243 - 11m 15s\n",
      "batch: 1300/1563 - train loss: 8.9716 - test loss: 14.1714 - train acc: 0.5547 - test acc: 0.4016 - 11m 20s\n",
      "batch: 1400/1563 - train loss: 8.8297 - test loss: 13.5344 - train acc: 0.5653 - test acc: 0.4172 - 11m 26s\n",
      "batch: 1500/1563 - train loss: 8.9195 - test loss: 13.5274 - train acc: 0.5562 - test acc: 0.4097 - 11m 32s\n",
      "batch: 1563/1563 - train loss: 8.6035 - test loss: 14.3366 - train acc: 0.5712 - test acc: 0.4008 - 11m 36s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4477 - test loss: 13.9646 - train acc: 0.6646 - test acc: 0.4060 - 11m 42s\n",
      "batch: 200/1563 - train loss: 6.8292 - test loss: 14.4136 - train acc: 0.6428 - test acc: 0.4080 - 11m 47s\n",
      "batch: 300/1563 - train loss: 6.9312 - test loss: 15.6893 - train acc: 0.6354 - test acc: 0.3805 - 11m 53s\n",
      "batch: 400/1563 - train loss: 6.8537 - test loss: 15.8455 - train acc: 0.6497 - test acc: 0.3720 - 11m 59s\n",
      "batch: 500/1563 - train loss: 7.4589 - test loss: 13.9894 - train acc: 0.6134 - test acc: 0.4195 - 12m 4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.3174 - test loss: 14.5966 - train acc: 0.6260 - test acc: 0.4140 - 12m 10s\n",
      "batch: 700/1563 - train loss: 7.3679 - test loss: 14.5649 - train acc: 0.6188 - test acc: 0.4128 - 12m 15s\n",
      "batch: 800/1563 - train loss: 7.4998 - test loss: 15.0364 - train acc: 0.6172 - test acc: 0.3921 - 12m 21s\n",
      "batch: 900/1563 - train loss: 8.1796 - test loss: 13.9973 - train acc: 0.5884 - test acc: 0.4205 - 12m 26s\n",
      "batch: 1000/1563 - train loss: 7.7033 - test loss: 13.6794 - train acc: 0.6137 - test acc: 0.4274 - 12m 32s\n",
      "batch: 1100/1563 - train loss: 8.0284 - test loss: 14.0793 - train acc: 0.5913 - test acc: 0.4178 - 12m 38s\n",
      "batch: 1200/1563 - train loss: 7.6917 - test loss: 14.3547 - train acc: 0.6137 - test acc: 0.4155 - 12m 44s\n",
      "batch: 1300/1563 - train loss: 8.1471 - test loss: 13.6808 - train acc: 0.5788 - test acc: 0.4291 - 12m 49s\n",
      "batch: 1400/1563 - train loss: 8.0812 - test loss: 13.9148 - train acc: 0.5940 - test acc: 0.4151 - 12m 54s\n",
      "batch: 1500/1563 - train loss: 7.8876 - test loss: 13.9067 - train acc: 0.5965 - test acc: 0.4221 - 13m 0s\n",
      "batch: 1563/1563 - train loss: 7.7597 - test loss: 13.5654 - train acc: 0.5994 - test acc: 0.4281 - 13m 5s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.5070 - test loss: 13.9367 - train acc: 0.7207 - test acc: 0.4271 - 13m 10s\n",
      "batch: 200/1563 - train loss: 5.7261 - test loss: 14.9886 - train acc: 0.6966 - test acc: 0.4088 - 13m 16s\n",
      "batch: 300/1563 - train loss: 5.9656 - test loss: 14.6477 - train acc: 0.6791 - test acc: 0.4206 - 13m 21s\n",
      "batch: 400/1563 - train loss: 5.8253 - test loss: 15.4941 - train acc: 0.6988 - test acc: 0.4048 - 13m 26s\n",
      "batch: 500/1563 - train loss: 6.1910 - test loss: 14.9267 - train acc: 0.6709 - test acc: 0.4069 - 13m 32s\n",
      "batch: 600/1563 - train loss: 6.1299 - test loss: 14.2733 - train acc: 0.6822 - test acc: 0.4274 - 13m 38s\n",
      "batch: 700/1563 - train loss: 6.4540 - test loss: 14.4731 - train acc: 0.6591 - test acc: 0.4228 - 13m 43s\n",
      "batch: 800/1563 - train loss: 6.6909 - test loss: 14.7402 - train acc: 0.6591 - test acc: 0.4177 - 13m 49s\n",
      "batch: 900/1563 - train loss: 6.5610 - test loss: 15.2908 - train acc: 0.6447 - test acc: 0.4017 - 13m 54s\n",
      "batch: 1000/1563 - train loss: 6.9338 - test loss: 14.2401 - train acc: 0.6439 - test acc: 0.4220 - 14m 0s\n",
      "batch: 1100/1563 - train loss: 6.9639 - test loss: 14.6324 - train acc: 0.6410 - test acc: 0.4215 - 14m 6s\n",
      "batch: 1200/1563 - train loss: 6.8822 - test loss: 16.0546 - train acc: 0.6365 - test acc: 0.3826 - 14m 11s\n",
      "batch: 1300/1563 - train loss: 7.2026 - test loss: 14.3587 - train acc: 0.6316 - test acc: 0.4252 - 14m 17s\n",
      "batch: 1400/1563 - train loss: 7.2184 - test loss: 13.9253 - train acc: 0.6316 - test acc: 0.4309 - 14m 22s\n",
      "batch: 1500/1563 - train loss: 7.2021 - test loss: 14.6305 - train acc: 0.6313 - test acc: 0.4251 - 14m 28s\n",
      "batch: 1563/1563 - train loss: 7.2363 - test loss: 13.9108 - train acc: 0.6269 - test acc: 0.4263 - 14m 32s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.7574 - test loss: 14.6919 - train acc: 0.7456 - test acc: 0.4322 - 14m 38s\n",
      "batch: 200/1563 - train loss: 4.8204 - test loss: 16.1117 - train acc: 0.7481 - test acc: 0.4003 - 14m 43s\n",
      "batch: 300/1563 - train loss: 5.1532 - test loss: 15.2509 - train acc: 0.7160 - test acc: 0.4151 - 14m 49s\n",
      "batch: 400/1563 - train loss: 5.1377 - test loss: 15.5335 - train acc: 0.7213 - test acc: 0.4178 - 14m 54s\n",
      "batch: 500/1563 - train loss: 5.4884 - test loss: 15.2154 - train acc: 0.7044 - test acc: 0.4228 - 14m 59s\n",
      "batch: 600/1563 - train loss: 5.5285 - test loss: 15.8356 - train acc: 0.6997 - test acc: 0.3990 - 15m 5s\n",
      "batch: 700/1563 - train loss: 5.6577 - test loss: 15.2874 - train acc: 0.6922 - test acc: 0.4161 - 15m 11s\n",
      "batch: 800/1563 - train loss: 5.8583 - test loss: 15.4479 - train acc: 0.6941 - test acc: 0.4099 - 15m 16s\n",
      "batch: 900/1563 - train loss: 5.9082 - test loss: 17.0016 - train acc: 0.6850 - test acc: 0.3861 - 15m 21s\n",
      "batch: 1000/1563 - train loss: 6.2166 - test loss: 14.9011 - train acc: 0.6688 - test acc: 0.4246 - 15m 27s\n",
      "batch: 1100/1563 - train loss: 5.8578 - test loss: 14.7460 - train acc: 0.6869 - test acc: 0.4211 - 15m 32s\n",
      "batch: 1200/1563 - train loss: 6.0713 - test loss: 14.8706 - train acc: 0.6835 - test acc: 0.4206 - 15m 38s\n",
      "batch: 1300/1563 - train loss: 6.1201 - test loss: 14.8514 - train acc: 0.6775 - test acc: 0.4222 - 15m 43s\n",
      "batch: 1400/1563 - train loss: 6.1027 - test loss: 16.0244 - train acc: 0.6741 - test acc: 0.3990 - 15m 49s\n",
      "batch: 1500/1563 - train loss: 6.1931 - test loss: 15.0750 - train acc: 0.6835 - test acc: 0.4159 - 15m 54s\n",
      "batch: 1563/1563 - train loss: 6.1460 - test loss: 14.5893 - train acc: 0.6735 - test acc: 0.4252 - 15m 59s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.9336 - test loss: 15.3440 - train acc: 0.7877 - test acc: 0.4249 - 16m 4s\n",
      "batch: 200/1563 - train loss: 4.1080 - test loss: 15.3007 - train acc: 0.7803 - test acc: 0.4283 - 16m 10s\n",
      "batch: 300/1563 - train loss: 4.0608 - test loss: 15.3757 - train acc: 0.7725 - test acc: 0.4249 - 16m 15s\n",
      "batch: 400/1563 - train loss: 4.3765 - test loss: 15.5166 - train acc: 0.7612 - test acc: 0.4238 - 16m 21s\n",
      "batch: 500/1563 - train loss: 4.5733 - test loss: 15.7859 - train acc: 0.7522 - test acc: 0.4196 - 16m 26s\n",
      "batch: 600/1563 - train loss: 4.7429 - test loss: 15.9311 - train acc: 0.7401 - test acc: 0.4210 - 16m 31s\n",
      "batch: 700/1563 - train loss: 4.8768 - test loss: 16.1505 - train acc: 0.7328 - test acc: 0.4120 - 16m 37s\n",
      "batch: 800/1563 - train loss: 5.0367 - test loss: 15.6367 - train acc: 0.7303 - test acc: 0.4252 - 16m 43s\n",
      "batch: 900/1563 - train loss: 5.2163 - test loss: 15.4866 - train acc: 0.7247 - test acc: 0.4244 - 16m 48s\n",
      "batch: 1000/1563 - train loss: 5.2353 - test loss: 15.7480 - train acc: 0.7144 - test acc: 0.4271 - 16m 54s\n",
      "batch: 1100/1563 - train loss: 5.3373 - test loss: 15.9683 - train acc: 0.7209 - test acc: 0.4172 - 16m 59s\n",
      "batch: 1200/1563 - train loss: 5.3628 - test loss: 15.7570 - train acc: 0.7069 - test acc: 0.4138 - 17m 5s\n",
      "batch: 1300/1563 - train loss: 5.5027 - test loss: 16.0854 - train acc: 0.7125 - test acc: 0.4089 - 17m 10s\n",
      "batch: 1400/1563 - train loss: 5.3396 - test loss: 15.5348 - train acc: 0.7118 - test acc: 0.4231 - 17m 16s\n",
      "batch: 1500/1563 - train loss: 5.2227 - test loss: 16.0057 - train acc: 0.7072 - test acc: 0.4134 - 17m 21s\n",
      "batch: 1563/1563 - train loss: 5.3320 - test loss: 15.6034 - train acc: 0.7119 - test acc: 0.4212 - 17m 26s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.3660 - test loss: 15.2393 - train acc: 0.8177 - test acc: 0.4442 - 17m 31s\n",
      "batch: 200/1563 - train loss: 3.4199 - test loss: 17.6570 - train acc: 0.8090 - test acc: 0.4001 - 17m 37s\n",
      "batch: 300/1563 - train loss: 3.3207 - test loss: 15.6575 - train acc: 0.8209 - test acc: 0.4355 - 17m 42s\n",
      "batch: 400/1563 - train loss: 3.5925 - test loss: 16.4310 - train acc: 0.8025 - test acc: 0.4262 - 17m 49s\n",
      "batch: 500/1563 - train loss: 4.1293 - test loss: 16.2944 - train acc: 0.7737 - test acc: 0.4245 - 17m 54s\n",
      "batch: 600/1563 - train loss: 4.1872 - test loss: 18.0751 - train acc: 0.7647 - test acc: 0.3927 - 18m 0s\n",
      "batch: 700/1563 - train loss: 4.3940 - test loss: 16.3761 - train acc: 0.7512 - test acc: 0.4205 - 18m 5s\n",
      "batch: 800/1563 - train loss: 4.3395 - test loss: 16.2132 - train acc: 0.7593 - test acc: 0.4206 - 18m 11s\n",
      "batch: 900/1563 - train loss: 4.2920 - test loss: 16.2057 - train acc: 0.7615 - test acc: 0.4218 - 18m 16s\n",
      "batch: 1000/1563 - train loss: 4.5488 - test loss: 16.5473 - train acc: 0.7565 - test acc: 0.4096 - 18m 22s\n",
      "batch: 1100/1563 - train loss: 4.6565 - test loss: 16.0742 - train acc: 0.7506 - test acc: 0.4323 - 18m 28s\n",
      "batch: 1200/1563 - train loss: 4.5138 - test loss: 17.0139 - train acc: 0.7478 - test acc: 0.4168 - 18m 33s\n",
      "batch: 1300/1563 - train loss: 4.7902 - test loss: 16.6442 - train acc: 0.7354 - test acc: 0.4183 - 18m 39s\n",
      "batch: 1400/1563 - train loss: 4.8156 - test loss: 15.5637 - train acc: 0.7410 - test acc: 0.4305 - 18m 44s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 4.8854 - test loss: 15.7136 - train acc: 0.7353 - test acc: 0.4334 - 18m 50s\n",
      "batch: 1563/1563 - train loss: 4.8798 - test loss: 15.8103 - train acc: 0.7297 - test acc: 0.4242 - 18m 54s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.9109 - test loss: 15.7346 - train acc: 0.8409 - test acc: 0.4418 - 19m 0s\n",
      "batch: 200/1563 - train loss: 2.5946 - test loss: 16.7891 - train acc: 0.8522 - test acc: 0.4213 - 19m 5s\n",
      "batch: 300/1563 - train loss: 2.8762 - test loss: 16.1674 - train acc: 0.8393 - test acc: 0.4382 - 19m 11s\n",
      "batch: 400/1563 - train loss: 2.9216 - test loss: 16.9866 - train acc: 0.8391 - test acc: 0.4256 - 19m 16s\n",
      "batch: 500/1563 - train loss: 3.4541 - test loss: 16.6481 - train acc: 0.8112 - test acc: 0.4303 - 19m 22s\n",
      "batch: 600/1563 - train loss: 3.2705 - test loss: 17.5932 - train acc: 0.8124 - test acc: 0.4172 - 19m 27s\n",
      "batch: 700/1563 - train loss: 3.6446 - test loss: 16.7482 - train acc: 0.7999 - test acc: 0.4286 - 19m 33s\n",
      "batch: 800/1563 - train loss: 3.5505 - test loss: 17.4594 - train acc: 0.8053 - test acc: 0.4179 - 19m 38s\n",
      "batch: 900/1563 - train loss: 3.7852 - test loss: 16.5299 - train acc: 0.7906 - test acc: 0.4326 - 19m 44s\n",
      "batch: 1000/1563 - train loss: 3.7794 - test loss: 16.9533 - train acc: 0.7922 - test acc: 0.4202 - 19m 49s\n",
      "batch: 1100/1563 - train loss: 4.0800 - test loss: 16.8940 - train acc: 0.7734 - test acc: 0.4255 - 19m 55s\n",
      "batch: 1200/1563 - train loss: 4.0312 - test loss: 16.8638 - train acc: 0.7853 - test acc: 0.4190 - 20m 0s\n",
      "time is up! finishing training\n",
      "batch: 1201/1563 - train loss: 4.0418 - test loss: 16.8229 - train acc: 0.7856 - test acc: 0.4206 - 20m 3s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 10\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1114 - test loss: 24.5778 - train acc: 0.0317 - test acc: 0.0611 - 0m 3s\n",
      "batch: 200/1563 - train loss: 24.0213 - test loss: 23.1403 - train acc: 0.0704 - test acc: 0.0837 - 0m 8s\n",
      "batch: 300/1563 - train loss: 22.7901 - test loss: 21.9559 - train acc: 0.1062 - test acc: 0.1147 - 0m 13s\n",
      "batch: 400/1563 - train loss: 22.0363 - test loss: 22.5161 - train acc: 0.1052 - test acc: 0.1037 - 0m 19s\n",
      "batch: 500/1563 - train loss: 21.6641 - test loss: 21.0314 - train acc: 0.1178 - test acc: 0.1285 - 0m 24s\n",
      "batch: 600/1563 - train loss: 21.2191 - test loss: 21.9312 - train acc: 0.1287 - test acc: 0.1004 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.7881 - test loss: 22.5651 - train acc: 0.1400 - test acc: 0.1102 - 0m 35s\n",
      "batch: 800/1563 - train loss: 20.3437 - test loss: 21.0571 - train acc: 0.1569 - test acc: 0.1329 - 0m 41s\n",
      "batch: 900/1563 - train loss: 20.0115 - test loss: 20.3460 - train acc: 0.1641 - test acc: 0.1625 - 0m 46s\n",
      "batch: 1000/1563 - train loss: 19.8013 - test loss: 19.2048 - train acc: 0.1691 - test acc: 0.1821 - 0m 51s\n",
      "batch: 1100/1563 - train loss: 19.6924 - test loss: 20.3233 - train acc: 0.1754 - test acc: 0.1668 - 0m 57s\n",
      "batch: 1200/1563 - train loss: 19.1627 - test loss: 22.5471 - train acc: 0.1923 - test acc: 0.1233 - 1m 2s\n",
      "batch: 1300/1563 - train loss: 18.8062 - test loss: 20.7045 - train acc: 0.1970 - test acc: 0.1657 - 1m 8s\n",
      "batch: 1400/1563 - train loss: 18.9297 - test loss: 18.9853 - train acc: 0.1900 - test acc: 0.1966 - 1m 13s\n",
      "batch: 1500/1563 - train loss: 18.9123 - test loss: 20.6085 - train acc: 0.1956 - test acc: 0.1602 - 1m 19s\n",
      "batch: 1563/1563 - train loss: 18.7084 - test loss: 18.4943 - train acc: 0.2001 - test acc: 0.2118 - 1m 23s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.6584 - test loss: 17.8248 - train acc: 0.2303 - test acc: 0.2303 - 1m 29s\n",
      "batch: 200/1563 - train loss: 17.5939 - test loss: 17.7557 - train acc: 0.2325 - test acc: 0.2304 - 1m 34s\n",
      "batch: 300/1563 - train loss: 17.5480 - test loss: 18.2989 - train acc: 0.2316 - test acc: 0.2274 - 1m 40s\n",
      "batch: 400/1563 - train loss: 17.4924 - test loss: 17.9559 - train acc: 0.2379 - test acc: 0.2259 - 1m 45s\n",
      "batch: 500/1563 - train loss: 17.3450 - test loss: 17.3544 - train acc: 0.2456 - test acc: 0.2465 - 1m 51s\n",
      "batch: 600/1563 - train loss: 17.3091 - test loss: 18.3229 - train acc: 0.2406 - test acc: 0.2191 - 1m 56s\n",
      "batch: 700/1563 - train loss: 17.1617 - test loss: 17.6462 - train acc: 0.2503 - test acc: 0.2430 - 2m 2s\n",
      "batch: 800/1563 - train loss: 17.0756 - test loss: 17.4123 - train acc: 0.2513 - test acc: 0.2503 - 2m 7s\n",
      "batch: 900/1563 - train loss: 16.9065 - test loss: 16.2586 - train acc: 0.2653 - test acc: 0.2775 - 2m 13s\n",
      "batch: 1000/1563 - train loss: 16.6085 - test loss: 16.4942 - train acc: 0.2738 - test acc: 0.2793 - 2m 18s\n",
      "batch: 1100/1563 - train loss: 16.2553 - test loss: 17.3852 - train acc: 0.2812 - test acc: 0.2590 - 2m 24s\n",
      "batch: 1200/1563 - train loss: 16.3644 - test loss: 17.3664 - train acc: 0.2718 - test acc: 0.2527 - 2m 29s\n",
      "batch: 1300/1563 - train loss: 16.2773 - test loss: 17.2008 - train acc: 0.2809 - test acc: 0.2496 - 2m 35s\n",
      "batch: 1400/1563 - train loss: 16.5679 - test loss: 16.1419 - train acc: 0.2756 - test acc: 0.2872 - 2m 40s\n",
      "batch: 1500/1563 - train loss: 16.0825 - test loss: 16.0806 - train acc: 0.2956 - test acc: 0.2917 - 2m 46s\n",
      "batch: 1563/1563 - train loss: 15.5749 - test loss: 16.7023 - train acc: 0.3065 - test acc: 0.2751 - 2m 51s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.7123 - test loss: 16.6297 - train acc: 0.3297 - test acc: 0.2874 - 2m 56s\n",
      "batch: 200/1563 - train loss: 15.1622 - test loss: 16.4568 - train acc: 0.3194 - test acc: 0.2882 - 3m 1s\n",
      "batch: 300/1563 - train loss: 14.8686 - test loss: 16.2217 - train acc: 0.3200 - test acc: 0.2909 - 3m 7s\n",
      "batch: 400/1563 - train loss: 15.1729 - test loss: 15.7158 - train acc: 0.3156 - test acc: 0.3038 - 3m 12s\n",
      "batch: 500/1563 - train loss: 14.7288 - test loss: 16.5875 - train acc: 0.3325 - test acc: 0.2851 - 3m 18s\n",
      "batch: 600/1563 - train loss: 14.6419 - test loss: 16.4146 - train acc: 0.3382 - test acc: 0.2865 - 3m 24s\n",
      "batch: 700/1563 - train loss: 14.8276 - test loss: 15.8487 - train acc: 0.3391 - test acc: 0.3027 - 3m 30s\n",
      "batch: 800/1563 - train loss: 14.8500 - test loss: 17.0673 - train acc: 0.3275 - test acc: 0.2840 - 3m 36s\n",
      "batch: 900/1563 - train loss: 14.8873 - test loss: 16.1769 - train acc: 0.3159 - test acc: 0.2921 - 3m 41s\n",
      "batch: 1000/1563 - train loss: 14.5808 - test loss: 15.5432 - train acc: 0.3372 - test acc: 0.3074 - 3m 47s\n",
      "batch: 1100/1563 - train loss: 14.4795 - test loss: 15.2365 - train acc: 0.3387 - test acc: 0.3204 - 3m 53s\n",
      "batch: 1200/1563 - train loss: 14.4028 - test loss: 15.4610 - train acc: 0.3522 - test acc: 0.3123 - 3m 58s\n",
      "batch: 1300/1563 - train loss: 14.6990 - test loss: 15.0188 - train acc: 0.3431 - test acc: 0.3259 - 4m 4s\n",
      "batch: 1400/1563 - train loss: 14.1701 - test loss: 15.5724 - train acc: 0.3509 - test acc: 0.3139 - 4m 9s\n",
      "batch: 1500/1563 - train loss: 14.1551 - test loss: 15.2527 - train acc: 0.3522 - test acc: 0.3304 - 4m 15s\n",
      "batch: 1563/1563 - train loss: 14.3142 - test loss: 15.1896 - train acc: 0.3528 - test acc: 0.3290 - 4m 19s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.2137 - test loss: 15.1622 - train acc: 0.3825 - test acc: 0.3267 - 4m 25s\n",
      "batch: 200/1563 - train loss: 13.0322 - test loss: 14.7049 - train acc: 0.3856 - test acc: 0.3382 - 4m 31s\n",
      "batch: 300/1563 - train loss: 13.2057 - test loss: 15.7441 - train acc: 0.3787 - test acc: 0.3200 - 4m 36s\n",
      "batch: 400/1563 - train loss: 13.2569 - test loss: 14.9385 - train acc: 0.3757 - test acc: 0.3400 - 4m 42s\n",
      "batch: 500/1563 - train loss: 13.2432 - test loss: 15.3665 - train acc: 0.3828 - test acc: 0.3260 - 4m 47s\n",
      "batch: 600/1563 - train loss: 13.1140 - test loss: 16.2001 - train acc: 0.3891 - test acc: 0.3042 - 4m 53s\n",
      "batch: 700/1563 - train loss: 13.1749 - test loss: 14.3998 - train acc: 0.3862 - test acc: 0.3582 - 4m 58s\n",
      "batch: 800/1563 - train loss: 13.1675 - test loss: 14.9748 - train acc: 0.3922 - test acc: 0.3382 - 5m 3s\n",
      "batch: 900/1563 - train loss: 13.0168 - test loss: 15.0453 - train acc: 0.4016 - test acc: 0.3402 - 5m 9s\n",
      "batch: 1000/1563 - train loss: 13.1394 - test loss: 13.9144 - train acc: 0.3944 - test acc: 0.3706 - 5m 14s\n",
      "batch: 1100/1563 - train loss: 13.0717 - test loss: 14.4644 - train acc: 0.3957 - test acc: 0.3505 - 5m 20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1200/1563 - train loss: 13.0559 - test loss: 14.5088 - train acc: 0.3922 - test acc: 0.3586 - 5m 26s\n",
      "batch: 1300/1563 - train loss: 13.2250 - test loss: 14.1723 - train acc: 0.3868 - test acc: 0.3620 - 5m 31s\n",
      "batch: 1400/1563 - train loss: 12.9659 - test loss: 14.2528 - train acc: 0.4191 - test acc: 0.3603 - 5m 36s\n",
      "batch: 1500/1563 - train loss: 12.6685 - test loss: 14.1398 - train acc: 0.4150 - test acc: 0.3631 - 5m 42s\n",
      "batch: 1563/1563 - train loss: 12.9772 - test loss: 14.2298 - train acc: 0.4031 - test acc: 0.3638 - 5m 46s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3410 - test loss: 13.6200 - train acc: 0.4594 - test acc: 0.3852 - 5m 52s\n",
      "batch: 200/1563 - train loss: 11.4834 - test loss: 15.1585 - train acc: 0.4532 - test acc: 0.3470 - 5m 58s\n",
      "batch: 300/1563 - train loss: 11.7172 - test loss: 15.1012 - train acc: 0.4532 - test acc: 0.3415 - 6m 3s\n",
      "batch: 400/1563 - train loss: 12.0563 - test loss: 15.0043 - train acc: 0.4397 - test acc: 0.3482 - 6m 9s\n",
      "batch: 500/1563 - train loss: 11.4688 - test loss: 14.8548 - train acc: 0.4631 - test acc: 0.3529 - 6m 14s\n",
      "batch: 600/1563 - train loss: 11.8978 - test loss: 14.0794 - train acc: 0.4416 - test acc: 0.3721 - 6m 19s\n",
      "batch: 700/1563 - train loss: 11.8187 - test loss: 14.0906 - train acc: 0.4456 - test acc: 0.3757 - 6m 25s\n",
      "batch: 800/1563 - train loss: 11.8836 - test loss: 14.0151 - train acc: 0.4326 - test acc: 0.3709 - 6m 31s\n",
      "batch: 900/1563 - train loss: 12.0395 - test loss: 13.5203 - train acc: 0.4294 - test acc: 0.3923 - 6m 36s\n",
      "batch: 1000/1563 - train loss: 11.8544 - test loss: 14.2278 - train acc: 0.4413 - test acc: 0.3737 - 6m 41s\n",
      "batch: 1100/1563 - train loss: 11.8408 - test loss: 15.4212 - train acc: 0.4316 - test acc: 0.3310 - 6m 47s\n",
      "batch: 1200/1563 - train loss: 11.8009 - test loss: 14.0768 - train acc: 0.4476 - test acc: 0.3774 - 6m 53s\n",
      "batch: 1300/1563 - train loss: 11.7942 - test loss: 14.5834 - train acc: 0.4388 - test acc: 0.3657 - 6m 58s\n",
      "batch: 1400/1563 - train loss: 11.8379 - test loss: 13.8809 - train acc: 0.4460 - test acc: 0.3808 - 7m 4s\n",
      "batch: 1500/1563 - train loss: 12.1389 - test loss: 14.6350 - train acc: 0.4309 - test acc: 0.3542 - 7m 10s\n",
      "batch: 1563/1563 - train loss: 12.1189 - test loss: 13.9439 - train acc: 0.4372 - test acc: 0.3774 - 7m 14s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.1457 - test loss: 13.6527 - train acc: 0.5144 - test acc: 0.3913 - 7m 20s\n",
      "batch: 200/1563 - train loss: 10.2373 - test loss: 13.6713 - train acc: 0.4891 - test acc: 0.3979 - 7m 25s\n",
      "batch: 300/1563 - train loss: 10.4807 - test loss: 13.3918 - train acc: 0.4900 - test acc: 0.4049 - 7m 31s\n",
      "batch: 400/1563 - train loss: 10.3129 - test loss: 13.7841 - train acc: 0.5074 - test acc: 0.3972 - 7m 36s\n",
      "batch: 500/1563 - train loss: 10.5910 - test loss: 13.3540 - train acc: 0.4888 - test acc: 0.4023 - 7m 42s\n",
      "batch: 600/1563 - train loss: 10.6108 - test loss: 13.6198 - train acc: 0.4803 - test acc: 0.3921 - 7m 47s\n",
      "batch: 700/1563 - train loss: 10.6707 - test loss: 14.1918 - train acc: 0.4881 - test acc: 0.3890 - 7m 53s\n",
      "batch: 800/1563 - train loss: 10.7042 - test loss: 14.4297 - train acc: 0.4925 - test acc: 0.3860 - 7m 58s\n",
      "batch: 900/1563 - train loss: 10.8038 - test loss: 13.5507 - train acc: 0.4813 - test acc: 0.3938 - 8m 4s\n",
      "batch: 1000/1563 - train loss: 10.6677 - test loss: 13.8566 - train acc: 0.4869 - test acc: 0.3948 - 8m 10s\n",
      "batch: 1100/1563 - train loss: 10.9658 - test loss: 15.1607 - train acc: 0.4746 - test acc: 0.3518 - 8m 15s\n",
      "batch: 1200/1563 - train loss: 10.8747 - test loss: 13.4897 - train acc: 0.4716 - test acc: 0.4008 - 8m 21s\n",
      "batch: 1300/1563 - train loss: 10.7762 - test loss: 13.1894 - train acc: 0.4906 - test acc: 0.4074 - 8m 26s\n",
      "batch: 1400/1563 - train loss: 10.6265 - test loss: 14.0878 - train acc: 0.5040 - test acc: 0.3795 - 8m 32s\n",
      "batch: 1500/1563 - train loss: 11.1064 - test loss: 13.7595 - train acc: 0.4713 - test acc: 0.3977 - 8m 38s\n",
      "batch: 1563/1563 - train loss: 10.7190 - test loss: 14.7107 - train acc: 0.4829 - test acc: 0.3689 - 8m 42s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 9.0002 - test loss: 13.2861 - train acc: 0.5531 - test acc: 0.4118 - 8m 48s\n",
      "batch: 200/1563 - train loss: 8.6498 - test loss: 13.6768 - train acc: 0.5728 - test acc: 0.4027 - 8m 53s\n",
      "batch: 300/1563 - train loss: 9.2210 - test loss: 14.6488 - train acc: 0.5459 - test acc: 0.3762 - 8m 59s\n",
      "batch: 400/1563 - train loss: 9.1472 - test loss: 14.3083 - train acc: 0.5469 - test acc: 0.3949 - 9m 5s\n",
      "batch: 500/1563 - train loss: 9.3694 - test loss: 13.5128 - train acc: 0.5353 - test acc: 0.4053 - 9m 10s\n",
      "batch: 600/1563 - train loss: 9.5746 - test loss: 15.0278 - train acc: 0.5332 - test acc: 0.3718 - 9m 16s\n",
      "batch: 700/1563 - train loss: 9.6972 - test loss: 13.9785 - train acc: 0.5216 - test acc: 0.3989 - 9m 21s\n",
      "batch: 800/1563 - train loss: 9.9375 - test loss: 14.3473 - train acc: 0.5134 - test acc: 0.3868 - 9m 26s\n",
      "batch: 900/1563 - train loss: 9.8881 - test loss: 13.2033 - train acc: 0.5225 - test acc: 0.4218 - 9m 32s\n",
      "batch: 1000/1563 - train loss: 9.7089 - test loss: 14.0827 - train acc: 0.5275 - test acc: 0.3932 - 9m 38s\n",
      "batch: 1100/1563 - train loss: 9.4715 - test loss: 13.3579 - train acc: 0.5297 - test acc: 0.4126 - 9m 43s\n",
      "batch: 1200/1563 - train loss: 9.8738 - test loss: 14.3529 - train acc: 0.5203 - test acc: 0.3853 - 9m 48s\n",
      "batch: 1300/1563 - train loss: 9.8867 - test loss: 13.5047 - train acc: 0.5138 - test acc: 0.4186 - 9m 54s\n",
      "batch: 1400/1563 - train loss: 9.7632 - test loss: 13.0454 - train acc: 0.5225 - test acc: 0.4212 - 10m 0s\n",
      "batch: 1500/1563 - train loss: 9.8897 - test loss: 14.6159 - train acc: 0.5188 - test acc: 0.3859 - 10m 5s\n",
      "batch: 1563/1563 - train loss: 9.7119 - test loss: 13.4523 - train acc: 0.5287 - test acc: 0.4127 - 10m 10s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.6826 - test loss: 13.2816 - train acc: 0.6049 - test acc: 0.4279 - 10m 15s\n",
      "batch: 200/1563 - train loss: 7.8099 - test loss: 14.7793 - train acc: 0.6059 - test acc: 0.3958 - 10m 21s\n",
      "batch: 300/1563 - train loss: 8.0387 - test loss: 13.8194 - train acc: 0.6009 - test acc: 0.4141 - 10m 26s\n",
      "batch: 400/1563 - train loss: 8.1829 - test loss: 13.7455 - train acc: 0.5797 - test acc: 0.4192 - 10m 32s\n",
      "batch: 500/1563 - train loss: 8.3601 - test loss: 13.9350 - train acc: 0.5769 - test acc: 0.4113 - 10m 37s\n",
      "batch: 600/1563 - train loss: 8.5430 - test loss: 13.8013 - train acc: 0.5809 - test acc: 0.4147 - 10m 43s\n",
      "batch: 700/1563 - train loss: 8.5322 - test loss: 13.7878 - train acc: 0.5718 - test acc: 0.4092 - 10m 48s\n",
      "batch: 800/1563 - train loss: 8.2623 - test loss: 14.0324 - train acc: 0.5813 - test acc: 0.4063 - 10m 54s\n",
      "batch: 900/1563 - train loss: 8.6679 - test loss: 13.6118 - train acc: 0.5713 - test acc: 0.4167 - 11m 0s\n",
      "batch: 1000/1563 - train loss: 8.7851 - test loss: 14.7454 - train acc: 0.5572 - test acc: 0.3917 - 11m 5s\n",
      "batch: 1100/1563 - train loss: 8.8049 - test loss: 13.7574 - train acc: 0.5669 - test acc: 0.4067 - 11m 11s\n",
      "batch: 1200/1563 - train loss: 8.7196 - test loss: 13.4619 - train acc: 0.5672 - test acc: 0.4222 - 11m 16s\n",
      "batch: 1300/1563 - train loss: 8.8560 - test loss: 13.3343 - train acc: 0.5703 - test acc: 0.4236 - 11m 22s\n",
      "batch: 1400/1563 - train loss: 8.8723 - test loss: 13.0717 - train acc: 0.5634 - test acc: 0.4325 - 11m 27s\n",
      "batch: 1500/1563 - train loss: 8.9907 - test loss: 13.6516 - train acc: 0.5588 - test acc: 0.4154 - 11m 32s\n",
      "batch: 1563/1563 - train loss: 9.0245 - test loss: 14.0395 - train acc: 0.5566 - test acc: 0.4030 - 11m 37s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.6975 - test loss: 13.7977 - train acc: 0.6560 - test acc: 0.4262 - 11m 42s\n",
      "batch: 200/1563 - train loss: 6.8986 - test loss: 14.0248 - train acc: 0.6494 - test acc: 0.4151 - 11m 48s\n",
      "batch: 300/1563 - train loss: 7.1192 - test loss: 15.3164 - train acc: 0.6372 - test acc: 0.3929 - 11m 53s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 400/1563 - train loss: 7.0634 - test loss: 14.4155 - train acc: 0.6331 - test acc: 0.4113 - 11m 59s\n",
      "batch: 500/1563 - train loss: 7.1771 - test loss: 14.2826 - train acc: 0.6197 - test acc: 0.4116 - 12m 4s\n",
      "batch: 600/1563 - train loss: 7.4854 - test loss: 14.7363 - train acc: 0.6145 - test acc: 0.3972 - 12m 9s\n",
      "batch: 700/1563 - train loss: 7.7468 - test loss: 13.9330 - train acc: 0.6075 - test acc: 0.4250 - 12m 15s\n",
      "batch: 800/1563 - train loss: 7.5627 - test loss: 14.4290 - train acc: 0.6147 - test acc: 0.4101 - 12m 21s\n",
      "batch: 900/1563 - train loss: 7.7999 - test loss: 14.5260 - train acc: 0.5968 - test acc: 0.4015 - 12m 26s\n",
      "batch: 1000/1563 - train loss: 8.1099 - test loss: 13.6483 - train acc: 0.5903 - test acc: 0.4235 - 12m 32s\n",
      "batch: 1100/1563 - train loss: 7.7768 - test loss: 14.6341 - train acc: 0.6062 - test acc: 0.3935 - 12m 37s\n",
      "batch: 1200/1563 - train loss: 8.0160 - test loss: 14.4299 - train acc: 0.5931 - test acc: 0.4032 - 12m 43s\n",
      "batch: 1300/1563 - train loss: 8.1716 - test loss: 14.5467 - train acc: 0.5910 - test acc: 0.4082 - 12m 49s\n",
      "batch: 1400/1563 - train loss: 7.6832 - test loss: 13.6542 - train acc: 0.6153 - test acc: 0.4235 - 12m 54s\n",
      "batch: 1500/1563 - train loss: 8.0531 - test loss: 13.7230 - train acc: 0.5968 - test acc: 0.4208 - 12m 59s\n",
      "batch: 1563/1563 - train loss: 8.1751 - test loss: 14.6028 - train acc: 0.5866 - test acc: 0.4038 - 13m 4s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.5779 - test loss: 13.6184 - train acc: 0.7063 - test acc: 0.4340 - 13m 9s\n",
      "batch: 200/1563 - train loss: 5.7432 - test loss: 14.6898 - train acc: 0.6894 - test acc: 0.4161 - 13m 14s\n",
      "batch: 300/1563 - train loss: 5.8024 - test loss: 14.5146 - train acc: 0.6879 - test acc: 0.4137 - 13m 20s\n",
      "batch: 400/1563 - train loss: 5.9898 - test loss: 14.6503 - train acc: 0.6881 - test acc: 0.4155 - 13m 25s\n",
      "batch: 500/1563 - train loss: 6.6137 - test loss: 15.5310 - train acc: 0.6606 - test acc: 0.3996 - 13m 31s\n",
      "batch: 600/1563 - train loss: 6.6051 - test loss: 16.8321 - train acc: 0.6493 - test acc: 0.3736 - 13m 36s\n",
      "batch: 700/1563 - train loss: 6.7893 - test loss: 14.2264 - train acc: 0.6462 - test acc: 0.4195 - 13m 41s\n",
      "batch: 800/1563 - train loss: 6.7036 - test loss: 14.4514 - train acc: 0.6547 - test acc: 0.4205 - 13m 47s\n",
      "batch: 900/1563 - train loss: 6.8510 - test loss: 14.5402 - train acc: 0.6510 - test acc: 0.4160 - 13m 52s\n",
      "batch: 1000/1563 - train loss: 6.6040 - test loss: 14.3141 - train acc: 0.6541 - test acc: 0.4250 - 13m 58s\n",
      "batch: 1100/1563 - train loss: 6.8222 - test loss: 14.2442 - train acc: 0.6397 - test acc: 0.4303 - 14m 3s\n",
      "batch: 1200/1563 - train loss: 6.9921 - test loss: 14.2960 - train acc: 0.6441 - test acc: 0.4311 - 14m 8s\n",
      "batch: 1300/1563 - train loss: 6.9641 - test loss: 14.6462 - train acc: 0.6504 - test acc: 0.4165 - 14m 14s\n",
      "batch: 1400/1563 - train loss: 7.1795 - test loss: 14.2742 - train acc: 0.6325 - test acc: 0.4273 - 14m 19s\n",
      "batch: 1500/1563 - train loss: 7.1461 - test loss: 14.4501 - train acc: 0.6322 - test acc: 0.4154 - 14m 25s\n",
      "batch: 1563/1563 - train loss: 7.1059 - test loss: 14.3895 - train acc: 0.6328 - test acc: 0.4189 - 14m 29s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.8334 - test loss: 14.7931 - train acc: 0.7397 - test acc: 0.4235 - 14m 35s\n",
      "batch: 200/1563 - train loss: 4.6026 - test loss: 15.3817 - train acc: 0.7565 - test acc: 0.4088 - 14m 40s\n",
      "batch: 300/1563 - train loss: 5.0210 - test loss: 14.9458 - train acc: 0.7331 - test acc: 0.4215 - 14m 45s\n",
      "batch: 400/1563 - train loss: 5.1640 - test loss: 14.9797 - train acc: 0.7222 - test acc: 0.4204 - 14m 51s\n",
      "batch: 500/1563 - train loss: 5.3730 - test loss: 16.7408 - train acc: 0.7125 - test acc: 0.3803 - 14m 57s\n",
      "batch: 600/1563 - train loss: 5.3680 - test loss: 14.5980 - train acc: 0.7150 - test acc: 0.4273 - 15m 2s\n",
      "batch: 700/1563 - train loss: 5.6090 - test loss: 15.4613 - train acc: 0.7016 - test acc: 0.4095 - 15m 7s\n",
      "batch: 800/1563 - train loss: 5.7551 - test loss: 14.9591 - train acc: 0.6945 - test acc: 0.4264 - 15m 12s\n",
      "batch: 900/1563 - train loss: 5.9943 - test loss: 14.9274 - train acc: 0.6823 - test acc: 0.4199 - 15m 18s\n",
      "batch: 1000/1563 - train loss: 6.2028 - test loss: 14.5703 - train acc: 0.6725 - test acc: 0.4273 - 15m 23s\n",
      "batch: 1100/1563 - train loss: 5.7900 - test loss: 14.9885 - train acc: 0.7001 - test acc: 0.4184 - 15m 29s\n",
      "batch: 1200/1563 - train loss: 6.3218 - test loss: 15.2701 - train acc: 0.6678 - test acc: 0.4192 - 15m 35s\n",
      "batch: 1300/1563 - train loss: 6.1162 - test loss: 15.1571 - train acc: 0.6834 - test acc: 0.4150 - 15m 40s\n",
      "batch: 1400/1563 - train loss: 6.1914 - test loss: 14.7754 - train acc: 0.6819 - test acc: 0.4259 - 15m 45s\n",
      "batch: 1500/1563 - train loss: 6.1991 - test loss: 15.1098 - train acc: 0.6803 - test acc: 0.4162 - 15m 51s\n",
      "batch: 1563/1563 - train loss: 6.3685 - test loss: 14.8619 - train acc: 0.6650 - test acc: 0.4223 - 15m 55s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.2321 - test loss: 14.6969 - train acc: 0.7753 - test acc: 0.4356 - 16m 1s\n",
      "batch: 200/1563 - train loss: 3.9434 - test loss: 16.5888 - train acc: 0.7825 - test acc: 0.4050 - 16m 7s\n",
      "batch: 300/1563 - train loss: 4.2292 - test loss: 15.4418 - train acc: 0.7694 - test acc: 0.4197 - 16m 12s\n",
      "batch: 400/1563 - train loss: 4.1493 - test loss: 15.8881 - train acc: 0.7806 - test acc: 0.4211 - 16m 18s\n",
      "batch: 500/1563 - train loss: 4.4492 - test loss: 16.1588 - train acc: 0.7662 - test acc: 0.4178 - 16m 23s\n",
      "batch: 600/1563 - train loss: 4.4952 - test loss: 15.8129 - train acc: 0.7528 - test acc: 0.4176 - 16m 28s\n",
      "batch: 700/1563 - train loss: 5.0332 - test loss: 15.8825 - train acc: 0.7313 - test acc: 0.4247 - 16m 34s\n",
      "batch: 800/1563 - train loss: 4.9744 - test loss: 15.4023 - train acc: 0.7268 - test acc: 0.4275 - 16m 40s\n",
      "batch: 900/1563 - train loss: 5.1172 - test loss: 15.7242 - train acc: 0.7216 - test acc: 0.4201 - 16m 46s\n",
      "batch: 1000/1563 - train loss: 5.1622 - test loss: 16.4973 - train acc: 0.7282 - test acc: 0.4108 - 16m 51s\n",
      "batch: 1100/1563 - train loss: 5.3032 - test loss: 16.0014 - train acc: 0.7216 - test acc: 0.4170 - 16m 56s\n",
      "batch: 1200/1563 - train loss: 5.2194 - test loss: 15.6248 - train acc: 0.7166 - test acc: 0.4191 - 17m 2s\n",
      "batch: 1300/1563 - train loss: 5.6654 - test loss: 15.4019 - train acc: 0.7037 - test acc: 0.4280 - 17m 8s\n",
      "batch: 1400/1563 - train loss: 5.3739 - test loss: 15.5065 - train acc: 0.7118 - test acc: 0.4315 - 17m 13s\n",
      "batch: 1500/1563 - train loss: 5.5133 - test loss: 15.3973 - train acc: 0.7085 - test acc: 0.4297 - 17m 19s\n",
      "batch: 1563/1563 - train loss: 5.5212 - test loss: 15.7853 - train acc: 0.7029 - test acc: 0.4188 - 17m 23s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.3249 - test loss: 15.9501 - train acc: 0.8193 - test acc: 0.4202 - 17m 28s\n",
      "batch: 200/1563 - train loss: 3.3336 - test loss: 15.9151 - train acc: 0.8224 - test acc: 0.4294 - 17m 34s\n",
      "batch: 300/1563 - train loss: 3.6466 - test loss: 15.9482 - train acc: 0.7953 - test acc: 0.4309 - 17m 40s\n",
      "batch: 400/1563 - train loss: 3.8570 - test loss: 15.9260 - train acc: 0.7918 - test acc: 0.4324 - 17m 45s\n",
      "batch: 500/1563 - train loss: 3.4294 - test loss: 17.8383 - train acc: 0.8075 - test acc: 0.3994 - 17m 51s\n",
      "batch: 600/1563 - train loss: 3.8852 - test loss: 16.5916 - train acc: 0.7812 - test acc: 0.4189 - 17m 56s\n",
      "batch: 700/1563 - train loss: 4.2000 - test loss: 16.8141 - train acc: 0.7646 - test acc: 0.4117 - 18m 2s\n",
      "batch: 800/1563 - train loss: 4.3370 - test loss: 15.7041 - train acc: 0.7559 - test acc: 0.4290 - 18m 7s\n",
      "batch: 900/1563 - train loss: 4.1135 - test loss: 16.4253 - train acc: 0.7693 - test acc: 0.4312 - 18m 13s\n",
      "batch: 1000/1563 - train loss: 4.5263 - test loss: 17.5203 - train acc: 0.7562 - test acc: 0.4030 - 18m 18s\n",
      "batch: 1100/1563 - train loss: 4.4915 - test loss: 16.1828 - train acc: 0.7559 - test acc: 0.4184 - 18m 24s\n",
      "batch: 1200/1563 - train loss: 4.8260 - test loss: 15.8154 - train acc: 0.7381 - test acc: 0.4258 - 18m 29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1300/1563 - train loss: 4.8097 - test loss: 20.0160 - train acc: 0.7422 - test acc: 0.3699 - 18m 35s\n",
      "batch: 1400/1563 - train loss: 4.9301 - test loss: 15.6338 - train acc: 0.7315 - test acc: 0.4303 - 18m 41s\n",
      "batch: 1500/1563 - train loss: 4.7651 - test loss: 15.8063 - train acc: 0.7428 - test acc: 0.4295 - 18m 46s\n",
      "batch: 1563/1563 - train loss: 4.6316 - test loss: 15.9769 - train acc: 0.7522 - test acc: 0.4244 - 18m 51s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8657 - test loss: 16.3397 - train acc: 0.8296 - test acc: 0.4310 - 18m 56s\n",
      "batch: 200/1563 - train loss: 2.7529 - test loss: 16.4913 - train acc: 0.8471 - test acc: 0.4385 - 19m 2s\n",
      "batch: 300/1563 - train loss: 2.9479 - test loss: 17.8014 - train acc: 0.8412 - test acc: 0.3986 - 19m 7s\n",
      "batch: 400/1563 - train loss: 2.8928 - test loss: 16.6016 - train acc: 0.8384 - test acc: 0.4344 - 19m 13s\n",
      "batch: 500/1563 - train loss: 3.1847 - test loss: 17.2864 - train acc: 0.8206 - test acc: 0.4191 - 19m 18s\n",
      "batch: 600/1563 - train loss: 3.4807 - test loss: 17.1106 - train acc: 0.8090 - test acc: 0.4189 - 19m 24s\n",
      "batch: 700/1563 - train loss: 3.4031 - test loss: 16.8049 - train acc: 0.8178 - test acc: 0.4247 - 19m 29s\n",
      "batch: 800/1563 - train loss: 3.5816 - test loss: 17.4292 - train acc: 0.8012 - test acc: 0.4180 - 19m 34s\n",
      "batch: 900/1563 - train loss: 3.4836 - test loss: 16.9257 - train acc: 0.8053 - test acc: 0.4219 - 19m 40s\n",
      "batch: 1000/1563 - train loss: 3.7025 - test loss: 16.5559 - train acc: 0.7896 - test acc: 0.4295 - 19m 45s\n",
      "batch: 1100/1563 - train loss: 3.7562 - test loss: 16.7927 - train acc: 0.7909 - test acc: 0.4269 - 19m 51s\n",
      "batch: 1200/1563 - train loss: 3.7391 - test loss: 16.4823 - train acc: 0.7950 - test acc: 0.4304 - 19m 56s\n",
      "time is up! finishing training\n",
      "batch: 1260/1563 - train loss: 3.6613 - test loss: 16.0645 - train acc: 0.7927 - test acc: 0.4336 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 11\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0127 - test loss: 25.1874 - train acc: 0.0399 - test acc: 0.0471 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.9771 - test loss: 23.4422 - train acc: 0.0639 - test acc: 0.0792 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.9280 - test loss: 22.3038 - train acc: 0.0855 - test acc: 0.1035 - 0m 13s\n",
      "batch: 400/1563 - train loss: 22.1030 - test loss: 21.8010 - train acc: 0.1090 - test acc: 0.1180 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.5853 - test loss: 20.8384 - train acc: 0.1175 - test acc: 0.1379 - 0m 24s\n",
      "batch: 600/1563 - train loss: 21.0833 - test loss: 22.5265 - train acc: 0.1262 - test acc: 0.1147 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.7301 - test loss: 20.3489 - train acc: 0.1344 - test acc: 0.1502 - 0m 34s\n",
      "batch: 800/1563 - train loss: 20.5201 - test loss: 20.5888 - train acc: 0.1488 - test acc: 0.1573 - 0m 40s\n",
      "batch: 900/1563 - train loss: 19.9572 - test loss: 20.1470 - train acc: 0.1660 - test acc: 0.1541 - 0m 45s\n",
      "batch: 1000/1563 - train loss: 19.8727 - test loss: 19.7928 - train acc: 0.1706 - test acc: 0.1676 - 0m 50s\n",
      "batch: 1100/1563 - train loss: 19.3467 - test loss: 20.2569 - train acc: 0.1738 - test acc: 0.1628 - 0m 56s\n",
      "batch: 1200/1563 - train loss: 19.5713 - test loss: 19.6784 - train acc: 0.1764 - test acc: 0.1729 - 1m 1s\n",
      "batch: 1300/1563 - train loss: 19.1072 - test loss: 20.5188 - train acc: 0.1794 - test acc: 0.1535 - 1m 6s\n",
      "batch: 1400/1563 - train loss: 18.9446 - test loss: 19.6944 - train acc: 0.1956 - test acc: 0.1790 - 1m 12s\n",
      "batch: 1500/1563 - train loss: 18.6662 - test loss: 18.5365 - train acc: 0.2007 - test acc: 0.2018 - 1m 17s\n",
      "batch: 1563/1563 - train loss: 18.7225 - test loss: 19.0567 - train acc: 0.1954 - test acc: 0.1868 - 1m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.4868 - test loss: 18.1723 - train acc: 0.2266 - test acc: 0.2170 - 1m 27s\n",
      "batch: 200/1563 - train loss: 17.7279 - test loss: 17.8930 - train acc: 0.2160 - test acc: 0.2262 - 1m 33s\n",
      "batch: 300/1563 - train loss: 17.7043 - test loss: 18.5311 - train acc: 0.2313 - test acc: 0.2025 - 1m 38s\n",
      "batch: 400/1563 - train loss: 17.5311 - test loss: 18.1626 - train acc: 0.2328 - test acc: 0.2189 - 1m 43s\n",
      "batch: 500/1563 - train loss: 17.3310 - test loss: 17.4744 - train acc: 0.2384 - test acc: 0.2415 - 1m 49s\n",
      "batch: 600/1563 - train loss: 17.2784 - test loss: 17.5752 - train acc: 0.2413 - test acc: 0.2388 - 1m 54s\n",
      "batch: 700/1563 - train loss: 17.1210 - test loss: 16.8228 - train acc: 0.2550 - test acc: 0.2643 - 2m 0s\n",
      "batch: 800/1563 - train loss: 17.1237 - test loss: 17.2879 - train acc: 0.2453 - test acc: 0.2456 - 2m 5s\n",
      "batch: 900/1563 - train loss: 16.8045 - test loss: 16.8635 - train acc: 0.2578 - test acc: 0.2652 - 2m 11s\n",
      "batch: 1000/1563 - train loss: 16.5112 - test loss: 17.2474 - train acc: 0.2656 - test acc: 0.2495 - 2m 16s\n",
      "batch: 1100/1563 - train loss: 16.6954 - test loss: 17.3697 - train acc: 0.2519 - test acc: 0.2410 - 2m 22s\n",
      "batch: 1200/1563 - train loss: 16.5392 - test loss: 16.4545 - train acc: 0.2696 - test acc: 0.2784 - 2m 27s\n",
      "batch: 1300/1563 - train loss: 16.2439 - test loss: 16.7236 - train acc: 0.2850 - test acc: 0.2673 - 2m 32s\n",
      "batch: 1400/1563 - train loss: 16.2894 - test loss: 15.8941 - train acc: 0.2722 - test acc: 0.2938 - 2m 38s\n",
      "batch: 1500/1563 - train loss: 16.0813 - test loss: 16.2497 - train acc: 0.2775 - test acc: 0.2840 - 2m 43s\n",
      "batch: 1563/1563 - train loss: 15.9256 - test loss: 17.7572 - train acc: 0.2838 - test acc: 0.2581 - 2m 47s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.0495 - test loss: 16.6402 - train acc: 0.3053 - test acc: 0.2726 - 2m 53s\n",
      "batch: 200/1563 - train loss: 14.5912 - test loss: 16.1791 - train acc: 0.3341 - test acc: 0.2853 - 2m 58s\n",
      "batch: 300/1563 - train loss: 14.8444 - test loss: 16.2385 - train acc: 0.3224 - test acc: 0.2818 - 3m 4s\n",
      "batch: 400/1563 - train loss: 14.9866 - test loss: 15.6388 - train acc: 0.3187 - test acc: 0.3002 - 3m 9s\n",
      "batch: 500/1563 - train loss: 14.8145 - test loss: 16.0578 - train acc: 0.3402 - test acc: 0.2933 - 3m 15s\n",
      "batch: 600/1563 - train loss: 15.0231 - test loss: 15.5458 - train acc: 0.3202 - test acc: 0.3031 - 3m 20s\n",
      "batch: 700/1563 - train loss: 15.0204 - test loss: 16.1049 - train acc: 0.3178 - test acc: 0.2928 - 3m 25s\n",
      "batch: 800/1563 - train loss: 14.7689 - test loss: 15.1495 - train acc: 0.3268 - test acc: 0.3180 - 3m 30s\n",
      "batch: 900/1563 - train loss: 14.7348 - test loss: 16.0969 - train acc: 0.3259 - test acc: 0.2979 - 3m 36s\n",
      "batch: 1000/1563 - train loss: 14.6545 - test loss: 15.3249 - train acc: 0.3397 - test acc: 0.3219 - 3m 42s\n",
      "batch: 1100/1563 - train loss: 14.7531 - test loss: 15.7893 - train acc: 0.3268 - test acc: 0.3087 - 3m 48s\n",
      "batch: 1200/1563 - train loss: 14.2984 - test loss: 15.3743 - train acc: 0.3475 - test acc: 0.3161 - 3m 53s\n",
      "batch: 1300/1563 - train loss: 14.6760 - test loss: 15.7154 - train acc: 0.3409 - test acc: 0.3067 - 3m 59s\n",
      "batch: 1400/1563 - train loss: 14.4847 - test loss: 14.7123 - train acc: 0.3366 - test acc: 0.3435 - 4m 4s\n",
      "batch: 1500/1563 - train loss: 14.2854 - test loss: 14.5951 - train acc: 0.3450 - test acc: 0.3462 - 4m 10s\n",
      "batch: 1563/1563 - train loss: 14.2297 - test loss: 15.6245 - train acc: 0.3353 - test acc: 0.3082 - 4m 14s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.7706 - test loss: 15.4290 - train acc: 0.4072 - test acc: 0.3227 - 4m 20s\n",
      "batch: 200/1563 - train loss: 12.7073 - test loss: 14.8032 - train acc: 0.4032 - test acc: 0.3369 - 4m 25s\n",
      "batch: 300/1563 - train loss: 13.0413 - test loss: 16.2144 - train acc: 0.4012 - test acc: 0.3000 - 4m 30s\n",
      "batch: 400/1563 - train loss: 13.1887 - test loss: 14.9018 - train acc: 0.3844 - test acc: 0.3438 - 4m 36s\n",
      "batch: 500/1563 - train loss: 13.2375 - test loss: 14.8650 - train acc: 0.3866 - test acc: 0.3411 - 4m 42s\n",
      "batch: 600/1563 - train loss: 13.1964 - test loss: 15.5671 - train acc: 0.4010 - test acc: 0.3198 - 4m 47s\n",
      "batch: 700/1563 - train loss: 13.2676 - test loss: 14.4167 - train acc: 0.3938 - test acc: 0.3560 - 4m 53s\n",
      "batch: 800/1563 - train loss: 13.0829 - test loss: 14.6255 - train acc: 0.3856 - test acc: 0.3508 - 4m 58s\n",
      "batch: 900/1563 - train loss: 13.1668 - test loss: 15.6038 - train acc: 0.4000 - test acc: 0.3285 - 5m 4s\n",
      "batch: 1000/1563 - train loss: 13.7489 - test loss: 15.4939 - train acc: 0.3594 - test acc: 0.3170 - 5m 9s\n",
      "batch: 1100/1563 - train loss: 13.1085 - test loss: 15.4030 - train acc: 0.3991 - test acc: 0.3322 - 5m 15s\n",
      "batch: 1200/1563 - train loss: 13.2363 - test loss: 14.3061 - train acc: 0.3896 - test acc: 0.3517 - 5m 20s\n",
      "batch: 1300/1563 - train loss: 13.1246 - test loss: 14.2172 - train acc: 0.4044 - test acc: 0.3617 - 5m 26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.4183 - test loss: 14.4260 - train acc: 0.3885 - test acc: 0.3536 - 5m 31s\n",
      "batch: 1500/1563 - train loss: 13.1527 - test loss: 15.5320 - train acc: 0.3928 - test acc: 0.3203 - 5m 36s\n",
      "batch: 1563/1563 - train loss: 13.1786 - test loss: 14.2941 - train acc: 0.3772 - test acc: 0.3621 - 5m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.4322 - test loss: 14.3951 - train acc: 0.4525 - test acc: 0.3629 - 5m 46s\n",
      "batch: 200/1563 - train loss: 11.1852 - test loss: 14.2194 - train acc: 0.4647 - test acc: 0.3692 - 5m 52s\n",
      "batch: 300/1563 - train loss: 11.7556 - test loss: 14.6730 - train acc: 0.4353 - test acc: 0.3589 - 5m 57s\n",
      "batch: 400/1563 - train loss: 11.7041 - test loss: 14.3915 - train acc: 0.4453 - test acc: 0.3667 - 6m 2s\n",
      "batch: 500/1563 - train loss: 11.8379 - test loss: 13.9582 - train acc: 0.4341 - test acc: 0.3780 - 6m 8s\n",
      "batch: 600/1563 - train loss: 11.7914 - test loss: 15.2183 - train acc: 0.4500 - test acc: 0.3438 - 6m 13s\n",
      "batch: 700/1563 - train loss: 11.9742 - test loss: 14.1599 - train acc: 0.4319 - test acc: 0.3713 - 6m 19s\n",
      "batch: 800/1563 - train loss: 12.1394 - test loss: 13.7361 - train acc: 0.4360 - test acc: 0.3811 - 6m 24s\n",
      "batch: 900/1563 - train loss: 12.1800 - test loss: 15.0082 - train acc: 0.4306 - test acc: 0.3445 - 6m 30s\n",
      "batch: 1000/1563 - train loss: 11.8239 - test loss: 14.1785 - train acc: 0.4447 - test acc: 0.3619 - 6m 35s\n",
      "batch: 1100/1563 - train loss: 12.1576 - test loss: 13.5823 - train acc: 0.4260 - test acc: 0.3827 - 6m 40s\n",
      "batch: 1200/1563 - train loss: 11.9946 - test loss: 13.7176 - train acc: 0.4316 - test acc: 0.3876 - 6m 46s\n",
      "batch: 1300/1563 - train loss: 11.9532 - test loss: 13.6476 - train acc: 0.4365 - test acc: 0.3774 - 6m 51s\n",
      "batch: 1400/1563 - train loss: 11.7151 - test loss: 15.3085 - train acc: 0.4425 - test acc: 0.3531 - 6m 56s\n",
      "batch: 1500/1563 - train loss: 11.8938 - test loss: 14.1104 - train acc: 0.4360 - test acc: 0.3727 - 7m 2s\n",
      "batch: 1563/1563 - train loss: 12.0594 - test loss: 13.8455 - train acc: 0.4375 - test acc: 0.3771 - 7m 6s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.8987 - test loss: 15.1309 - train acc: 0.5109 - test acc: 0.3574 - 7m 12s\n",
      "batch: 200/1563 - train loss: 10.1675 - test loss: 13.7233 - train acc: 0.5044 - test acc: 0.3920 - 7m 18s\n",
      "batch: 300/1563 - train loss: 10.2993 - test loss: 14.0300 - train acc: 0.4976 - test acc: 0.3821 - 7m 23s\n",
      "batch: 400/1563 - train loss: 10.3973 - test loss: 13.7588 - train acc: 0.4904 - test acc: 0.3900 - 7m 29s\n",
      "batch: 500/1563 - train loss: 10.7363 - test loss: 13.8671 - train acc: 0.4794 - test acc: 0.3962 - 7m 34s\n",
      "batch: 600/1563 - train loss: 10.5578 - test loss: 13.7655 - train acc: 0.4928 - test acc: 0.3909 - 7m 40s\n",
      "batch: 700/1563 - train loss: 10.6315 - test loss: 15.0219 - train acc: 0.4800 - test acc: 0.3656 - 7m 45s\n",
      "batch: 800/1563 - train loss: 10.7700 - test loss: 15.3532 - train acc: 0.4897 - test acc: 0.3533 - 7m 51s\n",
      "batch: 900/1563 - train loss: 10.8859 - test loss: 14.0440 - train acc: 0.4824 - test acc: 0.3747 - 7m 56s\n",
      "batch: 1000/1563 - train loss: 10.9171 - test loss: 13.4784 - train acc: 0.4707 - test acc: 0.3968 - 8m 2s\n",
      "batch: 1100/1563 - train loss: 10.7936 - test loss: 13.8940 - train acc: 0.4841 - test acc: 0.3859 - 8m 7s\n",
      "batch: 1200/1563 - train loss: 11.0326 - test loss: 14.4942 - train acc: 0.4731 - test acc: 0.3681 - 8m 13s\n",
      "batch: 1300/1563 - train loss: 10.8900 - test loss: 13.6814 - train acc: 0.4816 - test acc: 0.3956 - 8m 19s\n",
      "batch: 1400/1563 - train loss: 10.8015 - test loss: 14.6256 - train acc: 0.4738 - test acc: 0.3650 - 8m 25s\n",
      "batch: 1500/1563 - train loss: 10.8039 - test loss: 13.4667 - train acc: 0.4809 - test acc: 0.3986 - 8m 31s\n",
      "batch: 1563/1563 - train loss: 10.9735 - test loss: 13.8633 - train acc: 0.4775 - test acc: 0.3887 - 8m 35s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.8973 - test loss: 14.0755 - train acc: 0.5547 - test acc: 0.3906 - 8m 40s\n",
      "batch: 200/1563 - train loss: 8.7949 - test loss: 14.5544 - train acc: 0.5621 - test acc: 0.3786 - 8m 46s\n",
      "batch: 300/1563 - train loss: 9.5550 - test loss: 14.0668 - train acc: 0.5293 - test acc: 0.3932 - 8m 51s\n",
      "batch: 400/1563 - train loss: 9.1557 - test loss: 13.9394 - train acc: 0.5418 - test acc: 0.3934 - 8m 57s\n",
      "batch: 500/1563 - train loss: 9.7387 - test loss: 13.7716 - train acc: 0.5249 - test acc: 0.3982 - 9m 2s\n",
      "batch: 600/1563 - train loss: 9.5843 - test loss: 13.3536 - train acc: 0.5265 - test acc: 0.4083 - 9m 8s\n",
      "batch: 700/1563 - train loss: 9.8416 - test loss: 13.9716 - train acc: 0.5115 - test acc: 0.3989 - 9m 13s\n",
      "batch: 800/1563 - train loss: 9.9933 - test loss: 13.2375 - train acc: 0.5084 - test acc: 0.4118 - 9m 19s\n",
      "batch: 900/1563 - train loss: 9.5967 - test loss: 13.7434 - train acc: 0.5300 - test acc: 0.3977 - 9m 24s\n",
      "batch: 1000/1563 - train loss: 9.9055 - test loss: 13.4112 - train acc: 0.5290 - test acc: 0.4106 - 9m 30s\n",
      "batch: 1100/1563 - train loss: 9.6879 - test loss: 13.6451 - train acc: 0.5225 - test acc: 0.4037 - 9m 35s\n",
      "batch: 1200/1563 - train loss: 9.5745 - test loss: 13.7978 - train acc: 0.5297 - test acc: 0.3987 - 9m 40s\n",
      "batch: 1300/1563 - train loss: 9.7975 - test loss: 13.2449 - train acc: 0.5181 - test acc: 0.4233 - 9m 46s\n",
      "batch: 1400/1563 - train loss: 10.1281 - test loss: 13.3185 - train acc: 0.5081 - test acc: 0.4159 - 9m 51s\n",
      "batch: 1500/1563 - train loss: 9.8759 - test loss: 13.5821 - train acc: 0.5250 - test acc: 0.4084 - 9m 57s\n",
      "batch: 1563/1563 - train loss: 9.8220 - test loss: 12.9565 - train acc: 0.5247 - test acc: 0.4200 - 10m 2s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.7227 - test loss: 14.1253 - train acc: 0.6191 - test acc: 0.4075 - 10m 7s\n",
      "batch: 200/1563 - train loss: 7.5085 - test loss: 14.0036 - train acc: 0.6162 - test acc: 0.4056 - 10m 13s\n",
      "batch: 300/1563 - train loss: 8.1672 - test loss: 13.9876 - train acc: 0.5888 - test acc: 0.4072 - 10m 18s\n",
      "batch: 400/1563 - train loss: 8.4462 - test loss: 15.1633 - train acc: 0.5778 - test acc: 0.3847 - 10m 24s\n",
      "batch: 500/1563 - train loss: 8.3406 - test loss: 13.7518 - train acc: 0.5860 - test acc: 0.4101 - 10m 30s\n",
      "batch: 600/1563 - train loss: 8.3155 - test loss: 14.1699 - train acc: 0.5775 - test acc: 0.3923 - 10m 35s\n",
      "batch: 700/1563 - train loss: 8.3679 - test loss: 14.1078 - train acc: 0.5797 - test acc: 0.4087 - 10m 40s\n",
      "batch: 800/1563 - train loss: 8.7033 - test loss: 14.7146 - train acc: 0.5672 - test acc: 0.3825 - 10m 46s\n",
      "batch: 900/1563 - train loss: 8.6747 - test loss: 13.9409 - train acc: 0.5684 - test acc: 0.3986 - 10m 51s\n",
      "batch: 1000/1563 - train loss: 8.7614 - test loss: 13.7072 - train acc: 0.5712 - test acc: 0.4066 - 10m 57s\n",
      "batch: 1100/1563 - train loss: 9.1542 - test loss: 13.8425 - train acc: 0.5403 - test acc: 0.4056 - 11m 3s\n",
      "batch: 1200/1563 - train loss: 8.9995 - test loss: 13.5422 - train acc: 0.5512 - test acc: 0.4189 - 11m 8s\n",
      "batch: 1300/1563 - train loss: 8.7807 - test loss: 13.5163 - train acc: 0.5653 - test acc: 0.4156 - 11m 14s\n",
      "batch: 1400/1563 - train loss: 8.8591 - test loss: 14.0744 - train acc: 0.5572 - test acc: 0.4029 - 11m 20s\n",
      "batch: 1500/1563 - train loss: 8.9514 - test loss: 13.4725 - train acc: 0.5531 - test acc: 0.4144 - 11m 25s\n",
      "batch: 1563/1563 - train loss: 9.1909 - test loss: 13.4840 - train acc: 0.5421 - test acc: 0.4148 - 11m 30s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.5809 - test loss: 13.6470 - train acc: 0.6651 - test acc: 0.4276 - 11m 35s\n",
      "batch: 200/1563 - train loss: 6.7113 - test loss: 14.0447 - train acc: 0.6510 - test acc: 0.4136 - 11m 41s\n",
      "batch: 300/1563 - train loss: 6.9729 - test loss: 14.2212 - train acc: 0.6475 - test acc: 0.4169 - 11m 46s\n",
      "batch: 400/1563 - train loss: 7.1410 - test loss: 14.2888 - train acc: 0.6325 - test acc: 0.4097 - 11m 52s\n",
      "batch: 500/1563 - train loss: 7.2451 - test loss: 13.7140 - train acc: 0.6260 - test acc: 0.4274 - 11m 57s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.6231 - test loss: 14.7712 - train acc: 0.6144 - test acc: 0.3943 - 12m 3s\n",
      "batch: 700/1563 - train loss: 7.7023 - test loss: 14.9476 - train acc: 0.6184 - test acc: 0.3921 - 12m 9s\n",
      "batch: 800/1563 - train loss: 7.4699 - test loss: 13.9900 - train acc: 0.6219 - test acc: 0.4202 - 12m 14s\n",
      "batch: 900/1563 - train loss: 7.9476 - test loss: 14.3343 - train acc: 0.5943 - test acc: 0.4038 - 12m 20s\n",
      "batch: 1000/1563 - train loss: 7.7686 - test loss: 14.2524 - train acc: 0.6022 - test acc: 0.4073 - 12m 25s\n",
      "batch: 1100/1563 - train loss: 7.8915 - test loss: 14.1289 - train acc: 0.6006 - test acc: 0.4118 - 12m 31s\n",
      "batch: 1200/1563 - train loss: 8.2709 - test loss: 14.7843 - train acc: 0.5778 - test acc: 0.4038 - 12m 37s\n",
      "batch: 1300/1563 - train loss: 8.2023 - test loss: 14.0395 - train acc: 0.5934 - test acc: 0.4142 - 12m 42s\n",
      "batch: 1400/1563 - train loss: 8.0080 - test loss: 14.0388 - train acc: 0.5960 - test acc: 0.4131 - 12m 48s\n",
      "batch: 1500/1563 - train loss: 8.2952 - test loss: 14.2581 - train acc: 0.5866 - test acc: 0.4141 - 12m 53s\n",
      "batch: 1563/1563 - train loss: 8.4504 - test loss: 15.1494 - train acc: 0.5709 - test acc: 0.3799 - 12m 58s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.6582 - test loss: 14.5394 - train acc: 0.6969 - test acc: 0.4119 - 13m 3s\n",
      "batch: 200/1563 - train loss: 5.7872 - test loss: 14.9499 - train acc: 0.6938 - test acc: 0.4068 - 13m 10s\n",
      "batch: 300/1563 - train loss: 6.2275 - test loss: 14.9841 - train acc: 0.6744 - test acc: 0.4000 - 13m 15s\n",
      "batch: 400/1563 - train loss: 6.0938 - test loss: 14.6884 - train acc: 0.6853 - test acc: 0.4152 - 13m 21s\n",
      "batch: 500/1563 - train loss: 6.4279 - test loss: 15.1249 - train acc: 0.6647 - test acc: 0.4084 - 13m 26s\n",
      "batch: 600/1563 - train loss: 6.7072 - test loss: 15.0200 - train acc: 0.6485 - test acc: 0.4036 - 13m 32s\n",
      "batch: 700/1563 - train loss: 6.9788 - test loss: 14.6868 - train acc: 0.6410 - test acc: 0.4142 - 13m 38s\n",
      "batch: 800/1563 - train loss: 7.0379 - test loss: 15.3119 - train acc: 0.6475 - test acc: 0.3949 - 13m 43s\n",
      "batch: 900/1563 - train loss: 6.8743 - test loss: 14.6502 - train acc: 0.6441 - test acc: 0.4077 - 13m 49s\n",
      "batch: 1000/1563 - train loss: 7.2232 - test loss: 14.6429 - train acc: 0.6240 - test acc: 0.4106 - 13m 54s\n",
      "batch: 1100/1563 - train loss: 6.8468 - test loss: 14.5722 - train acc: 0.6397 - test acc: 0.4224 - 14m 0s\n",
      "batch: 1200/1563 - train loss: 7.0262 - test loss: 14.3452 - train acc: 0.6326 - test acc: 0.4264 - 14m 5s\n",
      "batch: 1300/1563 - train loss: 7.2134 - test loss: 14.2851 - train acc: 0.6250 - test acc: 0.4164 - 14m 11s\n",
      "batch: 1400/1563 - train loss: 7.1505 - test loss: 15.2108 - train acc: 0.6310 - test acc: 0.3894 - 14m 17s\n",
      "batch: 1500/1563 - train loss: 7.5444 - test loss: 14.2150 - train acc: 0.6206 - test acc: 0.4278 - 14m 22s\n",
      "batch: 1563/1563 - train loss: 7.5413 - test loss: 14.1283 - train acc: 0.6134 - test acc: 0.4168 - 14m 26s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.7153 - test loss: 15.0315 - train acc: 0.7485 - test acc: 0.4216 - 14m 32s\n",
      "batch: 200/1563 - train loss: 4.8156 - test loss: 15.6469 - train acc: 0.7375 - test acc: 0.4080 - 14m 37s\n",
      "batch: 300/1563 - train loss: 5.3207 - test loss: 15.0538 - train acc: 0.7172 - test acc: 0.4219 - 14m 43s\n",
      "batch: 400/1563 - train loss: 5.3093 - test loss: 16.1799 - train acc: 0.7200 - test acc: 0.4013 - 14m 49s\n",
      "batch: 500/1563 - train loss: 5.4721 - test loss: 15.2776 - train acc: 0.7094 - test acc: 0.4206 - 14m 54s\n",
      "batch: 600/1563 - train loss: 5.8985 - test loss: 15.4403 - train acc: 0.6891 - test acc: 0.4091 - 15m 0s\n",
      "batch: 700/1563 - train loss: 5.6688 - test loss: 15.3123 - train acc: 0.6978 - test acc: 0.4105 - 15m 5s\n",
      "batch: 800/1563 - train loss: 6.0523 - test loss: 15.3664 - train acc: 0.6972 - test acc: 0.4058 - 15m 10s\n",
      "batch: 900/1563 - train loss: 6.0735 - test loss: 14.6763 - train acc: 0.6807 - test acc: 0.4247 - 15m 16s\n",
      "batch: 1000/1563 - train loss: 6.1970 - test loss: 15.5512 - train acc: 0.6829 - test acc: 0.4146 - 15m 22s\n",
      "batch: 1100/1563 - train loss: 5.9559 - test loss: 14.7534 - train acc: 0.6931 - test acc: 0.4284 - 15m 27s\n",
      "batch: 1200/1563 - train loss: 6.1314 - test loss: 14.5891 - train acc: 0.6675 - test acc: 0.4256 - 15m 32s\n",
      "batch: 1300/1563 - train loss: 6.2788 - test loss: 15.1446 - train acc: 0.6735 - test acc: 0.4225 - 15m 38s\n",
      "batch: 1400/1563 - train loss: 6.3277 - test loss: 15.4083 - train acc: 0.6716 - test acc: 0.4112 - 15m 44s\n",
      "batch: 1500/1563 - train loss: 6.6102 - test loss: 14.7713 - train acc: 0.6522 - test acc: 0.4261 - 15m 49s\n",
      "batch: 1563/1563 - train loss: 6.8128 - test loss: 14.8688 - train acc: 0.6451 - test acc: 0.4233 - 15m 54s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.1790 - test loss: 15.5718 - train acc: 0.7775 - test acc: 0.4118 - 15m 59s\n",
      "batch: 200/1563 - train loss: 3.9125 - test loss: 15.8901 - train acc: 0.7928 - test acc: 0.4196 - 16m 5s\n",
      "batch: 300/1563 - train loss: 4.4893 - test loss: 15.8496 - train acc: 0.7484 - test acc: 0.4082 - 16m 10s\n",
      "batch: 400/1563 - train loss: 4.5145 - test loss: 16.2680 - train acc: 0.7559 - test acc: 0.4058 - 16m 16s\n",
      "batch: 500/1563 - train loss: 4.5895 - test loss: 15.5153 - train acc: 0.7456 - test acc: 0.4239 - 16m 21s\n",
      "batch: 600/1563 - train loss: 4.8126 - test loss: 15.9753 - train acc: 0.7422 - test acc: 0.4098 - 16m 27s\n",
      "batch: 700/1563 - train loss: 4.8933 - test loss: 15.9010 - train acc: 0.7378 - test acc: 0.4159 - 16m 32s\n",
      "batch: 800/1563 - train loss: 5.1403 - test loss: 15.8225 - train acc: 0.7219 - test acc: 0.4212 - 16m 38s\n",
      "batch: 900/1563 - train loss: 5.0162 - test loss: 15.6453 - train acc: 0.7313 - test acc: 0.4220 - 16m 43s\n",
      "batch: 1000/1563 - train loss: 5.2877 - test loss: 15.2411 - train acc: 0.7154 - test acc: 0.4239 - 16m 48s\n",
      "batch: 1100/1563 - train loss: 5.1639 - test loss: 15.2417 - train acc: 0.7310 - test acc: 0.4345 - 16m 54s\n",
      "batch: 1200/1563 - train loss: 5.3028 - test loss: 15.7287 - train acc: 0.7253 - test acc: 0.4171 - 16m 59s\n",
      "batch: 1300/1563 - train loss: 5.4649 - test loss: 15.8522 - train acc: 0.7044 - test acc: 0.4225 - 17m 5s\n",
      "batch: 1400/1563 - train loss: 5.3393 - test loss: 16.4874 - train acc: 0.7103 - test acc: 0.4036 - 17m 10s\n",
      "batch: 1500/1563 - train loss: 5.6892 - test loss: 15.5247 - train acc: 0.6972 - test acc: 0.4143 - 17m 16s\n",
      "batch: 1563/1563 - train loss: 5.6854 - test loss: 15.1210 - train acc: 0.7048 - test acc: 0.4254 - 17m 20s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.4557 - test loss: 15.6073 - train acc: 0.8106 - test acc: 0.4294 - 17m 26s\n",
      "batch: 200/1563 - train loss: 3.2270 - test loss: 16.5372 - train acc: 0.8193 - test acc: 0.4207 - 17m 31s\n",
      "batch: 300/1563 - train loss: 3.7392 - test loss: 16.3519 - train acc: 0.7924 - test acc: 0.4154 - 17m 36s\n",
      "batch: 400/1563 - train loss: 3.8392 - test loss: 16.0504 - train acc: 0.7769 - test acc: 0.4296 - 17m 42s\n",
      "batch: 500/1563 - train loss: 3.7080 - test loss: 16.8709 - train acc: 0.7987 - test acc: 0.4150 - 17m 47s\n",
      "batch: 600/1563 - train loss: 4.1318 - test loss: 17.2545 - train acc: 0.7769 - test acc: 0.4072 - 17m 53s\n",
      "batch: 700/1563 - train loss: 4.4568 - test loss: 15.9067 - train acc: 0.7500 - test acc: 0.4356 - 17m 58s\n",
      "batch: 800/1563 - train loss: 4.3997 - test loss: 16.1324 - train acc: 0.7547 - test acc: 0.4200 - 18m 4s\n",
      "batch: 900/1563 - train loss: 4.0163 - test loss: 16.0809 - train acc: 0.7821 - test acc: 0.4214 - 18m 9s\n",
      "batch: 1000/1563 - train loss: 4.7030 - test loss: 15.9127 - train acc: 0.7400 - test acc: 0.4217 - 18m 15s\n",
      "batch: 1100/1563 - train loss: 4.6129 - test loss: 16.5409 - train acc: 0.7424 - test acc: 0.4084 - 18m 21s\n",
      "batch: 1200/1563 - train loss: 5.0498 - test loss: 16.4823 - train acc: 0.7191 - test acc: 0.4096 - 18m 27s\n",
      "batch: 1300/1563 - train loss: 4.9480 - test loss: 16.7085 - train acc: 0.7328 - test acc: 0.4056 - 18m 32s\n",
      "batch: 1400/1563 - train loss: 5.0788 - test loss: 16.6901 - train acc: 0.7303 - test acc: 0.4099 - 18m 38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 5.0814 - test loss: 15.7325 - train acc: 0.7235 - test acc: 0.4199 - 18m 43s\n",
      "batch: 1563/1563 - train loss: 4.9820 - test loss: 15.8029 - train acc: 0.7241 - test acc: 0.4250 - 18m 48s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8765 - test loss: 15.7443 - train acc: 0.8422 - test acc: 0.4437 - 18m 54s\n",
      "batch: 200/1563 - train loss: 2.8279 - test loss: 17.0766 - train acc: 0.8453 - test acc: 0.4196 - 19m 0s\n",
      "batch: 300/1563 - train loss: 3.0189 - test loss: 16.7911 - train acc: 0.8309 - test acc: 0.4251 - 19m 5s\n",
      "batch: 400/1563 - train loss: 3.0995 - test loss: 16.5934 - train acc: 0.8197 - test acc: 0.4311 - 19m 11s\n",
      "batch: 500/1563 - train loss: 3.4913 - test loss: 17.4719 - train acc: 0.8109 - test acc: 0.4099 - 19m 17s\n",
      "batch: 600/1563 - train loss: 3.4368 - test loss: 16.8839 - train acc: 0.8068 - test acc: 0.4199 - 19m 22s\n",
      "batch: 700/1563 - train loss: 3.6900 - test loss: 16.1297 - train acc: 0.7997 - test acc: 0.4325 - 19m 28s\n",
      "batch: 800/1563 - train loss: 3.5978 - test loss: 16.7017 - train acc: 0.8015 - test acc: 0.4236 - 19m 34s\n",
      "batch: 900/1563 - train loss: 3.8160 - test loss: 16.3816 - train acc: 0.7934 - test acc: 0.4298 - 19m 40s\n",
      "batch: 1000/1563 - train loss: 3.7868 - test loss: 16.6334 - train acc: 0.7837 - test acc: 0.4280 - 19m 46s\n",
      "batch: 1100/1563 - train loss: 4.2277 - test loss: 16.6326 - train acc: 0.7662 - test acc: 0.4251 - 19m 52s\n",
      "batch: 1200/1563 - train loss: 4.2190 - test loss: 16.5145 - train acc: 0.7684 - test acc: 0.4224 - 19m 57s\n",
      "time is up! finishing training\n",
      "batch: 1201/1563 - train loss: 4.2275 - test loss: 16.5995 - train acc: 0.7668 - test acc: 0.4227 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 12\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 25.9472 - test loss: 25.1581 - train acc: 0.0411 - test acc: 0.0624 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.8988 - test loss: 23.6093 - train acc: 0.0714 - test acc: 0.0750 - 0m 8s\n",
      "batch: 300/1563 - train loss: 22.4476 - test loss: 22.2213 - train acc: 0.1015 - test acc: 0.1162 - 0m 13s\n",
      "batch: 400/1563 - train loss: 22.2965 - test loss: 21.7533 - train acc: 0.1012 - test acc: 0.1203 - 0m 19s\n",
      "batch: 500/1563 - train loss: 21.6133 - test loss: 21.1004 - train acc: 0.1197 - test acc: 0.1246 - 0m 24s\n",
      "batch: 600/1563 - train loss: 20.9690 - test loss: 20.2891 - train acc: 0.1344 - test acc: 0.1523 - 0m 30s\n",
      "batch: 700/1563 - train loss: 20.6303 - test loss: 20.5289 - train acc: 0.1425 - test acc: 0.1446 - 0m 35s\n",
      "batch: 800/1563 - train loss: 20.3719 - test loss: 20.2661 - train acc: 0.1513 - test acc: 0.1520 - 0m 41s\n",
      "batch: 900/1563 - train loss: 19.8024 - test loss: 19.2581 - train acc: 0.1600 - test acc: 0.1788 - 0m 47s\n",
      "batch: 1000/1563 - train loss: 19.4240 - test loss: 21.5666 - train acc: 0.1851 - test acc: 0.1386 - 0m 52s\n",
      "batch: 1100/1563 - train loss: 19.2573 - test loss: 19.1335 - train acc: 0.1847 - test acc: 0.1857 - 0m 57s\n",
      "batch: 1200/1563 - train loss: 19.0370 - test loss: 19.8406 - train acc: 0.1863 - test acc: 0.1760 - 1m 3s\n",
      "batch: 1300/1563 - train loss: 18.8995 - test loss: 19.0075 - train acc: 0.1916 - test acc: 0.1927 - 1m 8s\n",
      "batch: 1400/1563 - train loss: 18.5834 - test loss: 18.5966 - train acc: 0.2081 - test acc: 0.2036 - 1m 13s\n",
      "batch: 1500/1563 - train loss: 18.5217 - test loss: 18.2796 - train acc: 0.2088 - test acc: 0.2141 - 1m 19s\n",
      "batch: 1563/1563 - train loss: 18.3496 - test loss: 18.6704 - train acc: 0.2129 - test acc: 0.2104 - 1m 23s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.6938 - test loss: 18.0514 - train acc: 0.2301 - test acc: 0.2192 - 1m 28s\n",
      "batch: 200/1563 - train loss: 17.2665 - test loss: 17.4783 - train acc: 0.2459 - test acc: 0.2450 - 1m 34s\n",
      "batch: 300/1563 - train loss: 17.5324 - test loss: 17.4953 - train acc: 0.2444 - test acc: 0.2400 - 1m 39s\n",
      "batch: 400/1563 - train loss: 17.0497 - test loss: 17.7912 - train acc: 0.2594 - test acc: 0.2352 - 1m 45s\n",
      "batch: 500/1563 - train loss: 17.1512 - test loss: 17.5544 - train acc: 0.2472 - test acc: 0.2322 - 1m 50s\n",
      "batch: 600/1563 - train loss: 16.8670 - test loss: 17.0787 - train acc: 0.2597 - test acc: 0.2595 - 1m 56s\n",
      "batch: 700/1563 - train loss: 17.1368 - test loss: 17.2622 - train acc: 0.2506 - test acc: 0.2520 - 2m 1s\n",
      "batch: 800/1563 - train loss: 16.7700 - test loss: 16.9803 - train acc: 0.2722 - test acc: 0.2611 - 2m 6s\n",
      "batch: 900/1563 - train loss: 16.7797 - test loss: 17.5021 - train acc: 0.2603 - test acc: 0.2438 - 2m 11s\n",
      "batch: 1000/1563 - train loss: 16.4123 - test loss: 17.0598 - train acc: 0.2709 - test acc: 0.2567 - 2m 17s\n",
      "batch: 1100/1563 - train loss: 16.4486 - test loss: 16.3931 - train acc: 0.2747 - test acc: 0.2755 - 2m 22s\n",
      "batch: 1200/1563 - train loss: 16.7134 - test loss: 17.4204 - train acc: 0.2791 - test acc: 0.2469 - 2m 28s\n",
      "batch: 1300/1563 - train loss: 16.5271 - test loss: 16.8924 - train acc: 0.2719 - test acc: 0.2638 - 2m 33s\n",
      "batch: 1400/1563 - train loss: 15.8911 - test loss: 16.4740 - train acc: 0.2949 - test acc: 0.2778 - 2m 38s\n",
      "batch: 1500/1563 - train loss: 15.7183 - test loss: 17.5689 - train acc: 0.2962 - test acc: 0.2534 - 2m 44s\n",
      "batch: 1563/1563 - train loss: 15.8515 - test loss: 16.0159 - train acc: 0.2878 - test acc: 0.2975 - 2m 49s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.4868 - test loss: 15.4419 - train acc: 0.3381 - test acc: 0.3130 - 2m 54s\n",
      "batch: 200/1563 - train loss: 14.7822 - test loss: 15.5331 - train acc: 0.3247 - test acc: 0.3065 - 3m 0s\n",
      "batch: 300/1563 - train loss: 14.5383 - test loss: 17.3086 - train acc: 0.3281 - test acc: 0.2645 - 3m 5s\n",
      "batch: 400/1563 - train loss: 14.7929 - test loss: 15.7421 - train acc: 0.3250 - test acc: 0.2988 - 3m 11s\n",
      "batch: 500/1563 - train loss: 14.7053 - test loss: 15.4862 - train acc: 0.3375 - test acc: 0.3132 - 3m 16s\n",
      "batch: 600/1563 - train loss: 14.6055 - test loss: 15.4988 - train acc: 0.3278 - test acc: 0.3143 - 3m 22s\n",
      "batch: 700/1563 - train loss: 14.7738 - test loss: 15.5696 - train acc: 0.3284 - test acc: 0.3023 - 3m 27s\n",
      "batch: 800/1563 - train loss: 14.8138 - test loss: 14.9826 - train acc: 0.3221 - test acc: 0.3327 - 3m 32s\n",
      "batch: 900/1563 - train loss: 14.5394 - test loss: 15.9037 - train acc: 0.3393 - test acc: 0.3131 - 3m 38s\n",
      "batch: 1000/1563 - train loss: 14.4784 - test loss: 14.9731 - train acc: 0.3434 - test acc: 0.3329 - 3m 43s\n",
      "batch: 1100/1563 - train loss: 14.4348 - test loss: 15.4593 - train acc: 0.3356 - test acc: 0.3195 - 3m 48s\n",
      "batch: 1200/1563 - train loss: 14.4698 - test loss: 14.8654 - train acc: 0.3406 - test acc: 0.3383 - 3m 54s\n",
      "batch: 1300/1563 - train loss: 14.1928 - test loss: 14.5692 - train acc: 0.3585 - test acc: 0.3528 - 3m 59s\n",
      "batch: 1400/1563 - train loss: 14.0523 - test loss: 15.2564 - train acc: 0.3637 - test acc: 0.3317 - 4m 5s\n",
      "batch: 1500/1563 - train loss: 14.4023 - test loss: 15.5998 - train acc: 0.3497 - test acc: 0.3166 - 4m 10s\n",
      "batch: 1563/1563 - train loss: 14.2455 - test loss: 14.6259 - train acc: 0.3522 - test acc: 0.3489 - 4m 14s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.8314 - test loss: 14.5662 - train acc: 0.4125 - test acc: 0.3511 - 4m 20s\n",
      "batch: 200/1563 - train loss: 12.6244 - test loss: 15.0251 - train acc: 0.4028 - test acc: 0.3444 - 4m 25s\n",
      "batch: 300/1563 - train loss: 13.2994 - test loss: 15.2816 - train acc: 0.3906 - test acc: 0.3287 - 4m 31s\n",
      "batch: 400/1563 - train loss: 13.1005 - test loss: 14.8725 - train acc: 0.3969 - test acc: 0.3364 - 4m 36s\n",
      "batch: 500/1563 - train loss: 13.0615 - test loss: 14.5168 - train acc: 0.3931 - test acc: 0.3591 - 4m 41s\n",
      "batch: 600/1563 - train loss: 13.0252 - test loss: 14.4934 - train acc: 0.3953 - test acc: 0.3498 - 4m 47s\n",
      "batch: 700/1563 - train loss: 12.9718 - test loss: 14.5009 - train acc: 0.3937 - test acc: 0.3457 - 4m 52s\n",
      "batch: 800/1563 - train loss: 12.8684 - test loss: 14.7349 - train acc: 0.4041 - test acc: 0.3438 - 4m 57s\n",
      "batch: 900/1563 - train loss: 13.0998 - test loss: 15.0264 - train acc: 0.3994 - test acc: 0.3421 - 5m 3s\n",
      "batch: 1000/1563 - train loss: 12.7487 - test loss: 13.7435 - train acc: 0.4097 - test acc: 0.3798 - 5m 8s\n",
      "batch: 1100/1563 - train loss: 13.2577 - test loss: 14.1299 - train acc: 0.3788 - test acc: 0.3616 - 5m 14s\n",
      "batch: 1200/1563 - train loss: 12.9465 - test loss: 15.0070 - train acc: 0.4000 - test acc: 0.3422 - 5m 19s\n",
      "batch: 1300/1563 - train loss: 12.9849 - test loss: 14.6425 - train acc: 0.3987 - test acc: 0.3509 - 5m 24s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 12.8115 - test loss: 14.0473 - train acc: 0.4063 - test acc: 0.3718 - 5m 30s\n",
      "batch: 1500/1563 - train loss: 12.8502 - test loss: 13.5873 - train acc: 0.4078 - test acc: 0.3874 - 5m 35s\n",
      "batch: 1563/1563 - train loss: 12.9926 - test loss: 14.1739 - train acc: 0.3978 - test acc: 0.3692 - 5m 40s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3408 - test loss: 14.3478 - train acc: 0.4547 - test acc: 0.3653 - 5m 45s\n",
      "batch: 200/1563 - train loss: 11.5062 - test loss: 13.9527 - train acc: 0.4563 - test acc: 0.3822 - 5m 50s\n",
      "batch: 300/1563 - train loss: 11.3520 - test loss: 15.1235 - train acc: 0.4594 - test acc: 0.3511 - 5m 56s\n",
      "batch: 400/1563 - train loss: 11.5833 - test loss: 13.5707 - train acc: 0.4428 - test acc: 0.3859 - 6m 1s\n",
      "batch: 500/1563 - train loss: 11.1084 - test loss: 14.8120 - train acc: 0.4681 - test acc: 0.3551 - 6m 7s\n",
      "batch: 600/1563 - train loss: 11.6988 - test loss: 13.7851 - train acc: 0.4538 - test acc: 0.3873 - 6m 12s\n",
      "batch: 700/1563 - train loss: 11.7752 - test loss: 13.7076 - train acc: 0.4357 - test acc: 0.3881 - 6m 17s\n",
      "batch: 800/1563 - train loss: 11.7447 - test loss: 14.4546 - train acc: 0.4431 - test acc: 0.3624 - 6m 23s\n",
      "batch: 900/1563 - train loss: 12.2288 - test loss: 14.4394 - train acc: 0.4210 - test acc: 0.3673 - 6m 28s\n",
      "batch: 1000/1563 - train loss: 12.0281 - test loss: 13.9982 - train acc: 0.4231 - test acc: 0.3698 - 6m 34s\n",
      "batch: 1100/1563 - train loss: 11.8985 - test loss: 14.7997 - train acc: 0.4341 - test acc: 0.3544 - 6m 39s\n",
      "batch: 1200/1563 - train loss: 11.4212 - test loss: 13.5628 - train acc: 0.4488 - test acc: 0.3902 - 6m 44s\n",
      "batch: 1300/1563 - train loss: 12.1285 - test loss: 13.9787 - train acc: 0.4256 - test acc: 0.3742 - 6m 50s\n",
      "batch: 1400/1563 - train loss: 11.7261 - test loss: 13.5205 - train acc: 0.4487 - test acc: 0.3843 - 6m 55s\n",
      "batch: 1500/1563 - train loss: 11.5211 - test loss: 13.8444 - train acc: 0.4428 - test acc: 0.3849 - 7m 0s\n",
      "batch: 1563/1563 - train loss: 11.5341 - test loss: 13.7440 - train acc: 0.4529 - test acc: 0.3905 - 7m 5s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0876 - test loss: 13.3867 - train acc: 0.5122 - test acc: 0.4007 - 7m 10s\n",
      "batch: 200/1563 - train loss: 9.8368 - test loss: 14.1788 - train acc: 0.5203 - test acc: 0.3780 - 7m 16s\n",
      "batch: 300/1563 - train loss: 10.2536 - test loss: 13.9319 - train acc: 0.5106 - test acc: 0.3918 - 7m 21s\n",
      "batch: 400/1563 - train loss: 10.3604 - test loss: 13.4472 - train acc: 0.5066 - test acc: 0.3998 - 7m 27s\n",
      "batch: 500/1563 - train loss: 10.4849 - test loss: 13.9313 - train acc: 0.4904 - test acc: 0.3873 - 7m 32s\n",
      "batch: 600/1563 - train loss: 10.4324 - test loss: 14.1987 - train acc: 0.4875 - test acc: 0.3791 - 7m 38s\n",
      "batch: 700/1563 - train loss: 10.7346 - test loss: 13.5648 - train acc: 0.4856 - test acc: 0.3972 - 7m 43s\n",
      "batch: 800/1563 - train loss: 10.6067 - test loss: 14.0697 - train acc: 0.4913 - test acc: 0.3794 - 7m 49s\n",
      "batch: 900/1563 - train loss: 10.6394 - test loss: 13.2755 - train acc: 0.4904 - test acc: 0.4071 - 7m 54s\n",
      "batch: 1000/1563 - train loss: 10.3998 - test loss: 13.2122 - train acc: 0.4953 - test acc: 0.4162 - 7m 59s\n",
      "batch: 1100/1563 - train loss: 10.4002 - test loss: 13.8762 - train acc: 0.4942 - test acc: 0.3976 - 8m 5s\n",
      "batch: 1200/1563 - train loss: 10.3712 - test loss: 13.2270 - train acc: 0.4932 - test acc: 0.4116 - 8m 11s\n",
      "batch: 1300/1563 - train loss: 10.9150 - test loss: 14.0437 - train acc: 0.4757 - test acc: 0.3807 - 8m 16s\n",
      "batch: 1400/1563 - train loss: 10.9928 - test loss: 13.2017 - train acc: 0.4715 - test acc: 0.4167 - 8m 22s\n",
      "batch: 1500/1563 - train loss: 10.8003 - test loss: 13.3780 - train acc: 0.4819 - test acc: 0.4083 - 8m 27s\n",
      "batch: 1563/1563 - train loss: 10.6999 - test loss: 13.4973 - train acc: 0.4887 - test acc: 0.4081 - 8m 32s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.6519 - test loss: 14.2355 - train acc: 0.5700 - test acc: 0.3934 - 8m 37s\n",
      "batch: 200/1563 - train loss: 8.7997 - test loss: 14.7683 - train acc: 0.5656 - test acc: 0.3773 - 8m 43s\n",
      "batch: 300/1563 - train loss: 9.3589 - test loss: 14.1461 - train acc: 0.5372 - test acc: 0.3874 - 8m 48s\n",
      "batch: 400/1563 - train loss: 9.3901 - test loss: 13.7835 - train acc: 0.5278 - test acc: 0.4044 - 8m 53s\n",
      "batch: 500/1563 - train loss: 9.3662 - test loss: 13.7350 - train acc: 0.5515 - test acc: 0.4000 - 8m 59s\n",
      "batch: 600/1563 - train loss: 9.5008 - test loss: 13.8929 - train acc: 0.5244 - test acc: 0.3969 - 9m 5s\n",
      "batch: 700/1563 - train loss: 9.5332 - test loss: 13.4578 - train acc: 0.5287 - test acc: 0.4149 - 9m 11s\n",
      "batch: 800/1563 - train loss: 9.4635 - test loss: 14.0340 - train acc: 0.5368 - test acc: 0.3968 - 9m 16s\n",
      "batch: 900/1563 - train loss: 9.6218 - test loss: 13.9282 - train acc: 0.5331 - test acc: 0.3969 - 9m 21s\n",
      "batch: 1000/1563 - train loss: 9.8585 - test loss: 13.7167 - train acc: 0.5181 - test acc: 0.4023 - 9m 27s\n",
      "batch: 1100/1563 - train loss: 9.8232 - test loss: 13.3130 - train acc: 0.5206 - test acc: 0.4168 - 9m 33s\n",
      "batch: 1200/1563 - train loss: 9.9794 - test loss: 13.5615 - train acc: 0.5025 - test acc: 0.4066 - 9m 39s\n",
      "batch: 1300/1563 - train loss: 9.5378 - test loss: 13.5635 - train acc: 0.5369 - test acc: 0.4122 - 9m 44s\n",
      "batch: 1400/1563 - train loss: 9.9480 - test loss: 13.6234 - train acc: 0.5163 - test acc: 0.4066 - 9m 50s\n",
      "batch: 1500/1563 - train loss: 9.9100 - test loss: 13.6266 - train acc: 0.5271 - test acc: 0.4080 - 9m 55s\n",
      "batch: 1563/1563 - train loss: 9.8735 - test loss: 13.5047 - train acc: 0.5200 - test acc: 0.4111 - 10m 0s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.6861 - test loss: 13.8854 - train acc: 0.6156 - test acc: 0.4118 - 10m 6s\n",
      "batch: 200/1563 - train loss: 7.2772 - test loss: 13.4832 - train acc: 0.6378 - test acc: 0.4247 - 10m 11s\n",
      "batch: 300/1563 - train loss: 7.8400 - test loss: 13.4403 - train acc: 0.5934 - test acc: 0.4243 - 10m 17s\n",
      "batch: 400/1563 - train loss: 8.1877 - test loss: 14.2566 - train acc: 0.5827 - test acc: 0.4027 - 10m 23s\n",
      "batch: 500/1563 - train loss: 8.0842 - test loss: 14.0233 - train acc: 0.5994 - test acc: 0.4131 - 10m 28s\n",
      "batch: 600/1563 - train loss: 8.4146 - test loss: 13.8285 - train acc: 0.5722 - test acc: 0.4140 - 10m 34s\n",
      "batch: 700/1563 - train loss: 8.2922 - test loss: 13.7968 - train acc: 0.5853 - test acc: 0.4120 - 10m 39s\n",
      "batch: 800/1563 - train loss: 8.6201 - test loss: 14.8741 - train acc: 0.5622 - test acc: 0.3903 - 10m 45s\n",
      "batch: 900/1563 - train loss: 8.6801 - test loss: 13.5180 - train acc: 0.5628 - test acc: 0.4130 - 10m 50s\n",
      "batch: 1000/1563 - train loss: 8.6790 - test loss: 13.5922 - train acc: 0.5746 - test acc: 0.4221 - 10m 56s\n",
      "batch: 1100/1563 - train loss: 8.5282 - test loss: 13.8550 - train acc: 0.5750 - test acc: 0.4144 - 11m 1s\n",
      "batch: 1200/1563 - train loss: 8.9255 - test loss: 13.7851 - train acc: 0.5428 - test acc: 0.4255 - 11m 7s\n",
      "batch: 1300/1563 - train loss: 8.8287 - test loss: 13.2324 - train acc: 0.5563 - test acc: 0.4267 - 11m 13s\n",
      "batch: 1400/1563 - train loss: 8.5403 - test loss: 13.4439 - train acc: 0.5734 - test acc: 0.4229 - 11m 19s\n",
      "batch: 1500/1563 - train loss: 9.3621 - test loss: 14.0931 - train acc: 0.5353 - test acc: 0.4056 - 11m 24s\n",
      "batch: 1563/1563 - train loss: 9.0778 - test loss: 13.3395 - train acc: 0.5609 - test acc: 0.4234 - 11m 28s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.3167 - test loss: 13.6212 - train acc: 0.6816 - test acc: 0.4317 - 11m 34s\n",
      "batch: 200/1563 - train loss: 6.4400 - test loss: 14.1933 - train acc: 0.6641 - test acc: 0.4225 - 11m 40s\n",
      "batch: 300/1563 - train loss: 6.9678 - test loss: 13.9356 - train acc: 0.6334 - test acc: 0.4268 - 11m 45s\n",
      "batch: 400/1563 - train loss: 6.8402 - test loss: 14.4069 - train acc: 0.6403 - test acc: 0.4111 - 11m 51s\n",
      "batch: 500/1563 - train loss: 7.1453 - test loss: 13.8813 - train acc: 0.6310 - test acc: 0.4214 - 11m 56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.3109 - test loss: 14.7518 - train acc: 0.6237 - test acc: 0.4103 - 12m 1s\n",
      "batch: 700/1563 - train loss: 7.3994 - test loss: 13.9521 - train acc: 0.6209 - test acc: 0.4248 - 12m 7s\n",
      "batch: 800/1563 - train loss: 7.5884 - test loss: 14.0363 - train acc: 0.6163 - test acc: 0.4194 - 12m 12s\n",
      "batch: 900/1563 - train loss: 7.8003 - test loss: 14.3616 - train acc: 0.6140 - test acc: 0.4136 - 12m 18s\n",
      "batch: 1000/1563 - train loss: 7.4572 - test loss: 13.6770 - train acc: 0.6213 - test acc: 0.4285 - 12m 24s\n",
      "batch: 1100/1563 - train loss: 7.8563 - test loss: 13.7119 - train acc: 0.6010 - test acc: 0.4256 - 12m 29s\n",
      "batch: 1200/1563 - train loss: 7.7225 - test loss: 14.8721 - train acc: 0.6059 - test acc: 0.3979 - 12m 34s\n",
      "batch: 1300/1563 - train loss: 7.9070 - test loss: 14.1906 - train acc: 0.5940 - test acc: 0.4156 - 12m 39s\n",
      "batch: 1400/1563 - train loss: 8.3139 - test loss: 13.9361 - train acc: 0.5856 - test acc: 0.4190 - 12m 45s\n",
      "batch: 1500/1563 - train loss: 8.2393 - test loss: 14.3029 - train acc: 0.5941 - test acc: 0.4109 - 12m 50s\n",
      "batch: 1563/1563 - train loss: 8.1313 - test loss: 14.5032 - train acc: 0.5962 - test acc: 0.4042 - 12m 55s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.6166 - test loss: 13.9281 - train acc: 0.7075 - test acc: 0.4336 - 13m 0s\n",
      "batch: 200/1563 - train loss: 5.6354 - test loss: 14.0857 - train acc: 0.7057 - test acc: 0.4285 - 13m 6s\n",
      "batch: 300/1563 - train loss: 5.7423 - test loss: 14.2670 - train acc: 0.6932 - test acc: 0.4254 - 13m 11s\n",
      "batch: 400/1563 - train loss: 6.1499 - test loss: 14.3292 - train acc: 0.6695 - test acc: 0.4315 - 13m 17s\n",
      "batch: 500/1563 - train loss: 5.9964 - test loss: 14.8580 - train acc: 0.6816 - test acc: 0.4123 - 13m 22s\n",
      "batch: 600/1563 - train loss: 6.5172 - test loss: 14.2754 - train acc: 0.6550 - test acc: 0.4210 - 13m 28s\n",
      "batch: 700/1563 - train loss: 6.2535 - test loss: 15.5960 - train acc: 0.6716 - test acc: 0.3981 - 13m 33s\n",
      "batch: 800/1563 - train loss: 6.7663 - test loss: 14.4781 - train acc: 0.6550 - test acc: 0.4158 - 13m 38s\n",
      "batch: 900/1563 - train loss: 6.6739 - test loss: 14.5587 - train acc: 0.6491 - test acc: 0.4181 - 13m 44s\n",
      "batch: 1000/1563 - train loss: 6.8242 - test loss: 14.1238 - train acc: 0.6591 - test acc: 0.4265 - 13m 49s\n",
      "batch: 1100/1563 - train loss: 7.0094 - test loss: 14.0659 - train acc: 0.6362 - test acc: 0.4212 - 13m 54s\n",
      "batch: 1200/1563 - train loss: 7.0173 - test loss: 14.2920 - train acc: 0.6388 - test acc: 0.4182 - 14m 0s\n",
      "batch: 1300/1563 - train loss: 7.1557 - test loss: 14.4637 - train acc: 0.6238 - test acc: 0.4172 - 14m 5s\n",
      "batch: 1400/1563 - train loss: 7.2002 - test loss: 15.1907 - train acc: 0.6234 - test acc: 0.4012 - 14m 11s\n",
      "batch: 1500/1563 - train loss: 7.2334 - test loss: 14.2349 - train acc: 0.6350 - test acc: 0.4256 - 14m 16s\n",
      "batch: 1563/1563 - train loss: 7.1225 - test loss: 14.9260 - train acc: 0.6347 - test acc: 0.4041 - 14m 20s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.9081 - test loss: 14.6236 - train acc: 0.7371 - test acc: 0.4329 - 14m 26s\n",
      "batch: 200/1563 - train loss: 4.6642 - test loss: 14.5450 - train acc: 0.7515 - test acc: 0.4297 - 14m 32s\n",
      "batch: 300/1563 - train loss: 5.1201 - test loss: 14.9110 - train acc: 0.7210 - test acc: 0.4259 - 14m 37s\n",
      "batch: 400/1563 - train loss: 4.9495 - test loss: 15.0489 - train acc: 0.7337 - test acc: 0.4346 - 14m 42s\n",
      "batch: 500/1563 - train loss: 5.0609 - test loss: 15.0546 - train acc: 0.7172 - test acc: 0.4241 - 14m 48s\n",
      "batch: 600/1563 - train loss: 5.5196 - test loss: 15.7043 - train acc: 0.7097 - test acc: 0.4106 - 14m 53s\n",
      "batch: 700/1563 - train loss: 5.7367 - test loss: 15.2041 - train acc: 0.6963 - test acc: 0.4145 - 14m 59s\n",
      "batch: 800/1563 - train loss: 5.6579 - test loss: 14.8731 - train acc: 0.6997 - test acc: 0.4270 - 15m 4s\n",
      "batch: 900/1563 - train loss: 5.6354 - test loss: 15.0828 - train acc: 0.6963 - test acc: 0.4197 - 15m 10s\n",
      "batch: 1000/1563 - train loss: 5.8960 - test loss: 15.1017 - train acc: 0.6885 - test acc: 0.4174 - 15m 15s\n",
      "batch: 1100/1563 - train loss: 6.2132 - test loss: 14.8018 - train acc: 0.6688 - test acc: 0.4227 - 15m 20s\n",
      "batch: 1200/1563 - train loss: 6.2968 - test loss: 14.9044 - train acc: 0.6728 - test acc: 0.4245 - 15m 26s\n",
      "batch: 1300/1563 - train loss: 6.1919 - test loss: 14.5859 - train acc: 0.6741 - test acc: 0.4211 - 15m 32s\n",
      "batch: 1400/1563 - train loss: 6.0590 - test loss: 15.0802 - train acc: 0.6879 - test acc: 0.4154 - 15m 37s\n",
      "batch: 1500/1563 - train loss: 6.2664 - test loss: 14.6494 - train acc: 0.6663 - test acc: 0.4240 - 15m 43s\n",
      "batch: 1563/1563 - train loss: 6.4663 - test loss: 15.1576 - train acc: 0.6578 - test acc: 0.4225 - 15m 47s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.1013 - test loss: 14.8440 - train acc: 0.7840 - test acc: 0.4351 - 15m 53s\n",
      "batch: 200/1563 - train loss: 4.1273 - test loss: 15.1395 - train acc: 0.7778 - test acc: 0.4382 - 15m 59s\n",
      "batch: 300/1563 - train loss: 4.3100 - test loss: 16.3121 - train acc: 0.7616 - test acc: 0.4132 - 16m 5s\n",
      "batch: 400/1563 - train loss: 4.1339 - test loss: 15.8698 - train acc: 0.7762 - test acc: 0.4148 - 16m 10s\n",
      "batch: 500/1563 - train loss: 4.4595 - test loss: 16.2672 - train acc: 0.7603 - test acc: 0.4212 - 16m 15s\n",
      "batch: 600/1563 - train loss: 4.8305 - test loss: 15.2827 - train acc: 0.7378 - test acc: 0.4327 - 16m 21s\n",
      "batch: 700/1563 - train loss: 4.6358 - test loss: 15.5226 - train acc: 0.7506 - test acc: 0.4256 - 16m 27s\n",
      "batch: 800/1563 - train loss: 5.0437 - test loss: 15.9908 - train acc: 0.7243 - test acc: 0.4172 - 16m 32s\n",
      "batch: 900/1563 - train loss: 5.1431 - test loss: 15.7030 - train acc: 0.7303 - test acc: 0.4154 - 16m 38s\n",
      "batch: 1000/1563 - train loss: 5.1912 - test loss: 15.3803 - train acc: 0.7147 - test acc: 0.4246 - 16m 44s\n",
      "batch: 1100/1563 - train loss: 5.3536 - test loss: 15.3319 - train acc: 0.7156 - test acc: 0.4272 - 16m 50s\n",
      "batch: 1200/1563 - train loss: 5.7459 - test loss: 15.5028 - train acc: 0.6935 - test acc: 0.4194 - 16m 56s\n",
      "batch: 1300/1563 - train loss: 5.4223 - test loss: 15.4450 - train acc: 0.7025 - test acc: 0.4232 - 17m 1s\n",
      "batch: 1400/1563 - train loss: 5.7027 - test loss: 14.7138 - train acc: 0.6926 - test acc: 0.4383 - 17m 7s\n",
      "batch: 1500/1563 - train loss: 5.4414 - test loss: 15.1201 - train acc: 0.7075 - test acc: 0.4308 - 17m 13s\n",
      "batch: 1563/1563 - train loss: 5.5378 - test loss: 15.4440 - train acc: 0.7026 - test acc: 0.4242 - 17m 18s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.6927 - test loss: 14.9163 - train acc: 0.8034 - test acc: 0.4451 - 17m 23s\n",
      "batch: 200/1563 - train loss: 3.5012 - test loss: 15.9290 - train acc: 0.8078 - test acc: 0.4239 - 17m 29s\n",
      "batch: 300/1563 - train loss: 3.4610 - test loss: 15.7886 - train acc: 0.8071 - test acc: 0.4259 - 17m 34s\n",
      "batch: 400/1563 - train loss: 3.5365 - test loss: 16.5265 - train acc: 0.8046 - test acc: 0.4162 - 17m 40s\n",
      "batch: 500/1563 - train loss: 3.7122 - test loss: 16.3791 - train acc: 0.7972 - test acc: 0.4261 - 17m 46s\n",
      "batch: 600/1563 - train loss: 3.9952 - test loss: 16.3558 - train acc: 0.7830 - test acc: 0.4172 - 17m 51s\n",
      "batch: 700/1563 - train loss: 4.3481 - test loss: 16.0129 - train acc: 0.7678 - test acc: 0.4233 - 17m 56s\n",
      "batch: 800/1563 - train loss: 4.0439 - test loss: 16.3143 - train acc: 0.7772 - test acc: 0.4232 - 18m 2s\n",
      "batch: 900/1563 - train loss: 4.2063 - test loss: 15.9113 - train acc: 0.7656 - test acc: 0.4298 - 18m 7s\n",
      "batch: 1000/1563 - train loss: 4.3360 - test loss: 16.1802 - train acc: 0.7609 - test acc: 0.4257 - 18m 14s\n",
      "batch: 1100/1563 - train loss: 4.5502 - test loss: 16.0506 - train acc: 0.7500 - test acc: 0.4292 - 18m 19s\n",
      "batch: 1200/1563 - train loss: 4.7867 - test loss: 15.8071 - train acc: 0.7456 - test acc: 0.4311 - 18m 25s\n",
      "batch: 1300/1563 - train loss: 4.8015 - test loss: 16.5711 - train acc: 0.7340 - test acc: 0.4118 - 18m 30s\n",
      "batch: 1400/1563 - train loss: 4.5668 - test loss: 16.7277 - train acc: 0.7497 - test acc: 0.4184 - 18m 36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 5.1647 - test loss: 15.6982 - train acc: 0.7106 - test acc: 0.4266 - 18m 41s\n",
      "batch: 1563/1563 - train loss: 5.0049 - test loss: 15.5021 - train acc: 0.7235 - test acc: 0.4315 - 18m 46s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.9160 - test loss: 16.2641 - train acc: 0.8381 - test acc: 0.4389 - 18m 51s\n",
      "batch: 200/1563 - train loss: 2.9346 - test loss: 16.2731 - train acc: 0.8346 - test acc: 0.4329 - 18m 57s\n",
      "batch: 300/1563 - train loss: 2.8645 - test loss: 16.4977 - train acc: 0.8400 - test acc: 0.4325 - 19m 2s\n",
      "batch: 400/1563 - train loss: 3.1190 - test loss: 17.0503 - train acc: 0.8200 - test acc: 0.4263 - 19m 7s\n",
      "batch: 500/1563 - train loss: 3.1511 - test loss: 16.4205 - train acc: 0.8219 - test acc: 0.4332 - 19m 13s\n",
      "batch: 600/1563 - train loss: 3.3001 - test loss: 17.1135 - train acc: 0.8112 - test acc: 0.4212 - 19m 19s\n",
      "batch: 700/1563 - train loss: 3.6129 - test loss: 16.7780 - train acc: 0.8090 - test acc: 0.4315 - 19m 25s\n",
      "batch: 800/1563 - train loss: 3.5087 - test loss: 16.9442 - train acc: 0.7996 - test acc: 0.4222 - 19m 30s\n",
      "batch: 900/1563 - train loss: 3.7751 - test loss: 16.5054 - train acc: 0.7937 - test acc: 0.4327 - 19m 35s\n",
      "batch: 1000/1563 - train loss: 3.7592 - test loss: 17.0004 - train acc: 0.7806 - test acc: 0.4218 - 19m 41s\n",
      "batch: 1100/1563 - train loss: 4.0785 - test loss: 16.5425 - train acc: 0.7693 - test acc: 0.4321 - 19m 46s\n",
      "batch: 1200/1563 - train loss: 4.0160 - test loss: 16.3164 - train acc: 0.7728 - test acc: 0.4313 - 19m 52s\n",
      "batch: 1300/1563 - train loss: 3.9998 - test loss: 16.7631 - train acc: 0.7818 - test acc: 0.4269 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 1301/1563 - train loss: 3.9905 - test loss: 16.7029 - train acc: 0.7831 - test acc: 0.4277 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 13\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0310 - test loss: 24.7000 - train acc: 0.0339 - test acc: 0.0573 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.6334 - test loss: 23.6085 - train acc: 0.0842 - test acc: 0.0891 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.4912 - test loss: 21.9589 - train acc: 0.0990 - test acc: 0.1113 - 0m 13s\n",
      "batch: 400/1563 - train loss: 21.9281 - test loss: 21.7261 - train acc: 0.1103 - test acc: 0.1092 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.6348 - test loss: 21.4480 - train acc: 0.1159 - test acc: 0.1239 - 0m 24s\n",
      "batch: 600/1563 - train loss: 20.9835 - test loss: 21.3058 - train acc: 0.1316 - test acc: 0.1303 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.6737 - test loss: 21.4099 - train acc: 0.1434 - test acc: 0.1399 - 0m 35s\n",
      "batch: 800/1563 - train loss: 20.5255 - test loss: 20.3392 - train acc: 0.1528 - test acc: 0.1579 - 0m 40s\n",
      "batch: 900/1563 - train loss: 20.1829 - test loss: 19.7915 - train acc: 0.1501 - test acc: 0.1655 - 0m 46s\n",
      "batch: 1000/1563 - train loss: 19.8063 - test loss: 19.7879 - train acc: 0.1625 - test acc: 0.1689 - 0m 51s\n",
      "batch: 1100/1563 - train loss: 19.3799 - test loss: 19.0656 - train acc: 0.1801 - test acc: 0.1904 - 0m 57s\n",
      "batch: 1200/1563 - train loss: 19.1393 - test loss: 21.5833 - train acc: 0.1875 - test acc: 0.1355 - 1m 3s\n",
      "batch: 1300/1563 - train loss: 19.1031 - test loss: 20.0281 - train acc: 0.1894 - test acc: 0.1692 - 1m 8s\n",
      "batch: 1400/1563 - train loss: 18.7806 - test loss: 18.9072 - train acc: 0.2009 - test acc: 0.1919 - 1m 14s\n",
      "batch: 1500/1563 - train loss: 18.6122 - test loss: 18.0550 - train acc: 0.1997 - test acc: 0.2235 - 1m 19s\n",
      "batch: 1563/1563 - train loss: 18.5633 - test loss: 18.3567 - train acc: 0.1997 - test acc: 0.2139 - 1m 24s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.3400 - test loss: 18.1839 - train acc: 0.2388 - test acc: 0.2208 - 1m 29s\n",
      "batch: 200/1563 - train loss: 17.7344 - test loss: 17.6403 - train acc: 0.2301 - test acc: 0.2351 - 1m 35s\n",
      "batch: 300/1563 - train loss: 17.6494 - test loss: 17.9507 - train acc: 0.2347 - test acc: 0.2285 - 1m 40s\n",
      "batch: 400/1563 - train loss: 17.4488 - test loss: 17.4091 - train acc: 0.2341 - test acc: 0.2458 - 1m 45s\n",
      "batch: 500/1563 - train loss: 17.3176 - test loss: 17.9029 - train acc: 0.2444 - test acc: 0.2268 - 1m 51s\n",
      "batch: 600/1563 - train loss: 17.2243 - test loss: 18.0458 - train acc: 0.2434 - test acc: 0.2326 - 1m 56s\n",
      "batch: 700/1563 - train loss: 17.0542 - test loss: 17.2277 - train acc: 0.2610 - test acc: 0.2456 - 2m 1s\n",
      "batch: 800/1563 - train loss: 17.0206 - test loss: 17.0253 - train acc: 0.2463 - test acc: 0.2538 - 2m 7s\n",
      "batch: 900/1563 - train loss: 16.5239 - test loss: 17.1457 - train acc: 0.2657 - test acc: 0.2571 - 2m 12s\n",
      "batch: 1000/1563 - train loss: 16.5580 - test loss: 17.4146 - train acc: 0.2650 - test acc: 0.2434 - 2m 18s\n",
      "batch: 1100/1563 - train loss: 16.6186 - test loss: 16.9884 - train acc: 0.2618 - test acc: 0.2638 - 2m 23s\n",
      "batch: 1200/1563 - train loss: 16.4824 - test loss: 17.4202 - train acc: 0.2744 - test acc: 0.2496 - 2m 28s\n",
      "batch: 1300/1563 - train loss: 16.3066 - test loss: 16.5286 - train acc: 0.2852 - test acc: 0.2753 - 2m 33s\n",
      "batch: 1400/1563 - train loss: 15.9637 - test loss: 17.0444 - train acc: 0.2743 - test acc: 0.2681 - 2m 39s\n",
      "batch: 1500/1563 - train loss: 16.3203 - test loss: 17.5118 - train acc: 0.2890 - test acc: 0.2545 - 2m 44s\n",
      "batch: 1563/1563 - train loss: 16.0556 - test loss: 16.2999 - train acc: 0.2893 - test acc: 0.2861 - 2m 49s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.7135 - test loss: 15.9423 - train acc: 0.3181 - test acc: 0.2919 - 2m 54s\n",
      "batch: 200/1563 - train loss: 15.0898 - test loss: 16.2350 - train acc: 0.3259 - test acc: 0.2860 - 2m 59s\n",
      "batch: 300/1563 - train loss: 14.9503 - test loss: 18.6518 - train acc: 0.3312 - test acc: 0.2352 - 3m 4s\n",
      "batch: 400/1563 - train loss: 15.2184 - test loss: 15.6140 - train acc: 0.3078 - test acc: 0.3046 - 3m 10s\n",
      "batch: 500/1563 - train loss: 14.9649 - test loss: 15.3597 - train acc: 0.3271 - test acc: 0.3078 - 3m 15s\n",
      "batch: 600/1563 - train loss: 14.8951 - test loss: 16.1672 - train acc: 0.3234 - test acc: 0.2865 - 3m 21s\n",
      "batch: 700/1563 - train loss: 14.4589 - test loss: 16.1972 - train acc: 0.3397 - test acc: 0.2908 - 3m 26s\n",
      "batch: 800/1563 - train loss: 14.8798 - test loss: 16.1041 - train acc: 0.3312 - test acc: 0.3010 - 3m 31s\n",
      "batch: 900/1563 - train loss: 14.8252 - test loss: 14.9515 - train acc: 0.3315 - test acc: 0.3328 - 3m 36s\n",
      "batch: 1000/1563 - train loss: 14.5999 - test loss: 17.4634 - train acc: 0.3362 - test acc: 0.2776 - 3m 42s\n",
      "batch: 1100/1563 - train loss: 14.7174 - test loss: 14.6967 - train acc: 0.3278 - test acc: 0.3365 - 3m 48s\n",
      "batch: 1200/1563 - train loss: 14.2281 - test loss: 15.4794 - train acc: 0.3528 - test acc: 0.3187 - 3m 53s\n",
      "batch: 1300/1563 - train loss: 14.3130 - test loss: 14.9293 - train acc: 0.3494 - test acc: 0.3390 - 3m 58s\n",
      "batch: 1400/1563 - train loss: 14.2725 - test loss: 15.1340 - train acc: 0.3606 - test acc: 0.3213 - 4m 3s\n",
      "batch: 1500/1563 - train loss: 14.1592 - test loss: 15.1687 - train acc: 0.3516 - test acc: 0.3265 - 4m 9s\n",
      "batch: 1563/1563 - train loss: 14.5881 - test loss: 15.4707 - train acc: 0.3356 - test acc: 0.3186 - 4m 13s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.0176 - test loss: 14.6701 - train acc: 0.3975 - test acc: 0.3434 - 4m 19s\n",
      "batch: 200/1563 - train loss: 12.9362 - test loss: 16.3247 - train acc: 0.3941 - test acc: 0.3085 - 4m 24s\n",
      "batch: 300/1563 - train loss: 13.0586 - test loss: 14.7778 - train acc: 0.3784 - test acc: 0.3422 - 4m 30s\n",
      "batch: 400/1563 - train loss: 13.0658 - test loss: 14.4633 - train acc: 0.3825 - test acc: 0.3535 - 4m 35s\n",
      "batch: 500/1563 - train loss: 13.0397 - test loss: 14.4946 - train acc: 0.3969 - test acc: 0.3443 - 4m 40s\n",
      "batch: 600/1563 - train loss: 12.9933 - test loss: 16.3977 - train acc: 0.3819 - test acc: 0.2933 - 4m 46s\n",
      "batch: 700/1563 - train loss: 12.9758 - test loss: 15.1340 - train acc: 0.3994 - test acc: 0.3320 - 4m 52s\n",
      "batch: 800/1563 - train loss: 13.2660 - test loss: 14.7544 - train acc: 0.3887 - test acc: 0.3390 - 4m 57s\n",
      "batch: 900/1563 - train loss: 13.1695 - test loss: 14.7073 - train acc: 0.3997 - test acc: 0.3368 - 5m 2s\n",
      "batch: 1000/1563 - train loss: 13.3589 - test loss: 14.8477 - train acc: 0.3916 - test acc: 0.3409 - 5m 8s\n",
      "batch: 1100/1563 - train loss: 12.8289 - test loss: 14.4165 - train acc: 0.3966 - test acc: 0.3550 - 5m 13s\n",
      "batch: 1200/1563 - train loss: 12.7266 - test loss: 14.1401 - train acc: 0.3922 - test acc: 0.3657 - 5m 19s\n",
      "batch: 1300/1563 - train loss: 13.1498 - test loss: 13.9839 - train acc: 0.3947 - test acc: 0.3694 - 5m 25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.3058 - test loss: 14.3882 - train acc: 0.3922 - test acc: 0.3558 - 5m 30s\n",
      "batch: 1500/1563 - train loss: 13.0600 - test loss: 14.1405 - train acc: 0.4032 - test acc: 0.3641 - 5m 36s\n",
      "batch: 1563/1563 - train loss: 13.0592 - test loss: 14.0124 - train acc: 0.4047 - test acc: 0.3660 - 5m 40s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.4198 - test loss: 14.1518 - train acc: 0.4513 - test acc: 0.3644 - 5m 46s\n",
      "batch: 200/1563 - train loss: 11.5345 - test loss: 14.9007 - train acc: 0.4522 - test acc: 0.3560 - 5m 52s\n",
      "batch: 300/1563 - train loss: 11.5175 - test loss: 14.0469 - train acc: 0.4509 - test acc: 0.3729 - 5m 57s\n",
      "batch: 400/1563 - train loss: 11.4774 - test loss: 13.9663 - train acc: 0.4532 - test acc: 0.3789 - 6m 3s\n",
      "batch: 500/1563 - train loss: 11.6273 - test loss: 14.3115 - train acc: 0.4534 - test acc: 0.3704 - 6m 8s\n",
      "batch: 600/1563 - train loss: 11.7795 - test loss: 14.0640 - train acc: 0.4428 - test acc: 0.3710 - 6m 13s\n",
      "batch: 700/1563 - train loss: 11.7432 - test loss: 14.3717 - train acc: 0.4538 - test acc: 0.3633 - 6m 20s\n",
      "batch: 800/1563 - train loss: 11.6956 - test loss: 13.8242 - train acc: 0.4375 - test acc: 0.3789 - 6m 25s\n",
      "batch: 900/1563 - train loss: 12.0203 - test loss: 14.5215 - train acc: 0.4313 - test acc: 0.3621 - 6m 31s\n",
      "batch: 1000/1563 - train loss: 12.2278 - test loss: 13.6327 - train acc: 0.4240 - test acc: 0.3844 - 6m 36s\n",
      "batch: 1100/1563 - train loss: 11.7975 - test loss: 13.8376 - train acc: 0.4326 - test acc: 0.3825 - 6m 42s\n",
      "batch: 1200/1563 - train loss: 11.7409 - test loss: 14.0977 - train acc: 0.4482 - test acc: 0.3777 - 6m 47s\n",
      "batch: 1300/1563 - train loss: 11.5721 - test loss: 13.7385 - train acc: 0.4410 - test acc: 0.3807 - 6m 53s\n",
      "batch: 1400/1563 - train loss: 11.9099 - test loss: 13.9739 - train acc: 0.4456 - test acc: 0.3699 - 6m 58s\n",
      "batch: 1500/1563 - train loss: 12.0129 - test loss: 14.0925 - train acc: 0.4447 - test acc: 0.3699 - 7m 4s\n",
      "batch: 1563/1563 - train loss: 11.9897 - test loss: 13.9669 - train acc: 0.4272 - test acc: 0.3801 - 7m 8s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0698 - test loss: 13.6410 - train acc: 0.5019 - test acc: 0.4000 - 7m 14s\n",
      "batch: 200/1563 - train loss: 10.2786 - test loss: 14.1347 - train acc: 0.4944 - test acc: 0.3778 - 7m 19s\n",
      "batch: 300/1563 - train loss: 10.2791 - test loss: 13.1535 - train acc: 0.5088 - test acc: 0.4078 - 7m 25s\n",
      "batch: 400/1563 - train loss: 10.2736 - test loss: 14.2458 - train acc: 0.4975 - test acc: 0.3865 - 7m 30s\n",
      "batch: 500/1563 - train loss: 10.3333 - test loss: 17.4285 - train acc: 0.4962 - test acc: 0.3075 - 7m 36s\n",
      "batch: 600/1563 - train loss: 10.6199 - test loss: 13.8555 - train acc: 0.4863 - test acc: 0.3886 - 7m 42s\n",
      "batch: 700/1563 - train loss: 10.5092 - test loss: 15.1139 - train acc: 0.4990 - test acc: 0.3495 - 7m 47s\n",
      "batch: 800/1563 - train loss: 10.9374 - test loss: 13.3739 - train acc: 0.4775 - test acc: 0.3999 - 7m 53s\n",
      "batch: 900/1563 - train loss: 10.7204 - test loss: 14.4178 - train acc: 0.4781 - test acc: 0.3752 - 7m 58s\n",
      "batch: 1000/1563 - train loss: 11.0951 - test loss: 13.2665 - train acc: 0.4700 - test acc: 0.4025 - 8m 4s\n",
      "batch: 1100/1563 - train loss: 10.7511 - test loss: 13.2356 - train acc: 0.4844 - test acc: 0.4097 - 8m 9s\n",
      "batch: 1200/1563 - train loss: 10.7931 - test loss: 15.0598 - train acc: 0.4822 - test acc: 0.3534 - 8m 15s\n",
      "batch: 1300/1563 - train loss: 10.6617 - test loss: 13.5453 - train acc: 0.4934 - test acc: 0.3929 - 8m 20s\n",
      "batch: 1400/1563 - train loss: 10.9161 - test loss: 13.5058 - train acc: 0.4744 - test acc: 0.3991 - 8m 26s\n",
      "batch: 1500/1563 - train loss: 11.1072 - test loss: 13.2771 - train acc: 0.4672 - test acc: 0.4092 - 8m 32s\n",
      "batch: 1563/1563 - train loss: 11.1128 - test loss: 13.0495 - train acc: 0.4737 - test acc: 0.4124 - 8m 36s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.7399 - test loss: 14.7177 - train acc: 0.5675 - test acc: 0.3802 - 8m 42s\n",
      "batch: 200/1563 - train loss: 9.0220 - test loss: 15.6124 - train acc: 0.5469 - test acc: 0.3580 - 8m 48s\n",
      "batch: 300/1563 - train loss: 8.8798 - test loss: 13.7071 - train acc: 0.5651 - test acc: 0.4083 - 8m 53s\n",
      "batch: 400/1563 - train loss: 9.6313 - test loss: 15.7320 - train acc: 0.5322 - test acc: 0.3461 - 8m 59s\n",
      "batch: 500/1563 - train loss: 9.4769 - test loss: 14.7847 - train acc: 0.5272 - test acc: 0.3793 - 9m 5s\n",
      "batch: 600/1563 - train loss: 9.5773 - test loss: 13.8285 - train acc: 0.5275 - test acc: 0.4017 - 9m 10s\n",
      "batch: 700/1563 - train loss: 9.3919 - test loss: 13.9769 - train acc: 0.5363 - test acc: 0.3964 - 9m 16s\n",
      "batch: 800/1563 - train loss: 9.5662 - test loss: 13.6713 - train acc: 0.5322 - test acc: 0.3991 - 9m 21s\n",
      "batch: 900/1563 - train loss: 9.8773 - test loss: 13.3237 - train acc: 0.5178 - test acc: 0.4155 - 9m 26s\n",
      "batch: 1000/1563 - train loss: 9.9803 - test loss: 13.4783 - train acc: 0.5225 - test acc: 0.4078 - 9m 32s\n",
      "batch: 1100/1563 - train loss: 9.5476 - test loss: 15.1382 - train acc: 0.5163 - test acc: 0.3760 - 9m 37s\n",
      "batch: 1200/1563 - train loss: 9.9594 - test loss: 13.4515 - train acc: 0.5128 - test acc: 0.4110 - 9m 43s\n",
      "batch: 1300/1563 - train loss: 9.7189 - test loss: 14.5064 - train acc: 0.5228 - test acc: 0.3702 - 9m 48s\n",
      "batch: 1400/1563 - train loss: 9.6872 - test loss: 13.8747 - train acc: 0.5197 - test acc: 0.4151 - 9m 53s\n",
      "batch: 1500/1563 - train loss: 9.8891 - test loss: 13.6309 - train acc: 0.5181 - test acc: 0.4075 - 9m 59s\n",
      "batch: 1563/1563 - train loss: 10.1770 - test loss: 13.3216 - train acc: 0.5115 - test acc: 0.4128 - 10m 3s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.3561 - test loss: 13.4808 - train acc: 0.6319 - test acc: 0.4250 - 10m 9s\n",
      "batch: 200/1563 - train loss: 7.6892 - test loss: 13.5847 - train acc: 0.6144 - test acc: 0.4140 - 10m 14s\n",
      "batch: 300/1563 - train loss: 8.0230 - test loss: 13.9523 - train acc: 0.5859 - test acc: 0.4051 - 10m 19s\n",
      "batch: 400/1563 - train loss: 8.1305 - test loss: 14.3818 - train acc: 0.5859 - test acc: 0.4061 - 10m 25s\n",
      "batch: 500/1563 - train loss: 8.2832 - test loss: 13.5252 - train acc: 0.5878 - test acc: 0.4150 - 10m 30s\n",
      "batch: 600/1563 - train loss: 8.7938 - test loss: 14.1484 - train acc: 0.5593 - test acc: 0.4024 - 10m 36s\n",
      "batch: 700/1563 - train loss: 8.3413 - test loss: 13.6524 - train acc: 0.5847 - test acc: 0.4169 - 10m 41s\n",
      "batch: 800/1563 - train loss: 8.5910 - test loss: 13.7401 - train acc: 0.5731 - test acc: 0.4101 - 10m 47s\n",
      "batch: 900/1563 - train loss: 8.9071 - test loss: 14.4404 - train acc: 0.5519 - test acc: 0.3943 - 10m 52s\n",
      "batch: 1000/1563 - train loss: 8.4440 - test loss: 14.8475 - train acc: 0.5768 - test acc: 0.3957 - 10m 58s\n",
      "batch: 1100/1563 - train loss: 9.0237 - test loss: 14.1380 - train acc: 0.5543 - test acc: 0.3993 - 11m 3s\n",
      "batch: 1200/1563 - train loss: 9.0646 - test loss: 15.0971 - train acc: 0.5515 - test acc: 0.3815 - 11m 9s\n",
      "batch: 1300/1563 - train loss: 9.0512 - test loss: 13.6772 - train acc: 0.5506 - test acc: 0.4156 - 11m 14s\n",
      "batch: 1400/1563 - train loss: 9.1121 - test loss: 13.2436 - train acc: 0.5522 - test acc: 0.4246 - 11m 20s\n",
      "batch: 1500/1563 - train loss: 8.8671 - test loss: 13.4223 - train acc: 0.5515 - test acc: 0.4201 - 11m 25s\n",
      "batch: 1563/1563 - train loss: 8.8513 - test loss: 13.5225 - train acc: 0.5659 - test acc: 0.4217 - 11m 30s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.7032 - test loss: 13.9432 - train acc: 0.6566 - test acc: 0.4135 - 11m 35s\n",
      "batch: 200/1563 - train loss: 6.8088 - test loss: 14.3719 - train acc: 0.6494 - test acc: 0.4132 - 11m 41s\n",
      "batch: 300/1563 - train loss: 7.3015 - test loss: 13.6817 - train acc: 0.6256 - test acc: 0.4314 - 11m 46s\n",
      "batch: 400/1563 - train loss: 7.1403 - test loss: 14.0622 - train acc: 0.6287 - test acc: 0.4223 - 11m 52s\n",
      "batch: 500/1563 - train loss: 7.0254 - test loss: 14.2831 - train acc: 0.6416 - test acc: 0.4179 - 11m 57s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.6207 - test loss: 14.4335 - train acc: 0.6047 - test acc: 0.4084 - 12m 2s\n",
      "batch: 700/1563 - train loss: 7.8181 - test loss: 13.8198 - train acc: 0.6053 - test acc: 0.4245 - 12m 8s\n",
      "batch: 800/1563 - train loss: 7.5491 - test loss: 14.0014 - train acc: 0.6090 - test acc: 0.4205 - 12m 13s\n",
      "batch: 900/1563 - train loss: 7.6327 - test loss: 13.7608 - train acc: 0.6112 - test acc: 0.4309 - 12m 19s\n",
      "batch: 1000/1563 - train loss: 7.5766 - test loss: 14.6005 - train acc: 0.6138 - test acc: 0.4083 - 12m 24s\n",
      "batch: 1100/1563 - train loss: 8.1401 - test loss: 13.8967 - train acc: 0.5900 - test acc: 0.4136 - 12m 29s\n",
      "batch: 1200/1563 - train loss: 7.8169 - test loss: 13.9088 - train acc: 0.6013 - test acc: 0.4209 - 12m 35s\n",
      "batch: 1300/1563 - train loss: 7.6757 - test loss: 14.5770 - train acc: 0.6094 - test acc: 0.4130 - 12m 40s\n",
      "batch: 1400/1563 - train loss: 8.0766 - test loss: 13.8095 - train acc: 0.5844 - test acc: 0.4241 - 12m 46s\n",
      "batch: 1500/1563 - train loss: 8.3390 - test loss: 13.9956 - train acc: 0.5800 - test acc: 0.4146 - 12m 51s\n",
      "batch: 1563/1563 - train loss: 8.0620 - test loss: 14.4255 - train acc: 0.5850 - test acc: 0.4143 - 12m 56s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.6799 - test loss: 14.6756 - train acc: 0.7032 - test acc: 0.4159 - 13m 1s\n",
      "batch: 200/1563 - train loss: 5.6464 - test loss: 14.5697 - train acc: 0.6966 - test acc: 0.4225 - 13m 7s\n",
      "batch: 300/1563 - train loss: 5.9358 - test loss: 15.0188 - train acc: 0.6954 - test acc: 0.4054 - 13m 12s\n",
      "batch: 400/1563 - train loss: 5.9605 - test loss: 15.3705 - train acc: 0.6956 - test acc: 0.4065 - 13m 18s\n",
      "batch: 500/1563 - train loss: 6.3320 - test loss: 14.5284 - train acc: 0.6647 - test acc: 0.4239 - 13m 23s\n",
      "batch: 600/1563 - train loss: 6.1504 - test loss: 14.6002 - train acc: 0.6798 - test acc: 0.4211 - 13m 28s\n",
      "batch: 700/1563 - train loss: 6.6472 - test loss: 16.1030 - train acc: 0.6585 - test acc: 0.3816 - 13m 34s\n",
      "batch: 800/1563 - train loss: 6.8825 - test loss: 15.9842 - train acc: 0.6344 - test acc: 0.3866 - 13m 39s\n",
      "batch: 900/1563 - train loss: 6.8499 - test loss: 15.3429 - train acc: 0.6409 - test acc: 0.4032 - 13m 45s\n",
      "batch: 1000/1563 - train loss: 6.8273 - test loss: 14.1679 - train acc: 0.6512 - test acc: 0.4285 - 13m 50s\n",
      "batch: 1100/1563 - train loss: 6.7958 - test loss: 14.9793 - train acc: 0.6469 - test acc: 0.4068 - 13m 55s\n",
      "batch: 1200/1563 - train loss: 7.2390 - test loss: 14.2673 - train acc: 0.6310 - test acc: 0.4273 - 14m 1s\n",
      "batch: 1300/1563 - train loss: 7.4169 - test loss: 14.1060 - train acc: 0.6262 - test acc: 0.4208 - 14m 6s\n",
      "batch: 1400/1563 - train loss: 7.0215 - test loss: 14.1949 - train acc: 0.6394 - test acc: 0.4318 - 14m 12s\n",
      "batch: 1500/1563 - train loss: 7.2636 - test loss: 13.9615 - train acc: 0.6188 - test acc: 0.4279 - 14m 17s\n",
      "batch: 1563/1563 - train loss: 7.1233 - test loss: 14.2775 - train acc: 0.6256 - test acc: 0.4179 - 14m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.9933 - test loss: 14.4434 - train acc: 0.7453 - test acc: 0.4348 - 14m 27s\n",
      "batch: 200/1563 - train loss: 4.5784 - test loss: 14.4563 - train acc: 0.7569 - test acc: 0.4376 - 14m 32s\n",
      "batch: 300/1563 - train loss: 5.2184 - test loss: 15.4083 - train acc: 0.7247 - test acc: 0.4059 - 14m 38s\n",
      "batch: 400/1563 - train loss: 5.1861 - test loss: 14.7193 - train acc: 0.7266 - test acc: 0.4269 - 14m 43s\n",
      "batch: 500/1563 - train loss: 5.5565 - test loss: 15.1382 - train acc: 0.6976 - test acc: 0.4188 - 14m 49s\n",
      "batch: 600/1563 - train loss: 5.5130 - test loss: 15.7011 - train acc: 0.7010 - test acc: 0.4062 - 14m 54s\n",
      "batch: 700/1563 - train loss: 6.0315 - test loss: 14.5362 - train acc: 0.6760 - test acc: 0.4258 - 14m 59s\n",
      "batch: 800/1563 - train loss: 5.8183 - test loss: 15.0348 - train acc: 0.6950 - test acc: 0.4230 - 15m 5s\n",
      "batch: 900/1563 - train loss: 5.9618 - test loss: 15.3303 - train acc: 0.6789 - test acc: 0.4147 - 15m 10s\n",
      "batch: 1000/1563 - train loss: 6.2087 - test loss: 14.2940 - train acc: 0.6760 - test acc: 0.4266 - 15m 16s\n",
      "batch: 1100/1563 - train loss: 6.1528 - test loss: 15.0533 - train acc: 0.6681 - test acc: 0.4187 - 15m 21s\n",
      "batch: 1200/1563 - train loss: 6.4298 - test loss: 15.1080 - train acc: 0.6732 - test acc: 0.4188 - 15m 26s\n",
      "batch: 1300/1563 - train loss: 6.3893 - test loss: 14.6213 - train acc: 0.6637 - test acc: 0.4321 - 15m 32s\n",
      "batch: 1400/1563 - train loss: 6.1908 - test loss: 14.5370 - train acc: 0.6772 - test acc: 0.4298 - 15m 37s\n",
      "batch: 1500/1563 - train loss: 6.4045 - test loss: 15.4521 - train acc: 0.6713 - test acc: 0.4141 - 15m 42s\n",
      "batch: 1563/1563 - train loss: 6.3752 - test loss: 14.3724 - train acc: 0.6673 - test acc: 0.4331 - 15m 47s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.2558 - test loss: 14.9389 - train acc: 0.7662 - test acc: 0.4266 - 15m 53s\n",
      "batch: 200/1563 - train loss: 3.8188 - test loss: 16.4833 - train acc: 0.7884 - test acc: 0.4102 - 15m 58s\n",
      "batch: 300/1563 - train loss: 4.2096 - test loss: 15.9602 - train acc: 0.7700 - test acc: 0.4187 - 16m 3s\n",
      "batch: 400/1563 - train loss: 4.4578 - test loss: 16.4169 - train acc: 0.7587 - test acc: 0.4092 - 16m 9s\n",
      "batch: 500/1563 - train loss: 4.6943 - test loss: 15.3640 - train acc: 0.7522 - test acc: 0.4278 - 16m 14s\n",
      "batch: 600/1563 - train loss: 4.7257 - test loss: 15.6418 - train acc: 0.7406 - test acc: 0.4207 - 16m 19s\n",
      "batch: 700/1563 - train loss: 4.7924 - test loss: 16.3171 - train acc: 0.7409 - test acc: 0.4137 - 16m 25s\n",
      "batch: 800/1563 - train loss: 4.9493 - test loss: 15.6327 - train acc: 0.7353 - test acc: 0.4177 - 16m 31s\n",
      "batch: 900/1563 - train loss: 5.1075 - test loss: 15.4689 - train acc: 0.7229 - test acc: 0.4286 - 16m 36s\n",
      "batch: 1000/1563 - train loss: 5.3037 - test loss: 15.5110 - train acc: 0.7147 - test acc: 0.4158 - 16m 41s\n",
      "batch: 1100/1563 - train loss: 5.6093 - test loss: 15.5650 - train acc: 0.7097 - test acc: 0.4235 - 16m 47s\n",
      "batch: 1200/1563 - train loss: 5.6633 - test loss: 15.1146 - train acc: 0.7003 - test acc: 0.4375 - 16m 52s\n",
      "batch: 1300/1563 - train loss: 5.2487 - test loss: 15.3137 - train acc: 0.7116 - test acc: 0.4346 - 16m 58s\n",
      "batch: 1400/1563 - train loss: 5.6290 - test loss: 15.0971 - train acc: 0.7056 - test acc: 0.4330 - 17m 3s\n",
      "batch: 1500/1563 - train loss: 5.6962 - test loss: 15.1035 - train acc: 0.6966 - test acc: 0.4220 - 17m 9s\n",
      "batch: 1563/1563 - train loss: 5.7774 - test loss: 14.8173 - train acc: 0.6963 - test acc: 0.4281 - 17m 13s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.3320 - test loss: 15.3672 - train acc: 0.8225 - test acc: 0.4285 - 17m 19s\n",
      "batch: 200/1563 - train loss: 3.5483 - test loss: 16.7610 - train acc: 0.8024 - test acc: 0.4078 - 17m 24s\n",
      "batch: 300/1563 - train loss: 3.5464 - test loss: 15.5395 - train acc: 0.8087 - test acc: 0.4407 - 17m 30s\n",
      "batch: 400/1563 - train loss: 3.3501 - test loss: 16.2512 - train acc: 0.8130 - test acc: 0.4307 - 17m 35s\n",
      "batch: 500/1563 - train loss: 3.8929 - test loss: 16.2113 - train acc: 0.7827 - test acc: 0.4286 - 17m 41s\n",
      "batch: 600/1563 - train loss: 3.8403 - test loss: 16.4061 - train acc: 0.7862 - test acc: 0.4211 - 17m 46s\n",
      "batch: 700/1563 - train loss: 4.0974 - test loss: 16.0194 - train acc: 0.7765 - test acc: 0.4311 - 17m 52s\n",
      "batch: 800/1563 - train loss: 4.2242 - test loss: 16.4370 - train acc: 0.7716 - test acc: 0.4201 - 17m 57s\n",
      "batch: 900/1563 - train loss: 4.3755 - test loss: 15.7965 - train acc: 0.7641 - test acc: 0.4294 - 18m 3s\n",
      "batch: 1000/1563 - train loss: 4.4130 - test loss: 16.0289 - train acc: 0.7603 - test acc: 0.4235 - 18m 9s\n",
      "batch: 1100/1563 - train loss: 4.6316 - test loss: 17.3012 - train acc: 0.7444 - test acc: 0.3993 - 18m 14s\n",
      "batch: 1200/1563 - train loss: 4.8301 - test loss: 15.5753 - train acc: 0.7391 - test acc: 0.4311 - 18m 19s\n",
      "batch: 1300/1563 - train loss: 4.7479 - test loss: 15.9923 - train acc: 0.7503 - test acc: 0.4214 - 18m 25s\n",
      "batch: 1400/1563 - train loss: 4.6856 - test loss: 15.9323 - train acc: 0.7493 - test acc: 0.4255 - 18m 30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 5.0499 - test loss: 15.7947 - train acc: 0.7353 - test acc: 0.4161 - 18m 36s\n",
      "batch: 1563/1563 - train loss: 4.8871 - test loss: 15.5792 - train acc: 0.7353 - test acc: 0.4266 - 18m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8691 - test loss: 16.3005 - train acc: 0.8412 - test acc: 0.4283 - 18m 46s\n",
      "batch: 200/1563 - train loss: 2.7408 - test loss: 16.6929 - train acc: 0.8512 - test acc: 0.4267 - 18m 52s\n",
      "batch: 300/1563 - train loss: 2.7987 - test loss: 16.3470 - train acc: 0.8449 - test acc: 0.4270 - 18m 57s\n",
      "batch: 400/1563 - train loss: 3.1986 - test loss: 16.4437 - train acc: 0.8215 - test acc: 0.4324 - 19m 3s\n",
      "batch: 500/1563 - train loss: 3.3142 - test loss: 17.1961 - train acc: 0.8146 - test acc: 0.4223 - 19m 9s\n",
      "batch: 600/1563 - train loss: 3.3345 - test loss: 16.5961 - train acc: 0.8146 - test acc: 0.4279 - 19m 14s\n",
      "batch: 700/1563 - train loss: 3.3775 - test loss: 17.1574 - train acc: 0.8090 - test acc: 0.4220 - 19m 20s\n",
      "batch: 800/1563 - train loss: 3.4353 - test loss: 16.8912 - train acc: 0.8043 - test acc: 0.4276 - 19m 25s\n",
      "batch: 900/1563 - train loss: 3.9388 - test loss: 18.6467 - train acc: 0.7874 - test acc: 0.3903 - 19m 30s\n",
      "batch: 1000/1563 - train loss: 3.9681 - test loss: 16.5871 - train acc: 0.7878 - test acc: 0.4262 - 19m 36s\n",
      "batch: 1100/1563 - train loss: 3.7868 - test loss: 16.9098 - train acc: 0.7887 - test acc: 0.4201 - 19m 41s\n",
      "batch: 1200/1563 - train loss: 3.7711 - test loss: 16.4929 - train acc: 0.7828 - test acc: 0.4260 - 19m 47s\n",
      "batch: 1300/1563 - train loss: 4.2162 - test loss: 16.8191 - train acc: 0.7756 - test acc: 0.4147 - 19m 52s\n",
      "batch: 1400/1563 - train loss: 4.1033 - test loss: 16.3339 - train acc: 0.7797 - test acc: 0.4278 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 1401/1563 - train loss: 4.1185 - test loss: 16.2482 - train acc: 0.7784 - test acc: 0.4268 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 14\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0517 - test loss: 24.8379 - train acc: 0.0393 - test acc: 0.0602 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.9106 - test loss: 23.4161 - train acc: 0.0648 - test acc: 0.0798 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.5841 - test loss: 22.2433 - train acc: 0.0937 - test acc: 0.1119 - 0m 13s\n",
      "batch: 400/1563 - train loss: 21.9371 - test loss: 22.5851 - train acc: 0.1131 - test acc: 0.1126 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.3785 - test loss: 20.9863 - train acc: 0.1306 - test acc: 0.1330 - 0m 24s\n",
      "batch: 600/1563 - train loss: 20.7908 - test loss: 20.5836 - train acc: 0.1396 - test acc: 0.1419 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.5213 - test loss: 21.4264 - train acc: 0.1463 - test acc: 0.1380 - 0m 34s\n",
      "batch: 800/1563 - train loss: 20.2084 - test loss: 20.8549 - train acc: 0.1563 - test acc: 0.1476 - 0m 40s\n",
      "batch: 900/1563 - train loss: 19.9736 - test loss: 19.7937 - train acc: 0.1656 - test acc: 0.1704 - 0m 45s\n",
      "batch: 1000/1563 - train loss: 19.5861 - test loss: 19.5830 - train acc: 0.1688 - test acc: 0.1764 - 0m 51s\n",
      "batch: 1100/1563 - train loss: 19.6259 - test loss: 19.3270 - train acc: 0.1753 - test acc: 0.1809 - 0m 56s\n",
      "batch: 1200/1563 - train loss: 18.9002 - test loss: 19.2416 - train acc: 0.1991 - test acc: 0.1799 - 1m 2s\n",
      "batch: 1300/1563 - train loss: 19.0443 - test loss: 18.9313 - train acc: 0.1844 - test acc: 0.2056 - 1m 7s\n",
      "batch: 1400/1563 - train loss: 18.6694 - test loss: 18.4069 - train acc: 0.2047 - test acc: 0.2107 - 1m 12s\n",
      "batch: 1500/1563 - train loss: 18.5333 - test loss: 18.4530 - train acc: 0.2116 - test acc: 0.2046 - 1m 18s\n",
      "batch: 1563/1563 - train loss: 18.5139 - test loss: 18.9419 - train acc: 0.2119 - test acc: 0.1980 - 1m 23s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.5961 - test loss: 17.8802 - train acc: 0.2300 - test acc: 0.2263 - 1m 28s\n",
      "batch: 200/1563 - train loss: 17.4313 - test loss: 17.3940 - train acc: 0.2357 - test acc: 0.2397 - 1m 33s\n",
      "batch: 300/1563 - train loss: 17.4476 - test loss: 18.3961 - train acc: 0.2497 - test acc: 0.2198 - 1m 38s\n",
      "batch: 400/1563 - train loss: 17.2844 - test loss: 17.5093 - train acc: 0.2460 - test acc: 0.2410 - 1m 43s\n",
      "batch: 500/1563 - train loss: 17.0539 - test loss: 17.4313 - train acc: 0.2562 - test acc: 0.2419 - 1m 49s\n",
      "batch: 600/1563 - train loss: 16.6251 - test loss: 17.9295 - train acc: 0.2616 - test acc: 0.2439 - 1m 55s\n",
      "batch: 700/1563 - train loss: 17.1687 - test loss: 17.5702 - train acc: 0.2516 - test acc: 0.2367 - 2m 0s\n",
      "batch: 800/1563 - train loss: 16.7876 - test loss: 17.4913 - train acc: 0.2672 - test acc: 0.2411 - 2m 5s\n",
      "batch: 900/1563 - train loss: 16.4956 - test loss: 17.0896 - train acc: 0.2694 - test acc: 0.2532 - 2m 11s\n",
      "batch: 1000/1563 - train loss: 16.7760 - test loss: 16.8053 - train acc: 0.2575 - test acc: 0.2718 - 2m 16s\n",
      "batch: 1100/1563 - train loss: 16.1203 - test loss: 18.0185 - train acc: 0.2850 - test acc: 0.2215 - 2m 21s\n",
      "batch: 1200/1563 - train loss: 15.9866 - test loss: 16.4884 - train acc: 0.2906 - test acc: 0.2674 - 2m 27s\n",
      "batch: 1300/1563 - train loss: 16.4497 - test loss: 16.5649 - train acc: 0.2685 - test acc: 0.2744 - 2m 32s\n",
      "batch: 1400/1563 - train loss: 15.9898 - test loss: 15.5463 - train acc: 0.2837 - test acc: 0.3060 - 2m 37s\n",
      "batch: 1500/1563 - train loss: 16.2161 - test loss: 16.2819 - train acc: 0.2684 - test acc: 0.2793 - 2m 43s\n",
      "batch: 1563/1563 - train loss: 15.9910 - test loss: 17.1323 - train acc: 0.2822 - test acc: 0.2661 - 2m 47s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.8101 - test loss: 16.0874 - train acc: 0.3346 - test acc: 0.2977 - 2m 53s\n",
      "batch: 200/1563 - train loss: 14.7743 - test loss: 16.5573 - train acc: 0.3284 - test acc: 0.2746 - 2m 58s\n",
      "batch: 300/1563 - train loss: 14.6384 - test loss: 15.6450 - train acc: 0.3284 - test acc: 0.3023 - 3m 4s\n",
      "batch: 400/1563 - train loss: 14.7937 - test loss: 16.6665 - train acc: 0.3350 - test acc: 0.2811 - 3m 9s\n",
      "batch: 500/1563 - train loss: 14.6502 - test loss: 15.8523 - train acc: 0.3431 - test acc: 0.3016 - 3m 14s\n",
      "batch: 600/1563 - train loss: 14.6430 - test loss: 15.6149 - train acc: 0.3343 - test acc: 0.3153 - 3m 19s\n",
      "batch: 700/1563 - train loss: 14.3000 - test loss: 15.5341 - train acc: 0.3519 - test acc: 0.3149 - 3m 25s\n",
      "batch: 800/1563 - train loss: 15.0651 - test loss: 15.5451 - train acc: 0.3234 - test acc: 0.3123 - 3m 30s\n",
      "batch: 900/1563 - train loss: 14.5011 - test loss: 15.6493 - train acc: 0.3381 - test acc: 0.3153 - 3m 36s\n",
      "batch: 1000/1563 - train loss: 14.7949 - test loss: 14.8898 - train acc: 0.3325 - test acc: 0.3354 - 3m 41s\n",
      "batch: 1100/1563 - train loss: 14.4413 - test loss: 14.8890 - train acc: 0.3456 - test acc: 0.3305 - 3m 46s\n",
      "batch: 1200/1563 - train loss: 14.7231 - test loss: 17.6633 - train acc: 0.3394 - test acc: 0.2647 - 3m 52s\n",
      "batch: 1300/1563 - train loss: 14.4786 - test loss: 14.8544 - train acc: 0.3409 - test acc: 0.3339 - 3m 57s\n",
      "batch: 1400/1563 - train loss: 14.5511 - test loss: 15.1019 - train acc: 0.3331 - test acc: 0.3240 - 4m 3s\n",
      "batch: 1500/1563 - train loss: 14.5539 - test loss: 15.6661 - train acc: 0.3418 - test acc: 0.3031 - 4m 8s\n",
      "batch: 1563/1563 - train loss: 14.1831 - test loss: 15.5412 - train acc: 0.3490 - test acc: 0.3140 - 4m 12s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.6717 - test loss: 14.5566 - train acc: 0.4041 - test acc: 0.3486 - 4m 18s\n",
      "batch: 200/1563 - train loss: 12.9077 - test loss: 16.1413 - train acc: 0.3860 - test acc: 0.3184 - 4m 23s\n",
      "batch: 300/1563 - train loss: 13.2892 - test loss: 14.5929 - train acc: 0.3906 - test acc: 0.3568 - 4m 28s\n",
      "batch: 400/1563 - train loss: 13.3240 - test loss: 15.4649 - train acc: 0.3762 - test acc: 0.3259 - 4m 34s\n",
      "batch: 500/1563 - train loss: 12.9968 - test loss: 14.8005 - train acc: 0.4034 - test acc: 0.3454 - 4m 39s\n",
      "batch: 600/1563 - train loss: 13.2736 - test loss: 14.1286 - train acc: 0.3947 - test acc: 0.3648 - 4m 45s\n",
      "batch: 700/1563 - train loss: 13.1020 - test loss: 13.9724 - train acc: 0.3916 - test acc: 0.3693 - 4m 50s\n",
      "batch: 800/1563 - train loss: 13.2260 - test loss: 14.7035 - train acc: 0.3859 - test acc: 0.3445 - 4m 55s\n",
      "batch: 900/1563 - train loss: 13.1160 - test loss: 13.9788 - train acc: 0.3976 - test acc: 0.3754 - 5m 1s\n",
      "batch: 1000/1563 - train loss: 13.1387 - test loss: 14.3703 - train acc: 0.3987 - test acc: 0.3537 - 5m 6s\n",
      "batch: 1100/1563 - train loss: 13.4260 - test loss: 14.1683 - train acc: 0.3844 - test acc: 0.3648 - 5m 12s\n",
      "batch: 1200/1563 - train loss: 13.0209 - test loss: 13.9344 - train acc: 0.3879 - test acc: 0.3765 - 5m 17s\n",
      "batch: 1300/1563 - train loss: 13.1502 - test loss: 14.1971 - train acc: 0.3875 - test acc: 0.3524 - 5m 22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.2020 - test loss: 14.4403 - train acc: 0.3912 - test acc: 0.3542 - 5m 27s\n",
      "batch: 1500/1563 - train loss: 13.1420 - test loss: 14.4659 - train acc: 0.3916 - test acc: 0.3521 - 5m 33s\n",
      "batch: 1563/1563 - train loss: 13.2052 - test loss: 15.8891 - train acc: 0.3913 - test acc: 0.3112 - 5m 38s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.6415 - test loss: 14.4720 - train acc: 0.4485 - test acc: 0.3602 - 5m 43s\n",
      "batch: 200/1563 - train loss: 11.6210 - test loss: 14.3038 - train acc: 0.4534 - test acc: 0.3537 - 5m 48s\n",
      "batch: 300/1563 - train loss: 11.8983 - test loss: 14.8190 - train acc: 0.4341 - test acc: 0.3444 - 5m 53s\n",
      "batch: 400/1563 - train loss: 11.8858 - test loss: 14.8658 - train acc: 0.4282 - test acc: 0.3548 - 5m 59s\n",
      "batch: 500/1563 - train loss: 11.7510 - test loss: 13.8573 - train acc: 0.4341 - test acc: 0.3818 - 6m 4s\n",
      "batch: 600/1563 - train loss: 11.7059 - test loss: 13.9622 - train acc: 0.4419 - test acc: 0.3746 - 6m 10s\n",
      "batch: 700/1563 - train loss: 12.2617 - test loss: 14.1321 - train acc: 0.4244 - test acc: 0.3654 - 6m 15s\n",
      "batch: 800/1563 - train loss: 12.2104 - test loss: 14.5267 - train acc: 0.4282 - test acc: 0.3567 - 6m 21s\n",
      "batch: 900/1563 - train loss: 12.1560 - test loss: 13.4635 - train acc: 0.4207 - test acc: 0.3980 - 6m 26s\n",
      "batch: 1000/1563 - train loss: 11.8064 - test loss: 13.5324 - train acc: 0.4344 - test acc: 0.3926 - 6m 32s\n",
      "batch: 1100/1563 - train loss: 11.9851 - test loss: 13.9010 - train acc: 0.4362 - test acc: 0.3784 - 6m 38s\n",
      "batch: 1200/1563 - train loss: 12.0825 - test loss: 13.7244 - train acc: 0.4416 - test acc: 0.3742 - 6m 44s\n",
      "batch: 1300/1563 - train loss: 12.3742 - test loss: 14.2672 - train acc: 0.4287 - test acc: 0.3671 - 6m 49s\n",
      "batch: 1400/1563 - train loss: 11.9567 - test loss: 13.9795 - train acc: 0.4381 - test acc: 0.3798 - 6m 55s\n",
      "batch: 1500/1563 - train loss: 12.1540 - test loss: 14.9230 - train acc: 0.4347 - test acc: 0.3550 - 7m 0s\n",
      "batch: 1563/1563 - train loss: 11.8904 - test loss: 14.6763 - train acc: 0.4375 - test acc: 0.3539 - 7m 5s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.6399 - test loss: 14.3343 - train acc: 0.4834 - test acc: 0.3745 - 7m 10s\n",
      "batch: 200/1563 - train loss: 10.5246 - test loss: 13.6352 - train acc: 0.4928 - test acc: 0.3968 - 7m 16s\n",
      "batch: 300/1563 - train loss: 10.5417 - test loss: 13.8988 - train acc: 0.4950 - test acc: 0.3806 - 7m 21s\n",
      "batch: 400/1563 - train loss: 10.6278 - test loss: 13.9519 - train acc: 0.4763 - test acc: 0.3921 - 7m 27s\n",
      "batch: 500/1563 - train loss: 10.8090 - test loss: 13.8543 - train acc: 0.4829 - test acc: 0.3917 - 7m 32s\n",
      "batch: 600/1563 - train loss: 11.1977 - test loss: 13.4826 - train acc: 0.4638 - test acc: 0.3984 - 7m 37s\n",
      "batch: 700/1563 - train loss: 10.7451 - test loss: 14.1677 - train acc: 0.4893 - test acc: 0.3830 - 7m 43s\n",
      "batch: 800/1563 - train loss: 10.7155 - test loss: 13.6156 - train acc: 0.4944 - test acc: 0.3932 - 7m 49s\n",
      "batch: 900/1563 - train loss: 11.0247 - test loss: 13.5771 - train acc: 0.4719 - test acc: 0.3975 - 7m 54s\n",
      "batch: 1000/1563 - train loss: 11.0623 - test loss: 13.3082 - train acc: 0.4781 - test acc: 0.3932 - 8m 0s\n",
      "batch: 1100/1563 - train loss: 10.9166 - test loss: 13.6836 - train acc: 0.4669 - test acc: 0.3862 - 8m 5s\n",
      "batch: 1200/1563 - train loss: 11.3522 - test loss: 13.7422 - train acc: 0.4609 - test acc: 0.3917 - 8m 11s\n",
      "batch: 1300/1563 - train loss: 10.9581 - test loss: 13.2377 - train acc: 0.4803 - test acc: 0.4060 - 8m 17s\n",
      "batch: 1400/1563 - train loss: 11.0530 - test loss: 13.7784 - train acc: 0.4622 - test acc: 0.3931 - 8m 22s\n",
      "batch: 1500/1563 - train loss: 11.1346 - test loss: 13.3814 - train acc: 0.4688 - test acc: 0.4028 - 8m 28s\n",
      "batch: 1563/1563 - train loss: 11.2557 - test loss: 14.0390 - train acc: 0.4631 - test acc: 0.3877 - 8m 32s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.9426 - test loss: 13.6540 - train acc: 0.5484 - test acc: 0.4018 - 8m 37s\n",
      "batch: 200/1563 - train loss: 9.4041 - test loss: 14.0058 - train acc: 0.5212 - test acc: 0.3948 - 8m 43s\n",
      "batch: 300/1563 - train loss: 9.6016 - test loss: 13.3680 - train acc: 0.5222 - test acc: 0.4083 - 8m 49s\n",
      "batch: 400/1563 - train loss: 9.7287 - test loss: 13.1272 - train acc: 0.5263 - test acc: 0.4116 - 8m 54s\n",
      "batch: 500/1563 - train loss: 9.7420 - test loss: 13.7056 - train acc: 0.5213 - test acc: 0.3947 - 9m 0s\n",
      "batch: 600/1563 - train loss: 9.8519 - test loss: 14.5145 - train acc: 0.5282 - test acc: 0.3771 - 9m 5s\n",
      "batch: 700/1563 - train loss: 10.0162 - test loss: 14.3917 - train acc: 0.5062 - test acc: 0.3797 - 9m 10s\n",
      "batch: 800/1563 - train loss: 10.1441 - test loss: 13.7844 - train acc: 0.5047 - test acc: 0.3982 - 9m 16s\n",
      "batch: 900/1563 - train loss: 9.8160 - test loss: 14.3098 - train acc: 0.5240 - test acc: 0.3821 - 9m 21s\n",
      "batch: 1000/1563 - train loss: 10.3152 - test loss: 13.1958 - train acc: 0.5059 - test acc: 0.4137 - 9m 27s\n",
      "batch: 1100/1563 - train loss: 9.7993 - test loss: 13.5781 - train acc: 0.5184 - test acc: 0.4084 - 9m 32s\n",
      "batch: 1200/1563 - train loss: 10.4291 - test loss: 13.2619 - train acc: 0.4984 - test acc: 0.4142 - 9m 37s\n",
      "batch: 1300/1563 - train loss: 9.9020 - test loss: 13.5525 - train acc: 0.5116 - test acc: 0.4079 - 9m 43s\n",
      "batch: 1400/1563 - train loss: 10.1465 - test loss: 13.2330 - train acc: 0.5141 - test acc: 0.4127 - 9m 48s\n",
      "batch: 1500/1563 - train loss: 10.0694 - test loss: 13.3712 - train acc: 0.5234 - test acc: 0.4100 - 9m 54s\n",
      "batch: 1563/1563 - train loss: 10.2492 - test loss: 13.9499 - train acc: 0.5115 - test acc: 0.4010 - 9m 58s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 8.3746 - test loss: 13.9602 - train acc: 0.5725 - test acc: 0.3981 - 10m 4s\n",
      "batch: 200/1563 - train loss: 8.5764 - test loss: 15.5453 - train acc: 0.5716 - test acc: 0.3747 - 10m 9s\n",
      "batch: 300/1563 - train loss: 8.2609 - test loss: 13.5649 - train acc: 0.5837 - test acc: 0.4228 - 10m 14s\n",
      "batch: 400/1563 - train loss: 8.9259 - test loss: 13.9462 - train acc: 0.5628 - test acc: 0.4002 - 10m 19s\n",
      "batch: 500/1563 - train loss: 8.6278 - test loss: 13.7657 - train acc: 0.5728 - test acc: 0.4083 - 10m 25s\n",
      "batch: 600/1563 - train loss: 8.7851 - test loss: 13.7336 - train acc: 0.5562 - test acc: 0.4042 - 10m 30s\n",
      "batch: 700/1563 - train loss: 8.8470 - test loss: 13.6039 - train acc: 0.5578 - test acc: 0.4132 - 10m 36s\n",
      "batch: 800/1563 - train loss: 8.8858 - test loss: 14.4007 - train acc: 0.5581 - test acc: 0.3975 - 10m 41s\n",
      "batch: 900/1563 - train loss: 8.9937 - test loss: 13.7727 - train acc: 0.5515 - test acc: 0.4106 - 10m 47s\n",
      "batch: 1000/1563 - train loss: 9.2830 - test loss: 14.2669 - train acc: 0.5428 - test acc: 0.3937 - 10m 52s\n",
      "batch: 1100/1563 - train loss: 9.1951 - test loss: 13.6319 - train acc: 0.5476 - test acc: 0.4110 - 10m 58s\n",
      "batch: 1200/1563 - train loss: 9.3933 - test loss: 13.4829 - train acc: 0.5428 - test acc: 0.4150 - 11m 3s\n",
      "batch: 1300/1563 - train loss: 9.2082 - test loss: 14.4416 - train acc: 0.5434 - test acc: 0.3908 - 11m 8s\n",
      "batch: 1400/1563 - train loss: 9.1571 - test loss: 13.1988 - train acc: 0.5428 - test acc: 0.4224 - 11m 14s\n",
      "batch: 1500/1563 - train loss: 9.5494 - test loss: 13.2652 - train acc: 0.5272 - test acc: 0.4201 - 11m 19s\n",
      "batch: 1563/1563 - train loss: 9.1274 - test loss: 13.4718 - train acc: 0.5434 - test acc: 0.4168 - 11m 23s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 7.2864 - test loss: 13.3910 - train acc: 0.6306 - test acc: 0.4229 - 11m 29s\n",
      "batch: 200/1563 - train loss: 7.2167 - test loss: 13.8636 - train acc: 0.6263 - test acc: 0.4207 - 11m 34s\n",
      "batch: 300/1563 - train loss: 7.2842 - test loss: 15.2737 - train acc: 0.6366 - test acc: 0.3724 - 11m 40s\n",
      "batch: 400/1563 - train loss: 7.6100 - test loss: 14.9644 - train acc: 0.6119 - test acc: 0.3997 - 11m 45s\n",
      "batch: 500/1563 - train loss: 7.9078 - test loss: 14.3653 - train acc: 0.5897 - test acc: 0.3970 - 11m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.7251 - test loss: 13.9837 - train acc: 0.6053 - test acc: 0.4236 - 11m 56s\n",
      "batch: 700/1563 - train loss: 8.1509 - test loss: 13.7007 - train acc: 0.5941 - test acc: 0.4242 - 12m 2s\n",
      "batch: 800/1563 - train loss: 8.1601 - test loss: 14.2061 - train acc: 0.5834 - test acc: 0.4045 - 12m 7s\n",
      "batch: 900/1563 - train loss: 7.8798 - test loss: 14.4421 - train acc: 0.6022 - test acc: 0.4118 - 12m 12s\n",
      "batch: 1000/1563 - train loss: 8.3664 - test loss: 15.2448 - train acc: 0.5893 - test acc: 0.3893 - 12m 18s\n",
      "batch: 1100/1563 - train loss: 8.4472 - test loss: 13.5027 - train acc: 0.5759 - test acc: 0.4247 - 12m 23s\n",
      "batch: 1200/1563 - train loss: 8.6329 - test loss: 14.0132 - train acc: 0.5712 - test acc: 0.4100 - 12m 29s\n",
      "batch: 1300/1563 - train loss: 8.5692 - test loss: 14.6237 - train acc: 0.5840 - test acc: 0.3814 - 12m 34s\n",
      "batch: 1400/1563 - train loss: 8.1583 - test loss: 13.8167 - train acc: 0.5931 - test acc: 0.4168 - 12m 40s\n",
      "batch: 1500/1563 - train loss: 8.5695 - test loss: 13.4031 - train acc: 0.5727 - test acc: 0.4312 - 12m 45s\n",
      "batch: 1563/1563 - train loss: 8.4077 - test loss: 13.7438 - train acc: 0.5787 - test acc: 0.4224 - 12m 50s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 6.2042 - test loss: 14.4285 - train acc: 0.6569 - test acc: 0.4131 - 12m 55s\n",
      "batch: 200/1563 - train loss: 6.5626 - test loss: 14.1915 - train acc: 0.6634 - test acc: 0.4113 - 13m 1s\n",
      "batch: 300/1563 - train loss: 6.5031 - test loss: 14.7976 - train acc: 0.6566 - test acc: 0.4032 - 13m 6s\n",
      "batch: 400/1563 - train loss: 6.7590 - test loss: 13.6600 - train acc: 0.6497 - test acc: 0.4343 - 13m 11s\n",
      "batch: 500/1563 - train loss: 6.7978 - test loss: 14.1940 - train acc: 0.6428 - test acc: 0.4183 - 13m 17s\n",
      "batch: 600/1563 - train loss: 7.1774 - test loss: 14.4514 - train acc: 0.6244 - test acc: 0.4180 - 13m 22s\n",
      "batch: 700/1563 - train loss: 7.4668 - test loss: 14.3272 - train acc: 0.6235 - test acc: 0.4194 - 13m 27s\n",
      "batch: 800/1563 - train loss: 7.1814 - test loss: 14.3000 - train acc: 0.6369 - test acc: 0.4120 - 13m 33s\n",
      "batch: 900/1563 - train loss: 7.2582 - test loss: 14.0439 - train acc: 0.6369 - test acc: 0.4201 - 13m 39s\n",
      "batch: 1000/1563 - train loss: 7.3455 - test loss: 13.9315 - train acc: 0.6266 - test acc: 0.4226 - 13m 44s\n",
      "batch: 1100/1563 - train loss: 7.1641 - test loss: 15.7395 - train acc: 0.6353 - test acc: 0.3912 - 13m 50s\n",
      "batch: 1200/1563 - train loss: 7.3348 - test loss: 14.4784 - train acc: 0.6184 - test acc: 0.4089 - 13m 55s\n",
      "batch: 1300/1563 - train loss: 7.6088 - test loss: 14.8478 - train acc: 0.6190 - test acc: 0.4182 - 14m 0s\n",
      "batch: 1400/1563 - train loss: 7.7919 - test loss: 14.3624 - train acc: 0.5997 - test acc: 0.4103 - 14m 6s\n",
      "batch: 1500/1563 - train loss: 7.7720 - test loss: 13.7931 - train acc: 0.6060 - test acc: 0.4201 - 14m 11s\n",
      "batch: 1563/1563 - train loss: 8.0254 - test loss: 14.4459 - train acc: 0.6035 - test acc: 0.4114 - 14m 16s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 5.3042 - test loss: 13.8969 - train acc: 0.7197 - test acc: 0.4362 - 14m 21s\n",
      "batch: 200/1563 - train loss: 5.3917 - test loss: 15.1850 - train acc: 0.7150 - test acc: 0.4113 - 14m 27s\n",
      "batch: 300/1563 - train loss: 5.5541 - test loss: 14.9750 - train acc: 0.7075 - test acc: 0.4144 - 14m 32s\n",
      "batch: 400/1563 - train loss: 5.6909 - test loss: 14.6443 - train acc: 0.6984 - test acc: 0.4238 - 14m 38s\n",
      "batch: 500/1563 - train loss: 6.1157 - test loss: 14.5053 - train acc: 0.6841 - test acc: 0.4293 - 14m 43s\n",
      "batch: 600/1563 - train loss: 6.0576 - test loss: 14.9078 - train acc: 0.6741 - test acc: 0.4147 - 14m 49s\n",
      "batch: 700/1563 - train loss: 6.5403 - test loss: 14.9401 - train acc: 0.6528 - test acc: 0.4130 - 14m 55s\n",
      "batch: 800/1563 - train loss: 6.2021 - test loss: 15.0207 - train acc: 0.6750 - test acc: 0.4185 - 15m 0s\n",
      "batch: 900/1563 - train loss: 6.5260 - test loss: 14.6512 - train acc: 0.6635 - test acc: 0.4152 - 15m 5s\n",
      "batch: 1000/1563 - train loss: 6.4708 - test loss: 14.2503 - train acc: 0.6707 - test acc: 0.4288 - 15m 11s\n",
      "batch: 1100/1563 - train loss: 6.5007 - test loss: 15.4389 - train acc: 0.6547 - test acc: 0.4042 - 15m 17s\n",
      "batch: 1200/1563 - train loss: 6.7535 - test loss: 14.3199 - train acc: 0.6500 - test acc: 0.4205 - 15m 22s\n",
      "batch: 1300/1563 - train loss: 6.6716 - test loss: 14.3734 - train acc: 0.6497 - test acc: 0.4287 - 15m 28s\n",
      "batch: 1400/1563 - train loss: 6.7676 - test loss: 14.7448 - train acc: 0.6451 - test acc: 0.4140 - 15m 33s\n",
      "batch: 1500/1563 - train loss: 7.4491 - test loss: 14.3554 - train acc: 0.6219 - test acc: 0.4224 - 15m 38s\n",
      "batch: 1563/1563 - train loss: 7.4451 - test loss: 14.5700 - train acc: 0.6240 - test acc: 0.4235 - 15m 43s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.6557 - test loss: 14.8469 - train acc: 0.7481 - test acc: 0.4217 - 15m 48s\n",
      "batch: 200/1563 - train loss: 4.6352 - test loss: 15.2751 - train acc: 0.7497 - test acc: 0.4217 - 15m 54s\n",
      "batch: 300/1563 - train loss: 4.8145 - test loss: 14.9036 - train acc: 0.7447 - test acc: 0.4263 - 15m 59s\n",
      "batch: 400/1563 - train loss: 5.2569 - test loss: 14.8856 - train acc: 0.7113 - test acc: 0.4302 - 16m 4s\n",
      "batch: 500/1563 - train loss: 5.2816 - test loss: 15.1246 - train acc: 0.7234 - test acc: 0.4215 - 16m 10s\n",
      "batch: 600/1563 - train loss: 5.2253 - test loss: 15.0348 - train acc: 0.7176 - test acc: 0.4248 - 16m 15s\n",
      "batch: 700/1563 - train loss: 5.4310 - test loss: 18.2571 - train acc: 0.7069 - test acc: 0.3739 - 16m 21s\n",
      "batch: 800/1563 - train loss: 5.4603 - test loss: 15.4922 - train acc: 0.6997 - test acc: 0.4210 - 16m 26s\n",
      "batch: 900/1563 - train loss: 5.8745 - test loss: 15.2018 - train acc: 0.6963 - test acc: 0.4247 - 16m 31s\n",
      "batch: 1000/1563 - train loss: 5.8535 - test loss: 14.9795 - train acc: 0.6904 - test acc: 0.4233 - 16m 37s\n",
      "batch: 1100/1563 - train loss: 5.9387 - test loss: 16.2226 - train acc: 0.6922 - test acc: 0.4020 - 16m 42s\n",
      "batch: 1200/1563 - train loss: 5.9288 - test loss: 14.8490 - train acc: 0.6950 - test acc: 0.4264 - 16m 48s\n",
      "batch: 1300/1563 - train loss: 6.2858 - test loss: 14.7294 - train acc: 0.6751 - test acc: 0.4242 - 16m 53s\n",
      "batch: 1400/1563 - train loss: 6.1679 - test loss: 14.7946 - train acc: 0.6722 - test acc: 0.4215 - 16m 59s\n",
      "batch: 1500/1563 - train loss: 6.1534 - test loss: 14.8732 - train acc: 0.6785 - test acc: 0.4277 - 17m 4s\n",
      "batch: 1563/1563 - train loss: 6.2989 - test loss: 14.8729 - train acc: 0.6682 - test acc: 0.4237 - 17m 9s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 4.0664 - test loss: 14.8302 - train acc: 0.7849 - test acc: 0.4327 - 17m 14s\n",
      "batch: 200/1563 - train loss: 3.9993 - test loss: 15.4390 - train acc: 0.7765 - test acc: 0.4241 - 17m 20s\n",
      "batch: 300/1563 - train loss: 4.0714 - test loss: 15.2882 - train acc: 0.7822 - test acc: 0.4331 - 17m 25s\n",
      "batch: 400/1563 - train loss: 4.2113 - test loss: 15.2630 - train acc: 0.7793 - test acc: 0.4221 - 17m 31s\n",
      "batch: 500/1563 - train loss: 4.2095 - test loss: 15.3974 - train acc: 0.7647 - test acc: 0.4271 - 17m 36s\n",
      "batch: 600/1563 - train loss: 4.7228 - test loss: 15.3462 - train acc: 0.7522 - test acc: 0.4358 - 17m 42s\n",
      "batch: 700/1563 - train loss: 4.7817 - test loss: 15.5576 - train acc: 0.7331 - test acc: 0.4278 - 17m 47s\n",
      "batch: 800/1563 - train loss: 4.8851 - test loss: 15.3135 - train acc: 0.7406 - test acc: 0.4284 - 17m 53s\n",
      "batch: 900/1563 - train loss: 4.9710 - test loss: 15.5079 - train acc: 0.7332 - test acc: 0.4193 - 17m 58s\n",
      "batch: 1000/1563 - train loss: 4.7604 - test loss: 15.4721 - train acc: 0.7410 - test acc: 0.4303 - 18m 3s\n",
      "batch: 1100/1563 - train loss: 4.8572 - test loss: 15.2678 - train acc: 0.7366 - test acc: 0.4303 - 18m 9s\n",
      "batch: 1200/1563 - train loss: 4.9684 - test loss: 15.5902 - train acc: 0.7294 - test acc: 0.4296 - 18m 15s\n",
      "batch: 1300/1563 - train loss: 5.3267 - test loss: 17.8593 - train acc: 0.7050 - test acc: 0.3890 - 18m 20s\n",
      "batch: 1400/1563 - train loss: 5.4325 - test loss: 15.3326 - train acc: 0.7196 - test acc: 0.4323 - 18m 26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 5.4825 - test loss: 15.2528 - train acc: 0.7044 - test acc: 0.4229 - 18m 31s\n",
      "batch: 1563/1563 - train loss: 5.9067 - test loss: 15.7347 - train acc: 0.6882 - test acc: 0.4079 - 18m 35s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 3.3002 - test loss: 15.2144 - train acc: 0.8228 - test acc: 0.4383 - 18m 41s\n",
      "batch: 200/1563 - train loss: 3.4628 - test loss: 15.9554 - train acc: 0.8074 - test acc: 0.4241 - 18m 46s\n",
      "batch: 300/1563 - train loss: 3.5517 - test loss: 17.0190 - train acc: 0.8081 - test acc: 0.4141 - 18m 52s\n",
      "batch: 400/1563 - train loss: 3.5931 - test loss: 16.4777 - train acc: 0.7999 - test acc: 0.4232 - 18m 57s\n",
      "batch: 500/1563 - train loss: 3.7836 - test loss: 16.2026 - train acc: 0.8012 - test acc: 0.4324 - 19m 3s\n",
      "batch: 600/1563 - train loss: 3.7828 - test loss: 16.0195 - train acc: 0.7909 - test acc: 0.4289 - 19m 8s\n",
      "batch: 700/1563 - train loss: 4.0628 - test loss: 16.2621 - train acc: 0.7687 - test acc: 0.4245 - 19m 14s\n",
      "batch: 800/1563 - train loss: 4.4539 - test loss: 15.9186 - train acc: 0.7647 - test acc: 0.4243 - 19m 19s\n",
      "batch: 900/1563 - train loss: 4.3177 - test loss: 16.2269 - train acc: 0.7581 - test acc: 0.4196 - 19m 25s\n",
      "batch: 1000/1563 - train loss: 4.3674 - test loss: 16.0159 - train acc: 0.7581 - test acc: 0.4267 - 19m 30s\n",
      "batch: 1100/1563 - train loss: 4.4502 - test loss: 15.8627 - train acc: 0.7525 - test acc: 0.4320 - 19m 36s\n",
      "batch: 1200/1563 - train loss: 4.6366 - test loss: 15.8023 - train acc: 0.7431 - test acc: 0.4350 - 19m 41s\n",
      "batch: 1300/1563 - train loss: 4.6361 - test loss: 15.6868 - train acc: 0.7479 - test acc: 0.4294 - 19m 46s\n",
      "batch: 1400/1563 - train loss: 4.5969 - test loss: 16.0604 - train acc: 0.7484 - test acc: 0.4205 - 19m 52s\n",
      "batch: 1500/1563 - train loss: 4.8035 - test loss: 16.1683 - train acc: 0.7331 - test acc: 0.4111 - 19m 57s\n",
      "time is up! finishing training\n",
      "batch: 1517/1563 - train loss: 4.8687 - test loss: 18.8782 - train acc: 0.7360 - test acc: 0.3783 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 15\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0251 - test loss: 24.2662 - train acc: 0.0386 - test acc: 0.0616 - 0m 2s\n",
      "batch: 200/1563 - train loss: 24.0472 - test loss: 23.2059 - train acc: 0.0639 - test acc: 0.0764 - 0m 8s\n",
      "batch: 300/1563 - train loss: 22.7208 - test loss: 22.6157 - train acc: 0.0890 - test acc: 0.0953 - 0m 13s\n",
      "batch: 400/1563 - train loss: 22.1400 - test loss: 22.0974 - train acc: 0.1037 - test acc: 0.1099 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.4566 - test loss: 21.1378 - train acc: 0.1221 - test acc: 0.1320 - 0m 24s\n",
      "batch: 600/1563 - train loss: 21.2311 - test loss: 20.7045 - train acc: 0.1310 - test acc: 0.1416 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.7751 - test loss: 20.4305 - train acc: 0.1469 - test acc: 0.1575 - 0m 34s\n",
      "batch: 800/1563 - train loss: 20.3468 - test loss: 20.8098 - train acc: 0.1534 - test acc: 0.1520 - 0m 40s\n",
      "batch: 900/1563 - train loss: 20.2037 - test loss: 19.9687 - train acc: 0.1497 - test acc: 0.1609 - 0m 45s\n",
      "batch: 1000/1563 - train loss: 20.0478 - test loss: 20.0418 - train acc: 0.1703 - test acc: 0.1625 - 0m 50s\n",
      "batch: 1100/1563 - train loss: 19.5315 - test loss: 20.3058 - train acc: 0.1744 - test acc: 0.1525 - 0m 56s\n",
      "batch: 1200/1563 - train loss: 19.5374 - test loss: 20.3281 - train acc: 0.1782 - test acc: 0.1657 - 1m 1s\n",
      "batch: 1300/1563 - train loss: 19.1716 - test loss: 19.0214 - train acc: 0.1776 - test acc: 0.1845 - 1m 6s\n",
      "batch: 1400/1563 - train loss: 18.7756 - test loss: 18.2016 - train acc: 0.1919 - test acc: 0.2197 - 1m 12s\n",
      "batch: 1500/1563 - train loss: 18.8368 - test loss: 18.2406 - train acc: 0.1953 - test acc: 0.2031 - 1m 17s\n",
      "batch: 1563/1563 - train loss: 18.6438 - test loss: 19.1360 - train acc: 0.1987 - test acc: 0.1976 - 1m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.4243 - test loss: 18.8840 - train acc: 0.2425 - test acc: 0.2050 - 1m 27s\n",
      "batch: 200/1563 - train loss: 17.8662 - test loss: 18.1379 - train acc: 0.2235 - test acc: 0.2184 - 1m 33s\n",
      "batch: 300/1563 - train loss: 17.3518 - test loss: 17.7749 - train acc: 0.2319 - test acc: 0.2233 - 1m 39s\n",
      "batch: 400/1563 - train loss: 17.4361 - test loss: 17.1922 - train acc: 0.2394 - test acc: 0.2511 - 1m 44s\n",
      "batch: 500/1563 - train loss: 17.1156 - test loss: 17.4279 - train acc: 0.2442 - test acc: 0.2388 - 1m 49s\n",
      "batch: 600/1563 - train loss: 17.0854 - test loss: 18.3909 - train acc: 0.2488 - test acc: 0.2156 - 1m 55s\n",
      "batch: 700/1563 - train loss: 16.8462 - test loss: 18.1579 - train acc: 0.2553 - test acc: 0.2220 - 2m 0s\n",
      "batch: 800/1563 - train loss: 16.8009 - test loss: 17.5976 - train acc: 0.2625 - test acc: 0.2449 - 2m 5s\n",
      "batch: 900/1563 - train loss: 16.8298 - test loss: 18.4238 - train acc: 0.2491 - test acc: 0.2334 - 2m 11s\n",
      "batch: 1000/1563 - train loss: 16.4627 - test loss: 17.5446 - train acc: 0.2656 - test acc: 0.2438 - 2m 16s\n",
      "batch: 1100/1563 - train loss: 16.5927 - test loss: 16.7910 - train acc: 0.2756 - test acc: 0.2564 - 2m 22s\n",
      "batch: 1200/1563 - train loss: 16.3851 - test loss: 16.4194 - train acc: 0.2713 - test acc: 0.2713 - 2m 27s\n",
      "batch: 1300/1563 - train loss: 16.0378 - test loss: 16.6858 - train acc: 0.2863 - test acc: 0.2646 - 2m 33s\n",
      "batch: 1400/1563 - train loss: 16.1834 - test loss: 16.1535 - train acc: 0.2747 - test acc: 0.2841 - 2m 39s\n",
      "batch: 1500/1563 - train loss: 16.0789 - test loss: 16.2943 - train acc: 0.2759 - test acc: 0.2799 - 2m 45s\n",
      "batch: 1563/1563 - train loss: 15.8456 - test loss: 15.5077 - train acc: 0.2909 - test acc: 0.3113 - 2m 49s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.9596 - test loss: 15.5057 - train acc: 0.3153 - test acc: 0.3063 - 2m 55s\n",
      "batch: 200/1563 - train loss: 14.7254 - test loss: 17.3410 - train acc: 0.3325 - test acc: 0.2676 - 3m 0s\n",
      "batch: 300/1563 - train loss: 14.8169 - test loss: 15.7445 - train acc: 0.3249 - test acc: 0.3018 - 3m 5s\n",
      "batch: 400/1563 - train loss: 14.8476 - test loss: 15.6945 - train acc: 0.3206 - test acc: 0.3036 - 3m 11s\n",
      "batch: 500/1563 - train loss: 14.7268 - test loss: 15.6595 - train acc: 0.3300 - test acc: 0.3046 - 3m 16s\n",
      "batch: 600/1563 - train loss: 14.9019 - test loss: 16.0897 - train acc: 0.3290 - test acc: 0.2911 - 3m 22s\n",
      "batch: 700/1563 - train loss: 14.5487 - test loss: 15.0501 - train acc: 0.3290 - test acc: 0.3243 - 3m 28s\n",
      "batch: 800/1563 - train loss: 14.7821 - test loss: 15.8332 - train acc: 0.3293 - test acc: 0.2982 - 3m 33s\n",
      "batch: 900/1563 - train loss: 14.6412 - test loss: 15.5324 - train acc: 0.3403 - test acc: 0.3152 - 3m 39s\n",
      "batch: 1000/1563 - train loss: 14.5911 - test loss: 15.1014 - train acc: 0.3406 - test acc: 0.3233 - 3m 44s\n",
      "batch: 1100/1563 - train loss: 14.6398 - test loss: 14.6810 - train acc: 0.3488 - test acc: 0.3390 - 3m 50s\n",
      "batch: 1200/1563 - train loss: 14.7542 - test loss: 14.7346 - train acc: 0.3230 - test acc: 0.3435 - 3m 55s\n",
      "batch: 1300/1563 - train loss: 14.4365 - test loss: 14.9590 - train acc: 0.3366 - test acc: 0.3317 - 4m 1s\n",
      "batch: 1400/1563 - train loss: 14.2130 - test loss: 14.8969 - train acc: 0.3503 - test acc: 0.3324 - 4m 6s\n",
      "batch: 1500/1563 - train loss: 14.2046 - test loss: 17.1204 - train acc: 0.3491 - test acc: 0.2621 - 4m 12s\n",
      "batch: 1563/1563 - train loss: 14.4390 - test loss: 15.0458 - train acc: 0.3372 - test acc: 0.3313 - 4m 16s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.9451 - test loss: 14.9442 - train acc: 0.4019 - test acc: 0.3402 - 4m 22s\n",
      "batch: 200/1563 - train loss: 13.0623 - test loss: 16.0159 - train acc: 0.3831 - test acc: 0.3008 - 4m 27s\n",
      "batch: 300/1563 - train loss: 13.3734 - test loss: 14.9187 - train acc: 0.3813 - test acc: 0.3407 - 4m 33s\n",
      "batch: 400/1563 - train loss: 13.0058 - test loss: 14.7948 - train acc: 0.4038 - test acc: 0.3457 - 4m 38s\n",
      "batch: 500/1563 - train loss: 13.1903 - test loss: 14.5227 - train acc: 0.3769 - test acc: 0.3500 - 4m 44s\n",
      "batch: 600/1563 - train loss: 13.1046 - test loss: 15.5747 - train acc: 0.3882 - test acc: 0.3165 - 4m 50s\n",
      "batch: 700/1563 - train loss: 13.2401 - test loss: 14.4143 - train acc: 0.3925 - test acc: 0.3578 - 4m 55s\n",
      "batch: 800/1563 - train loss: 13.2652 - test loss: 14.0996 - train acc: 0.3903 - test acc: 0.3623 - 5m 1s\n",
      "batch: 900/1563 - train loss: 13.0991 - test loss: 14.8458 - train acc: 0.3881 - test acc: 0.3435 - 5m 6s\n",
      "batch: 1000/1563 - train loss: 13.1483 - test loss: 14.5772 - train acc: 0.3868 - test acc: 0.3509 - 5m 12s\n",
      "batch: 1100/1563 - train loss: 13.1044 - test loss: 15.1367 - train acc: 0.3951 - test acc: 0.3328 - 5m 17s\n",
      "batch: 1200/1563 - train loss: 12.9526 - test loss: 14.0417 - train acc: 0.3953 - test acc: 0.3758 - 5m 23s\n",
      "batch: 1300/1563 - train loss: 13.4302 - test loss: 14.4594 - train acc: 0.3759 - test acc: 0.3463 - 5m 28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 12.9678 - test loss: 13.7631 - train acc: 0.3884 - test acc: 0.3761 - 5m 34s\n",
      "batch: 1500/1563 - train loss: 13.4553 - test loss: 14.5099 - train acc: 0.3819 - test acc: 0.3510 - 5m 39s\n",
      "batch: 1563/1563 - train loss: 13.0736 - test loss: 13.9023 - train acc: 0.3950 - test acc: 0.3664 - 5m 44s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.6291 - test loss: 13.5802 - train acc: 0.4462 - test acc: 0.3860 - 5m 49s\n",
      "batch: 200/1563 - train loss: 11.3980 - test loss: 14.0720 - train acc: 0.4591 - test acc: 0.3703 - 5m 55s\n",
      "batch: 300/1563 - train loss: 11.4943 - test loss: 15.1213 - train acc: 0.4556 - test acc: 0.3403 - 6m 1s\n",
      "batch: 400/1563 - train loss: 11.6854 - test loss: 14.0194 - train acc: 0.4432 - test acc: 0.3783 - 6m 6s\n",
      "batch: 500/1563 - train loss: 11.6887 - test loss: 14.0584 - train acc: 0.4450 - test acc: 0.3710 - 6m 11s\n",
      "batch: 600/1563 - train loss: 11.6862 - test loss: 17.2249 - train acc: 0.4432 - test acc: 0.3027 - 6m 17s\n",
      "batch: 700/1563 - train loss: 12.0367 - test loss: 14.0558 - train acc: 0.4360 - test acc: 0.3697 - 6m 22s\n",
      "batch: 800/1563 - train loss: 11.8736 - test loss: 13.7079 - train acc: 0.4288 - test acc: 0.3834 - 6m 28s\n",
      "batch: 900/1563 - train loss: 11.8828 - test loss: 13.4906 - train acc: 0.4541 - test acc: 0.3933 - 6m 33s\n",
      "batch: 1000/1563 - train loss: 11.9728 - test loss: 13.7384 - train acc: 0.4413 - test acc: 0.3837 - 6m 38s\n",
      "batch: 1100/1563 - train loss: 12.1343 - test loss: 13.5023 - train acc: 0.4366 - test acc: 0.3923 - 6m 44s\n",
      "batch: 1200/1563 - train loss: 12.1681 - test loss: 14.5627 - train acc: 0.4278 - test acc: 0.3598 - 6m 49s\n",
      "batch: 1300/1563 - train loss: 11.8864 - test loss: 13.5644 - train acc: 0.4385 - test acc: 0.3908 - 6m 55s\n",
      "batch: 1400/1563 - train loss: 11.8560 - test loss: 13.7735 - train acc: 0.4487 - test acc: 0.3785 - 7m 0s\n",
      "batch: 1500/1563 - train loss: 11.6013 - test loss: 13.5021 - train acc: 0.4441 - test acc: 0.3887 - 7m 6s\n",
      "batch: 1563/1563 - train loss: 11.8360 - test loss: 15.5407 - train acc: 0.4338 - test acc: 0.3429 - 7m 10s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.9430 - test loss: 15.7284 - train acc: 0.5078 - test acc: 0.3409 - 7m 15s\n",
      "batch: 200/1563 - train loss: 9.9575 - test loss: 13.9098 - train acc: 0.5147 - test acc: 0.3845 - 7m 21s\n",
      "batch: 300/1563 - train loss: 10.4388 - test loss: 13.6093 - train acc: 0.4997 - test acc: 0.3941 - 7m 26s\n",
      "batch: 400/1563 - train loss: 10.6278 - test loss: 14.0657 - train acc: 0.4879 - test acc: 0.3892 - 7m 32s\n",
      "batch: 500/1563 - train loss: 10.1392 - test loss: 14.0145 - train acc: 0.5068 - test acc: 0.3890 - 7m 37s\n",
      "batch: 600/1563 - train loss: 10.6843 - test loss: 14.3038 - train acc: 0.4890 - test acc: 0.3806 - 7m 43s\n",
      "batch: 700/1563 - train loss: 10.6572 - test loss: 14.0376 - train acc: 0.4851 - test acc: 0.3814 - 7m 49s\n",
      "batch: 800/1563 - train loss: 10.6247 - test loss: 15.3597 - train acc: 0.4866 - test acc: 0.3546 - 7m 54s\n",
      "batch: 900/1563 - train loss: 10.7824 - test loss: 15.2338 - train acc: 0.4849 - test acc: 0.3573 - 8m 0s\n",
      "batch: 1000/1563 - train loss: 11.1084 - test loss: 14.0616 - train acc: 0.4675 - test acc: 0.3856 - 8m 6s\n",
      "batch: 1100/1563 - train loss: 10.7683 - test loss: 13.9825 - train acc: 0.4834 - test acc: 0.3832 - 8m 11s\n",
      "batch: 1200/1563 - train loss: 10.8414 - test loss: 13.1394 - train acc: 0.4826 - test acc: 0.4059 - 8m 16s\n",
      "batch: 1300/1563 - train loss: 10.8265 - test loss: 13.4021 - train acc: 0.4760 - test acc: 0.3979 - 8m 22s\n",
      "batch: 1400/1563 - train loss: 10.8235 - test loss: 14.4944 - train acc: 0.4891 - test acc: 0.3727 - 8m 27s\n",
      "batch: 1500/1563 - train loss: 10.8846 - test loss: 14.2139 - train acc: 0.4747 - test acc: 0.3793 - 8m 33s\n",
      "batch: 1563/1563 - train loss: 11.2095 - test loss: 13.5663 - train acc: 0.4688 - test acc: 0.3943 - 8m 37s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.8660 - test loss: 13.1576 - train acc: 0.5728 - test acc: 0.4197 - 8m 43s\n",
      "batch: 200/1563 - train loss: 9.1720 - test loss: 14.8791 - train acc: 0.5559 - test acc: 0.3681 - 8m 48s\n",
      "batch: 300/1563 - train loss: 8.8199 - test loss: 13.4955 - train acc: 0.5709 - test acc: 0.4103 - 8m 54s\n",
      "batch: 400/1563 - train loss: 9.2157 - test loss: 13.4228 - train acc: 0.5431 - test acc: 0.4141 - 8m 59s\n",
      "batch: 500/1563 - train loss: 9.4863 - test loss: 13.5552 - train acc: 0.5352 - test acc: 0.4058 - 9m 5s\n",
      "batch: 600/1563 - train loss: 9.3481 - test loss: 14.0890 - train acc: 0.5409 - test acc: 0.3930 - 9m 11s\n",
      "batch: 700/1563 - train loss: 9.5119 - test loss: 13.7269 - train acc: 0.5244 - test acc: 0.4098 - 9m 16s\n",
      "batch: 800/1563 - train loss: 9.6190 - test loss: 13.5430 - train acc: 0.5290 - test acc: 0.4055 - 9m 22s\n",
      "batch: 900/1563 - train loss: 9.8505 - test loss: 13.3036 - train acc: 0.5206 - test acc: 0.4055 - 9m 27s\n",
      "batch: 1000/1563 - train loss: 9.8021 - test loss: 14.1003 - train acc: 0.5151 - test acc: 0.3912 - 9m 33s\n",
      "batch: 1100/1563 - train loss: 9.6089 - test loss: 13.2959 - train acc: 0.5310 - test acc: 0.4101 - 9m 39s\n",
      "batch: 1200/1563 - train loss: 9.7546 - test loss: 13.3935 - train acc: 0.5259 - test acc: 0.4074 - 9m 45s\n",
      "batch: 1300/1563 - train loss: 9.7731 - test loss: 13.8670 - train acc: 0.5237 - test acc: 0.3950 - 9m 51s\n",
      "batch: 1400/1563 - train loss: 9.6391 - test loss: 13.5265 - train acc: 0.5328 - test acc: 0.4094 - 9m 56s\n",
      "batch: 1500/1563 - train loss: 9.7422 - test loss: 13.6099 - train acc: 0.5290 - test acc: 0.4048 - 10m 1s\n",
      "batch: 1563/1563 - train loss: 10.1769 - test loss: 13.3350 - train acc: 0.5056 - test acc: 0.4100 - 10m 6s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.6437 - test loss: 13.4962 - train acc: 0.6165 - test acc: 0.4185 - 10m 12s\n",
      "batch: 200/1563 - train loss: 7.8630 - test loss: 14.6769 - train acc: 0.6018 - test acc: 0.3830 - 10m 18s\n",
      "batch: 300/1563 - train loss: 8.2002 - test loss: 13.8929 - train acc: 0.5862 - test acc: 0.4083 - 10m 23s\n",
      "batch: 400/1563 - train loss: 8.6498 - test loss: 14.3444 - train acc: 0.5562 - test acc: 0.3892 - 10m 28s\n",
      "batch: 500/1563 - train loss: 8.2213 - test loss: 14.0877 - train acc: 0.5815 - test acc: 0.4030 - 10m 34s\n",
      "batch: 600/1563 - train loss: 8.4808 - test loss: 14.1114 - train acc: 0.5834 - test acc: 0.3986 - 10m 40s\n",
      "batch: 700/1563 - train loss: 8.7742 - test loss: 13.3999 - train acc: 0.5572 - test acc: 0.4182 - 10m 45s\n",
      "batch: 800/1563 - train loss: 8.6962 - test loss: 13.3290 - train acc: 0.5612 - test acc: 0.4209 - 10m 51s\n",
      "batch: 900/1563 - train loss: 8.5704 - test loss: 13.8327 - train acc: 0.5693 - test acc: 0.4059 - 10m 56s\n",
      "batch: 1000/1563 - train loss: 8.7355 - test loss: 13.6312 - train acc: 0.5637 - test acc: 0.4072 - 11m 2s\n",
      "batch: 1100/1563 - train loss: 8.7443 - test loss: 13.4999 - train acc: 0.5581 - test acc: 0.4141 - 11m 7s\n",
      "batch: 1200/1563 - train loss: 8.7865 - test loss: 13.5052 - train acc: 0.5668 - test acc: 0.4165 - 11m 13s\n",
      "batch: 1300/1563 - train loss: 9.1294 - test loss: 13.1767 - train acc: 0.5597 - test acc: 0.4263 - 11m 18s\n",
      "batch: 1400/1563 - train loss: 8.9754 - test loss: 13.0245 - train acc: 0.5559 - test acc: 0.4271 - 11m 24s\n",
      "batch: 1500/1563 - train loss: 8.9769 - test loss: 13.4576 - train acc: 0.5568 - test acc: 0.4204 - 11m 29s\n",
      "batch: 1563/1563 - train loss: 9.0229 - test loss: 14.8862 - train acc: 0.5543 - test acc: 0.3925 - 11m 33s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4378 - test loss: 13.8072 - train acc: 0.6759 - test acc: 0.4270 - 11m 39s\n",
      "batch: 200/1563 - train loss: 6.7243 - test loss: 16.0293 - train acc: 0.6529 - test acc: 0.3686 - 11m 44s\n",
      "batch: 300/1563 - train loss: 7.1803 - test loss: 15.6358 - train acc: 0.6300 - test acc: 0.3664 - 11m 50s\n",
      "batch: 400/1563 - train loss: 7.0436 - test loss: 13.9716 - train acc: 0.6431 - test acc: 0.4226 - 11m 55s\n",
      "batch: 500/1563 - train loss: 7.2039 - test loss: 14.2597 - train acc: 0.6334 - test acc: 0.4168 - 12m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.3579 - test loss: 14.1318 - train acc: 0.6109 - test acc: 0.4155 - 12m 6s\n",
      "batch: 700/1563 - train loss: 7.7131 - test loss: 14.1464 - train acc: 0.6031 - test acc: 0.4120 - 12m 12s\n",
      "batch: 800/1563 - train loss: 7.8455 - test loss: 13.7115 - train acc: 0.6075 - test acc: 0.4305 - 12m 18s\n",
      "batch: 900/1563 - train loss: 7.6916 - test loss: 13.6549 - train acc: 0.6130 - test acc: 0.4246 - 12m 23s\n",
      "batch: 1000/1563 - train loss: 7.7102 - test loss: 14.8434 - train acc: 0.6022 - test acc: 0.3973 - 12m 28s\n",
      "batch: 1100/1563 - train loss: 7.6777 - test loss: 13.8981 - train acc: 0.6125 - test acc: 0.4305 - 12m 34s\n",
      "batch: 1200/1563 - train loss: 7.7599 - test loss: 14.0009 - train acc: 0.6112 - test acc: 0.4191 - 12m 39s\n",
      "batch: 1300/1563 - train loss: 8.0042 - test loss: 13.9364 - train acc: 0.5994 - test acc: 0.4230 - 12m 45s\n",
      "batch: 1400/1563 - train loss: 8.0643 - test loss: 13.7184 - train acc: 0.5884 - test acc: 0.4210 - 12m 51s\n",
      "batch: 1500/1563 - train loss: 8.3535 - test loss: 13.4093 - train acc: 0.5881 - test acc: 0.4286 - 12m 57s\n",
      "batch: 1563/1563 - train loss: 8.0399 - test loss: 14.3476 - train acc: 0.5947 - test acc: 0.4096 - 13m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.4374 - test loss: 14.1137 - train acc: 0.7144 - test acc: 0.4234 - 13m 7s\n",
      "batch: 200/1563 - train loss: 5.5435 - test loss: 14.0933 - train acc: 0.7075 - test acc: 0.4272 - 13m 12s\n",
      "batch: 300/1563 - train loss: 5.9778 - test loss: 14.4881 - train acc: 0.6822 - test acc: 0.4214 - 13m 18s\n",
      "batch: 400/1563 - train loss: 6.4617 - test loss: 14.9013 - train acc: 0.6697 - test acc: 0.4080 - 13m 23s\n",
      "batch: 500/1563 - train loss: 6.2339 - test loss: 15.1031 - train acc: 0.6754 - test acc: 0.4055 - 13m 29s\n",
      "batch: 600/1563 - train loss: 6.6785 - test loss: 15.6918 - train acc: 0.6575 - test acc: 0.3837 - 13m 34s\n",
      "batch: 700/1563 - train loss: 6.8434 - test loss: 14.7078 - train acc: 0.6494 - test acc: 0.4140 - 13m 39s\n",
      "batch: 800/1563 - train loss: 6.8846 - test loss: 13.9366 - train acc: 0.6438 - test acc: 0.4272 - 13m 45s\n",
      "batch: 900/1563 - train loss: 6.9042 - test loss: 14.6368 - train acc: 0.6435 - test acc: 0.4135 - 13m 51s\n",
      "batch: 1000/1563 - train loss: 6.9124 - test loss: 14.0275 - train acc: 0.6370 - test acc: 0.4300 - 13m 57s\n",
      "batch: 1100/1563 - train loss: 6.8368 - test loss: 14.3236 - train acc: 0.6434 - test acc: 0.4185 - 14m 2s\n",
      "batch: 1200/1563 - train loss: 7.0328 - test loss: 14.2493 - train acc: 0.6406 - test acc: 0.4219 - 14m 8s\n",
      "batch: 1300/1563 - train loss: 7.0907 - test loss: 14.4837 - train acc: 0.6306 - test acc: 0.4110 - 14m 13s\n",
      "batch: 1400/1563 - train loss: 7.0662 - test loss: 14.5745 - train acc: 0.6291 - test acc: 0.4183 - 14m 19s\n",
      "batch: 1500/1563 - train loss: 7.0824 - test loss: 14.4085 - train acc: 0.6334 - test acc: 0.4210 - 14m 25s\n",
      "batch: 1563/1563 - train loss: 6.9660 - test loss: 14.6268 - train acc: 0.6341 - test acc: 0.4128 - 14m 29s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.6217 - test loss: 14.6457 - train acc: 0.7525 - test acc: 0.4316 - 14m 35s\n",
      "batch: 200/1563 - train loss: 4.8241 - test loss: 15.5708 - train acc: 0.7437 - test acc: 0.4128 - 14m 40s\n",
      "batch: 300/1563 - train loss: 4.7672 - test loss: 15.1295 - train acc: 0.7425 - test acc: 0.4220 - 14m 45s\n",
      "batch: 400/1563 - train loss: 5.2871 - test loss: 14.8609 - train acc: 0.7238 - test acc: 0.4213 - 14m 51s\n",
      "batch: 500/1563 - train loss: 5.2207 - test loss: 15.1390 - train acc: 0.7213 - test acc: 0.4146 - 14m 56s\n",
      "batch: 600/1563 - train loss: 5.6861 - test loss: 15.8326 - train acc: 0.6995 - test acc: 0.3984 - 15m 2s\n",
      "batch: 700/1563 - train loss: 5.9212 - test loss: 15.1045 - train acc: 0.6928 - test acc: 0.4232 - 15m 7s\n",
      "batch: 800/1563 - train loss: 6.0554 - test loss: 15.2161 - train acc: 0.6760 - test acc: 0.4120 - 15m 12s\n",
      "batch: 900/1563 - train loss: 5.7949 - test loss: 15.7224 - train acc: 0.6919 - test acc: 0.4078 - 15m 18s\n",
      "batch: 1000/1563 - train loss: 6.1278 - test loss: 14.7611 - train acc: 0.6878 - test acc: 0.4277 - 15m 23s\n",
      "batch: 1100/1563 - train loss: 6.4472 - test loss: 14.3972 - train acc: 0.6650 - test acc: 0.4315 - 15m 29s\n",
      "batch: 1200/1563 - train loss: 6.3416 - test loss: 14.6516 - train acc: 0.6694 - test acc: 0.4264 - 15m 35s\n",
      "batch: 1300/1563 - train loss: 6.3287 - test loss: 14.5663 - train acc: 0.6907 - test acc: 0.4210 - 15m 41s\n",
      "batch: 1400/1563 - train loss: 6.3943 - test loss: 14.3094 - train acc: 0.6650 - test acc: 0.4280 - 15m 46s\n",
      "batch: 1500/1563 - train loss: 6.2298 - test loss: 14.8371 - train acc: 0.6819 - test acc: 0.4199 - 15m 52s\n",
      "batch: 1563/1563 - train loss: 6.3216 - test loss: 14.5830 - train acc: 0.6713 - test acc: 0.4261 - 15m 57s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.8347 - test loss: 14.6215 - train acc: 0.7941 - test acc: 0.4343 - 16m 2s\n",
      "batch: 200/1563 - train loss: 3.9896 - test loss: 15.5161 - train acc: 0.7806 - test acc: 0.4132 - 16m 8s\n",
      "batch: 300/1563 - train loss: 4.1203 - test loss: 15.1896 - train acc: 0.7815 - test acc: 0.4334 - 16m 14s\n",
      "batch: 400/1563 - train loss: 4.1968 - test loss: 15.3304 - train acc: 0.7809 - test acc: 0.4276 - 16m 19s\n",
      "batch: 500/1563 - train loss: 4.6622 - test loss: 15.5609 - train acc: 0.7475 - test acc: 0.4182 - 16m 24s\n",
      "batch: 600/1563 - train loss: 4.9648 - test loss: 15.6229 - train acc: 0.7300 - test acc: 0.4204 - 16m 31s\n",
      "batch: 700/1563 - train loss: 5.0813 - test loss: 15.4093 - train acc: 0.7354 - test acc: 0.4205 - 16m 36s\n",
      "batch: 800/1563 - train loss: 4.8617 - test loss: 15.5015 - train acc: 0.7331 - test acc: 0.4257 - 16m 41s\n",
      "batch: 900/1563 - train loss: 5.2407 - test loss: 15.6215 - train acc: 0.7134 - test acc: 0.4268 - 16m 47s\n",
      "batch: 1000/1563 - train loss: 5.1457 - test loss: 15.3665 - train acc: 0.7225 - test acc: 0.4226 - 16m 52s\n",
      "batch: 1100/1563 - train loss: 5.2234 - test loss: 15.0759 - train acc: 0.7163 - test acc: 0.4333 - 16m 58s\n",
      "batch: 1200/1563 - train loss: 5.5707 - test loss: 15.9023 - train acc: 0.7003 - test acc: 0.4127 - 17m 4s\n",
      "batch: 1300/1563 - train loss: 5.5235 - test loss: 15.4947 - train acc: 0.6990 - test acc: 0.4136 - 17m 10s\n",
      "batch: 1400/1563 - train loss: 5.6004 - test loss: 15.5368 - train acc: 0.7063 - test acc: 0.4159 - 17m 15s\n",
      "batch: 1500/1563 - train loss: 5.8870 - test loss: 15.5966 - train acc: 0.6810 - test acc: 0.4154 - 17m 21s\n",
      "batch: 1563/1563 - train loss: 5.8164 - test loss: 14.8991 - train acc: 0.6931 - test acc: 0.4308 - 17m 26s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.3843 - test loss: 15.4580 - train acc: 0.8193 - test acc: 0.4349 - 17m 32s\n",
      "batch: 200/1563 - train loss: 3.4230 - test loss: 16.0782 - train acc: 0.8136 - test acc: 0.4243 - 17m 37s\n",
      "batch: 300/1563 - train loss: 3.4209 - test loss: 16.0663 - train acc: 0.8156 - test acc: 0.4309 - 17m 43s\n",
      "batch: 400/1563 - train loss: 3.6936 - test loss: 15.9733 - train acc: 0.7924 - test acc: 0.4193 - 17m 48s\n",
      "batch: 500/1563 - train loss: 3.7007 - test loss: 16.1893 - train acc: 0.8005 - test acc: 0.4367 - 17m 54s\n",
      "batch: 600/1563 - train loss: 4.0163 - test loss: 16.2801 - train acc: 0.7809 - test acc: 0.4327 - 18m 0s\n",
      "batch: 700/1563 - train loss: 4.1161 - test loss: 15.7739 - train acc: 0.7722 - test acc: 0.4296 - 18m 6s\n",
      "batch: 800/1563 - train loss: 4.2656 - test loss: 17.3765 - train acc: 0.7674 - test acc: 0.3985 - 18m 11s\n",
      "batch: 900/1563 - train loss: 4.4076 - test loss: 15.9649 - train acc: 0.7522 - test acc: 0.4315 - 18m 17s\n",
      "batch: 1000/1563 - train loss: 4.5500 - test loss: 15.6579 - train acc: 0.7493 - test acc: 0.4352 - 18m 23s\n",
      "batch: 1100/1563 - train loss: 4.3315 - test loss: 16.0888 - train acc: 0.7578 - test acc: 0.4249 - 18m 28s\n",
      "batch: 1200/1563 - train loss: 4.7087 - test loss: 15.9107 - train acc: 0.7475 - test acc: 0.4221 - 18m 34s\n",
      "batch: 1300/1563 - train loss: 4.6902 - test loss: 15.7468 - train acc: 0.7344 - test acc: 0.4285 - 18m 40s\n",
      "batch: 1400/1563 - train loss: 4.9216 - test loss: 16.2433 - train acc: 0.7272 - test acc: 0.4225 - 18m 45s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 4.6222 - test loss: 15.8953 - train acc: 0.7485 - test acc: 0.4294 - 18m 51s\n",
      "batch: 1563/1563 - train loss: 4.9399 - test loss: 16.2438 - train acc: 0.7403 - test acc: 0.4090 - 18m 55s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.9860 - test loss: 17.0245 - train acc: 0.8299 - test acc: 0.4095 - 19m 1s\n",
      "batch: 200/1563 - train loss: 2.8511 - test loss: 16.0334 - train acc: 0.8415 - test acc: 0.4366 - 19m 6s\n",
      "batch: 300/1563 - train loss: 2.9479 - test loss: 16.1857 - train acc: 0.8359 - test acc: 0.4317 - 19m 11s\n",
      "batch: 400/1563 - train loss: 3.0596 - test loss: 16.7983 - train acc: 0.8287 - test acc: 0.4317 - 19m 17s\n",
      "batch: 500/1563 - train loss: 3.0657 - test loss: 17.5765 - train acc: 0.8252 - test acc: 0.4120 - 19m 22s\n",
      "batch: 600/1563 - train loss: 3.3003 - test loss: 17.8133 - train acc: 0.8153 - test acc: 0.4055 - 19m 28s\n",
      "batch: 700/1563 - train loss: 3.5834 - test loss: 16.4161 - train acc: 0.7950 - test acc: 0.4344 - 19m 33s\n",
      "batch: 800/1563 - train loss: 3.4648 - test loss: 16.8051 - train acc: 0.7965 - test acc: 0.4227 - 19m 38s\n",
      "batch: 900/1563 - train loss: 3.7456 - test loss: 18.3431 - train acc: 0.7893 - test acc: 0.3978 - 19m 44s\n",
      "batch: 1000/1563 - train loss: 3.8573 - test loss: 17.1760 - train acc: 0.7894 - test acc: 0.4141 - 19m 49s\n",
      "batch: 1100/1563 - train loss: 4.0927 - test loss: 16.5367 - train acc: 0.7774 - test acc: 0.4287 - 19m 55s\n",
      "batch: 1200/1563 - train loss: 4.1559 - test loss: 16.4052 - train acc: 0.7718 - test acc: 0.4265 - 20m 0s\n",
      "time is up! finishing training\n",
      "batch: 1201/1563 - train loss: 4.1533 - test loss: 16.4014 - train acc: 0.7721 - test acc: 0.4285 - 20m 3s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 16\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 25.9729 - test loss: 24.3588 - train acc: 0.0320 - test acc: 0.0605 - 0m 2s\n",
      "batch: 200/1563 - train loss: 24.1472 - test loss: 23.3728 - train acc: 0.0658 - test acc: 0.0779 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.8445 - test loss: 22.4657 - train acc: 0.0890 - test acc: 0.0958 - 0m 13s\n",
      "batch: 400/1563 - train loss: 21.8556 - test loss: 22.7282 - train acc: 0.1159 - test acc: 0.1092 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.5763 - test loss: 22.9037 - train acc: 0.1206 - test acc: 0.1029 - 0m 24s\n",
      "batch: 600/1563 - train loss: 21.3375 - test loss: 20.8302 - train acc: 0.1303 - test acc: 0.1357 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.5798 - test loss: 19.8860 - train acc: 0.1412 - test acc: 0.1448 - 0m 34s\n",
      "batch: 800/1563 - train loss: 20.1622 - test loss: 20.9758 - train acc: 0.1478 - test acc: 0.1436 - 0m 40s\n",
      "batch: 900/1563 - train loss: 20.1440 - test loss: 20.2857 - train acc: 0.1594 - test acc: 0.1630 - 0m 45s\n",
      "batch: 1000/1563 - train loss: 19.8704 - test loss: 21.9665 - train acc: 0.1538 - test acc: 0.1271 - 0m 50s\n",
      "batch: 1100/1563 - train loss: 19.6241 - test loss: 20.2951 - train acc: 0.1701 - test acc: 0.1703 - 0m 56s\n",
      "batch: 1200/1563 - train loss: 19.2879 - test loss: 18.6582 - train acc: 0.1860 - test acc: 0.2081 - 1m 1s\n",
      "batch: 1300/1563 - train loss: 18.9878 - test loss: 18.2537 - train acc: 0.1969 - test acc: 0.2129 - 1m 6s\n",
      "batch: 1400/1563 - train loss: 18.8057 - test loss: 18.5642 - train acc: 0.2025 - test acc: 0.2060 - 1m 11s\n",
      "batch: 1500/1563 - train loss: 18.7652 - test loss: 20.2323 - train acc: 0.1916 - test acc: 0.1606 - 1m 17s\n",
      "batch: 1563/1563 - train loss: 18.3317 - test loss: 18.0731 - train acc: 0.2129 - test acc: 0.2238 - 1m 21s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.5016 - test loss: 17.9746 - train acc: 0.2332 - test acc: 0.2203 - 1m 27s\n",
      "batch: 200/1563 - train loss: 17.4656 - test loss: 17.9932 - train acc: 0.2300 - test acc: 0.2243 - 1m 32s\n",
      "batch: 300/1563 - train loss: 17.3817 - test loss: 17.8921 - train acc: 0.2347 - test acc: 0.2307 - 1m 38s\n",
      "batch: 400/1563 - train loss: 17.2697 - test loss: 17.5957 - train acc: 0.2525 - test acc: 0.2395 - 1m 43s\n",
      "batch: 500/1563 - train loss: 17.3393 - test loss: 17.4790 - train acc: 0.2366 - test acc: 0.2461 - 1m 49s\n",
      "batch: 600/1563 - train loss: 17.2488 - test loss: 17.1896 - train acc: 0.2488 - test acc: 0.2480 - 1m 54s\n",
      "batch: 700/1563 - train loss: 16.9345 - test loss: 17.9342 - train acc: 0.2600 - test acc: 0.2359 - 2m 0s\n",
      "batch: 800/1563 - train loss: 16.9365 - test loss: 17.9202 - train acc: 0.2562 - test acc: 0.2320 - 2m 5s\n",
      "batch: 900/1563 - train loss: 16.7210 - test loss: 16.6100 - train acc: 0.2675 - test acc: 0.2726 - 2m 10s\n",
      "batch: 1000/1563 - train loss: 16.4166 - test loss: 17.0153 - train acc: 0.2735 - test acc: 0.2558 - 2m 16s\n",
      "batch: 1100/1563 - train loss: 16.7284 - test loss: 16.7145 - train acc: 0.2651 - test acc: 0.2659 - 2m 21s\n",
      "batch: 1200/1563 - train loss: 16.0665 - test loss: 19.3861 - train acc: 0.2919 - test acc: 0.2051 - 2m 27s\n",
      "batch: 1300/1563 - train loss: 16.3071 - test loss: 17.2054 - train acc: 0.2775 - test acc: 0.2582 - 2m 32s\n",
      "batch: 1400/1563 - train loss: 16.0042 - test loss: 18.2904 - train acc: 0.2922 - test acc: 0.2269 - 2m 37s\n",
      "batch: 1500/1563 - train loss: 15.9439 - test loss: 15.9032 - train acc: 0.2984 - test acc: 0.2924 - 2m 43s\n",
      "batch: 1563/1563 - train loss: 16.1938 - test loss: 15.9090 - train acc: 0.2784 - test acc: 0.2910 - 2m 47s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.9979 - test loss: 16.2089 - train acc: 0.3268 - test acc: 0.2860 - 2m 53s\n",
      "batch: 200/1563 - train loss: 14.8142 - test loss: 16.4061 - train acc: 0.3284 - test acc: 0.2892 - 2m 58s\n",
      "batch: 300/1563 - train loss: 14.5206 - test loss: 16.3676 - train acc: 0.3346 - test acc: 0.2873 - 3m 4s\n",
      "batch: 400/1563 - train loss: 14.7108 - test loss: 15.5179 - train acc: 0.3331 - test acc: 0.3067 - 3m 9s\n",
      "batch: 500/1563 - train loss: 14.8629 - test loss: 15.8330 - train acc: 0.3353 - test acc: 0.2972 - 3m 15s\n",
      "batch: 600/1563 - train loss: 14.6601 - test loss: 15.2866 - train acc: 0.3219 - test acc: 0.3149 - 3m 21s\n",
      "batch: 700/1563 - train loss: 14.7589 - test loss: 15.6580 - train acc: 0.3377 - test acc: 0.3056 - 3m 26s\n",
      "batch: 800/1563 - train loss: 14.8331 - test loss: 15.1572 - train acc: 0.3278 - test acc: 0.3247 - 3m 32s\n",
      "batch: 900/1563 - train loss: 14.4627 - test loss: 15.9672 - train acc: 0.3390 - test acc: 0.2937 - 3m 37s\n",
      "batch: 1000/1563 - train loss: 14.6571 - test loss: 15.7041 - train acc: 0.3325 - test acc: 0.3047 - 3m 42s\n",
      "batch: 1100/1563 - train loss: 14.5597 - test loss: 16.3754 - train acc: 0.3409 - test acc: 0.2857 - 3m 48s\n",
      "batch: 1200/1563 - train loss: 14.4860 - test loss: 15.0452 - train acc: 0.3422 - test acc: 0.3268 - 3m 54s\n",
      "batch: 1300/1563 - train loss: 14.5813 - test loss: 15.1404 - train acc: 0.3309 - test acc: 0.3226 - 3m 59s\n",
      "batch: 1400/1563 - train loss: 14.2716 - test loss: 15.1523 - train acc: 0.3541 - test acc: 0.3263 - 4m 5s\n",
      "batch: 1500/1563 - train loss: 14.2893 - test loss: 15.0889 - train acc: 0.3525 - test acc: 0.3273 - 4m 11s\n",
      "batch: 1563/1563 - train loss: 14.3442 - test loss: 16.1794 - train acc: 0.3506 - test acc: 0.2925 - 4m 16s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.9832 - test loss: 14.7038 - train acc: 0.3901 - test acc: 0.3422 - 4m 21s\n",
      "batch: 200/1563 - train loss: 13.0346 - test loss: 15.2845 - train acc: 0.3937 - test acc: 0.3281 - 4m 27s\n",
      "batch: 300/1563 - train loss: 13.1278 - test loss: 14.5128 - train acc: 0.3874 - test acc: 0.3482 - 4m 32s\n",
      "batch: 400/1563 - train loss: 13.0939 - test loss: 14.9144 - train acc: 0.3903 - test acc: 0.3422 - 4m 38s\n",
      "batch: 500/1563 - train loss: 13.0266 - test loss: 14.6465 - train acc: 0.3884 - test acc: 0.3487 - 4m 44s\n",
      "batch: 600/1563 - train loss: 13.2791 - test loss: 15.8195 - train acc: 0.3850 - test acc: 0.3209 - 4m 49s\n",
      "batch: 700/1563 - train loss: 12.9931 - test loss: 14.4376 - train acc: 0.4000 - test acc: 0.3521 - 4m 55s\n",
      "batch: 800/1563 - train loss: 13.2262 - test loss: 16.2756 - train acc: 0.3841 - test acc: 0.2985 - 5m 1s\n",
      "batch: 900/1563 - train loss: 13.2911 - test loss: 14.4831 - train acc: 0.3862 - test acc: 0.3480 - 5m 7s\n",
      "batch: 1000/1563 - train loss: 13.0814 - test loss: 14.5381 - train acc: 0.3960 - test acc: 0.3507 - 5m 13s\n",
      "batch: 1100/1563 - train loss: 12.9157 - test loss: 14.4317 - train acc: 0.3991 - test acc: 0.3554 - 5m 18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1200/1563 - train loss: 13.0936 - test loss: 14.9661 - train acc: 0.4041 - test acc: 0.3359 - 5m 24s\n",
      "batch: 1300/1563 - train loss: 13.2703 - test loss: 13.8208 - train acc: 0.4025 - test acc: 0.3770 - 5m 29s\n",
      "batch: 1400/1563 - train loss: 12.9956 - test loss: 13.9070 - train acc: 0.3896 - test acc: 0.3650 - 5m 35s\n",
      "batch: 1500/1563 - train loss: 12.8826 - test loss: 15.3007 - train acc: 0.3950 - test acc: 0.3374 - 5m 41s\n",
      "batch: 1563/1563 - train loss: 12.8485 - test loss: 14.4961 - train acc: 0.3891 - test acc: 0.3513 - 5m 46s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3414 - test loss: 14.6343 - train acc: 0.4619 - test acc: 0.3537 - 5m 51s\n",
      "batch: 200/1563 - train loss: 11.5209 - test loss: 14.1840 - train acc: 0.4479 - test acc: 0.3669 - 5m 57s\n",
      "batch: 300/1563 - train loss: 11.8762 - test loss: 13.9758 - train acc: 0.4419 - test acc: 0.3706 - 6m 3s\n",
      "batch: 400/1563 - train loss: 11.7630 - test loss: 14.7355 - train acc: 0.4394 - test acc: 0.3498 - 6m 9s\n",
      "batch: 500/1563 - train loss: 12.1776 - test loss: 15.2967 - train acc: 0.4156 - test acc: 0.3385 - 6m 15s\n",
      "batch: 600/1563 - train loss: 11.7730 - test loss: 14.2054 - train acc: 0.4451 - test acc: 0.3632 - 6m 20s\n",
      "batch: 700/1563 - train loss: 11.6576 - test loss: 14.1327 - train acc: 0.4481 - test acc: 0.3752 - 6m 26s\n",
      "batch: 800/1563 - train loss: 11.9493 - test loss: 14.5198 - train acc: 0.4316 - test acc: 0.3584 - 6m 32s\n",
      "batch: 900/1563 - train loss: 11.6808 - test loss: 14.1402 - train acc: 0.4578 - test acc: 0.3654 - 6m 38s\n",
      "batch: 1000/1563 - train loss: 11.6978 - test loss: 14.4476 - train acc: 0.4359 - test acc: 0.3613 - 6m 44s\n",
      "batch: 1100/1563 - train loss: 11.9460 - test loss: 13.6890 - train acc: 0.4392 - test acc: 0.3875 - 6m 50s\n",
      "batch: 1200/1563 - train loss: 11.7279 - test loss: 13.7021 - train acc: 0.4528 - test acc: 0.3843 - 6m 55s\n",
      "batch: 1300/1563 - train loss: 11.8400 - test loss: 13.6691 - train acc: 0.4413 - test acc: 0.3859 - 7m 1s\n",
      "batch: 1400/1563 - train loss: 11.8118 - test loss: 14.1102 - train acc: 0.4588 - test acc: 0.3732 - 7m 6s\n",
      "batch: 1500/1563 - train loss: 11.9406 - test loss: 14.0441 - train acc: 0.4444 - test acc: 0.3693 - 7m 12s\n",
      "batch: 1563/1563 - train loss: 11.8615 - test loss: 13.3245 - train acc: 0.4444 - test acc: 0.3944 - 7m 17s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.2299 - test loss: 14.3225 - train acc: 0.4903 - test acc: 0.3725 - 7m 22s\n",
      "batch: 200/1563 - train loss: 10.1486 - test loss: 14.0188 - train acc: 0.4897 - test acc: 0.3838 - 7m 28s\n",
      "batch: 300/1563 - train loss: 10.2534 - test loss: 14.3457 - train acc: 0.5010 - test acc: 0.3786 - 7m 33s\n",
      "batch: 400/1563 - train loss: 10.3907 - test loss: 13.7655 - train acc: 0.4956 - test acc: 0.3939 - 7m 39s\n",
      "batch: 500/1563 - train loss: 10.7930 - test loss: 13.5859 - train acc: 0.4772 - test acc: 0.3985 - 7m 45s\n",
      "batch: 600/1563 - train loss: 10.7164 - test loss: 13.6239 - train acc: 0.4791 - test acc: 0.3889 - 7m 51s\n",
      "batch: 700/1563 - train loss: 10.7015 - test loss: 13.9841 - train acc: 0.4878 - test acc: 0.3806 - 7m 56s\n",
      "batch: 800/1563 - train loss: 10.5817 - test loss: 13.5904 - train acc: 0.4881 - test acc: 0.3951 - 8m 2s\n",
      "batch: 900/1563 - train loss: 10.4953 - test loss: 13.8485 - train acc: 0.4931 - test acc: 0.3937 - 8m 7s\n",
      "batch: 1000/1563 - train loss: 10.7401 - test loss: 13.4783 - train acc: 0.4894 - test acc: 0.3976 - 8m 13s\n",
      "batch: 1100/1563 - train loss: 10.7477 - test loss: 13.4046 - train acc: 0.4862 - test acc: 0.4027 - 8m 18s\n",
      "batch: 1200/1563 - train loss: 11.1991 - test loss: 14.4500 - train acc: 0.4681 - test acc: 0.3692 - 8m 24s\n",
      "batch: 1300/1563 - train loss: 10.8510 - test loss: 13.5654 - train acc: 0.4894 - test acc: 0.3963 - 8m 29s\n",
      "batch: 1400/1563 - train loss: 10.7578 - test loss: 14.2587 - train acc: 0.4893 - test acc: 0.3758 - 8m 35s\n",
      "batch: 1500/1563 - train loss: 11.1016 - test loss: 13.7724 - train acc: 0.4723 - test acc: 0.3919 - 8m 40s\n",
      "batch: 1563/1563 - train loss: 11.1236 - test loss: 13.7484 - train acc: 0.4728 - test acc: 0.3870 - 8m 45s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.6483 - test loss: 13.5854 - train acc: 0.5675 - test acc: 0.4133 - 8m 51s\n",
      "batch: 200/1563 - train loss: 8.7946 - test loss: 13.4005 - train acc: 0.5660 - test acc: 0.4105 - 8m 57s\n",
      "batch: 300/1563 - train loss: 9.0593 - test loss: 13.5377 - train acc: 0.5516 - test acc: 0.4123 - 9m 3s\n",
      "batch: 400/1563 - train loss: 9.0801 - test loss: 14.3945 - train acc: 0.5527 - test acc: 0.3916 - 9m 8s\n",
      "batch: 500/1563 - train loss: 9.6301 - test loss: 13.4346 - train acc: 0.5252 - test acc: 0.4050 - 9m 13s\n",
      "batch: 600/1563 - train loss: 9.4480 - test loss: 14.0578 - train acc: 0.5381 - test acc: 0.3953 - 9m 19s\n",
      "batch: 700/1563 - train loss: 9.5767 - test loss: 14.4456 - train acc: 0.5325 - test acc: 0.3805 - 9m 24s\n",
      "batch: 800/1563 - train loss: 9.8996 - test loss: 14.1095 - train acc: 0.5235 - test acc: 0.3892 - 9m 30s\n",
      "batch: 900/1563 - train loss: 9.5611 - test loss: 13.9151 - train acc: 0.5281 - test acc: 0.3965 - 9m 35s\n",
      "batch: 1000/1563 - train loss: 9.4724 - test loss: 13.4552 - train acc: 0.5315 - test acc: 0.4176 - 9m 41s\n",
      "batch: 1100/1563 - train loss: 9.9886 - test loss: 14.4553 - train acc: 0.5216 - test acc: 0.3776 - 9m 46s\n",
      "batch: 1200/1563 - train loss: 10.0677 - test loss: 13.4298 - train acc: 0.5050 - test acc: 0.4111 - 9m 51s\n",
      "batch: 1300/1563 - train loss: 9.6950 - test loss: 14.2912 - train acc: 0.5322 - test acc: 0.3944 - 9m 57s\n",
      "batch: 1400/1563 - train loss: 9.7533 - test loss: 13.8597 - train acc: 0.5231 - test acc: 0.3974 - 10m 3s\n",
      "batch: 1500/1563 - train loss: 10.0830 - test loss: 13.6961 - train acc: 0.5044 - test acc: 0.4026 - 10m 8s\n",
      "batch: 1563/1563 - train loss: 10.1266 - test loss: 13.6540 - train acc: 0.5066 - test acc: 0.4051 - 10m 13s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.5625 - test loss: 13.7853 - train acc: 0.6113 - test acc: 0.4154 - 10m 18s\n",
      "batch: 200/1563 - train loss: 8.0284 - test loss: 14.1323 - train acc: 0.5991 - test acc: 0.4008 - 10m 24s\n",
      "batch: 300/1563 - train loss: 8.1253 - test loss: 13.4838 - train acc: 0.5834 - test acc: 0.4201 - 10m 30s\n",
      "batch: 400/1563 - train loss: 8.5223 - test loss: 14.2188 - train acc: 0.5775 - test acc: 0.4014 - 10m 36s\n",
      "batch: 500/1563 - train loss: 8.6295 - test loss: 13.6458 - train acc: 0.5627 - test acc: 0.4099 - 10m 41s\n",
      "batch: 600/1563 - train loss: 8.6435 - test loss: 13.6018 - train acc: 0.5581 - test acc: 0.4175 - 10m 47s\n",
      "batch: 700/1563 - train loss: 8.5808 - test loss: 13.6052 - train acc: 0.5806 - test acc: 0.4169 - 10m 52s\n",
      "batch: 800/1563 - train loss: 8.7897 - test loss: 14.6119 - train acc: 0.5675 - test acc: 0.3914 - 10m 58s\n",
      "batch: 900/1563 - train loss: 8.4798 - test loss: 13.8667 - train acc: 0.5731 - test acc: 0.4156 - 11m 3s\n",
      "batch: 1000/1563 - train loss: 8.8968 - test loss: 13.3491 - train acc: 0.5487 - test acc: 0.4199 - 11m 9s\n",
      "batch: 1100/1563 - train loss: 8.6003 - test loss: 13.6340 - train acc: 0.5732 - test acc: 0.4113 - 11m 14s\n",
      "batch: 1200/1563 - train loss: 8.9753 - test loss: 13.7916 - train acc: 0.5491 - test acc: 0.4148 - 11m 20s\n",
      "batch: 1300/1563 - train loss: 9.0099 - test loss: 13.7334 - train acc: 0.5553 - test acc: 0.4085 - 11m 25s\n",
      "batch: 1400/1563 - train loss: 8.9574 - test loss: 13.6495 - train acc: 0.5550 - test acc: 0.4136 - 11m 31s\n",
      "batch: 1500/1563 - train loss: 8.7157 - test loss: 14.0432 - train acc: 0.5619 - test acc: 0.4020 - 11m 37s\n",
      "batch: 1563/1563 - train loss: 8.7306 - test loss: 14.9170 - train acc: 0.5569 - test acc: 0.3722 - 11m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.5217 - test loss: 13.8279 - train acc: 0.6631 - test acc: 0.4217 - 11m 47s\n",
      "batch: 200/1563 - train loss: 6.6940 - test loss: 14.4483 - train acc: 0.6538 - test acc: 0.4020 - 11m 52s\n",
      "batch: 300/1563 - train loss: 6.9881 - test loss: 14.3473 - train acc: 0.6372 - test acc: 0.4127 - 11m 58s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 400/1563 - train loss: 7.3324 - test loss: 13.6058 - train acc: 0.6197 - test acc: 0.4285 - 12m 4s\n",
      "batch: 500/1563 - train loss: 7.4315 - test loss: 14.1041 - train acc: 0.6219 - test acc: 0.4153 - 12m 9s\n",
      "batch: 600/1563 - train loss: 7.4614 - test loss: 13.9213 - train acc: 0.6197 - test acc: 0.4179 - 12m 15s\n",
      "batch: 700/1563 - train loss: 7.2891 - test loss: 14.7276 - train acc: 0.6269 - test acc: 0.4030 - 12m 20s\n",
      "batch: 800/1563 - train loss: 7.6618 - test loss: 15.0423 - train acc: 0.5987 - test acc: 0.3929 - 12m 26s\n",
      "batch: 900/1563 - train loss: 7.7504 - test loss: 14.0927 - train acc: 0.6047 - test acc: 0.4226 - 12m 31s\n",
      "batch: 1000/1563 - train loss: 8.4909 - test loss: 14.1887 - train acc: 0.5662 - test acc: 0.4095 - 12m 37s\n",
      "batch: 1100/1563 - train loss: 7.8233 - test loss: 14.6712 - train acc: 0.6094 - test acc: 0.4025 - 12m 42s\n",
      "batch: 1200/1563 - train loss: 8.0544 - test loss: 13.4991 - train acc: 0.5855 - test acc: 0.4240 - 12m 48s\n",
      "batch: 1300/1563 - train loss: 8.0674 - test loss: 13.9194 - train acc: 0.5937 - test acc: 0.4165 - 12m 53s\n",
      "batch: 1400/1563 - train loss: 8.1336 - test loss: 13.8391 - train acc: 0.6050 - test acc: 0.4209 - 12m 59s\n",
      "batch: 1500/1563 - train loss: 8.1661 - test loss: 13.7843 - train acc: 0.5819 - test acc: 0.4217 - 13m 5s\n",
      "batch: 1563/1563 - train loss: 8.2994 - test loss: 13.9078 - train acc: 0.5853 - test acc: 0.4187 - 13m 9s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 6.0144 - test loss: 14.1911 - train acc: 0.6891 - test acc: 0.4191 - 13m 15s\n",
      "batch: 200/1563 - train loss: 5.9300 - test loss: 14.1004 - train acc: 0.6816 - test acc: 0.4287 - 13m 21s\n",
      "batch: 300/1563 - train loss: 5.8280 - test loss: 14.0539 - train acc: 0.6875 - test acc: 0.4178 - 13m 27s\n",
      "batch: 400/1563 - train loss: 6.1142 - test loss: 14.5804 - train acc: 0.6753 - test acc: 0.4193 - 13m 33s\n",
      "batch: 500/1563 - train loss: 6.2687 - test loss: 14.3699 - train acc: 0.6794 - test acc: 0.4247 - 13m 39s\n",
      "batch: 600/1563 - train loss: 6.6765 - test loss: 14.6313 - train acc: 0.6582 - test acc: 0.4164 - 13m 44s\n",
      "batch: 700/1563 - train loss: 6.6073 - test loss: 14.9399 - train acc: 0.6582 - test acc: 0.4087 - 13m 49s\n",
      "batch: 800/1563 - train loss: 6.9886 - test loss: 14.1368 - train acc: 0.6431 - test acc: 0.4231 - 13m 55s\n",
      "batch: 900/1563 - train loss: 6.7884 - test loss: 14.8054 - train acc: 0.6488 - test acc: 0.4107 - 14m 1s\n",
      "batch: 1000/1563 - train loss: 7.0351 - test loss: 14.4455 - train acc: 0.6269 - test acc: 0.4186 - 14m 6s\n",
      "batch: 1100/1563 - train loss: 6.8487 - test loss: 14.5376 - train acc: 0.6456 - test acc: 0.4111 - 14m 12s\n",
      "batch: 1200/1563 - train loss: 6.9503 - test loss: 14.1610 - train acc: 0.6356 - test acc: 0.4257 - 14m 18s\n",
      "batch: 1300/1563 - train loss: 7.5080 - test loss: 15.0758 - train acc: 0.6131 - test acc: 0.3947 - 14m 25s\n",
      "batch: 1400/1563 - train loss: 7.3395 - test loss: 14.4656 - train acc: 0.6178 - test acc: 0.4243 - 14m 30s\n",
      "batch: 1500/1563 - train loss: 7.1041 - test loss: 14.3613 - train acc: 0.6316 - test acc: 0.4160 - 14m 36s\n",
      "batch: 1563/1563 - train loss: 7.1758 - test loss: 14.1455 - train acc: 0.6294 - test acc: 0.4423 - 14m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.5968 - test loss: 14.4542 - train acc: 0.7453 - test acc: 0.4344 - 14m 47s\n",
      "batch: 200/1563 - train loss: 4.9823 - test loss: 14.9586 - train acc: 0.7241 - test acc: 0.4199 - 14m 52s\n",
      "batch: 300/1563 - train loss: 5.0790 - test loss: 15.0199 - train acc: 0.7328 - test acc: 0.4240 - 14m 58s\n",
      "batch: 400/1563 - train loss: 5.5065 - test loss: 15.0314 - train acc: 0.7088 - test acc: 0.4280 - 15m 3s\n",
      "batch: 500/1563 - train loss: 5.1988 - test loss: 15.0270 - train acc: 0.7253 - test acc: 0.4160 - 15m 9s\n",
      "batch: 600/1563 - train loss: 5.8010 - test loss: 14.6606 - train acc: 0.7050 - test acc: 0.4288 - 15m 14s\n",
      "batch: 700/1563 - train loss: 5.6283 - test loss: 14.8688 - train acc: 0.7031 - test acc: 0.4256 - 15m 20s\n",
      "batch: 800/1563 - train loss: 5.7814 - test loss: 14.8597 - train acc: 0.6862 - test acc: 0.4268 - 15m 25s\n",
      "batch: 900/1563 - train loss: 5.7975 - test loss: 15.4976 - train acc: 0.6932 - test acc: 0.4080 - 15m 31s\n",
      "batch: 1000/1563 - train loss: 6.0723 - test loss: 14.7602 - train acc: 0.6744 - test acc: 0.4276 - 15m 36s\n",
      "batch: 1100/1563 - train loss: 6.2362 - test loss: 14.9212 - train acc: 0.6715 - test acc: 0.4153 - 15m 42s\n",
      "batch: 1200/1563 - train loss: 6.2635 - test loss: 14.8891 - train acc: 0.6713 - test acc: 0.4200 - 15m 47s\n",
      "batch: 1300/1563 - train loss: 6.5022 - test loss: 14.6304 - train acc: 0.6631 - test acc: 0.4178 - 15m 53s\n",
      "batch: 1400/1563 - train loss: 6.3779 - test loss: 15.0075 - train acc: 0.6666 - test acc: 0.4256 - 15m 58s\n",
      "batch: 1500/1563 - train loss: 6.6091 - test loss: 15.0480 - train acc: 0.6469 - test acc: 0.4203 - 16m 4s\n",
      "batch: 1563/1563 - train loss: 6.6398 - test loss: 14.9760 - train acc: 0.6607 - test acc: 0.4268 - 16m 8s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.1336 - test loss: 15.4332 - train acc: 0.7794 - test acc: 0.4310 - 16m 14s\n",
      "batch: 200/1563 - train loss: 4.1960 - test loss: 15.2661 - train acc: 0.7712 - test acc: 0.4326 - 16m 19s\n",
      "batch: 300/1563 - train loss: 4.3225 - test loss: 15.7550 - train acc: 0.7640 - test acc: 0.4217 - 16m 25s\n",
      "batch: 400/1563 - train loss: 4.4062 - test loss: 15.6271 - train acc: 0.7516 - test acc: 0.4261 - 16m 30s\n",
      "batch: 500/1563 - train loss: 4.6334 - test loss: 15.6509 - train acc: 0.7385 - test acc: 0.4241 - 16m 35s\n",
      "batch: 600/1563 - train loss: 4.8829 - test loss: 15.7498 - train acc: 0.7312 - test acc: 0.4191 - 16m 41s\n",
      "batch: 700/1563 - train loss: 5.0070 - test loss: 16.7998 - train acc: 0.7316 - test acc: 0.4053 - 16m 46s\n",
      "batch: 800/1563 - train loss: 5.0886 - test loss: 15.5672 - train acc: 0.7178 - test acc: 0.4275 - 16m 52s\n",
      "batch: 900/1563 - train loss: 5.3466 - test loss: 17.3330 - train acc: 0.7172 - test acc: 0.3867 - 16m 57s\n",
      "batch: 1000/1563 - train loss: 5.4395 - test loss: 15.4556 - train acc: 0.7038 - test acc: 0.4273 - 17m 3s\n",
      "batch: 1100/1563 - train loss: 5.3790 - test loss: 15.2566 - train acc: 0.7125 - test acc: 0.4227 - 17m 8s\n",
      "batch: 1200/1563 - train loss: 5.5469 - test loss: 15.4239 - train acc: 0.7047 - test acc: 0.4226 - 17m 13s\n",
      "batch: 1300/1563 - train loss: 5.5165 - test loss: 16.5996 - train acc: 0.7050 - test acc: 0.3909 - 17m 19s\n",
      "batch: 1400/1563 - train loss: 5.5515 - test loss: 15.2311 - train acc: 0.7069 - test acc: 0.4302 - 17m 25s\n",
      "batch: 1500/1563 - train loss: 5.8513 - test loss: 15.3371 - train acc: 0.6940 - test acc: 0.4206 - 17m 30s\n",
      "batch: 1563/1563 - train loss: 5.8336 - test loss: 15.0219 - train acc: 0.6856 - test acc: 0.4308 - 17m 35s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.4726 - test loss: 15.6428 - train acc: 0.8037 - test acc: 0.4289 - 17m 40s\n",
      "batch: 200/1563 - train loss: 3.5585 - test loss: 15.9990 - train acc: 0.8118 - test acc: 0.4248 - 17m 46s\n",
      "batch: 300/1563 - train loss: 3.6317 - test loss: 16.2362 - train acc: 0.7971 - test acc: 0.4208 - 17m 51s\n",
      "batch: 400/1563 - train loss: 3.8036 - test loss: 16.3907 - train acc: 0.7937 - test acc: 0.4208 - 17m 57s\n",
      "batch: 500/1563 - train loss: 3.9238 - test loss: 16.4161 - train acc: 0.7809 - test acc: 0.4241 - 18m 2s\n",
      "batch: 600/1563 - train loss: 4.0564 - test loss: 15.7417 - train acc: 0.7818 - test acc: 0.4302 - 18m 7s\n",
      "batch: 700/1563 - train loss: 4.1838 - test loss: 16.1553 - train acc: 0.7725 - test acc: 0.4316 - 18m 12s\n",
      "batch: 800/1563 - train loss: 4.2298 - test loss: 16.1858 - train acc: 0.7625 - test acc: 0.4223 - 18m 18s\n",
      "batch: 900/1563 - train loss: 4.2762 - test loss: 16.8078 - train acc: 0.7716 - test acc: 0.4103 - 18m 23s\n",
      "batch: 1000/1563 - train loss: 4.3418 - test loss: 16.2124 - train acc: 0.7663 - test acc: 0.4192 - 18m 29s\n",
      "batch: 1100/1563 - train loss: 4.6403 - test loss: 15.7560 - train acc: 0.7500 - test acc: 0.4289 - 18m 34s\n",
      "batch: 1200/1563 - train loss: 4.7122 - test loss: 16.5186 - train acc: 0.7416 - test acc: 0.4191 - 18m 40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1300/1563 - train loss: 4.8312 - test loss: 15.7780 - train acc: 0.7372 - test acc: 0.4309 - 18m 45s\n",
      "batch: 1400/1563 - train loss: 4.9465 - test loss: 15.6844 - train acc: 0.7309 - test acc: 0.4278 - 18m 51s\n",
      "batch: 1500/1563 - train loss: 4.8061 - test loss: 16.5530 - train acc: 0.7422 - test acc: 0.4121 - 18m 57s\n",
      "batch: 1563/1563 - train loss: 5.3224 - test loss: 16.7108 - train acc: 0.7176 - test acc: 0.4124 - 19m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 3.0252 - test loss: 16.1671 - train acc: 0.8305 - test acc: 0.4294 - 19m 6s\n",
      "batch: 200/1563 - train loss: 2.8816 - test loss: 17.0599 - train acc: 0.8396 - test acc: 0.4226 - 19m 12s\n",
      "batch: 300/1563 - train loss: 3.0206 - test loss: 16.3133 - train acc: 0.8381 - test acc: 0.4355 - 19m 17s\n",
      "batch: 400/1563 - train loss: 3.2123 - test loss: 16.5153 - train acc: 0.8315 - test acc: 0.4310 - 19m 23s\n",
      "batch: 500/1563 - train loss: 3.3888 - test loss: 16.7835 - train acc: 0.8090 - test acc: 0.4312 - 19m 30s\n",
      "batch: 600/1563 - train loss: 3.6999 - test loss: 15.9847 - train acc: 0.7961 - test acc: 0.4344 - 19m 36s\n",
      "batch: 700/1563 - train loss: 3.5126 - test loss: 17.9382 - train acc: 0.8006 - test acc: 0.4125 - 19m 41s\n",
      "batch: 800/1563 - train loss: 3.6330 - test loss: 16.7278 - train acc: 0.8056 - test acc: 0.4179 - 19m 47s\n",
      "batch: 900/1563 - train loss: 3.6051 - test loss: 16.3134 - train acc: 0.8040 - test acc: 0.4369 - 19m 52s\n",
      "batch: 1000/1563 - train loss: 3.9037 - test loss: 16.4245 - train acc: 0.7893 - test acc: 0.4296 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 1001/1563 - train loss: 3.8746 - test loss: 16.4039 - train acc: 0.7903 - test acc: 0.4286 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 17\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 25.9393 - test loss: 25.2189 - train acc: 0.0386 - test acc: 0.0616 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.7664 - test loss: 24.1436 - train acc: 0.0783 - test acc: 0.0730 - 0m 8s\n",
      "batch: 300/1563 - train loss: 22.9490 - test loss: 22.0303 - train acc: 0.0896 - test acc: 0.1014 - 0m 14s\n",
      "batch: 400/1563 - train loss: 22.1843 - test loss: 21.3500 - train acc: 0.0956 - test acc: 0.1187 - 0m 19s\n",
      "batch: 500/1563 - train loss: 21.5981 - test loss: 21.2172 - train acc: 0.1175 - test acc: 0.1323 - 0m 25s\n",
      "batch: 600/1563 - train loss: 21.1780 - test loss: 21.7110 - train acc: 0.1316 - test acc: 0.1257 - 0m 30s\n",
      "batch: 700/1563 - train loss: 20.7889 - test loss: 20.2134 - train acc: 0.1334 - test acc: 0.1431 - 0m 36s\n",
      "batch: 800/1563 - train loss: 20.4988 - test loss: 19.8084 - train acc: 0.1434 - test acc: 0.1652 - 0m 42s\n",
      "batch: 900/1563 - train loss: 19.9386 - test loss: 21.6109 - train acc: 0.1638 - test acc: 0.1268 - 0m 47s\n",
      "batch: 1000/1563 - train loss: 19.6620 - test loss: 19.0742 - train acc: 0.1725 - test acc: 0.1924 - 0m 52s\n",
      "batch: 1100/1563 - train loss: 19.6388 - test loss: 19.0990 - train acc: 0.1810 - test acc: 0.1922 - 0m 58s\n",
      "batch: 1200/1563 - train loss: 19.1342 - test loss: 19.2139 - train acc: 0.1813 - test acc: 0.1920 - 1m 3s\n",
      "batch: 1300/1563 - train loss: 19.0174 - test loss: 19.7379 - train acc: 0.1875 - test acc: 0.1718 - 1m 8s\n",
      "batch: 1400/1563 - train loss: 18.7955 - test loss: 18.8553 - train acc: 0.2003 - test acc: 0.2066 - 1m 14s\n",
      "batch: 1500/1563 - train loss: 18.3724 - test loss: 18.2932 - train acc: 0.2141 - test acc: 0.2120 - 1m 19s\n",
      "batch: 1563/1563 - train loss: 18.4155 - test loss: 18.0539 - train acc: 0.2088 - test acc: 0.2293 - 1m 24s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.5047 - test loss: 17.6009 - train acc: 0.2350 - test acc: 0.2412 - 1m 29s\n",
      "batch: 200/1563 - train loss: 17.1655 - test loss: 17.8081 - train acc: 0.2484 - test acc: 0.2388 - 1m 35s\n",
      "batch: 300/1563 - train loss: 17.7861 - test loss: 18.2200 - train acc: 0.2351 - test acc: 0.2328 - 1m 40s\n",
      "batch: 400/1563 - train loss: 17.4409 - test loss: 17.3597 - train acc: 0.2379 - test acc: 0.2536 - 1m 46s\n",
      "batch: 500/1563 - train loss: 17.1266 - test loss: 17.4046 - train acc: 0.2522 - test acc: 0.2438 - 1m 52s\n",
      "batch: 600/1563 - train loss: 17.1086 - test loss: 17.4304 - train acc: 0.2534 - test acc: 0.2436 - 1m 57s\n",
      "batch: 700/1563 - train loss: 16.8540 - test loss: 17.2656 - train acc: 0.2609 - test acc: 0.2504 - 2m 3s\n",
      "batch: 800/1563 - train loss: 16.8696 - test loss: 17.4380 - train acc: 0.2647 - test acc: 0.2397 - 2m 9s\n",
      "batch: 900/1563 - train loss: 16.6463 - test loss: 16.9375 - train acc: 0.2600 - test acc: 0.2714 - 2m 14s\n",
      "batch: 1000/1563 - train loss: 16.7121 - test loss: 17.0288 - train acc: 0.2712 - test acc: 0.2547 - 2m 20s\n",
      "batch: 1100/1563 - train loss: 16.4696 - test loss: 16.3717 - train acc: 0.2756 - test acc: 0.2834 - 2m 25s\n",
      "batch: 1200/1563 - train loss: 16.3646 - test loss: 16.3128 - train acc: 0.2740 - test acc: 0.2872 - 2m 30s\n",
      "batch: 1300/1563 - train loss: 16.3105 - test loss: 16.2785 - train acc: 0.2765 - test acc: 0.2805 - 2m 36s\n",
      "batch: 1400/1563 - train loss: 16.2588 - test loss: 16.0948 - train acc: 0.2772 - test acc: 0.2936 - 2m 41s\n",
      "batch: 1500/1563 - train loss: 15.6983 - test loss: 17.6713 - train acc: 0.2991 - test acc: 0.2516 - 2m 47s\n",
      "batch: 1563/1563 - train loss: 16.1242 - test loss: 16.3958 - train acc: 0.2909 - test acc: 0.2779 - 2m 51s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.9088 - test loss: 15.6414 - train acc: 0.3249 - test acc: 0.3055 - 2m 56s\n",
      "batch: 200/1563 - train loss: 15.0382 - test loss: 19.5311 - train acc: 0.3080 - test acc: 0.2187 - 3m 2s\n",
      "batch: 300/1563 - train loss: 14.9543 - test loss: 15.7338 - train acc: 0.3277 - test acc: 0.3075 - 3m 7s\n",
      "batch: 400/1563 - train loss: 14.8822 - test loss: 16.2245 - train acc: 0.3215 - test acc: 0.2970 - 3m 12s\n",
      "batch: 500/1563 - train loss: 15.2401 - test loss: 15.6460 - train acc: 0.3274 - test acc: 0.3027 - 3m 18s\n",
      "batch: 600/1563 - train loss: 14.9253 - test loss: 15.7392 - train acc: 0.3310 - test acc: 0.3025 - 3m 24s\n",
      "batch: 700/1563 - train loss: 14.7013 - test loss: 15.3766 - train acc: 0.3356 - test acc: 0.3235 - 3m 29s\n",
      "batch: 800/1563 - train loss: 14.6144 - test loss: 15.3119 - train acc: 0.3418 - test acc: 0.3205 - 3m 34s\n",
      "batch: 900/1563 - train loss: 14.3139 - test loss: 15.5042 - train acc: 0.3562 - test acc: 0.3137 - 3m 40s\n",
      "batch: 1000/1563 - train loss: 14.4124 - test loss: 14.9308 - train acc: 0.3431 - test acc: 0.3286 - 3m 45s\n",
      "batch: 1100/1563 - train loss: 14.8468 - test loss: 15.8321 - train acc: 0.3378 - test acc: 0.3063 - 3m 51s\n",
      "batch: 1200/1563 - train loss: 14.5001 - test loss: 15.0693 - train acc: 0.3462 - test acc: 0.3258 - 3m 56s\n",
      "batch: 1300/1563 - train loss: 14.6207 - test loss: 15.0404 - train acc: 0.3535 - test acc: 0.3292 - 4m 2s\n",
      "batch: 1400/1563 - train loss: 14.4959 - test loss: 15.5922 - train acc: 0.3450 - test acc: 0.3111 - 4m 7s\n",
      "batch: 1500/1563 - train loss: 14.2365 - test loss: 14.3798 - train acc: 0.3550 - test acc: 0.3446 - 4m 12s\n",
      "batch: 1563/1563 - train loss: 14.5076 - test loss: 14.9368 - train acc: 0.3410 - test acc: 0.3247 - 4m 16s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.1351 - test loss: 15.1826 - train acc: 0.3853 - test acc: 0.3331 - 4m 22s\n",
      "batch: 200/1563 - train loss: 12.9624 - test loss: 15.5714 - train acc: 0.3969 - test acc: 0.3224 - 4m 28s\n",
      "batch: 300/1563 - train loss: 12.8709 - test loss: 14.5417 - train acc: 0.3934 - test acc: 0.3508 - 4m 34s\n",
      "batch: 400/1563 - train loss: 12.9764 - test loss: 14.8461 - train acc: 0.3944 - test acc: 0.3479 - 4m 39s\n",
      "batch: 500/1563 - train loss: 13.0753 - test loss: 14.5553 - train acc: 0.3984 - test acc: 0.3504 - 4m 44s\n",
      "batch: 600/1563 - train loss: 12.9807 - test loss: 16.8671 - train acc: 0.3988 - test acc: 0.2809 - 4m 50s\n",
      "batch: 700/1563 - train loss: 13.0585 - test loss: 15.2311 - train acc: 0.4022 - test acc: 0.3365 - 4m 55s\n",
      "batch: 800/1563 - train loss: 13.1007 - test loss: 14.3497 - train acc: 0.3956 - test acc: 0.3517 - 5m 1s\n",
      "batch: 900/1563 - train loss: 13.0979 - test loss: 15.1619 - train acc: 0.3819 - test acc: 0.3299 - 5m 6s\n",
      "batch: 1000/1563 - train loss: 13.3424 - test loss: 14.4821 - train acc: 0.3973 - test acc: 0.3519 - 5m 11s\n",
      "batch: 1100/1563 - train loss: 13.2970 - test loss: 15.0204 - train acc: 0.3915 - test acc: 0.3378 - 5m 17s\n",
      "batch: 1200/1563 - train loss: 12.9417 - test loss: 15.9787 - train acc: 0.3966 - test acc: 0.3068 - 5m 22s\n",
      "batch: 1300/1563 - train loss: 13.1815 - test loss: 13.9896 - train acc: 0.3899 - test acc: 0.3707 - 5m 28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.1407 - test loss: 14.1630 - train acc: 0.3925 - test acc: 0.3644 - 5m 33s\n",
      "batch: 1500/1563 - train loss: 13.4940 - test loss: 13.7808 - train acc: 0.3819 - test acc: 0.3789 - 5m 38s\n",
      "batch: 1563/1563 - train loss: 13.2106 - test loss: 15.0598 - train acc: 0.3900 - test acc: 0.3445 - 5m 43s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.4931 - test loss: 14.3092 - train acc: 0.4441 - test acc: 0.3632 - 5m 48s\n",
      "batch: 200/1563 - train loss: 11.3381 - test loss: 14.7987 - train acc: 0.4506 - test acc: 0.3567 - 5m 53s\n",
      "batch: 300/1563 - train loss: 11.5236 - test loss: 14.2380 - train acc: 0.4479 - test acc: 0.3727 - 5m 59s\n",
      "batch: 400/1563 - train loss: 11.5197 - test loss: 15.1620 - train acc: 0.4466 - test acc: 0.3557 - 6m 4s\n",
      "batch: 500/1563 - train loss: 12.1647 - test loss: 16.2367 - train acc: 0.4294 - test acc: 0.3044 - 6m 9s\n",
      "batch: 600/1563 - train loss: 11.8423 - test loss: 13.7176 - train acc: 0.4294 - test acc: 0.3851 - 6m 14s\n",
      "batch: 700/1563 - train loss: 12.0592 - test loss: 14.2415 - train acc: 0.4228 - test acc: 0.3639 - 6m 19s\n",
      "batch: 800/1563 - train loss: 12.0648 - test loss: 14.2528 - train acc: 0.4460 - test acc: 0.3738 - 6m 25s\n",
      "batch: 900/1563 - train loss: 11.9959 - test loss: 15.3236 - train acc: 0.4319 - test acc: 0.3334 - 6m 30s\n",
      "batch: 1000/1563 - train loss: 11.9598 - test loss: 13.4883 - train acc: 0.4422 - test acc: 0.3908 - 6m 36s\n",
      "batch: 1100/1563 - train loss: 11.8304 - test loss: 13.8978 - train acc: 0.4460 - test acc: 0.3850 - 6m 41s\n",
      "batch: 1200/1563 - train loss: 11.9387 - test loss: 15.7281 - train acc: 0.4359 - test acc: 0.3300 - 6m 46s\n",
      "batch: 1300/1563 - train loss: 11.8846 - test loss: 15.5736 - train acc: 0.4434 - test acc: 0.3419 - 6m 51s\n",
      "batch: 1400/1563 - train loss: 12.1504 - test loss: 13.6907 - train acc: 0.4347 - test acc: 0.3844 - 6m 57s\n",
      "batch: 1500/1563 - train loss: 11.8897 - test loss: 13.8317 - train acc: 0.4322 - test acc: 0.3855 - 7m 2s\n",
      "batch: 1563/1563 - train loss: 12.0949 - test loss: 14.6296 - train acc: 0.4391 - test acc: 0.3558 - 7m 7s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.2982 - test loss: 14.4857 - train acc: 0.4997 - test acc: 0.3606 - 7m 12s\n",
      "batch: 200/1563 - train loss: 10.1211 - test loss: 13.5859 - train acc: 0.5149 - test acc: 0.3921 - 7m 17s\n",
      "batch: 300/1563 - train loss: 10.4763 - test loss: 14.0890 - train acc: 0.4907 - test acc: 0.3745 - 7m 22s\n",
      "batch: 400/1563 - train loss: 10.4510 - test loss: 14.0548 - train acc: 0.4975 - test acc: 0.3871 - 7m 28s\n",
      "batch: 500/1563 - train loss: 10.4295 - test loss: 14.1742 - train acc: 0.4938 - test acc: 0.3808 - 7m 33s\n",
      "batch: 600/1563 - train loss: 10.5952 - test loss: 14.3365 - train acc: 0.4847 - test acc: 0.3758 - 7m 39s\n",
      "batch: 700/1563 - train loss: 10.6120 - test loss: 13.4428 - train acc: 0.4947 - test acc: 0.3998 - 7m 44s\n",
      "batch: 800/1563 - train loss: 10.9803 - test loss: 14.1709 - train acc: 0.4837 - test acc: 0.3812 - 7m 49s\n",
      "batch: 900/1563 - train loss: 10.8151 - test loss: 14.0514 - train acc: 0.4766 - test acc: 0.3801 - 7m 54s\n",
      "batch: 1000/1563 - train loss: 10.9207 - test loss: 13.7761 - train acc: 0.4734 - test acc: 0.3904 - 8m 0s\n",
      "batch: 1100/1563 - train loss: 10.7685 - test loss: 14.0796 - train acc: 0.4847 - test acc: 0.3826 - 8m 5s\n",
      "batch: 1200/1563 - train loss: 10.8648 - test loss: 13.2756 - train acc: 0.4722 - test acc: 0.4124 - 8m 11s\n",
      "batch: 1300/1563 - train loss: 10.7287 - test loss: 13.3301 - train acc: 0.4798 - test acc: 0.4085 - 8m 16s\n",
      "batch: 1400/1563 - train loss: 11.1516 - test loss: 13.3214 - train acc: 0.4647 - test acc: 0.4007 - 8m 21s\n",
      "batch: 1500/1563 - train loss: 10.8635 - test loss: 13.7685 - train acc: 0.4859 - test acc: 0.3933 - 8m 26s\n",
      "batch: 1563/1563 - train loss: 10.8417 - test loss: 13.3202 - train acc: 0.4868 - test acc: 0.4106 - 8m 31s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.5276 - test loss: 13.4926 - train acc: 0.5715 - test acc: 0.4087 - 8m 37s\n",
      "batch: 200/1563 - train loss: 8.8235 - test loss: 13.6380 - train acc: 0.5609 - test acc: 0.4078 - 8m 42s\n",
      "batch: 300/1563 - train loss: 9.0096 - test loss: 13.9271 - train acc: 0.5497 - test acc: 0.3930 - 8m 47s\n",
      "batch: 400/1563 - train loss: 9.5839 - test loss: 13.8294 - train acc: 0.5197 - test acc: 0.4017 - 8m 52s\n",
      "batch: 500/1563 - train loss: 9.4555 - test loss: 14.0257 - train acc: 0.5269 - test acc: 0.3957 - 8m 58s\n",
      "batch: 600/1563 - train loss: 9.5199 - test loss: 14.2360 - train acc: 0.5343 - test acc: 0.3886 - 9m 3s\n",
      "batch: 700/1563 - train loss: 9.7231 - test loss: 14.3093 - train acc: 0.5209 - test acc: 0.3804 - 9m 9s\n",
      "batch: 800/1563 - train loss: 9.5347 - test loss: 13.6148 - train acc: 0.5222 - test acc: 0.4067 - 9m 14s\n",
      "batch: 900/1563 - train loss: 9.5931 - test loss: 13.8572 - train acc: 0.5297 - test acc: 0.3968 - 9m 20s\n",
      "batch: 1000/1563 - train loss: 9.6582 - test loss: 13.3647 - train acc: 0.5327 - test acc: 0.4124 - 9m 25s\n",
      "batch: 1100/1563 - train loss: 9.6927 - test loss: 14.2895 - train acc: 0.5435 - test acc: 0.3857 - 9m 30s\n",
      "batch: 1200/1563 - train loss: 9.8413 - test loss: 13.5374 - train acc: 0.5144 - test acc: 0.4035 - 9m 35s\n",
      "batch: 1300/1563 - train loss: 9.7827 - test loss: 13.7552 - train acc: 0.5241 - test acc: 0.3942 - 9m 41s\n",
      "batch: 1400/1563 - train loss: 10.0557 - test loss: 13.1204 - train acc: 0.5172 - test acc: 0.4176 - 9m 46s\n",
      "batch: 1500/1563 - train loss: 9.8141 - test loss: 13.2156 - train acc: 0.5150 - test acc: 0.4164 - 9m 52s\n",
      "batch: 1563/1563 - train loss: 9.9620 - test loss: 13.3953 - train acc: 0.5134 - test acc: 0.4060 - 9m 57s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.7759 - test loss: 13.9887 - train acc: 0.5963 - test acc: 0.4000 - 10m 2s\n",
      "batch: 200/1563 - train loss: 7.6619 - test loss: 13.6423 - train acc: 0.6146 - test acc: 0.4138 - 10m 8s\n",
      "batch: 300/1563 - train loss: 8.0889 - test loss: 13.8813 - train acc: 0.5934 - test acc: 0.4085 - 10m 13s\n",
      "batch: 400/1563 - train loss: 8.1790 - test loss: 13.8684 - train acc: 0.5891 - test acc: 0.4195 - 10m 19s\n",
      "batch: 500/1563 - train loss: 8.3792 - test loss: 14.0087 - train acc: 0.5859 - test acc: 0.4097 - 10m 24s\n",
      "batch: 600/1563 - train loss: 8.3477 - test loss: 15.1096 - train acc: 0.5743 - test acc: 0.3881 - 10m 29s\n",
      "batch: 700/1563 - train loss: 8.6020 - test loss: 14.1011 - train acc: 0.5547 - test acc: 0.4041 - 10m 35s\n",
      "batch: 800/1563 - train loss: 8.4146 - test loss: 13.7022 - train acc: 0.5828 - test acc: 0.4189 - 10m 40s\n",
      "batch: 900/1563 - train loss: 8.6827 - test loss: 14.3091 - train acc: 0.5647 - test acc: 0.3953 - 10m 46s\n",
      "batch: 1000/1563 - train loss: 8.7979 - test loss: 14.6242 - train acc: 0.5516 - test acc: 0.3891 - 10m 51s\n",
      "batch: 1100/1563 - train loss: 8.6710 - test loss: 13.5007 - train acc: 0.5771 - test acc: 0.4143 - 10m 56s\n",
      "batch: 1200/1563 - train loss: 9.0586 - test loss: 13.7574 - train acc: 0.5459 - test acc: 0.4173 - 11m 2s\n",
      "batch: 1300/1563 - train loss: 8.9479 - test loss: 17.7913 - train acc: 0.5624 - test acc: 0.3398 - 11m 7s\n",
      "batch: 1400/1563 - train loss: 9.0672 - test loss: 13.2930 - train acc: 0.5572 - test acc: 0.4224 - 11m 12s\n",
      "batch: 1500/1563 - train loss: 9.0398 - test loss: 14.2030 - train acc: 0.5444 - test acc: 0.4015 - 11m 18s\n",
      "batch: 1563/1563 - train loss: 9.1550 - test loss: 13.6087 - train acc: 0.5494 - test acc: 0.4122 - 11m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4287 - test loss: 13.6642 - train acc: 0.6644 - test acc: 0.4283 - 11m 27s\n",
      "batch: 200/1563 - train loss: 6.9519 - test loss: 13.8558 - train acc: 0.6413 - test acc: 0.4185 - 11m 33s\n",
      "batch: 300/1563 - train loss: 6.8716 - test loss: 13.9490 - train acc: 0.6550 - test acc: 0.4130 - 11m 38s\n",
      "batch: 400/1563 - train loss: 6.9902 - test loss: 13.9720 - train acc: 0.6466 - test acc: 0.4273 - 11m 43s\n",
      "batch: 500/1563 - train loss: 7.2698 - test loss: 14.6493 - train acc: 0.6369 - test acc: 0.4029 - 11m 49s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.7082 - test loss: 13.7641 - train acc: 0.6091 - test acc: 0.4216 - 11m 54s\n",
      "batch: 700/1563 - train loss: 7.5006 - test loss: 13.9941 - train acc: 0.6225 - test acc: 0.4168 - 12m 0s\n",
      "batch: 800/1563 - train loss: 7.4369 - test loss: 13.7995 - train acc: 0.6228 - test acc: 0.4235 - 12m 5s\n",
      "batch: 900/1563 - train loss: 7.7453 - test loss: 14.4181 - train acc: 0.6072 - test acc: 0.3992 - 12m 10s\n",
      "batch: 1000/1563 - train loss: 7.9457 - test loss: 13.7456 - train acc: 0.6003 - test acc: 0.4188 - 12m 16s\n",
      "batch: 1100/1563 - train loss: 7.7842 - test loss: 14.4815 - train acc: 0.6059 - test acc: 0.4042 - 12m 21s\n",
      "batch: 1200/1563 - train loss: 7.9255 - test loss: 14.1188 - train acc: 0.6122 - test acc: 0.4112 - 12m 27s\n",
      "batch: 1300/1563 - train loss: 8.1501 - test loss: 13.9683 - train acc: 0.5900 - test acc: 0.4106 - 12m 32s\n",
      "batch: 1400/1563 - train loss: 7.9410 - test loss: 13.5186 - train acc: 0.5887 - test acc: 0.4285 - 12m 37s\n",
      "batch: 1500/1563 - train loss: 7.7138 - test loss: 14.5626 - train acc: 0.6087 - test acc: 0.4008 - 12m 43s\n",
      "batch: 1563/1563 - train loss: 8.0434 - test loss: 13.9779 - train acc: 0.6031 - test acc: 0.4167 - 12m 47s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.6272 - test loss: 15.1560 - train acc: 0.7057 - test acc: 0.4042 - 12m 53s\n",
      "batch: 200/1563 - train loss: 5.7416 - test loss: 14.2224 - train acc: 0.7001 - test acc: 0.4267 - 12m 58s\n",
      "batch: 300/1563 - train loss: 5.9232 - test loss: 14.4180 - train acc: 0.6819 - test acc: 0.4257 - 13m 3s\n",
      "batch: 400/1563 - train loss: 6.0468 - test loss: 14.2387 - train acc: 0.6819 - test acc: 0.4273 - 13m 9s\n",
      "batch: 500/1563 - train loss: 6.3011 - test loss: 14.6662 - train acc: 0.6732 - test acc: 0.4180 - 13m 14s\n",
      "batch: 600/1563 - train loss: 6.8776 - test loss: 14.3838 - train acc: 0.6378 - test acc: 0.4164 - 13m 19s\n",
      "batch: 700/1563 - train loss: 6.5960 - test loss: 15.4247 - train acc: 0.6572 - test acc: 0.3895 - 13m 25s\n",
      "batch: 800/1563 - train loss: 6.8385 - test loss: 15.1290 - train acc: 0.6382 - test acc: 0.4053 - 13m 30s\n",
      "batch: 900/1563 - train loss: 6.6566 - test loss: 14.3448 - train acc: 0.6519 - test acc: 0.4180 - 13m 36s\n",
      "batch: 1000/1563 - train loss: 6.8198 - test loss: 14.3074 - train acc: 0.6438 - test acc: 0.4254 - 13m 41s\n",
      "batch: 1100/1563 - train loss: 7.0210 - test loss: 14.2319 - train acc: 0.6331 - test acc: 0.4133 - 13m 46s\n",
      "batch: 1200/1563 - train loss: 6.9836 - test loss: 14.0916 - train acc: 0.6344 - test acc: 0.4193 - 13m 52s\n",
      "batch: 1300/1563 - train loss: 7.2530 - test loss: 14.1969 - train acc: 0.6312 - test acc: 0.4188 - 13m 57s\n",
      "batch: 1400/1563 - train loss: 7.2487 - test loss: 15.2846 - train acc: 0.6266 - test acc: 0.3902 - 14m 3s\n",
      "batch: 1500/1563 - train loss: 7.3012 - test loss: 14.2968 - train acc: 0.6212 - test acc: 0.4179 - 14m 8s\n",
      "batch: 1563/1563 - train loss: 7.4726 - test loss: 15.2354 - train acc: 0.6147 - test acc: 0.3992 - 14m 13s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.8617 - test loss: 15.0880 - train acc: 0.7472 - test acc: 0.4105 - 14m 18s\n",
      "batch: 200/1563 - train loss: 4.7346 - test loss: 14.6255 - train acc: 0.7444 - test acc: 0.4261 - 14m 23s\n",
      "batch: 300/1563 - train loss: 4.8679 - test loss: 14.8431 - train acc: 0.7472 - test acc: 0.4326 - 14m 29s\n",
      "batch: 400/1563 - train loss: 5.0568 - test loss: 15.1291 - train acc: 0.7360 - test acc: 0.4208 - 14m 35s\n",
      "batch: 500/1563 - train loss: 5.4754 - test loss: 14.6057 - train acc: 0.7113 - test acc: 0.4319 - 14m 41s\n",
      "batch: 600/1563 - train loss: 5.3872 - test loss: 15.2605 - train acc: 0.7129 - test acc: 0.4125 - 14m 47s\n",
      "batch: 700/1563 - train loss: 5.7436 - test loss: 15.8225 - train acc: 0.6985 - test acc: 0.4046 - 14m 52s\n",
      "batch: 800/1563 - train loss: 5.5839 - test loss: 14.9564 - train acc: 0.7113 - test acc: 0.4156 - 14m 58s\n",
      "batch: 900/1563 - train loss: 5.8171 - test loss: 15.8715 - train acc: 0.6916 - test acc: 0.4077 - 15m 3s\n",
      "batch: 1000/1563 - train loss: 6.0974 - test loss: 15.0157 - train acc: 0.6834 - test acc: 0.4157 - 15m 9s\n",
      "batch: 1100/1563 - train loss: 6.2197 - test loss: 14.2960 - train acc: 0.6719 - test acc: 0.4307 - 15m 14s\n",
      "batch: 1200/1563 - train loss: 6.2967 - test loss: 14.6606 - train acc: 0.6787 - test acc: 0.4261 - 15m 20s\n",
      "batch: 1300/1563 - train loss: 6.3163 - test loss: 14.5037 - train acc: 0.6676 - test acc: 0.4261 - 15m 25s\n",
      "batch: 1400/1563 - train loss: 6.3352 - test loss: 15.0033 - train acc: 0.6635 - test acc: 0.4234 - 15m 31s\n",
      "batch: 1500/1563 - train loss: 6.3296 - test loss: 14.8179 - train acc: 0.6597 - test acc: 0.4224 - 15m 36s\n",
      "batch: 1563/1563 - train loss: 6.6650 - test loss: 14.7557 - train acc: 0.6575 - test acc: 0.4234 - 15m 40s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.0924 - test loss: 15.6785 - train acc: 0.7744 - test acc: 0.4220 - 15m 46s\n",
      "batch: 200/1563 - train loss: 4.1226 - test loss: 15.5987 - train acc: 0.7846 - test acc: 0.4218 - 15m 51s\n",
      "batch: 300/1563 - train loss: 4.3071 - test loss: 15.4538 - train acc: 0.7615 - test acc: 0.4210 - 15m 57s\n",
      "batch: 400/1563 - train loss: 4.4009 - test loss: 15.3120 - train acc: 0.7672 - test acc: 0.4332 - 16m 3s\n",
      "batch: 500/1563 - train loss: 4.6195 - test loss: 15.8156 - train acc: 0.7496 - test acc: 0.4275 - 16m 8s\n",
      "batch: 600/1563 - train loss: 5.0151 - test loss: 15.7952 - train acc: 0.7263 - test acc: 0.4177 - 16m 13s\n",
      "batch: 700/1563 - train loss: 4.9325 - test loss: 15.6733 - train acc: 0.7406 - test acc: 0.4189 - 16m 19s\n",
      "batch: 800/1563 - train loss: 4.9264 - test loss: 15.5571 - train acc: 0.7332 - test acc: 0.4227 - 16m 24s\n",
      "batch: 900/1563 - train loss: 5.1168 - test loss: 15.8568 - train acc: 0.7203 - test acc: 0.4148 - 16m 30s\n",
      "batch: 1000/1563 - train loss: 5.0921 - test loss: 15.6107 - train acc: 0.7228 - test acc: 0.4182 - 16m 35s\n",
      "batch: 1100/1563 - train loss: 5.3535 - test loss: 14.8531 - train acc: 0.7185 - test acc: 0.4302 - 16m 41s\n",
      "batch: 1200/1563 - train loss: 5.2603 - test loss: 15.6862 - train acc: 0.7163 - test acc: 0.4276 - 16m 46s\n",
      "batch: 1300/1563 - train loss: 5.3420 - test loss: 15.3918 - train acc: 0.7156 - test acc: 0.4287 - 16m 52s\n",
      "batch: 1400/1563 - train loss: 5.8018 - test loss: 15.2347 - train acc: 0.6963 - test acc: 0.4276 - 16m 57s\n",
      "batch: 1500/1563 - train loss: 5.4392 - test loss: 15.7024 - train acc: 0.7075 - test acc: 0.4179 - 17m 2s\n",
      "batch: 1563/1563 - train loss: 5.5145 - test loss: 16.4884 - train acc: 0.7003 - test acc: 0.4051 - 17m 7s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.5130 - test loss: 15.8262 - train acc: 0.8118 - test acc: 0.4340 - 17m 13s\n",
      "batch: 200/1563 - train loss: 3.2832 - test loss: 15.7668 - train acc: 0.8177 - test acc: 0.4288 - 17m 18s\n",
      "batch: 300/1563 - train loss: 3.6444 - test loss: 16.1305 - train acc: 0.7956 - test acc: 0.4240 - 17m 23s\n",
      "batch: 400/1563 - train loss: 3.8195 - test loss: 17.2111 - train acc: 0.7899 - test acc: 0.4126 - 17m 29s\n",
      "batch: 500/1563 - train loss: 3.8796 - test loss: 16.2461 - train acc: 0.7822 - test acc: 0.4187 - 17m 34s\n",
      "batch: 600/1563 - train loss: 3.9566 - test loss: 16.6822 - train acc: 0.7762 - test acc: 0.4215 - 17m 40s\n",
      "batch: 700/1563 - train loss: 4.0841 - test loss: 16.2961 - train acc: 0.7775 - test acc: 0.4235 - 17m 45s\n",
      "batch: 800/1563 - train loss: 4.1167 - test loss: 15.6224 - train acc: 0.7759 - test acc: 0.4321 - 17m 50s\n",
      "batch: 900/1563 - train loss: 4.2452 - test loss: 16.4837 - train acc: 0.7625 - test acc: 0.4192 - 17m 56s\n",
      "batch: 1000/1563 - train loss: 4.6885 - test loss: 16.4059 - train acc: 0.7466 - test acc: 0.4197 - 18m 1s\n",
      "batch: 1100/1563 - train loss: 4.5892 - test loss: 16.2559 - train acc: 0.7534 - test acc: 0.4185 - 18m 7s\n",
      "batch: 1200/1563 - train loss: 4.7446 - test loss: 15.9480 - train acc: 0.7478 - test acc: 0.4317 - 18m 13s\n",
      "batch: 1300/1563 - train loss: 4.9886 - test loss: 16.0378 - train acc: 0.7275 - test acc: 0.4179 - 18m 18s\n",
      "batch: 1400/1563 - train loss: 4.7656 - test loss: 16.6763 - train acc: 0.7409 - test acc: 0.4063 - 18m 23s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 4.7244 - test loss: 16.0862 - train acc: 0.7481 - test acc: 0.4283 - 18m 29s\n",
      "batch: 1563/1563 - train loss: 4.8039 - test loss: 16.0488 - train acc: 0.7450 - test acc: 0.4308 - 18m 33s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 3.0369 - test loss: 16.2930 - train acc: 0.8363 - test acc: 0.4367 - 18m 39s\n",
      "batch: 200/1563 - train loss: 2.8499 - test loss: 16.8242 - train acc: 0.8418 - test acc: 0.4244 - 18m 44s\n",
      "batch: 300/1563 - train loss: 3.0944 - test loss: 16.8748 - train acc: 0.8321 - test acc: 0.4211 - 18m 50s\n",
      "batch: 400/1563 - train loss: 3.0251 - test loss: 16.4159 - train acc: 0.8356 - test acc: 0.4330 - 18m 55s\n",
      "batch: 500/1563 - train loss: 3.4653 - test loss: 16.5974 - train acc: 0.8028 - test acc: 0.4260 - 19m 1s\n",
      "batch: 600/1563 - train loss: 3.2196 - test loss: 17.1608 - train acc: 0.8240 - test acc: 0.4217 - 19m 6s\n",
      "batch: 700/1563 - train loss: 3.3542 - test loss: 16.7962 - train acc: 0.8150 - test acc: 0.4331 - 19m 12s\n",
      "batch: 800/1563 - train loss: 3.6719 - test loss: 16.9111 - train acc: 0.7990 - test acc: 0.4293 - 19m 17s\n",
      "batch: 900/1563 - train loss: 3.6616 - test loss: 16.8329 - train acc: 0.7956 - test acc: 0.4242 - 19m 23s\n",
      "batch: 1000/1563 - train loss: 4.1066 - test loss: 16.8253 - train acc: 0.7678 - test acc: 0.4266 - 19m 28s\n",
      "batch: 1100/1563 - train loss: 4.0886 - test loss: 16.7166 - train acc: 0.7749 - test acc: 0.4250 - 19m 34s\n",
      "batch: 1200/1563 - train loss: 4.1279 - test loss: 16.6933 - train acc: 0.7703 - test acc: 0.4262 - 19m 39s\n",
      "batch: 1300/1563 - train loss: 4.2608 - test loss: 16.4976 - train acc: 0.7619 - test acc: 0.4268 - 19m 45s\n",
      "batch: 1400/1563 - train loss: 4.2577 - test loss: 16.8249 - train acc: 0.7693 - test acc: 0.4245 - 19m 50s\n",
      "batch: 1500/1563 - train loss: 4.2480 - test loss: 17.0576 - train acc: 0.7606 - test acc: 0.4293 - 19m 56s\n",
      "batch: 1563/1563 - train loss: 4.4050 - test loss: 17.0050 - train acc: 0.7568 - test acc: 0.4217 - 20m 0s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "time is up! finishing training\n",
      "batch: 1/1563 - train loss: 4.4318 - test loss: 17.1215 - train acc: 0.7559 - test acc: 0.4203 - 20m 3s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 18\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1900 - test loss: 25.8718 - train acc: 0.0351 - test acc: 0.0507 - 0m 2s\n",
      "batch: 200/1563 - train loss: 24.0211 - test loss: 23.8909 - train acc: 0.0746 - test acc: 0.0725 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.7280 - test loss: 22.1400 - train acc: 0.0937 - test acc: 0.1154 - 0m 13s\n",
      "batch: 400/1563 - train loss: 22.2309 - test loss: 22.3272 - train acc: 0.1081 - test acc: 0.1091 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.2770 - test loss: 21.0967 - train acc: 0.1256 - test acc: 0.1326 - 0m 23s\n",
      "batch: 600/1563 - train loss: 20.8932 - test loss: 21.2768 - train acc: 0.1416 - test acc: 0.1350 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.7505 - test loss: 20.5716 - train acc: 0.1338 - test acc: 0.1478 - 0m 34s\n",
      "batch: 800/1563 - train loss: 20.4335 - test loss: 20.5983 - train acc: 0.1588 - test acc: 0.1435 - 0m 40s\n",
      "batch: 900/1563 - train loss: 20.1633 - test loss: 20.0649 - train acc: 0.1566 - test acc: 0.1534 - 0m 45s\n",
      "batch: 1000/1563 - train loss: 19.7197 - test loss: 20.6153 - train acc: 0.1722 - test acc: 0.1603 - 0m 51s\n",
      "batch: 1100/1563 - train loss: 19.6318 - test loss: 19.0663 - train acc: 0.1725 - test acc: 0.1952 - 0m 56s\n",
      "batch: 1200/1563 - train loss: 19.1048 - test loss: 19.5621 - train acc: 0.1863 - test acc: 0.1835 - 1m 2s\n",
      "batch: 1300/1563 - train loss: 19.2792 - test loss: 19.4466 - train acc: 0.1885 - test acc: 0.1812 - 1m 7s\n",
      "batch: 1400/1563 - train loss: 18.9753 - test loss: 18.2284 - train acc: 0.1894 - test acc: 0.2156 - 1m 13s\n",
      "batch: 1500/1563 - train loss: 18.5410 - test loss: 19.7415 - train acc: 0.2104 - test acc: 0.1754 - 1m 18s\n",
      "batch: 1563/1563 - train loss: 18.2805 - test loss: 18.4891 - train acc: 0.2166 - test acc: 0.2133 - 1m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.4794 - test loss: 18.7563 - train acc: 0.2316 - test acc: 0.2040 - 1m 28s\n",
      "batch: 200/1563 - train loss: 17.3639 - test loss: 17.8060 - train acc: 0.2406 - test acc: 0.2363 - 1m 33s\n",
      "batch: 300/1563 - train loss: 17.5100 - test loss: 17.9478 - train acc: 0.2428 - test acc: 0.2194 - 1m 39s\n",
      "batch: 400/1563 - train loss: 17.0743 - test loss: 18.5079 - train acc: 0.2578 - test acc: 0.2078 - 1m 45s\n",
      "batch: 500/1563 - train loss: 17.1087 - test loss: 18.1660 - train acc: 0.2401 - test acc: 0.2283 - 1m 50s\n",
      "batch: 600/1563 - train loss: 17.1430 - test loss: 17.5355 - train acc: 0.2525 - test acc: 0.2390 - 1m 55s\n",
      "batch: 700/1563 - train loss: 17.2981 - test loss: 19.3458 - train acc: 0.2444 - test acc: 0.2095 - 2m 1s\n",
      "batch: 800/1563 - train loss: 16.6738 - test loss: 16.7950 - train acc: 0.2672 - test acc: 0.2677 - 2m 6s\n",
      "batch: 900/1563 - train loss: 16.4116 - test loss: 17.0106 - train acc: 0.2828 - test acc: 0.2624 - 2m 12s\n",
      "batch: 1000/1563 - train loss: 16.8243 - test loss: 17.2008 - train acc: 0.2665 - test acc: 0.2489 - 2m 17s\n",
      "batch: 1100/1563 - train loss: 16.2968 - test loss: 16.6125 - train acc: 0.2784 - test acc: 0.2778 - 2m 22s\n",
      "batch: 1200/1563 - train loss: 16.5885 - test loss: 16.4099 - train acc: 0.2710 - test acc: 0.2844 - 2m 28s\n",
      "batch: 1300/1563 - train loss: 16.4067 - test loss: 16.5917 - train acc: 0.2753 - test acc: 0.2638 - 2m 33s\n",
      "batch: 1400/1563 - train loss: 16.3007 - test loss: 16.5922 - train acc: 0.2794 - test acc: 0.2738 - 2m 39s\n",
      "batch: 1500/1563 - train loss: 15.5978 - test loss: 16.9995 - train acc: 0.3050 - test acc: 0.2717 - 2m 44s\n",
      "batch: 1563/1563 - train loss: 16.0940 - test loss: 16.2043 - train acc: 0.2816 - test acc: 0.2878 - 2m 49s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.0219 - test loss: 15.5426 - train acc: 0.3228 - test acc: 0.3112 - 2m 54s\n",
      "batch: 200/1563 - train loss: 14.6577 - test loss: 15.7516 - train acc: 0.3341 - test acc: 0.3059 - 3m 0s\n",
      "batch: 300/1563 - train loss: 14.8078 - test loss: 15.6908 - train acc: 0.3274 - test acc: 0.3049 - 3m 5s\n",
      "batch: 400/1563 - train loss: 14.5702 - test loss: 17.8231 - train acc: 0.3488 - test acc: 0.2539 - 3m 11s\n",
      "batch: 500/1563 - train loss: 14.7450 - test loss: 16.5741 - train acc: 0.3294 - test acc: 0.2825 - 3m 16s\n",
      "batch: 600/1563 - train loss: 14.4883 - test loss: 16.0561 - train acc: 0.3325 - test acc: 0.3038 - 3m 21s\n",
      "batch: 700/1563 - train loss: 14.7189 - test loss: 15.5847 - train acc: 0.3353 - test acc: 0.3110 - 3m 26s\n",
      "batch: 800/1563 - train loss: 14.9729 - test loss: 15.7580 - train acc: 0.3247 - test acc: 0.3100 - 3m 32s\n",
      "batch: 900/1563 - train loss: 14.5599 - test loss: 15.3068 - train acc: 0.3416 - test acc: 0.3200 - 3m 38s\n",
      "batch: 1000/1563 - train loss: 14.4728 - test loss: 15.0927 - train acc: 0.3315 - test acc: 0.3266 - 3m 43s\n",
      "batch: 1100/1563 - train loss: 14.9204 - test loss: 16.8084 - train acc: 0.3103 - test acc: 0.2810 - 3m 48s\n",
      "batch: 1200/1563 - train loss: 14.4487 - test loss: 15.3722 - train acc: 0.3472 - test acc: 0.3191 - 3m 54s\n",
      "batch: 1300/1563 - train loss: 14.3107 - test loss: 14.5674 - train acc: 0.3438 - test acc: 0.3442 - 3m 59s\n",
      "batch: 1400/1563 - train loss: 14.5857 - test loss: 15.2654 - train acc: 0.3321 - test acc: 0.3220 - 4m 5s\n",
      "batch: 1500/1563 - train loss: 14.3365 - test loss: 15.4582 - train acc: 0.3456 - test acc: 0.3148 - 4m 10s\n",
      "batch: 1563/1563 - train loss: 14.2032 - test loss: 16.0709 - train acc: 0.3547 - test acc: 0.3025 - 4m 15s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.0854 - test loss: 14.5675 - train acc: 0.3904 - test acc: 0.3521 - 4m 20s\n",
      "batch: 200/1563 - train loss: 13.0960 - test loss: 14.8379 - train acc: 0.3969 - test acc: 0.3427 - 4m 26s\n",
      "batch: 300/1563 - train loss: 13.0534 - test loss: 17.2968 - train acc: 0.4022 - test acc: 0.2804 - 4m 31s\n",
      "batch: 400/1563 - train loss: 13.0507 - test loss: 14.9346 - train acc: 0.3947 - test acc: 0.3375 - 4m 37s\n",
      "batch: 500/1563 - train loss: 13.2888 - test loss: 14.8388 - train acc: 0.3922 - test acc: 0.3411 - 4m 42s\n",
      "batch: 600/1563 - train loss: 12.9240 - test loss: 14.4144 - train acc: 0.4016 - test acc: 0.3571 - 4m 48s\n",
      "batch: 700/1563 - train loss: 13.1982 - test loss: 14.5887 - train acc: 0.3894 - test acc: 0.3444 - 4m 53s\n",
      "batch: 800/1563 - train loss: 12.9853 - test loss: 14.8034 - train acc: 0.3991 - test acc: 0.3426 - 4m 58s\n",
      "batch: 900/1563 - train loss: 13.4278 - test loss: 14.5228 - train acc: 0.3769 - test acc: 0.3459 - 5m 4s\n",
      "batch: 1000/1563 - train loss: 13.1189 - test loss: 14.5223 - train acc: 0.3813 - test acc: 0.3510 - 5m 10s\n",
      "batch: 1100/1563 - train loss: 13.1536 - test loss: 14.5138 - train acc: 0.4025 - test acc: 0.3433 - 5m 15s\n",
      "batch: 1200/1563 - train loss: 12.8525 - test loss: 15.1622 - train acc: 0.3953 - test acc: 0.3340 - 5m 20s\n",
      "batch: 1300/1563 - train loss: 13.4720 - test loss: 14.5703 - train acc: 0.3559 - test acc: 0.3406 - 5m 26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.2473 - test loss: 14.3024 - train acc: 0.3816 - test acc: 0.3589 - 5m 31s\n",
      "batch: 1500/1563 - train loss: 13.1430 - test loss: 15.4259 - train acc: 0.3925 - test acc: 0.3178 - 5m 37s\n",
      "batch: 1563/1563 - train loss: 13.2168 - test loss: 15.6836 - train acc: 0.3966 - test acc: 0.3167 - 5m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.2398 - test loss: 15.6691 - train acc: 0.4663 - test acc: 0.3270 - 5m 47s\n",
      "batch: 200/1563 - train loss: 11.5597 - test loss: 14.0775 - train acc: 0.4550 - test acc: 0.3746 - 5m 52s\n",
      "batch: 300/1563 - train loss: 11.3375 - test loss: 13.9679 - train acc: 0.4519 - test acc: 0.3747 - 5m 57s\n",
      "batch: 400/1563 - train loss: 11.5967 - test loss: 14.5818 - train acc: 0.4463 - test acc: 0.3583 - 6m 3s\n",
      "batch: 500/1563 - train loss: 11.8503 - test loss: 15.7343 - train acc: 0.4312 - test acc: 0.3298 - 6m 8s\n",
      "batch: 600/1563 - train loss: 11.6741 - test loss: 14.1037 - train acc: 0.4525 - test acc: 0.3731 - 6m 14s\n",
      "batch: 700/1563 - train loss: 12.2596 - test loss: 14.1567 - train acc: 0.4241 - test acc: 0.3698 - 6m 19s\n",
      "batch: 800/1563 - train loss: 12.1677 - test loss: 13.7443 - train acc: 0.4213 - test acc: 0.3839 - 6m 24s\n",
      "batch: 900/1563 - train loss: 11.6488 - test loss: 14.8478 - train acc: 0.4574 - test acc: 0.3536 - 6m 30s\n",
      "batch: 1000/1563 - train loss: 11.7114 - test loss: 14.2231 - train acc: 0.4344 - test acc: 0.3771 - 6m 35s\n",
      "batch: 1100/1563 - train loss: 12.1653 - test loss: 13.8282 - train acc: 0.4253 - test acc: 0.3797 - 6m 41s\n",
      "batch: 1200/1563 - train loss: 11.9673 - test loss: 16.3498 - train acc: 0.4319 - test acc: 0.3031 - 6m 46s\n",
      "batch: 1300/1563 - train loss: 11.8482 - test loss: 13.9090 - train acc: 0.4432 - test acc: 0.3736 - 6m 52s\n",
      "batch: 1400/1563 - train loss: 11.9296 - test loss: 15.0688 - train acc: 0.4425 - test acc: 0.3412 - 6m 57s\n",
      "batch: 1500/1563 - train loss: 12.3351 - test loss: 14.1004 - train acc: 0.4275 - test acc: 0.3662 - 7m 2s\n",
      "batch: 1563/1563 - train loss: 12.2456 - test loss: 14.0568 - train acc: 0.4200 - test acc: 0.3737 - 7m 7s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0119 - test loss: 14.2182 - train acc: 0.5134 - test acc: 0.3754 - 7m 12s\n",
      "batch: 200/1563 - train loss: 10.1692 - test loss: 15.0395 - train acc: 0.5178 - test acc: 0.3618 - 7m 18s\n",
      "batch: 300/1563 - train loss: 10.1953 - test loss: 13.5780 - train acc: 0.4984 - test acc: 0.4006 - 7m 23s\n",
      "batch: 400/1563 - train loss: 10.4260 - test loss: 14.6360 - train acc: 0.4953 - test acc: 0.3628 - 7m 29s\n",
      "batch: 500/1563 - train loss: 10.6368 - test loss: 14.0644 - train acc: 0.4869 - test acc: 0.3805 - 7m 34s\n",
      "batch: 600/1563 - train loss: 10.4998 - test loss: 13.8926 - train acc: 0.4844 - test acc: 0.3856 - 7m 40s\n",
      "batch: 700/1563 - train loss: 10.3313 - test loss: 13.5557 - train acc: 0.4969 - test acc: 0.3970 - 7m 45s\n",
      "batch: 800/1563 - train loss: 10.5005 - test loss: 14.3583 - train acc: 0.4953 - test acc: 0.3717 - 7m 51s\n",
      "batch: 900/1563 - train loss: 11.0775 - test loss: 14.6644 - train acc: 0.4797 - test acc: 0.3580 - 7m 56s\n",
      "batch: 1000/1563 - train loss: 10.4155 - test loss: 13.4037 - train acc: 0.4866 - test acc: 0.3996 - 8m 2s\n",
      "batch: 1100/1563 - train loss: 11.2361 - test loss: 14.7636 - train acc: 0.4794 - test acc: 0.3618 - 8m 7s\n",
      "batch: 1200/1563 - train loss: 10.6375 - test loss: 13.9209 - train acc: 0.4828 - test acc: 0.3846 - 8m 13s\n",
      "batch: 1300/1563 - train loss: 11.0922 - test loss: 14.2503 - train acc: 0.4688 - test acc: 0.3814 - 8m 18s\n",
      "batch: 1400/1563 - train loss: 11.1469 - test loss: 13.4350 - train acc: 0.4650 - test acc: 0.3944 - 8m 24s\n",
      "batch: 1500/1563 - train loss: 11.3022 - test loss: 13.9747 - train acc: 0.4619 - test acc: 0.3814 - 8m 29s\n",
      "batch: 1563/1563 - train loss: 10.8643 - test loss: 13.8478 - train acc: 0.4809 - test acc: 0.3872 - 8m 33s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.6681 - test loss: 14.0911 - train acc: 0.5762 - test acc: 0.3859 - 8m 39s\n",
      "batch: 200/1563 - train loss: 8.8560 - test loss: 13.5604 - train acc: 0.5625 - test acc: 0.4094 - 8m 44s\n",
      "batch: 300/1563 - train loss: 8.8860 - test loss: 14.0455 - train acc: 0.5500 - test acc: 0.3979 - 8m 49s\n",
      "batch: 400/1563 - train loss: 9.3796 - test loss: 13.7888 - train acc: 0.5331 - test acc: 0.4000 - 8m 55s\n",
      "batch: 500/1563 - train loss: 9.2482 - test loss: 13.9098 - train acc: 0.5357 - test acc: 0.3893 - 9m 0s\n",
      "batch: 600/1563 - train loss: 9.4393 - test loss: 14.8609 - train acc: 0.5312 - test acc: 0.3790 - 9m 6s\n",
      "batch: 700/1563 - train loss: 9.5493 - test loss: 14.1015 - train acc: 0.5322 - test acc: 0.3906 - 9m 11s\n",
      "batch: 800/1563 - train loss: 9.6515 - test loss: 17.5194 - train acc: 0.5247 - test acc: 0.3196 - 9m 17s\n",
      "batch: 900/1563 - train loss: 9.7790 - test loss: 13.8051 - train acc: 0.5128 - test acc: 0.3959 - 9m 22s\n",
      "batch: 1000/1563 - train loss: 9.7836 - test loss: 13.4438 - train acc: 0.5172 - test acc: 0.4065 - 9m 28s\n",
      "batch: 1100/1563 - train loss: 10.1236 - test loss: 13.4363 - train acc: 0.5141 - test acc: 0.4086 - 9m 33s\n",
      "batch: 1200/1563 - train loss: 9.7829 - test loss: 13.2759 - train acc: 0.5153 - test acc: 0.4084 - 9m 38s\n",
      "batch: 1300/1563 - train loss: 9.9158 - test loss: 14.4558 - train acc: 0.5228 - test acc: 0.3731 - 9m 43s\n",
      "batch: 1400/1563 - train loss: 9.9929 - test loss: 13.5358 - train acc: 0.5169 - test acc: 0.4004 - 9m 49s\n",
      "batch: 1500/1563 - train loss: 10.1314 - test loss: 13.5252 - train acc: 0.5025 - test acc: 0.4091 - 9m 54s\n",
      "batch: 1563/1563 - train loss: 9.8850 - test loss: 13.2823 - train acc: 0.5157 - test acc: 0.4124 - 9m 59s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.9846 - test loss: 14.3674 - train acc: 0.5975 - test acc: 0.3902 - 10m 4s\n",
      "batch: 200/1563 - train loss: 7.6461 - test loss: 14.4437 - train acc: 0.6050 - test acc: 0.3845 - 10m 10s\n",
      "batch: 300/1563 - train loss: 8.0051 - test loss: 14.1028 - train acc: 0.5909 - test acc: 0.3973 - 10m 15s\n",
      "batch: 400/1563 - train loss: 7.7687 - test loss: 15.0851 - train acc: 0.5978 - test acc: 0.3813 - 10m 20s\n",
      "batch: 500/1563 - train loss: 8.4365 - test loss: 14.8306 - train acc: 0.5825 - test acc: 0.3825 - 10m 26s\n",
      "batch: 600/1563 - train loss: 8.7439 - test loss: 14.1180 - train acc: 0.5622 - test acc: 0.4042 - 10m 31s\n",
      "batch: 700/1563 - train loss: 8.6068 - test loss: 13.5229 - train acc: 0.5672 - test acc: 0.4117 - 10m 37s\n",
      "batch: 800/1563 - train loss: 8.4140 - test loss: 15.1267 - train acc: 0.5753 - test acc: 0.3858 - 10m 42s\n",
      "batch: 900/1563 - train loss: 8.5086 - test loss: 14.3991 - train acc: 0.5678 - test acc: 0.3868 - 10m 47s\n",
      "batch: 1000/1563 - train loss: 8.7254 - test loss: 13.7061 - train acc: 0.5646 - test acc: 0.4058 - 10m 53s\n",
      "batch: 1100/1563 - train loss: 8.9034 - test loss: 14.7788 - train acc: 0.5519 - test acc: 0.3849 - 10m 58s\n",
      "batch: 1200/1563 - train loss: 9.0067 - test loss: 13.7010 - train acc: 0.5556 - test acc: 0.4167 - 11m 4s\n",
      "batch: 1300/1563 - train loss: 9.0154 - test loss: 13.2167 - train acc: 0.5456 - test acc: 0.4261 - 11m 9s\n",
      "batch: 1400/1563 - train loss: 8.7784 - test loss: 13.9309 - train acc: 0.5541 - test acc: 0.4123 - 11m 15s\n",
      "batch: 1500/1563 - train loss: 9.1042 - test loss: 14.0039 - train acc: 0.5481 - test acc: 0.4031 - 11m 20s\n",
      "batch: 1563/1563 - train loss: 9.1524 - test loss: 13.4949 - train acc: 0.5463 - test acc: 0.4197 - 11m 24s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4982 - test loss: 15.6048 - train acc: 0.6666 - test acc: 0.3755 - 11m 30s\n",
      "batch: 200/1563 - train loss: 6.6389 - test loss: 14.7910 - train acc: 0.6534 - test acc: 0.4036 - 11m 35s\n",
      "batch: 300/1563 - train loss: 7.1799 - test loss: 14.5697 - train acc: 0.6291 - test acc: 0.4086 - 11m 41s\n",
      "batch: 400/1563 - train loss: 7.1976 - test loss: 13.9263 - train acc: 0.6326 - test acc: 0.4158 - 11m 46s\n",
      "batch: 500/1563 - train loss: 7.1462 - test loss: 14.4753 - train acc: 0.6356 - test acc: 0.4072 - 11m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.5771 - test loss: 15.8307 - train acc: 0.6109 - test acc: 0.3749 - 11m 57s\n",
      "batch: 700/1563 - train loss: 7.4878 - test loss: 14.3279 - train acc: 0.6234 - test acc: 0.4106 - 12m 3s\n",
      "batch: 800/1563 - train loss: 7.6403 - test loss: 15.0851 - train acc: 0.6241 - test acc: 0.3869 - 12m 8s\n",
      "batch: 900/1563 - train loss: 7.7329 - test loss: 14.0748 - train acc: 0.6084 - test acc: 0.4192 - 12m 13s\n",
      "batch: 1000/1563 - train loss: 7.8073 - test loss: 14.2772 - train acc: 0.6122 - test acc: 0.4097 - 12m 18s\n",
      "batch: 1100/1563 - train loss: 7.9461 - test loss: 14.3034 - train acc: 0.5997 - test acc: 0.4074 - 12m 24s\n",
      "batch: 1200/1563 - train loss: 8.0045 - test loss: 14.3972 - train acc: 0.5949 - test acc: 0.4062 - 12m 29s\n",
      "batch: 1300/1563 - train loss: 7.9847 - test loss: 13.6942 - train acc: 0.5925 - test acc: 0.4251 - 12m 35s\n",
      "batch: 1400/1563 - train loss: 8.1198 - test loss: 14.1536 - train acc: 0.5897 - test acc: 0.4098 - 12m 40s\n",
      "batch: 1500/1563 - train loss: 8.2006 - test loss: 14.4918 - train acc: 0.5903 - test acc: 0.4048 - 12m 46s\n",
      "batch: 1563/1563 - train loss: 8.0079 - test loss: 13.9549 - train acc: 0.5994 - test acc: 0.4141 - 12m 50s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.8106 - test loss: 13.9815 - train acc: 0.6982 - test acc: 0.4186 - 12m 55s\n",
      "batch: 200/1563 - train loss: 5.8635 - test loss: 15.2282 - train acc: 0.6966 - test acc: 0.3971 - 13m 1s\n",
      "batch: 300/1563 - train loss: 5.7872 - test loss: 14.5595 - train acc: 0.6929 - test acc: 0.4215 - 13m 6s\n",
      "batch: 400/1563 - train loss: 6.3834 - test loss: 15.3404 - train acc: 0.6719 - test acc: 0.3994 - 13m 12s\n",
      "batch: 500/1563 - train loss: 6.2018 - test loss: 15.0318 - train acc: 0.6641 - test acc: 0.4008 - 13m 17s\n",
      "batch: 600/1563 - train loss: 6.3367 - test loss: 14.6514 - train acc: 0.6663 - test acc: 0.4098 - 13m 22s\n",
      "batch: 700/1563 - train loss: 6.4725 - test loss: 14.7645 - train acc: 0.6588 - test acc: 0.4157 - 13m 28s\n",
      "batch: 800/1563 - train loss: 6.6876 - test loss: 14.7291 - train acc: 0.6625 - test acc: 0.4128 - 13m 33s\n",
      "batch: 900/1563 - train loss: 7.0883 - test loss: 14.7425 - train acc: 0.6341 - test acc: 0.4016 - 13m 39s\n",
      "batch: 1000/1563 - train loss: 7.1556 - test loss: 14.0850 - train acc: 0.6259 - test acc: 0.4220 - 13m 44s\n",
      "batch: 1100/1563 - train loss: 6.9655 - test loss: 14.3040 - train acc: 0.6363 - test acc: 0.4231 - 13m 49s\n",
      "batch: 1200/1563 - train loss: 6.9165 - test loss: 14.5829 - train acc: 0.6441 - test acc: 0.4147 - 13m 55s\n",
      "batch: 1300/1563 - train loss: 7.0141 - test loss: 15.2984 - train acc: 0.6372 - test acc: 0.3980 - 14m 0s\n",
      "batch: 1400/1563 - train loss: 7.3043 - test loss: 14.3619 - train acc: 0.6172 - test acc: 0.4207 - 14m 6s\n",
      "batch: 1500/1563 - train loss: 7.3355 - test loss: 14.9353 - train acc: 0.6259 - test acc: 0.4056 - 14m 12s\n",
      "batch: 1563/1563 - train loss: 7.3765 - test loss: 14.4660 - train acc: 0.6206 - test acc: 0.4203 - 14m 16s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.9660 - test loss: 15.0234 - train acc: 0.7397 - test acc: 0.4208 - 14m 22s\n",
      "batch: 200/1563 - train loss: 4.8449 - test loss: 14.7349 - train acc: 0.7422 - test acc: 0.4332 - 14m 27s\n",
      "batch: 300/1563 - train loss: 4.6697 - test loss: 15.4561 - train acc: 0.7466 - test acc: 0.4158 - 14m 32s\n",
      "batch: 400/1563 - train loss: 5.2373 - test loss: 15.6198 - train acc: 0.7213 - test acc: 0.4064 - 14m 38s\n",
      "batch: 500/1563 - train loss: 5.4497 - test loss: 17.6355 - train acc: 0.7119 - test acc: 0.3755 - 14m 44s\n",
      "batch: 600/1563 - train loss: 5.6265 - test loss: 15.2499 - train acc: 0.7066 - test acc: 0.4106 - 14m 50s\n",
      "batch: 700/1563 - train loss: 5.5609 - test loss: 15.8763 - train acc: 0.7101 - test acc: 0.3946 - 14m 55s\n",
      "batch: 800/1563 - train loss: 5.8710 - test loss: 15.5632 - train acc: 0.6853 - test acc: 0.4030 - 15m 1s\n",
      "batch: 900/1563 - train loss: 5.9588 - test loss: 14.6843 - train acc: 0.6838 - test acc: 0.4257 - 15m 6s\n",
      "batch: 1000/1563 - train loss: 5.9284 - test loss: 15.0193 - train acc: 0.6815 - test acc: 0.4184 - 15m 12s\n",
      "batch: 1100/1563 - train loss: 6.0910 - test loss: 16.0061 - train acc: 0.6756 - test acc: 0.4023 - 15m 17s\n",
      "batch: 1200/1563 - train loss: 6.2602 - test loss: 16.4426 - train acc: 0.6619 - test acc: 0.3907 - 15m 23s\n",
      "batch: 1300/1563 - train loss: 5.9717 - test loss: 15.5009 - train acc: 0.6847 - test acc: 0.4120 - 15m 28s\n",
      "batch: 1400/1563 - train loss: 6.5548 - test loss: 14.9993 - train acc: 0.6616 - test acc: 0.4127 - 15m 33s\n",
      "batch: 1500/1563 - train loss: 6.5420 - test loss: 14.9590 - train acc: 0.6676 - test acc: 0.4237 - 15m 39s\n",
      "batch: 1563/1563 - train loss: 6.6293 - test loss: 16.1267 - train acc: 0.6657 - test acc: 0.4005 - 15m 44s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.2979 - test loss: 15.1458 - train acc: 0.7684 - test acc: 0.4287 - 15m 49s\n",
      "batch: 200/1563 - train loss: 4.0971 - test loss: 15.6451 - train acc: 0.7850 - test acc: 0.4183 - 15m 54s\n",
      "batch: 300/1563 - train loss: 4.3428 - test loss: 16.1207 - train acc: 0.7650 - test acc: 0.4090 - 16m 0s\n",
      "batch: 400/1563 - train loss: 4.2538 - test loss: 16.4931 - train acc: 0.7678 - test acc: 0.4029 - 16m 6s\n",
      "batch: 500/1563 - train loss: 4.7305 - test loss: 15.5825 - train acc: 0.7465 - test acc: 0.4260 - 16m 11s\n",
      "batch: 600/1563 - train loss: 4.6678 - test loss: 15.3724 - train acc: 0.7518 - test acc: 0.4200 - 16m 18s\n",
      "batch: 700/1563 - train loss: 4.7436 - test loss: 15.6969 - train acc: 0.7406 - test acc: 0.4216 - 16m 23s\n",
      "batch: 800/1563 - train loss: 4.7820 - test loss: 16.8817 - train acc: 0.7375 - test acc: 0.3901 - 16m 28s\n",
      "batch: 900/1563 - train loss: 5.2313 - test loss: 15.9042 - train acc: 0.7147 - test acc: 0.4155 - 16m 34s\n",
      "batch: 1000/1563 - train loss: 5.3057 - test loss: 15.7895 - train acc: 0.7182 - test acc: 0.4227 - 16m 39s\n",
      "batch: 1100/1563 - train loss: 5.3206 - test loss: 15.9519 - train acc: 0.7241 - test acc: 0.4093 - 16m 45s\n",
      "batch: 1200/1563 - train loss: 5.4420 - test loss: 15.1143 - train acc: 0.7163 - test acc: 0.4222 - 16m 51s\n",
      "batch: 1300/1563 - train loss: 5.5819 - test loss: 15.5911 - train acc: 0.7050 - test acc: 0.4196 - 16m 56s\n",
      "batch: 1400/1563 - train loss: 5.6444 - test loss: 15.2759 - train acc: 0.6938 - test acc: 0.4284 - 17m 1s\n",
      "batch: 1500/1563 - train loss: 5.5293 - test loss: 15.6863 - train acc: 0.7059 - test acc: 0.4222 - 17m 7s\n",
      "batch: 1563/1563 - train loss: 5.5035 - test loss: 16.5335 - train acc: 0.7069 - test acc: 0.4064 - 17m 11s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.4995 - test loss: 15.5757 - train acc: 0.8081 - test acc: 0.4282 - 17m 17s\n",
      "batch: 200/1563 - train loss: 3.3584 - test loss: 16.1746 - train acc: 0.8175 - test acc: 0.4252 - 17m 23s\n",
      "batch: 300/1563 - train loss: 3.3405 - test loss: 16.3781 - train acc: 0.8181 - test acc: 0.4217 - 17m 28s\n",
      "batch: 400/1563 - train loss: 3.7574 - test loss: 16.6419 - train acc: 0.7918 - test acc: 0.4105 - 17m 33s\n",
      "batch: 500/1563 - train loss: 3.9734 - test loss: 16.4121 - train acc: 0.7718 - test acc: 0.4185 - 17m 39s\n",
      "batch: 600/1563 - train loss: 4.1536 - test loss: 17.2483 - train acc: 0.7678 - test acc: 0.3902 - 17m 44s\n",
      "batch: 700/1563 - train loss: 3.9801 - test loss: 16.1316 - train acc: 0.7749 - test acc: 0.4158 - 17m 49s\n",
      "batch: 800/1563 - train loss: 4.2022 - test loss: 16.1353 - train acc: 0.7703 - test acc: 0.4290 - 17m 55s\n",
      "batch: 900/1563 - train loss: 4.2973 - test loss: 16.6319 - train acc: 0.7688 - test acc: 0.4178 - 18m 1s\n",
      "batch: 1000/1563 - train loss: 4.5280 - test loss: 16.1163 - train acc: 0.7491 - test acc: 0.4257 - 18m 6s\n",
      "batch: 1100/1563 - train loss: 4.7327 - test loss: 17.0087 - train acc: 0.7428 - test acc: 0.4045 - 18m 12s\n",
      "batch: 1200/1563 - train loss: 4.5065 - test loss: 16.4492 - train acc: 0.7531 - test acc: 0.4158 - 18m 17s\n",
      "batch: 1300/1563 - train loss: 4.8723 - test loss: 16.5511 - train acc: 0.7375 - test acc: 0.4089 - 18m 23s\n",
      "batch: 1400/1563 - train loss: 4.8688 - test loss: 16.3907 - train acc: 0.7303 - test acc: 0.4130 - 18m 28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 4.9550 - test loss: 15.7995 - train acc: 0.7306 - test acc: 0.4296 - 18m 34s\n",
      "batch: 1563/1563 - train loss: 4.9153 - test loss: 16.0846 - train acc: 0.7285 - test acc: 0.4207 - 18m 38s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8630 - test loss: 15.5883 - train acc: 0.8406 - test acc: 0.4345 - 18m 43s\n",
      "batch: 200/1563 - train loss: 2.9700 - test loss: 16.5457 - train acc: 0.8305 - test acc: 0.4284 - 18m 49s\n",
      "batch: 300/1563 - train loss: 2.9114 - test loss: 16.8453 - train acc: 0.8381 - test acc: 0.4230 - 18m 55s\n",
      "batch: 400/1563 - train loss: 3.0369 - test loss: 16.6679 - train acc: 0.8268 - test acc: 0.4235 - 19m 0s\n",
      "batch: 500/1563 - train loss: 3.1572 - test loss: 16.3949 - train acc: 0.8255 - test acc: 0.4325 - 19m 6s\n",
      "batch: 600/1563 - train loss: 3.4532 - test loss: 17.0753 - train acc: 0.8025 - test acc: 0.4105 - 19m 11s\n",
      "batch: 700/1563 - train loss: 3.5705 - test loss: 17.1130 - train acc: 0.8034 - test acc: 0.4196 - 19m 16s\n",
      "batch: 800/1563 - train loss: 3.5377 - test loss: 17.0581 - train acc: 0.8062 - test acc: 0.4157 - 19m 22s\n",
      "batch: 900/1563 - train loss: 3.8603 - test loss: 16.7880 - train acc: 0.7903 - test acc: 0.4222 - 19m 28s\n",
      "batch: 1000/1563 - train loss: 3.9876 - test loss: 16.3879 - train acc: 0.7909 - test acc: 0.4263 - 19m 33s\n",
      "batch: 1100/1563 - train loss: 4.0809 - test loss: 16.7261 - train acc: 0.7775 - test acc: 0.4274 - 19m 39s\n",
      "batch: 1200/1563 - train loss: 3.9667 - test loss: 16.5064 - train acc: 0.7809 - test acc: 0.4292 - 19m 44s\n",
      "batch: 1300/1563 - train loss: 4.2472 - test loss: 16.6711 - train acc: 0.7653 - test acc: 0.4253 - 19m 49s\n",
      "batch: 1400/1563 - train loss: 4.2580 - test loss: 16.4091 - train acc: 0.7597 - test acc: 0.4300 - 19m 55s\n",
      "time is up! finishing training\n",
      "batch: 1491/1563 - train loss: 4.3746 - test loss: 16.4378 - train acc: 0.7684 - test acc: 0.4327 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 19\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 25.7955 - test loss: 25.2488 - train acc: 0.0461 - test acc: 0.0483 - 0m 2s\n",
      "batch: 200/1563 - train loss: 24.1464 - test loss: 24.3630 - train acc: 0.0664 - test acc: 0.0682 - 0m 8s\n",
      "batch: 300/1563 - train loss: 22.9196 - test loss: 22.6637 - train acc: 0.0918 - test acc: 0.0953 - 0m 14s\n",
      "batch: 400/1563 - train loss: 21.9965 - test loss: 22.0207 - train acc: 0.1034 - test acc: 0.1061 - 0m 19s\n",
      "batch: 500/1563 - train loss: 21.3947 - test loss: 22.8890 - train acc: 0.1259 - test acc: 0.1093 - 0m 24s\n",
      "batch: 600/1563 - train loss: 21.0647 - test loss: 21.4328 - train acc: 0.1298 - test acc: 0.1268 - 0m 30s\n",
      "batch: 700/1563 - train loss: 20.7009 - test loss: 20.8738 - train acc: 0.1353 - test acc: 0.1343 - 0m 35s\n",
      "batch: 800/1563 - train loss: 20.4107 - test loss: 20.1608 - train acc: 0.1479 - test acc: 0.1580 - 0m 41s\n",
      "batch: 900/1563 - train loss: 19.7901 - test loss: 20.4945 - train acc: 0.1644 - test acc: 0.1581 - 0m 47s\n",
      "batch: 1000/1563 - train loss: 19.6008 - test loss: 20.0147 - train acc: 0.1654 - test acc: 0.1684 - 0m 52s\n",
      "batch: 1100/1563 - train loss: 19.4089 - test loss: 19.2558 - train acc: 0.1789 - test acc: 0.1864 - 0m 58s\n",
      "batch: 1200/1563 - train loss: 19.2858 - test loss: 19.0463 - train acc: 0.1794 - test acc: 0.1940 - 1m 3s\n",
      "batch: 1300/1563 - train loss: 19.2488 - test loss: 19.8248 - train acc: 0.1822 - test acc: 0.1699 - 1m 9s\n",
      "batch: 1400/1563 - train loss: 18.9231 - test loss: 18.9776 - train acc: 0.1950 - test acc: 0.1929 - 1m 14s\n",
      "batch: 1500/1563 - train loss: 18.6634 - test loss: 19.1033 - train acc: 0.1969 - test acc: 0.1880 - 1m 20s\n",
      "batch: 1563/1563 - train loss: 18.6508 - test loss: 19.9717 - train acc: 0.1973 - test acc: 0.1642 - 1m 24s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.5469 - test loss: 19.9228 - train acc: 0.2322 - test acc: 0.1849 - 1m 30s\n",
      "batch: 200/1563 - train loss: 17.5283 - test loss: 18.1537 - train acc: 0.2307 - test acc: 0.2282 - 1m 35s\n",
      "batch: 300/1563 - train loss: 17.3448 - test loss: 17.4516 - train acc: 0.2341 - test acc: 0.2430 - 1m 41s\n",
      "batch: 400/1563 - train loss: 17.4191 - test loss: 17.3779 - train acc: 0.2344 - test acc: 0.2447 - 1m 47s\n",
      "batch: 500/1563 - train loss: 16.8345 - test loss: 17.1901 - train acc: 0.2503 - test acc: 0.2464 - 1m 52s\n",
      "batch: 600/1563 - train loss: 17.3188 - test loss: 17.5277 - train acc: 0.2506 - test acc: 0.2421 - 1m 57s\n",
      "batch: 700/1563 - train loss: 16.8285 - test loss: 17.6947 - train acc: 0.2578 - test acc: 0.2377 - 2m 3s\n",
      "batch: 800/1563 - train loss: 16.9716 - test loss: 16.8631 - train acc: 0.2553 - test acc: 0.2606 - 2m 8s\n",
      "batch: 900/1563 - train loss: 16.6028 - test loss: 17.5800 - train acc: 0.2719 - test acc: 0.2456 - 2m 14s\n",
      "batch: 1000/1563 - train loss: 16.7745 - test loss: 17.6373 - train acc: 0.2637 - test acc: 0.2294 - 2m 19s\n",
      "batch: 1100/1563 - train loss: 16.5756 - test loss: 16.6403 - train acc: 0.2715 - test acc: 0.2689 - 2m 24s\n",
      "batch: 1200/1563 - train loss: 16.3272 - test loss: 17.5815 - train acc: 0.2793 - test acc: 0.2399 - 2m 30s\n",
      "batch: 1300/1563 - train loss: 16.0283 - test loss: 17.0235 - train acc: 0.2896 - test acc: 0.2595 - 2m 35s\n",
      "batch: 1400/1563 - train loss: 16.2166 - test loss: 17.6492 - train acc: 0.2706 - test acc: 0.2497 - 2m 40s\n",
      "batch: 1500/1563 - train loss: 16.0337 - test loss: 16.0702 - train acc: 0.2825 - test acc: 0.2821 - 2m 46s\n",
      "batch: 1563/1563 - train loss: 15.8644 - test loss: 16.0786 - train acc: 0.2881 - test acc: 0.2906 - 2m 51s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.1188 - test loss: 19.3053 - train acc: 0.3127 - test acc: 0.2193 - 2m 56s\n",
      "batch: 200/1563 - train loss: 15.1606 - test loss: 15.6707 - train acc: 0.3131 - test acc: 0.3070 - 3m 1s\n",
      "batch: 300/1563 - train loss: 14.6848 - test loss: 16.0171 - train acc: 0.3343 - test acc: 0.2959 - 3m 7s\n",
      "batch: 400/1563 - train loss: 14.9934 - test loss: 15.5997 - train acc: 0.3243 - test acc: 0.3057 - 3m 12s\n",
      "batch: 500/1563 - train loss: 14.7614 - test loss: 16.4536 - train acc: 0.3337 - test acc: 0.2827 - 3m 18s\n",
      "batch: 600/1563 - train loss: 14.6939 - test loss: 15.9048 - train acc: 0.3415 - test acc: 0.2962 - 3m 23s\n",
      "batch: 700/1563 - train loss: 14.6234 - test loss: 15.8810 - train acc: 0.3422 - test acc: 0.2963 - 3m 28s\n",
      "batch: 800/1563 - train loss: 14.8011 - test loss: 15.1587 - train acc: 0.3359 - test acc: 0.3277 - 3m 34s\n",
      "batch: 900/1563 - train loss: 14.7217 - test loss: 15.9311 - train acc: 0.3334 - test acc: 0.3047 - 3m 39s\n",
      "batch: 1000/1563 - train loss: 14.6971 - test loss: 15.2184 - train acc: 0.3387 - test acc: 0.3202 - 3m 45s\n",
      "batch: 1100/1563 - train loss: 14.5827 - test loss: 14.6754 - train acc: 0.3328 - test acc: 0.3403 - 3m 50s\n",
      "batch: 1200/1563 - train loss: 14.5595 - test loss: 16.0685 - train acc: 0.3481 - test acc: 0.2990 - 3m 56s\n",
      "batch: 1300/1563 - train loss: 14.2083 - test loss: 14.7278 - train acc: 0.3550 - test acc: 0.3400 - 4m 1s\n",
      "batch: 1400/1563 - train loss: 14.4651 - test loss: 15.4488 - train acc: 0.3394 - test acc: 0.3239 - 4m 6s\n",
      "batch: 1500/1563 - train loss: 14.2427 - test loss: 14.5508 - train acc: 0.3516 - test acc: 0.3467 - 4m 12s\n",
      "batch: 1563/1563 - train loss: 14.1992 - test loss: 14.4352 - train acc: 0.3591 - test acc: 0.3540 - 4m 16s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.7503 - test loss: 14.5697 - train acc: 0.3984 - test acc: 0.3447 - 4m 22s\n",
      "batch: 200/1563 - train loss: 12.9774 - test loss: 17.1639 - train acc: 0.3928 - test acc: 0.2778 - 4m 27s\n",
      "batch: 300/1563 - train loss: 13.2766 - test loss: 14.7226 - train acc: 0.3847 - test acc: 0.3382 - 4m 33s\n",
      "batch: 400/1563 - train loss: 12.8474 - test loss: 14.1335 - train acc: 0.3995 - test acc: 0.3639 - 4m 38s\n",
      "batch: 500/1563 - train loss: 13.0256 - test loss: 15.3253 - train acc: 0.3884 - test acc: 0.3345 - 4m 43s\n",
      "batch: 600/1563 - train loss: 13.1986 - test loss: 14.2348 - train acc: 0.3957 - test acc: 0.3650 - 4m 49s\n",
      "batch: 700/1563 - train loss: 12.9989 - test loss: 15.4801 - train acc: 0.3956 - test acc: 0.3274 - 4m 54s\n",
      "batch: 800/1563 - train loss: 13.1423 - test loss: 14.8691 - train acc: 0.3878 - test acc: 0.3428 - 4m 59s\n",
      "batch: 900/1563 - train loss: 13.1786 - test loss: 14.6673 - train acc: 0.3984 - test acc: 0.3525 - 5m 5s\n",
      "batch: 1000/1563 - train loss: 13.3430 - test loss: 14.4144 - train acc: 0.3838 - test acc: 0.3513 - 5m 10s\n",
      "batch: 1100/1563 - train loss: 13.4355 - test loss: 13.9873 - train acc: 0.3791 - test acc: 0.3642 - 5m 16s\n",
      "batch: 1200/1563 - train loss: 13.0560 - test loss: 14.7749 - train acc: 0.3997 - test acc: 0.3454 - 5m 21s\n",
      "batch: 1300/1563 - train loss: 13.1196 - test loss: 14.5315 - train acc: 0.3988 - test acc: 0.3575 - 5m 27s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.0160 - test loss: 15.2929 - train acc: 0.3956 - test acc: 0.3350 - 5m 32s\n",
      "batch: 1500/1563 - train loss: 13.1316 - test loss: 14.8903 - train acc: 0.3975 - test acc: 0.3434 - 5m 38s\n",
      "batch: 1563/1563 - train loss: 12.7507 - test loss: 14.0456 - train acc: 0.4094 - test acc: 0.3700 - 5m 42s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3308 - test loss: 14.0926 - train acc: 0.4638 - test acc: 0.3691 - 5m 47s\n",
      "batch: 200/1563 - train loss: 11.3616 - test loss: 14.4192 - train acc: 0.4537 - test acc: 0.3644 - 5m 53s\n",
      "batch: 300/1563 - train loss: 11.5675 - test loss: 15.3566 - train acc: 0.4481 - test acc: 0.3414 - 5m 59s\n",
      "batch: 400/1563 - train loss: 11.5861 - test loss: 14.1948 - train acc: 0.4528 - test acc: 0.3669 - 6m 4s\n",
      "batch: 500/1563 - train loss: 11.8850 - test loss: 14.1636 - train acc: 0.4332 - test acc: 0.3675 - 6m 9s\n",
      "batch: 600/1563 - train loss: 11.7500 - test loss: 15.9166 - train acc: 0.4432 - test acc: 0.3192 - 6m 15s\n",
      "batch: 700/1563 - train loss: 11.8489 - test loss: 13.8516 - train acc: 0.4285 - test acc: 0.3882 - 6m 20s\n",
      "batch: 800/1563 - train loss: 11.7289 - test loss: 14.2804 - train acc: 0.4307 - test acc: 0.3682 - 6m 25s\n",
      "batch: 900/1563 - train loss: 11.7659 - test loss: 14.4287 - train acc: 0.4357 - test acc: 0.3694 - 6m 31s\n",
      "batch: 1000/1563 - train loss: 11.9257 - test loss: 13.9484 - train acc: 0.4544 - test acc: 0.3761 - 6m 36s\n",
      "batch: 1100/1563 - train loss: 12.1919 - test loss: 13.6956 - train acc: 0.4391 - test acc: 0.3917 - 6m 42s\n",
      "batch: 1200/1563 - train loss: 11.7831 - test loss: 14.1593 - train acc: 0.4356 - test acc: 0.3762 - 6m 47s\n",
      "batch: 1300/1563 - train loss: 12.2936 - test loss: 14.8118 - train acc: 0.4219 - test acc: 0.3476 - 6m 52s\n",
      "batch: 1400/1563 - train loss: 11.7994 - test loss: 13.3229 - train acc: 0.4447 - test acc: 0.3995 - 6m 57s\n",
      "batch: 1500/1563 - train loss: 12.0888 - test loss: 14.3112 - train acc: 0.4288 - test acc: 0.3594 - 7m 3s\n",
      "batch: 1563/1563 - train loss: 11.9668 - test loss: 13.9480 - train acc: 0.4297 - test acc: 0.3776 - 7m 7s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.3828 - test loss: 13.8029 - train acc: 0.4935 - test acc: 0.3871 - 7m 13s\n",
      "batch: 200/1563 - train loss: 10.1928 - test loss: 13.6314 - train acc: 0.5025 - test acc: 0.4003 - 7m 18s\n",
      "batch: 300/1563 - train loss: 10.0363 - test loss: 14.2868 - train acc: 0.5034 - test acc: 0.3760 - 7m 23s\n",
      "batch: 400/1563 - train loss: 10.5419 - test loss: 13.5363 - train acc: 0.4843 - test acc: 0.4005 - 7m 29s\n",
      "batch: 500/1563 - train loss: 10.4386 - test loss: 13.3170 - train acc: 0.4984 - test acc: 0.4015 - 7m 34s\n",
      "batch: 600/1563 - train loss: 10.7098 - test loss: 14.0059 - train acc: 0.4781 - test acc: 0.3845 - 7m 40s\n",
      "batch: 700/1563 - train loss: 10.8427 - test loss: 14.5671 - train acc: 0.4853 - test acc: 0.3668 - 7m 45s\n",
      "batch: 800/1563 - train loss: 10.5198 - test loss: 14.0856 - train acc: 0.4978 - test acc: 0.3800 - 7m 50s\n",
      "batch: 900/1563 - train loss: 10.7643 - test loss: 14.0540 - train acc: 0.4760 - test acc: 0.3727 - 7m 56s\n",
      "batch: 1000/1563 - train loss: 11.0113 - test loss: 13.8262 - train acc: 0.4684 - test acc: 0.3902 - 8m 1s\n",
      "batch: 1100/1563 - train loss: 10.5628 - test loss: 16.1863 - train acc: 0.4931 - test acc: 0.3383 - 8m 6s\n",
      "batch: 1200/1563 - train loss: 10.8158 - test loss: 14.0734 - train acc: 0.4787 - test acc: 0.3842 - 8m 12s\n",
      "batch: 1300/1563 - train loss: 11.0004 - test loss: 15.6747 - train acc: 0.4791 - test acc: 0.3391 - 8m 17s\n",
      "batch: 1400/1563 - train loss: 10.7549 - test loss: 14.0329 - train acc: 0.4797 - test acc: 0.3877 - 8m 23s\n",
      "batch: 1500/1563 - train loss: 10.8776 - test loss: 13.5233 - train acc: 0.4763 - test acc: 0.4021 - 8m 28s\n",
      "batch: 1563/1563 - train loss: 10.7898 - test loss: 13.7510 - train acc: 0.4931 - test acc: 0.3965 - 8m 32s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.9744 - test loss: 15.6102 - train acc: 0.5550 - test acc: 0.3647 - 8m 38s\n",
      "batch: 200/1563 - train loss: 8.8591 - test loss: 13.3705 - train acc: 0.5515 - test acc: 0.4111 - 8m 43s\n",
      "batch: 300/1563 - train loss: 8.7903 - test loss: 14.5605 - train acc: 0.5549 - test acc: 0.3919 - 8m 49s\n",
      "batch: 400/1563 - train loss: 9.2573 - test loss: 13.6248 - train acc: 0.5447 - test acc: 0.4068 - 8m 54s\n",
      "batch: 500/1563 - train loss: 9.3959 - test loss: 13.6544 - train acc: 0.5494 - test acc: 0.4043 - 8m 59s\n",
      "batch: 600/1563 - train loss: 9.6606 - test loss: 14.3085 - train acc: 0.5225 - test acc: 0.3865 - 9m 5s\n",
      "batch: 700/1563 - train loss: 9.5347 - test loss: 14.2665 - train acc: 0.5375 - test acc: 0.3908 - 9m 10s\n",
      "batch: 800/1563 - train loss: 9.5839 - test loss: 13.5928 - train acc: 0.5397 - test acc: 0.4136 - 9m 16s\n",
      "batch: 900/1563 - train loss: 9.6898 - test loss: 13.9584 - train acc: 0.5185 - test acc: 0.3976 - 9m 21s\n",
      "batch: 1000/1563 - train loss: 9.6008 - test loss: 13.4233 - train acc: 0.5219 - test acc: 0.4155 - 9m 27s\n",
      "batch: 1100/1563 - train loss: 9.8247 - test loss: 13.9648 - train acc: 0.5272 - test acc: 0.3963 - 9m 32s\n",
      "batch: 1200/1563 - train loss: 10.0425 - test loss: 14.0092 - train acc: 0.5143 - test acc: 0.4034 - 9m 38s\n",
      "batch: 1300/1563 - train loss: 9.7954 - test loss: 13.9436 - train acc: 0.5191 - test acc: 0.3899 - 9m 43s\n",
      "batch: 1400/1563 - train loss: 9.7738 - test loss: 13.5065 - train acc: 0.5184 - test acc: 0.4097 - 9m 48s\n",
      "batch: 1500/1563 - train loss: 9.9260 - test loss: 13.2079 - train acc: 0.5163 - test acc: 0.4168 - 9m 53s\n",
      "batch: 1563/1563 - train loss: 9.9480 - test loss: 13.3682 - train acc: 0.5153 - test acc: 0.4074 - 9m 58s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.5775 - test loss: 13.9916 - train acc: 0.6112 - test acc: 0.4128 - 10m 4s\n",
      "batch: 200/1563 - train loss: 7.8446 - test loss: 13.7414 - train acc: 0.6066 - test acc: 0.4161 - 10m 10s\n",
      "batch: 300/1563 - train loss: 8.0436 - test loss: 14.1591 - train acc: 0.6005 - test acc: 0.4034 - 10m 15s\n",
      "batch: 400/1563 - train loss: 7.9995 - test loss: 14.4605 - train acc: 0.5937 - test acc: 0.4000 - 10m 21s\n",
      "batch: 500/1563 - train loss: 8.3326 - test loss: 14.1371 - train acc: 0.5859 - test acc: 0.4025 - 10m 26s\n",
      "batch: 600/1563 - train loss: 8.4021 - test loss: 13.7599 - train acc: 0.5765 - test acc: 0.4085 - 10m 31s\n",
      "batch: 700/1563 - train loss: 8.5850 - test loss: 14.1929 - train acc: 0.5763 - test acc: 0.3987 - 10m 37s\n",
      "batch: 800/1563 - train loss: 8.6584 - test loss: 14.1119 - train acc: 0.5663 - test acc: 0.4086 - 10m 43s\n",
      "batch: 900/1563 - train loss: 8.7361 - test loss: 14.1349 - train acc: 0.5743 - test acc: 0.3984 - 10m 48s\n",
      "batch: 1000/1563 - train loss: 8.9233 - test loss: 13.8525 - train acc: 0.5518 - test acc: 0.4031 - 10m 53s\n",
      "batch: 1100/1563 - train loss: 8.8921 - test loss: 14.6335 - train acc: 0.5660 - test acc: 0.3816 - 10m 59s\n",
      "batch: 1200/1563 - train loss: 8.9316 - test loss: 14.4520 - train acc: 0.5547 - test acc: 0.3917 - 11m 4s\n",
      "batch: 1300/1563 - train loss: 8.9369 - test loss: 14.0594 - train acc: 0.5581 - test acc: 0.3993 - 11m 9s\n",
      "batch: 1400/1563 - train loss: 8.9864 - test loss: 13.9923 - train acc: 0.5565 - test acc: 0.4044 - 11m 15s\n",
      "batch: 1500/1563 - train loss: 9.2046 - test loss: 13.8699 - train acc: 0.5463 - test acc: 0.4041 - 11m 20s\n",
      "batch: 1563/1563 - train loss: 9.0556 - test loss: 14.1065 - train acc: 0.5534 - test acc: 0.4087 - 11m 25s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.5673 - test loss: 13.4734 - train acc: 0.6647 - test acc: 0.4265 - 11m 30s\n",
      "batch: 200/1563 - train loss: 6.8006 - test loss: 13.9363 - train acc: 0.6472 - test acc: 0.4219 - 11m 36s\n",
      "batch: 300/1563 - train loss: 6.7311 - test loss: 14.9526 - train acc: 0.6519 - test acc: 0.3976 - 11m 41s\n",
      "batch: 400/1563 - train loss: 7.0125 - test loss: 14.5104 - train acc: 0.6450 - test acc: 0.4113 - 11m 47s\n",
      "batch: 500/1563 - train loss: 7.4369 - test loss: 14.2195 - train acc: 0.6225 - test acc: 0.4184 - 11m 52s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.2366 - test loss: 15.4787 - train acc: 0.6194 - test acc: 0.3848 - 11m 58s\n",
      "batch: 700/1563 - train loss: 7.7028 - test loss: 14.9122 - train acc: 0.6034 - test acc: 0.3933 - 12m 4s\n",
      "batch: 800/1563 - train loss: 7.6778 - test loss: 14.2275 - train acc: 0.6047 - test acc: 0.4127 - 12m 9s\n",
      "batch: 900/1563 - train loss: 7.8373 - test loss: 16.5945 - train acc: 0.6040 - test acc: 0.3584 - 12m 14s\n",
      "batch: 1000/1563 - train loss: 7.9061 - test loss: 14.4491 - train acc: 0.5975 - test acc: 0.4054 - 12m 20s\n",
      "batch: 1100/1563 - train loss: 7.9863 - test loss: 14.0066 - train acc: 0.5969 - test acc: 0.4162 - 12m 25s\n",
      "batch: 1200/1563 - train loss: 7.7145 - test loss: 14.5644 - train acc: 0.6090 - test acc: 0.4052 - 12m 31s\n",
      "batch: 1300/1563 - train loss: 8.0068 - test loss: 14.4611 - train acc: 0.5981 - test acc: 0.4020 - 12m 36s\n",
      "batch: 1400/1563 - train loss: 7.9445 - test loss: 13.9616 - train acc: 0.5931 - test acc: 0.4183 - 12m 42s\n",
      "batch: 1500/1563 - train loss: 8.1292 - test loss: 13.9009 - train acc: 0.5853 - test acc: 0.4191 - 12m 47s\n",
      "batch: 1563/1563 - train loss: 7.8575 - test loss: 13.4639 - train acc: 0.5987 - test acc: 0.4316 - 12m 52s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.7141 - test loss: 14.1826 - train acc: 0.6978 - test acc: 0.4226 - 12m 58s\n",
      "batch: 200/1563 - train loss: 5.5396 - test loss: 14.7256 - train acc: 0.7022 - test acc: 0.4214 - 13m 4s\n",
      "batch: 300/1563 - train loss: 6.3201 - test loss: 14.3508 - train acc: 0.6697 - test acc: 0.4247 - 13m 9s\n",
      "batch: 400/1563 - train loss: 5.9164 - test loss: 15.0193 - train acc: 0.6835 - test acc: 0.4132 - 13m 15s\n",
      "batch: 500/1563 - train loss: 6.7482 - test loss: 15.1971 - train acc: 0.6529 - test acc: 0.4009 - 13m 21s\n",
      "batch: 600/1563 - train loss: 6.5339 - test loss: 15.2766 - train acc: 0.6575 - test acc: 0.4071 - 13m 26s\n",
      "batch: 700/1563 - train loss: 6.7433 - test loss: 14.0433 - train acc: 0.6453 - test acc: 0.4302 - 13m 31s\n",
      "batch: 800/1563 - train loss: 6.4746 - test loss: 14.4293 - train acc: 0.6688 - test acc: 0.4257 - 13m 37s\n",
      "batch: 900/1563 - train loss: 6.7285 - test loss: 14.8265 - train acc: 0.6528 - test acc: 0.4147 - 13m 42s\n",
      "batch: 1000/1563 - train loss: 6.8327 - test loss: 14.3269 - train acc: 0.6519 - test acc: 0.4267 - 13m 48s\n",
      "batch: 1100/1563 - train loss: 6.9459 - test loss: 14.6332 - train acc: 0.6381 - test acc: 0.4184 - 13m 53s\n",
      "batch: 1200/1563 - train loss: 6.8299 - test loss: 14.5518 - train acc: 0.6506 - test acc: 0.4196 - 13m 59s\n",
      "batch: 1300/1563 - train loss: 7.0022 - test loss: 14.5947 - train acc: 0.6331 - test acc: 0.4183 - 14m 4s\n",
      "batch: 1400/1563 - train loss: 7.4278 - test loss: 14.6955 - train acc: 0.6200 - test acc: 0.4097 - 14m 10s\n",
      "batch: 1500/1563 - train loss: 7.5189 - test loss: 14.4032 - train acc: 0.6119 - test acc: 0.4156 - 14m 15s\n",
      "batch: 1563/1563 - train loss: 7.3426 - test loss: 15.2323 - train acc: 0.6109 - test acc: 0.3937 - 14m 19s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.9397 - test loss: 15.2929 - train acc: 0.7347 - test acc: 0.4189 - 14m 25s\n",
      "batch: 200/1563 - train loss: 4.8267 - test loss: 15.2285 - train acc: 0.7372 - test acc: 0.4202 - 14m 31s\n",
      "batch: 300/1563 - train loss: 4.9594 - test loss: 15.0145 - train acc: 0.7381 - test acc: 0.4218 - 14m 36s\n",
      "batch: 400/1563 - train loss: 5.3436 - test loss: 15.1409 - train acc: 0.7078 - test acc: 0.4195 - 14m 42s\n",
      "batch: 500/1563 - train loss: 5.2851 - test loss: 15.6842 - train acc: 0.7237 - test acc: 0.4154 - 14m 47s\n",
      "batch: 600/1563 - train loss: 5.5767 - test loss: 15.7532 - train acc: 0.6910 - test acc: 0.4129 - 14m 52s\n",
      "batch: 700/1563 - train loss: 5.6032 - test loss: 16.0815 - train acc: 0.7050 - test acc: 0.4014 - 14m 58s\n",
      "batch: 800/1563 - train loss: 5.7773 - test loss: 15.5179 - train acc: 0.6813 - test acc: 0.4091 - 15m 4s\n",
      "batch: 900/1563 - train loss: 6.0523 - test loss: 14.9092 - train acc: 0.6885 - test acc: 0.4200 - 15m 9s\n",
      "batch: 1000/1563 - train loss: 6.0948 - test loss: 15.1621 - train acc: 0.6794 - test acc: 0.4179 - 15m 14s\n",
      "batch: 1100/1563 - train loss: 6.3071 - test loss: 14.8885 - train acc: 0.6700 - test acc: 0.4200 - 15m 20s\n",
      "batch: 1200/1563 - train loss: 6.3795 - test loss: 14.7268 - train acc: 0.6672 - test acc: 0.4282 - 15m 25s\n",
      "batch: 1300/1563 - train loss: 6.2308 - test loss: 14.7079 - train acc: 0.6778 - test acc: 0.4172 - 15m 31s\n",
      "batch: 1400/1563 - train loss: 6.5514 - test loss: 14.5468 - train acc: 0.6566 - test acc: 0.4305 - 15m 36s\n",
      "batch: 1500/1563 - train loss: 6.4206 - test loss: 14.7023 - train acc: 0.6691 - test acc: 0.4274 - 15m 42s\n",
      "batch: 1563/1563 - train loss: 6.3755 - test loss: 15.2245 - train acc: 0.6609 - test acc: 0.4119 - 15m 46s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.9949 - test loss: 14.9977 - train acc: 0.7869 - test acc: 0.4325 - 15m 52s\n",
      "batch: 200/1563 - train loss: 4.0891 - test loss: 16.0448 - train acc: 0.7752 - test acc: 0.4174 - 15m 57s\n",
      "batch: 300/1563 - train loss: 4.2134 - test loss: 16.6677 - train acc: 0.7715 - test acc: 0.4061 - 16m 3s\n",
      "batch: 400/1563 - train loss: 4.4322 - test loss: 16.6190 - train acc: 0.7521 - test acc: 0.4111 - 16m 8s\n",
      "batch: 500/1563 - train loss: 4.6916 - test loss: 16.4932 - train acc: 0.7425 - test acc: 0.4055 - 16m 13s\n",
      "batch: 600/1563 - train loss: 5.0928 - test loss: 17.1122 - train acc: 0.7288 - test acc: 0.3904 - 16m 18s\n",
      "batch: 700/1563 - train loss: 4.7682 - test loss: 15.7827 - train acc: 0.7453 - test acc: 0.4222 - 16m 24s\n",
      "batch: 800/1563 - train loss: 4.7850 - test loss: 16.1144 - train acc: 0.7406 - test acc: 0.4117 - 16m 29s\n",
      "batch: 900/1563 - train loss: 5.4675 - test loss: 15.5936 - train acc: 0.7047 - test acc: 0.4234 - 16m 35s\n",
      "batch: 1000/1563 - train loss: 5.3555 - test loss: 16.0092 - train acc: 0.7175 - test acc: 0.4142 - 16m 40s\n",
      "batch: 1100/1563 - train loss: 5.3359 - test loss: 16.1929 - train acc: 0.7206 - test acc: 0.4112 - 16m 46s\n",
      "batch: 1200/1563 - train loss: 5.4414 - test loss: 15.5790 - train acc: 0.7116 - test acc: 0.4146 - 16m 51s\n",
      "batch: 1300/1563 - train loss: 5.4276 - test loss: 15.8218 - train acc: 0.7050 - test acc: 0.4179 - 16m 57s\n",
      "batch: 1400/1563 - train loss: 5.6393 - test loss: 15.7011 - train acc: 0.6981 - test acc: 0.4254 - 17m 2s\n",
      "batch: 1500/1563 - train loss: 5.5412 - test loss: 16.3141 - train acc: 0.7032 - test acc: 0.4105 - 17m 8s\n",
      "batch: 1563/1563 - train loss: 5.7302 - test loss: 16.5936 - train acc: 0.6975 - test acc: 0.4069 - 17m 12s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.4105 - test loss: 15.3903 - train acc: 0.8214 - test acc: 0.4388 - 17m 18s\n",
      "batch: 200/1563 - train loss: 3.4210 - test loss: 16.2688 - train acc: 0.8100 - test acc: 0.4206 - 17m 23s\n",
      "batch: 300/1563 - train loss: 3.5190 - test loss: 16.5881 - train acc: 0.8159 - test acc: 0.4223 - 17m 28s\n",
      "batch: 400/1563 - train loss: 3.8074 - test loss: 16.0002 - train acc: 0.7899 - test acc: 0.4319 - 17m 33s\n",
      "batch: 500/1563 - train loss: 3.9739 - test loss: 16.4068 - train acc: 0.7803 - test acc: 0.4145 - 17m 39s\n",
      "batch: 600/1563 - train loss: 4.2271 - test loss: 16.1725 - train acc: 0.7596 - test acc: 0.4269 - 17m 45s\n",
      "batch: 700/1563 - train loss: 4.3312 - test loss: 16.5429 - train acc: 0.7609 - test acc: 0.4167 - 17m 50s\n",
      "batch: 800/1563 - train loss: 4.4677 - test loss: 16.4299 - train acc: 0.7578 - test acc: 0.4124 - 17m 55s\n",
      "batch: 900/1563 - train loss: 4.3271 - test loss: 16.9914 - train acc: 0.7491 - test acc: 0.4009 - 18m 1s\n",
      "batch: 1000/1563 - train loss: 4.1828 - test loss: 15.9330 - train acc: 0.7750 - test acc: 0.4261 - 18m 7s\n",
      "batch: 1100/1563 - train loss: 4.7048 - test loss: 17.3587 - train acc: 0.7441 - test acc: 0.3956 - 18m 12s\n",
      "batch: 1200/1563 - train loss: 4.9597 - test loss: 16.6503 - train acc: 0.7359 - test acc: 0.4091 - 18m 17s\n",
      "batch: 1300/1563 - train loss: 4.6952 - test loss: 15.9068 - train acc: 0.7512 - test acc: 0.4288 - 18m 23s\n",
      "batch: 1400/1563 - train loss: 4.5158 - test loss: 15.7346 - train acc: 0.7503 - test acc: 0.4291 - 18m 28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 4.9460 - test loss: 15.8154 - train acc: 0.7341 - test acc: 0.4289 - 18m 34s\n",
      "batch: 1563/1563 - train loss: 5.1564 - test loss: 15.9478 - train acc: 0.7257 - test acc: 0.4223 - 18m 39s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.7470 - test loss: 16.3441 - train acc: 0.8428 - test acc: 0.4285 - 18m 44s\n",
      "batch: 200/1563 - train loss: 2.7951 - test loss: 17.0944 - train acc: 0.8497 - test acc: 0.4148 - 18m 49s\n",
      "batch: 300/1563 - train loss: 3.0584 - test loss: 17.0704 - train acc: 0.8371 - test acc: 0.4216 - 18m 55s\n",
      "batch: 400/1563 - train loss: 3.2474 - test loss: 16.9926 - train acc: 0.8249 - test acc: 0.4296 - 19m 0s\n",
      "batch: 500/1563 - train loss: 3.4730 - test loss: 16.6137 - train acc: 0.8065 - test acc: 0.4256 - 19m 6s\n",
      "batch: 600/1563 - train loss: 3.4379 - test loss: 16.7765 - train acc: 0.8037 - test acc: 0.4221 - 19m 12s\n",
      "batch: 700/1563 - train loss: 3.6643 - test loss: 17.9213 - train acc: 0.7934 - test acc: 0.4104 - 19m 17s\n",
      "batch: 800/1563 - train loss: 3.7700 - test loss: 17.5874 - train acc: 0.7943 - test acc: 0.4081 - 19m 22s\n",
      "batch: 900/1563 - train loss: 3.7965 - test loss: 16.5154 - train acc: 0.7828 - test acc: 0.4293 - 19m 27s\n",
      "batch: 1000/1563 - train loss: 3.8405 - test loss: 17.7848 - train acc: 0.7896 - test acc: 0.4071 - 19m 33s\n",
      "batch: 1100/1563 - train loss: 4.1609 - test loss: 17.1503 - train acc: 0.7775 - test acc: 0.4110 - 19m 39s\n",
      "batch: 1200/1563 - train loss: 4.1155 - test loss: 16.9908 - train acc: 0.7693 - test acc: 0.4119 - 19m 44s\n",
      "batch: 1300/1563 - train loss: 4.2721 - test loss: 16.9398 - train acc: 0.7709 - test acc: 0.4058 - 19m 50s\n",
      "batch: 1400/1563 - train loss: 3.9744 - test loss: 16.5733 - train acc: 0.7806 - test acc: 0.4308 - 19m 55s\n",
      "time is up! finishing training\n",
      "batch: 1494/1563 - train loss: 4.5010 - test loss: 16.5623 - train acc: 0.7503 - test acc: 0.4270 - 20m 1s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n",
      "testing - buf size: 140 - part size: 30 - block upd: 360 - combination nº: 20\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "FisherPartitioner: param: 9408 - partition: 30 - nº part: 314 - block updates: 314\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 36864 - partition: 30 - nº part: 1229 - block updates: 360\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 64 - partition: 30 - nº part: 3 - block updates: 3\n",
      "FisherPartitioner: param: 73728 - partition: 30 - nº part: 2458 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 8192 - partition: 30 - nº part: 274 - block updates: 274\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 147456 - partition: 30 - nº part: 4916 - block updates: 360\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 128 - partition: 30 - nº part: 5 - block updates: 5\n",
      "FisherPartitioner: param: 294912 - partition: 30 - nº part: 9831 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 32768 - partition: 30 - nº part: 1093 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 589824 - partition: 30 - nº part: 19661 - block updates: 360\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 256 - partition: 30 - nº part: 9 - block updates: 9\n",
      "FisherPartitioner: param: 1179648 - partition: 30 - nº part: 39322 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 131072 - partition: 30 - nº part: 4370 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 2359296 - partition: 30 - nº part: 78644 - block updates: 360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 512 - partition: 30 - nº part: 18 - block updates: 18\n",
      "FisherPartitioner: param: 51200 - partition: 30 - nº part: 1707 - block updates: 360\n",
      "FisherPartitioner: param: 100 - partition: 30 - nº part: 4 - block updates: 4\n",
      "total partitions: 374302 - effective block updates: 7782\n",
      "initializing buffers and inverses...\n",
      "partition 1/374302\n",
      "partition 10000/374302\n",
      "partition 20000/374302\n",
      "partition 30000/374302\n",
      "partition 40000/374302\n",
      "partition 50000/374302\n",
      "partition 60000/374302\n",
      "partition 70000/374302\n",
      "partition 80000/374302\n",
      "partition 90000/374302\n",
      "partition 100000/374302\n",
      "partition 110000/374302\n",
      "partition 120000/374302\n",
      "partition 130000/374302\n",
      "partition 140000/374302\n",
      "partition 150000/374302\n",
      "partition 160000/374302\n",
      "partition 170000/374302\n",
      "partition 180000/374302\n",
      "partition 190000/374302\n",
      "partition 200000/374302\n",
      "partition 210000/374302\n",
      "partition 220000/374302\n",
      "partition 230000/374302\n",
      "partition 240000/374302\n",
      "partition 250000/374302\n",
      "partition 260000/374302\n",
      "partition 270000/374302\n",
      "partition 280000/374302\n",
      "partition 290000/374302\n",
      "partition 300000/374302\n",
      "partition 310000/374302\n",
      "partition 320000/374302\n",
      "partition 330000/374302\n",
      "partition 340000/374302\n",
      "partition 350000/374302\n",
      "partition 360000/374302\n",
      "partition 370000/374302\n",
      "partition 374302/374302\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 25.9466 - test loss: 24.8682 - train acc: 0.0349 - test acc: 0.0616 - 0m 2s\n",
      "batch: 200/1563 - train loss: 23.7096 - test loss: 23.4064 - train acc: 0.0802 - test acc: 0.0820 - 0m 7s\n",
      "batch: 300/1563 - train loss: 22.6900 - test loss: 22.9844 - train acc: 0.0861 - test acc: 0.0855 - 0m 13s\n",
      "batch: 400/1563 - train loss: 21.9351 - test loss: 21.9652 - train acc: 0.1002 - test acc: 0.1117 - 0m 18s\n",
      "batch: 500/1563 - train loss: 21.7105 - test loss: 21.5051 - train acc: 0.1144 - test acc: 0.1255 - 0m 24s\n",
      "batch: 600/1563 - train loss: 21.1564 - test loss: 20.9217 - train acc: 0.1231 - test acc: 0.1469 - 0m 29s\n",
      "batch: 700/1563 - train loss: 20.6777 - test loss: 20.8787 - train acc: 0.1419 - test acc: 0.1424 - 0m 34s\n",
      "batch: 800/1563 - train loss: 20.3048 - test loss: 20.5195 - train acc: 0.1522 - test acc: 0.1450 - 0m 40s\n",
      "batch: 900/1563 - train loss: 19.9733 - test loss: 20.4985 - train acc: 0.1503 - test acc: 0.1595 - 0m 45s\n",
      "batch: 1000/1563 - train loss: 19.3730 - test loss: 19.5410 - train acc: 0.1854 - test acc: 0.1717 - 0m 50s\n",
      "batch: 1100/1563 - train loss: 19.6210 - test loss: 20.0057 - train acc: 0.1629 - test acc: 0.1645 - 0m 56s\n",
      "batch: 1200/1563 - train loss: 19.3960 - test loss: 19.2389 - train acc: 0.1841 - test acc: 0.1837 - 1m 1s\n",
      "batch: 1300/1563 - train loss: 19.2755 - test loss: 18.5925 - train acc: 0.1788 - test acc: 0.2012 - 1m 7s\n",
      "batch: 1400/1563 - train loss: 18.8147 - test loss: 18.6118 - train acc: 0.1879 - test acc: 0.1991 - 1m 12s\n",
      "batch: 1500/1563 - train loss: 18.7733 - test loss: 18.0162 - train acc: 0.2067 - test acc: 0.2214 - 1m 17s\n",
      "batch: 1563/1563 - train loss: 18.6479 - test loss: 18.7058 - train acc: 0.1973 - test acc: 0.1975 - 1m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.6712 - test loss: 18.4911 - train acc: 0.2313 - test acc: 0.2169 - 1m 28s\n",
      "batch: 200/1563 - train loss: 18.0290 - test loss: 18.1211 - train acc: 0.2200 - test acc: 0.2193 - 1m 33s\n",
      "batch: 300/1563 - train loss: 17.7222 - test loss: 17.6680 - train acc: 0.2300 - test acc: 0.2340 - 1m 38s\n",
      "batch: 400/1563 - train loss: 17.4616 - test loss: 17.4816 - train acc: 0.2373 - test acc: 0.2458 - 1m 44s\n",
      "batch: 500/1563 - train loss: 17.4717 - test loss: 17.9199 - train acc: 0.2360 - test acc: 0.2157 - 1m 49s\n",
      "batch: 600/1563 - train loss: 17.2189 - test loss: 17.1325 - train acc: 0.2422 - test acc: 0.2541 - 1m 54s\n",
      "batch: 700/1563 - train loss: 17.0847 - test loss: 17.3026 - train acc: 0.2572 - test acc: 0.2519 - 2m 0s\n",
      "batch: 800/1563 - train loss: 16.8753 - test loss: 18.9435 - train acc: 0.2528 - test acc: 0.2131 - 2m 5s\n",
      "batch: 900/1563 - train loss: 16.8745 - test loss: 16.7527 - train acc: 0.2606 - test acc: 0.2629 - 2m 11s\n",
      "batch: 1000/1563 - train loss: 16.9364 - test loss: 16.7189 - train acc: 0.2556 - test acc: 0.2632 - 2m 16s\n",
      "batch: 1100/1563 - train loss: 16.5326 - test loss: 16.6067 - train acc: 0.2694 - test acc: 0.2678 - 2m 21s\n",
      "batch: 1200/1563 - train loss: 16.3546 - test loss: 16.6568 - train acc: 0.2762 - test acc: 0.2664 - 2m 27s\n",
      "batch: 1300/1563 - train loss: 16.3081 - test loss: 16.1530 - train acc: 0.2968 - test acc: 0.2839 - 2m 32s\n",
      "batch: 1400/1563 - train loss: 16.1359 - test loss: 16.5758 - train acc: 0.2850 - test acc: 0.2712 - 2m 38s\n",
      "batch: 1500/1563 - train loss: 16.1070 - test loss: 16.2370 - train acc: 0.2884 - test acc: 0.2824 - 2m 43s\n",
      "batch: 1563/1563 - train loss: 16.2079 - test loss: 17.7531 - train acc: 0.2803 - test acc: 0.2396 - 2m 48s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.7407 - test loss: 15.9677 - train acc: 0.3288 - test acc: 0.2964 - 2m 53s\n",
      "batch: 200/1563 - train loss: 15.1583 - test loss: 16.0370 - train acc: 0.3153 - test acc: 0.2935 - 2m 58s\n",
      "batch: 300/1563 - train loss: 14.9995 - test loss: 15.8711 - train acc: 0.3218 - test acc: 0.2963 - 3m 4s\n",
      "batch: 400/1563 - train loss: 15.4186 - test loss: 16.1037 - train acc: 0.3059 - test acc: 0.2926 - 3m 10s\n",
      "batch: 500/1563 - train loss: 14.9695 - test loss: 15.7457 - train acc: 0.3262 - test acc: 0.3061 - 3m 15s\n",
      "batch: 600/1563 - train loss: 15.2001 - test loss: 16.9839 - train acc: 0.3044 - test acc: 0.2564 - 3m 21s\n",
      "batch: 700/1563 - train loss: 14.7466 - test loss: 16.0598 - train acc: 0.3309 - test acc: 0.2956 - 3m 26s\n",
      "batch: 800/1563 - train loss: 14.3595 - test loss: 15.7567 - train acc: 0.3401 - test acc: 0.3155 - 3m 31s\n",
      "batch: 900/1563 - train loss: 14.9223 - test loss: 16.8740 - train acc: 0.3237 - test acc: 0.2729 - 3m 37s\n",
      "batch: 1000/1563 - train loss: 14.6491 - test loss: 15.6644 - train acc: 0.3356 - test acc: 0.3039 - 3m 43s\n",
      "batch: 1100/1563 - train loss: 14.8085 - test loss: 15.1296 - train acc: 0.3338 - test acc: 0.3208 - 3m 48s\n",
      "batch: 1200/1563 - train loss: 14.5061 - test loss: 15.1019 - train acc: 0.3331 - test acc: 0.3277 - 3m 53s\n",
      "batch: 1300/1563 - train loss: 14.1654 - test loss: 16.8705 - train acc: 0.3578 - test acc: 0.2866 - 3m 59s\n",
      "batch: 1400/1563 - train loss: 14.4142 - test loss: 15.5528 - train acc: 0.3547 - test acc: 0.3181 - 4m 4s\n",
      "batch: 1500/1563 - train loss: 14.0757 - test loss: 15.2356 - train acc: 0.3616 - test acc: 0.3218 - 4m 10s\n",
      "batch: 1563/1563 - train loss: 14.5976 - test loss: 15.4079 - train acc: 0.3459 - test acc: 0.3169 - 4m 15s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.9886 - test loss: 15.1817 - train acc: 0.3938 - test acc: 0.3323 - 4m 20s\n",
      "batch: 200/1563 - train loss: 13.1225 - test loss: 14.5632 - train acc: 0.3847 - test acc: 0.3465 - 4m 26s\n",
      "batch: 300/1563 - train loss: 13.2819 - test loss: 15.1953 - train acc: 0.3756 - test acc: 0.3309 - 4m 31s\n",
      "batch: 400/1563 - train loss: 13.4221 - test loss: 15.1669 - train acc: 0.3853 - test acc: 0.3318 - 4m 36s\n",
      "batch: 500/1563 - train loss: 13.4035 - test loss: 14.7362 - train acc: 0.3790 - test acc: 0.3375 - 4m 42s\n",
      "batch: 600/1563 - train loss: 13.2847 - test loss: 14.4144 - train acc: 0.3969 - test acc: 0.3540 - 4m 47s\n",
      "batch: 700/1563 - train loss: 13.4852 - test loss: 14.9773 - train acc: 0.3732 - test acc: 0.3351 - 4m 53s\n",
      "batch: 800/1563 - train loss: 13.4762 - test loss: 15.2995 - train acc: 0.3750 - test acc: 0.3269 - 4m 58s\n",
      "batch: 900/1563 - train loss: 13.0729 - test loss: 14.4420 - train acc: 0.3985 - test acc: 0.3477 - 5m 3s\n",
      "batch: 1000/1563 - train loss: 13.3033 - test loss: 15.2739 - train acc: 0.3832 - test acc: 0.3306 - 5m 9s\n",
      "batch: 1100/1563 - train loss: 13.1477 - test loss: 14.2606 - train acc: 0.3854 - test acc: 0.3562 - 5m 15s\n",
      "batch: 1200/1563 - train loss: 12.9954 - test loss: 14.0813 - train acc: 0.4043 - test acc: 0.3618 - 5m 20s\n",
      "batch: 1300/1563 - train loss: 13.0291 - test loss: 14.8007 - train acc: 0.3909 - test acc: 0.3465 - 5m 25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 13.1178 - test loss: 14.1043 - train acc: 0.3990 - test acc: 0.3626 - 5m 31s\n",
      "batch: 1500/1563 - train loss: 13.1247 - test loss: 14.4040 - train acc: 0.3925 - test acc: 0.3528 - 5m 36s\n",
      "batch: 1563/1563 - train loss: 13.0547 - test loss: 14.6232 - train acc: 0.3928 - test acc: 0.3491 - 5m 41s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.5288 - test loss: 14.1530 - train acc: 0.4478 - test acc: 0.3724 - 5m 47s\n",
      "batch: 200/1563 - train loss: 11.5956 - test loss: 15.3814 - train acc: 0.4453 - test acc: 0.3316 - 5m 52s\n",
      "batch: 300/1563 - train loss: 11.9684 - test loss: 14.4792 - train acc: 0.4332 - test acc: 0.3613 - 5m 57s\n",
      "batch: 400/1563 - train loss: 11.7726 - test loss: 15.0461 - train acc: 0.4475 - test acc: 0.3561 - 6m 3s\n",
      "batch: 500/1563 - train loss: 11.8725 - test loss: 14.6411 - train acc: 0.4385 - test acc: 0.3639 - 6m 8s\n",
      "batch: 600/1563 - train loss: 11.9369 - test loss: 14.6748 - train acc: 0.4316 - test acc: 0.3522 - 6m 14s\n",
      "batch: 700/1563 - train loss: 11.9276 - test loss: 13.8339 - train acc: 0.4363 - test acc: 0.3837 - 6m 19s\n",
      "batch: 800/1563 - train loss: 11.7306 - test loss: 13.6798 - train acc: 0.4300 - test acc: 0.3859 - 6m 24s\n",
      "batch: 900/1563 - train loss: 11.7966 - test loss: 14.3720 - train acc: 0.4507 - test acc: 0.3638 - 6m 30s\n",
      "batch: 1000/1563 - train loss: 12.1997 - test loss: 14.2432 - train acc: 0.4266 - test acc: 0.3704 - 6m 35s\n",
      "batch: 1100/1563 - train loss: 12.2154 - test loss: 13.5771 - train acc: 0.4359 - test acc: 0.3916 - 6m 40s\n",
      "batch: 1200/1563 - train loss: 11.9276 - test loss: 14.0683 - train acc: 0.4266 - test acc: 0.3751 - 6m 46s\n",
      "batch: 1300/1563 - train loss: 12.0186 - test loss: 13.5545 - train acc: 0.4354 - test acc: 0.3941 - 6m 51s\n",
      "batch: 1400/1563 - train loss: 11.8935 - test loss: 13.4296 - train acc: 0.4435 - test acc: 0.3918 - 6m 56s\n",
      "batch: 1500/1563 - train loss: 12.2164 - test loss: 13.9708 - train acc: 0.4166 - test acc: 0.3711 - 7m 2s\n",
      "batch: 1563/1563 - train loss: 12.1269 - test loss: 13.4981 - train acc: 0.4275 - test acc: 0.3864 - 7m 6s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.9403 - test loss: 13.4475 - train acc: 0.5152 - test acc: 0.4050 - 7m 11s\n",
      "batch: 200/1563 - train loss: 10.4137 - test loss: 13.3523 - train acc: 0.4956 - test acc: 0.3955 - 7m 17s\n",
      "batch: 300/1563 - train loss: 10.4556 - test loss: 13.8385 - train acc: 0.4975 - test acc: 0.3905 - 7m 22s\n",
      "batch: 400/1563 - train loss: 10.2948 - test loss: 13.6240 - train acc: 0.5072 - test acc: 0.3903 - 7m 28s\n",
      "batch: 500/1563 - train loss: 10.8621 - test loss: 13.5607 - train acc: 0.4810 - test acc: 0.3920 - 7m 33s\n",
      "batch: 600/1563 - train loss: 10.5806 - test loss: 14.3756 - train acc: 0.4807 - test acc: 0.3735 - 7m 38s\n",
      "batch: 700/1563 - train loss: 10.6729 - test loss: 13.4873 - train acc: 0.4975 - test acc: 0.4006 - 7m 43s\n",
      "batch: 800/1563 - train loss: 10.4496 - test loss: 14.0983 - train acc: 0.5062 - test acc: 0.3845 - 7m 49s\n",
      "batch: 900/1563 - train loss: 10.9626 - test loss: 16.1862 - train acc: 0.4806 - test acc: 0.3405 - 7m 54s\n",
      "batch: 1000/1563 - train loss: 11.0885 - test loss: 13.7331 - train acc: 0.4688 - test acc: 0.3940 - 8m 0s\n",
      "batch: 1100/1563 - train loss: 11.0011 - test loss: 13.8870 - train acc: 0.4743 - test acc: 0.3923 - 8m 5s\n",
      "batch: 1200/1563 - train loss: 10.9901 - test loss: 14.2443 - train acc: 0.4650 - test acc: 0.3718 - 8m 10s\n",
      "batch: 1300/1563 - train loss: 11.1019 - test loss: 13.4789 - train acc: 0.4729 - test acc: 0.3995 - 8m 16s\n",
      "batch: 1400/1563 - train loss: 10.9661 - test loss: 13.7181 - train acc: 0.4700 - test acc: 0.4003 - 8m 22s\n",
      "batch: 1500/1563 - train loss: 11.1883 - test loss: 13.6680 - train acc: 0.4641 - test acc: 0.3878 - 8m 27s\n",
      "batch: 1563/1563 - train loss: 10.9431 - test loss: 13.4370 - train acc: 0.4828 - test acc: 0.4035 - 8m 31s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.9499 - test loss: 13.5542 - train acc: 0.5584 - test acc: 0.4079 - 8m 37s\n",
      "batch: 200/1563 - train loss: 9.0147 - test loss: 13.4834 - train acc: 0.5509 - test acc: 0.4082 - 8m 42s\n",
      "batch: 300/1563 - train loss: 9.5603 - test loss: 15.2137 - train acc: 0.5368 - test acc: 0.3675 - 8m 47s\n",
      "batch: 400/1563 - train loss: 9.2968 - test loss: 13.7136 - train acc: 0.5474 - test acc: 0.4032 - 8m 53s\n",
      "batch: 500/1563 - train loss: 9.6026 - test loss: 13.3635 - train acc: 0.5224 - test acc: 0.4114 - 8m 58s\n",
      "batch: 600/1563 - train loss: 9.4170 - test loss: 14.7585 - train acc: 0.5319 - test acc: 0.3778 - 9m 3s\n",
      "batch: 700/1563 - train loss: 9.7027 - test loss: 14.2437 - train acc: 0.5365 - test acc: 0.3951 - 9m 9s\n",
      "batch: 800/1563 - train loss: 9.8948 - test loss: 14.5693 - train acc: 0.5162 - test acc: 0.3781 - 9m 14s\n",
      "batch: 900/1563 - train loss: 9.6036 - test loss: 13.4128 - train acc: 0.5256 - test acc: 0.4109 - 9m 20s\n",
      "batch: 1000/1563 - train loss: 10.0062 - test loss: 14.3938 - train acc: 0.5113 - test acc: 0.3918 - 9m 26s\n",
      "batch: 1100/1563 - train loss: 9.6024 - test loss: 14.0560 - train acc: 0.5362 - test acc: 0.3943 - 9m 31s\n",
      "batch: 1200/1563 - train loss: 9.9723 - test loss: 13.5244 - train acc: 0.5087 - test acc: 0.4064 - 9m 37s\n",
      "batch: 1300/1563 - train loss: 10.0656 - test loss: 13.4145 - train acc: 0.5106 - test acc: 0.4154 - 9m 42s\n",
      "batch: 1400/1563 - train loss: 10.0427 - test loss: 14.4000 - train acc: 0.5182 - test acc: 0.3816 - 9m 48s\n",
      "batch: 1500/1563 - train loss: 9.9990 - test loss: 13.5447 - train acc: 0.5191 - test acc: 0.4069 - 9m 53s\n",
      "batch: 1563/1563 - train loss: 10.0820 - test loss: 13.4431 - train acc: 0.5172 - test acc: 0.4128 - 9m 58s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.5762 - test loss: 13.8919 - train acc: 0.6235 - test acc: 0.4063 - 10m 3s\n",
      "batch: 200/1563 - train loss: 7.9597 - test loss: 14.5104 - train acc: 0.5978 - test acc: 0.3977 - 10m 9s\n",
      "batch: 300/1563 - train loss: 8.0462 - test loss: 14.1788 - train acc: 0.5928 - test acc: 0.4009 - 10m 15s\n",
      "batch: 400/1563 - train loss: 8.0406 - test loss: 14.2870 - train acc: 0.5919 - test acc: 0.4034 - 10m 21s\n",
      "batch: 500/1563 - train loss: 8.6680 - test loss: 13.5319 - train acc: 0.5618 - test acc: 0.4239 - 10m 26s\n",
      "batch: 600/1563 - train loss: 8.4876 - test loss: 14.7567 - train acc: 0.5794 - test acc: 0.3916 - 10m 32s\n",
      "batch: 700/1563 - train loss: 8.5149 - test loss: 14.0550 - train acc: 0.5691 - test acc: 0.4049 - 10m 38s\n",
      "batch: 800/1563 - train loss: 8.7679 - test loss: 14.1693 - train acc: 0.5637 - test acc: 0.4099 - 10m 44s\n",
      "batch: 900/1563 - train loss: 8.8415 - test loss: 14.2579 - train acc: 0.5562 - test acc: 0.4007 - 10m 50s\n",
      "batch: 1000/1563 - train loss: 8.7215 - test loss: 14.3187 - train acc: 0.5637 - test acc: 0.3963 - 10m 55s\n",
      "batch: 1100/1563 - train loss: 8.9280 - test loss: 13.5307 - train acc: 0.5644 - test acc: 0.4160 - 11m 1s\n",
      "batch: 1200/1563 - train loss: 9.3733 - test loss: 14.0533 - train acc: 0.5397 - test acc: 0.4015 - 11m 7s\n",
      "batch: 1300/1563 - train loss: 9.0505 - test loss: 14.1534 - train acc: 0.5555 - test acc: 0.4004 - 11m 12s\n",
      "batch: 1400/1563 - train loss: 8.9671 - test loss: 13.7689 - train acc: 0.5572 - test acc: 0.4127 - 11m 18s\n",
      "batch: 1500/1563 - train loss: 9.0644 - test loss: 13.2275 - train acc: 0.5456 - test acc: 0.4247 - 11m 23s\n",
      "batch: 1563/1563 - train loss: 8.7917 - test loss: 13.9358 - train acc: 0.5684 - test acc: 0.4074 - 11m 28s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4986 - test loss: 14.2183 - train acc: 0.6666 - test acc: 0.4095 - 11m 34s\n",
      "batch: 200/1563 - train loss: 6.6033 - test loss: 14.0896 - train acc: 0.6656 - test acc: 0.4221 - 11m 40s\n",
      "batch: 300/1563 - train loss: 7.2499 - test loss: 14.0665 - train acc: 0.6281 - test acc: 0.4120 - 11m 45s\n",
      "batch: 400/1563 - train loss: 7.1263 - test loss: 14.8949 - train acc: 0.6343 - test acc: 0.4010 - 11m 50s\n",
      "batch: 500/1563 - train loss: 7.3869 - test loss: 14.9645 - train acc: 0.6122 - test acc: 0.4041 - 11m 56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 7.6263 - test loss: 13.6577 - train acc: 0.6068 - test acc: 0.4148 - 12m 1s\n",
      "batch: 700/1563 - train loss: 7.5079 - test loss: 14.1026 - train acc: 0.6156 - test acc: 0.4149 - 12m 7s\n",
      "batch: 800/1563 - train loss: 7.8148 - test loss: 13.9718 - train acc: 0.6153 - test acc: 0.4188 - 12m 13s\n",
      "batch: 900/1563 - train loss: 7.8648 - test loss: 14.2381 - train acc: 0.5984 - test acc: 0.4149 - 12m 18s\n",
      "batch: 1000/1563 - train loss: 7.9081 - test loss: 13.5798 - train acc: 0.6063 - test acc: 0.4183 - 12m 24s\n",
      "batch: 1100/1563 - train loss: 7.7278 - test loss: 13.7997 - train acc: 0.6112 - test acc: 0.4235 - 12m 29s\n",
      "batch: 1200/1563 - train loss: 7.7515 - test loss: 14.1103 - train acc: 0.6100 - test acc: 0.4114 - 12m 35s\n",
      "batch: 1300/1563 - train loss: 7.9826 - test loss: 14.1105 - train acc: 0.5972 - test acc: 0.4216 - 12m 40s\n",
      "batch: 1400/1563 - train loss: 8.3468 - test loss: 14.3711 - train acc: 0.5787 - test acc: 0.4107 - 12m 46s\n",
      "batch: 1500/1563 - train loss: 8.2782 - test loss: 15.3130 - train acc: 0.5844 - test acc: 0.3811 - 12m 51s\n",
      "batch: 1563/1563 - train loss: 8.2971 - test loss: 13.7639 - train acc: 0.5803 - test acc: 0.4234 - 12m 55s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.5405 - test loss: 14.3829 - train acc: 0.7090 - test acc: 0.4201 - 13m 1s\n",
      "batch: 200/1563 - train loss: 5.5870 - test loss: 14.1766 - train acc: 0.7016 - test acc: 0.4276 - 13m 7s\n",
      "batch: 300/1563 - train loss: 6.0481 - test loss: 14.2624 - train acc: 0.6832 - test acc: 0.4268 - 13m 12s\n",
      "batch: 400/1563 - train loss: 6.4836 - test loss: 14.7467 - train acc: 0.6616 - test acc: 0.4107 - 13m 18s\n",
      "batch: 500/1563 - train loss: 6.3825 - test loss: 15.1381 - train acc: 0.6610 - test acc: 0.3983 - 13m 23s\n",
      "batch: 600/1563 - train loss: 6.4226 - test loss: 14.5602 - train acc: 0.6591 - test acc: 0.4193 - 13m 28s\n",
      "batch: 700/1563 - train loss: 6.8227 - test loss: 15.4141 - train acc: 0.6494 - test acc: 0.4022 - 13m 34s\n",
      "batch: 800/1563 - train loss: 6.8710 - test loss: 14.4206 - train acc: 0.6497 - test acc: 0.4206 - 13m 40s\n",
      "batch: 900/1563 - train loss: 7.1075 - test loss: 14.4332 - train acc: 0.6266 - test acc: 0.4159 - 13m 45s\n",
      "batch: 1000/1563 - train loss: 6.7437 - test loss: 14.2159 - train acc: 0.6428 - test acc: 0.4266 - 13m 50s\n",
      "batch: 1100/1563 - train loss: 7.2498 - test loss: 14.1117 - train acc: 0.6272 - test acc: 0.4189 - 13m 56s\n",
      "batch: 1200/1563 - train loss: 6.9946 - test loss: 14.8809 - train acc: 0.6369 - test acc: 0.4073 - 14m 1s\n",
      "batch: 1300/1563 - train loss: 7.2191 - test loss: 14.8545 - train acc: 0.6291 - test acc: 0.4120 - 14m 6s\n",
      "batch: 1400/1563 - train loss: 7.1449 - test loss: 14.5000 - train acc: 0.6271 - test acc: 0.4099 - 14m 12s\n",
      "batch: 1500/1563 - train loss: 7.4105 - test loss: 14.5694 - train acc: 0.6188 - test acc: 0.4136 - 14m 17s\n",
      "batch: 1563/1563 - train loss: 7.5814 - test loss: 14.2305 - train acc: 0.6057 - test acc: 0.4141 - 14m 22s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.8782 - test loss: 14.2886 - train acc: 0.7406 - test acc: 0.4268 - 14m 27s\n",
      "batch: 200/1563 - train loss: 4.9211 - test loss: 14.6445 - train acc: 0.7435 - test acc: 0.4344 - 14m 32s\n",
      "batch: 300/1563 - train loss: 5.1933 - test loss: 14.8275 - train acc: 0.7291 - test acc: 0.4312 - 14m 38s\n",
      "batch: 400/1563 - train loss: 5.3891 - test loss: 16.3327 - train acc: 0.7032 - test acc: 0.4054 - 14m 43s\n",
      "batch: 500/1563 - train loss: 5.4314 - test loss: 14.9219 - train acc: 0.7063 - test acc: 0.4213 - 14m 49s\n",
      "batch: 600/1563 - train loss: 5.8436 - test loss: 15.1372 - train acc: 0.6891 - test acc: 0.4230 - 14m 54s\n",
      "batch: 700/1563 - train loss: 5.7491 - test loss: 15.2602 - train acc: 0.6875 - test acc: 0.4117 - 15m 0s\n",
      "batch: 800/1563 - train loss: 5.8189 - test loss: 15.3303 - train acc: 0.6878 - test acc: 0.4161 - 15m 5s\n",
      "batch: 900/1563 - train loss: 6.4024 - test loss: 15.0702 - train acc: 0.6657 - test acc: 0.4167 - 15m 10s\n",
      "batch: 1000/1563 - train loss: 6.1520 - test loss: 15.1403 - train acc: 0.6822 - test acc: 0.4210 - 15m 16s\n",
      "batch: 1100/1563 - train loss: 6.4232 - test loss: 15.0452 - train acc: 0.6582 - test acc: 0.4122 - 15m 21s\n",
      "batch: 1200/1563 - train loss: 6.5402 - test loss: 14.6276 - train acc: 0.6610 - test acc: 0.4249 - 15m 27s\n",
      "batch: 1300/1563 - train loss: 6.2449 - test loss: 14.6971 - train acc: 0.6732 - test acc: 0.4256 - 15m 32s\n",
      "batch: 1400/1563 - train loss: 6.1454 - test loss: 14.7185 - train acc: 0.6779 - test acc: 0.4191 - 15m 37s\n",
      "batch: 1500/1563 - train loss: 6.4426 - test loss: 14.6857 - train acc: 0.6665 - test acc: 0.4271 - 15m 43s\n",
      "batch: 1563/1563 - train loss: 6.3450 - test loss: 14.7060 - train acc: 0.6666 - test acc: 0.4260 - 15m 48s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.2136 - test loss: 15.6533 - train acc: 0.7715 - test acc: 0.4202 - 15m 53s\n",
      "batch: 200/1563 - train loss: 3.9234 - test loss: 15.1236 - train acc: 0.7893 - test acc: 0.4322 - 15m 59s\n",
      "batch: 300/1563 - train loss: 3.9076 - test loss: 15.6661 - train acc: 0.7930 - test acc: 0.4259 - 16m 4s\n",
      "batch: 400/1563 - train loss: 4.6150 - test loss: 15.4815 - train acc: 0.7568 - test acc: 0.4281 - 16m 10s\n",
      "batch: 500/1563 - train loss: 4.6383 - test loss: 16.0784 - train acc: 0.7525 - test acc: 0.4112 - 16m 15s\n",
      "batch: 600/1563 - train loss: 4.7802 - test loss: 15.3647 - train acc: 0.7418 - test acc: 0.4244 - 16m 21s\n",
      "batch: 700/1563 - train loss: 4.9618 - test loss: 15.2396 - train acc: 0.7360 - test acc: 0.4318 - 16m 26s\n",
      "batch: 800/1563 - train loss: 5.2325 - test loss: 14.9712 - train acc: 0.7378 - test acc: 0.4354 - 16m 32s\n",
      "batch: 900/1563 - train loss: 5.1888 - test loss: 15.7338 - train acc: 0.7137 - test acc: 0.4112 - 16m 37s\n",
      "batch: 1000/1563 - train loss: 5.0785 - test loss: 15.1971 - train acc: 0.7300 - test acc: 0.4283 - 16m 43s\n",
      "batch: 1100/1563 - train loss: 5.4243 - test loss: 15.0021 - train acc: 0.7103 - test acc: 0.4263 - 16m 50s\n",
      "batch: 1200/1563 - train loss: 5.5279 - test loss: 15.7350 - train acc: 0.7082 - test acc: 0.4167 - 16m 55s\n",
      "batch: 1300/1563 - train loss: 5.8313 - test loss: 15.5638 - train acc: 0.6850 - test acc: 0.4182 - 17m 1s\n",
      "batch: 1400/1563 - train loss: 5.9175 - test loss: 15.7898 - train acc: 0.6750 - test acc: 0.4209 - 17m 7s\n",
      "batch: 1500/1563 - train loss: 5.7418 - test loss: 14.9164 - train acc: 0.6888 - test acc: 0.4279 - 17m 12s\n",
      "batch: 1563/1563 - train loss: 5.7211 - test loss: 15.4098 - train acc: 0.6963 - test acc: 0.4247 - 17m 16s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.6463 - test loss: 15.1988 - train acc: 0.8036 - test acc: 0.4337 - 17m 22s\n",
      "batch: 200/1563 - train loss: 3.2036 - test loss: 15.8293 - train acc: 0.8287 - test acc: 0.4298 - 17m 27s\n",
      "batch: 300/1563 - train loss: 3.5327 - test loss: 16.0295 - train acc: 0.8049 - test acc: 0.4270 - 17m 33s\n",
      "batch: 400/1563 - train loss: 3.7593 - test loss: 15.8939 - train acc: 0.7940 - test acc: 0.4285 - 17m 39s\n",
      "batch: 500/1563 - train loss: 3.9306 - test loss: 16.6875 - train acc: 0.7840 - test acc: 0.4036 - 17m 44s\n",
      "batch: 600/1563 - train loss: 3.8879 - test loss: 16.7384 - train acc: 0.7869 - test acc: 0.4134 - 17m 50s\n",
      "batch: 700/1563 - train loss: 3.9325 - test loss: 16.6927 - train acc: 0.7846 - test acc: 0.4175 - 17m 55s\n",
      "batch: 800/1563 - train loss: 4.1714 - test loss: 16.9405 - train acc: 0.7772 - test acc: 0.4216 - 18m 1s\n",
      "batch: 900/1563 - train loss: 4.2711 - test loss: 15.9885 - train acc: 0.7622 - test acc: 0.4267 - 18m 6s\n",
      "batch: 1000/1563 - train loss: 4.5404 - test loss: 16.6097 - train acc: 0.7538 - test acc: 0.4129 - 18m 12s\n",
      "batch: 1100/1563 - train loss: 4.7861 - test loss: 16.1017 - train acc: 0.7375 - test acc: 0.4188 - 18m 17s\n",
      "batch: 1200/1563 - train loss: 4.7817 - test loss: 15.9514 - train acc: 0.7434 - test acc: 0.4248 - 18m 22s\n",
      "batch: 1300/1563 - train loss: 5.0139 - test loss: 16.3241 - train acc: 0.7338 - test acc: 0.4075 - 18m 28s\n",
      "batch: 1400/1563 - train loss: 5.2264 - test loss: 16.1792 - train acc: 0.7225 - test acc: 0.4256 - 18m 34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 4.8618 - test loss: 15.8962 - train acc: 0.7359 - test acc: 0.4271 - 18m 39s\n",
      "batch: 1563/1563 - train loss: 5.1462 - test loss: 16.4639 - train acc: 0.7285 - test acc: 0.4069 - 18m 44s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.9321 - test loss: 15.9505 - train acc: 0.8356 - test acc: 0.4329 - 18m 49s\n",
      "batch: 200/1563 - train loss: 3.0128 - test loss: 16.4292 - train acc: 0.8296 - test acc: 0.4265 - 18m 55s\n",
      "batch: 300/1563 - train loss: 3.0304 - test loss: 16.5596 - train acc: 0.8337 - test acc: 0.4287 - 19m 0s\n",
      "batch: 400/1563 - train loss: 3.0562 - test loss: 16.5810 - train acc: 0.8350 - test acc: 0.4219 - 19m 5s\n",
      "batch: 500/1563 - train loss: 3.4083 - test loss: 16.4013 - train acc: 0.8118 - test acc: 0.4371 - 19m 11s\n",
      "batch: 600/1563 - train loss: 3.5625 - test loss: 16.8888 - train acc: 0.7965 - test acc: 0.4215 - 19m 16s\n",
      "batch: 700/1563 - train loss: 3.5852 - test loss: 16.8267 - train acc: 0.8025 - test acc: 0.4318 - 19m 22s\n",
      "batch: 800/1563 - train loss: 3.6760 - test loss: 16.5057 - train acc: 0.8090 - test acc: 0.4283 - 19m 27s\n",
      "batch: 900/1563 - train loss: 3.5310 - test loss: 17.1204 - train acc: 0.8065 - test acc: 0.4189 - 19m 33s\n",
      "batch: 1000/1563 - train loss: 4.0022 - test loss: 16.8313 - train acc: 0.7809 - test acc: 0.4157 - 19m 38s\n",
      "batch: 1100/1563 - train loss: 3.9881 - test loss: 16.7479 - train acc: 0.7812 - test acc: 0.4284 - 19m 44s\n",
      "batch: 1200/1563 - train loss: 4.0852 - test loss: 16.6241 - train acc: 0.7715 - test acc: 0.4228 - 19m 49s\n",
      "batch: 1300/1563 - train loss: 4.2717 - test loss: 16.9109 - train acc: 0.7715 - test acc: 0.4161 - 19m 55s\n",
      "batch: 1400/1563 - train loss: 4.1793 - test loss: 16.7227 - train acc: 0.7706 - test acc: 0.4206 - 20m 0s\n",
      "time is up! finishing training\n",
      "batch: 1401/1563 - train loss: 4.1986 - test loss: 16.6542 - train acc: 0.7684 - test acc: 0.4210 - 20m 3s\n",
      "GPU memory used: 7.24 GB - max: 7.56 GB - memory reserved: 7.62 GB - max: 7.62 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "step_i = 0\n",
    "\n",
    "buffer_size = 140\n",
    "partition_size = 30\n",
    "block_updates = 360\n",
    "\n",
    "for _ in range(nruns):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'testing - buf size: {buffer_size} - part size: {partition_size} - block upd: {block_updates} - combination nº: {step_i + 1}')\n",
    "\n",
    "    default_metrics, _ = train_network_fisher_optimization(apply_fisher = True,\n",
    "                                                           buffer_size = buffer_size,\n",
    "                                                           partition_size = partition_size,\n",
    "                                                           block_updates = block_updates,\n",
    "                                                           net_params = {'p': 0.1},\n",
    "                                                           epochs = 100,\n",
    "                                                           time_limit_secs = 1200)\n",
    "\n",
    "    results_list.append( (default_metrics, buffer_size, partition_size, block_updates) )\n",
    "    results_list_to_json(results_list, step=step_i)\n",
    "    step_i += 1\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f32754",
   "metadata": {
    "papermill": {
     "duration": 0.285466,
     "end_time": "2022-09-29T11:22:11.516751",
     "exception": false,
     "start_time": "2022-09-29T11:22:11.231285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## testing baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dee4dce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T11:22:12.108098Z",
     "iopub.status.busy": "2022-09-29T11:22:12.107733Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2022-09-29T11:22:11.801526",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1840 - test loss: 25.2592 - train acc: 0.0424 - test acc: 0.0487 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.1417 - test loss: 23.6348 - train acc: 0.0702 - test acc: 0.0783 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.3848 - test loss: 22.8241 - train acc: 0.0761 - test acc: 0.0939 - 0m 10s\n",
      "batch: 400/1563 - train loss: 22.7609 - test loss: 22.7511 - train acc: 0.0943 - test acc: 0.0978 - 0m 15s\n",
      "batch: 500/1563 - train loss: 22.0464 - test loss: 21.1781 - train acc: 0.1043 - test acc: 0.1212 - 0m 20s\n",
      "batch: 600/1563 - train loss: 21.6560 - test loss: 21.7653 - train acc: 0.1128 - test acc: 0.1192 - 0m 24s\n",
      "batch: 700/1563 - train loss: 21.0866 - test loss: 21.5039 - train acc: 0.1275 - test acc: 0.1184 - 0m 29s\n",
      "batch: 800/1563 - train loss: 21.0256 - test loss: 20.6697 - train acc: 0.1300 - test acc: 0.1444 - 0m 34s\n",
      "batch: 900/1563 - train loss: 20.3913 - test loss: 20.2517 - train acc: 0.1469 - test acc: 0.1484 - 0m 38s\n",
      "batch: 1000/1563 - train loss: 20.4055 - test loss: 19.8312 - train acc: 0.1579 - test acc: 0.1637 - 0m 43s\n",
      "batch: 1100/1563 - train loss: 19.9198 - test loss: 19.9767 - train acc: 0.1673 - test acc: 0.1700 - 0m 48s\n",
      "batch: 1200/1563 - train loss: 19.4501 - test loss: 19.9424 - train acc: 0.1685 - test acc: 0.1713 - 0m 53s\n",
      "batch: 1300/1563 - train loss: 19.3096 - test loss: 19.4826 - train acc: 0.1772 - test acc: 0.1885 - 0m 57s\n",
      "batch: 1400/1563 - train loss: 19.1753 - test loss: 19.4357 - train acc: 0.1866 - test acc: 0.1811 - 1m 2s\n",
      "batch: 1500/1563 - train loss: 18.8890 - test loss: 18.5213 - train acc: 0.2014 - test acc: 0.2082 - 1m 7s\n",
      "batch: 1563/1563 - train loss: 19.0556 - test loss: 19.0823 - train acc: 0.1892 - test acc: 0.1953 - 1m 11s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.9051 - test loss: 18.9108 - train acc: 0.2329 - test acc: 0.2030 - 1m 15s\n",
      "batch: 200/1563 - train loss: 18.3736 - test loss: 20.0878 - train acc: 0.2036 - test acc: 0.1748 - 1m 21s\n",
      "batch: 300/1563 - train loss: 17.8785 - test loss: 17.3584 - train acc: 0.2310 - test acc: 0.2484 - 1m 25s\n",
      "batch: 400/1563 - train loss: 17.5845 - test loss: 18.2184 - train acc: 0.2322 - test acc: 0.2190 - 1m 30s\n",
      "batch: 500/1563 - train loss: 17.7620 - test loss: 17.6499 - train acc: 0.2407 - test acc: 0.2362 - 1m 35s\n",
      "batch: 600/1563 - train loss: 17.2162 - test loss: 17.4710 - train acc: 0.2397 - test acc: 0.2459 - 1m 40s\n",
      "batch: 700/1563 - train loss: 17.5900 - test loss: 18.0167 - train acc: 0.2366 - test acc: 0.2403 - 1m 45s\n",
      "batch: 800/1563 - train loss: 17.0443 - test loss: 17.6437 - train acc: 0.2515 - test acc: 0.2394 - 1m 49s\n",
      "batch: 900/1563 - train loss: 17.3351 - test loss: 17.0174 - train acc: 0.2419 - test acc: 0.2586 - 1m 54s\n",
      "batch: 1000/1563 - train loss: 16.8716 - test loss: 17.1173 - train acc: 0.2525 - test acc: 0.2521 - 1m 59s\n",
      "batch: 1100/1563 - train loss: 16.9854 - test loss: 17.5292 - train acc: 0.2597 - test acc: 0.2406 - 2m 4s\n",
      "batch: 1200/1563 - train loss: 16.7838 - test loss: 17.7358 - train acc: 0.2651 - test acc: 0.2409 - 2m 9s\n",
      "batch: 1300/1563 - train loss: 16.5017 - test loss: 18.4525 - train acc: 0.2669 - test acc: 0.2248 - 2m 14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 16.5994 - test loss: 16.3922 - train acc: 0.2650 - test acc: 0.2778 - 2m 18s\n",
      "batch: 1500/1563 - train loss: 16.6086 - test loss: 16.2864 - train acc: 0.2759 - test acc: 0.2772 - 2m 24s\n",
      "batch: 1563/1563 - train loss: 16.0743 - test loss: 17.1867 - train acc: 0.2912 - test acc: 0.2572 - 2m 28s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.4027 - test loss: 16.0116 - train acc: 0.2919 - test acc: 0.2940 - 2m 32s\n",
      "batch: 200/1563 - train loss: 14.9696 - test loss: 16.0886 - train acc: 0.3199 - test acc: 0.2873 - 2m 37s\n",
      "batch: 300/1563 - train loss: 15.0037 - test loss: 16.4906 - train acc: 0.3256 - test acc: 0.2757 - 2m 42s\n",
      "batch: 400/1563 - train loss: 15.0565 - test loss: 16.3459 - train acc: 0.3187 - test acc: 0.2868 - 2m 47s\n",
      "batch: 500/1563 - train loss: 15.2491 - test loss: 15.8935 - train acc: 0.3115 - test acc: 0.2981 - 2m 51s\n",
      "batch: 600/1563 - train loss: 15.0938 - test loss: 16.8829 - train acc: 0.3240 - test acc: 0.2777 - 2m 56s\n",
      "batch: 700/1563 - train loss: 14.9653 - test loss: 15.9592 - train acc: 0.3299 - test acc: 0.3118 - 3m 1s\n",
      "batch: 800/1563 - train loss: 14.9839 - test loss: 16.1075 - train acc: 0.3281 - test acc: 0.2920 - 3m 6s\n",
      "batch: 900/1563 - train loss: 14.9021 - test loss: 15.6390 - train acc: 0.3303 - test acc: 0.2994 - 3m 11s\n",
      "batch: 1000/1563 - train loss: 14.6364 - test loss: 15.6731 - train acc: 0.3425 - test acc: 0.3021 - 3m 16s\n",
      "batch: 1100/1563 - train loss: 15.0263 - test loss: 15.6436 - train acc: 0.3284 - test acc: 0.3153 - 3m 21s\n",
      "batch: 1200/1563 - train loss: 14.9223 - test loss: 15.9903 - train acc: 0.3228 - test acc: 0.2962 - 3m 25s\n",
      "batch: 1300/1563 - train loss: 14.4876 - test loss: 15.4455 - train acc: 0.3393 - test acc: 0.3087 - 3m 31s\n",
      "batch: 1400/1563 - train loss: 14.6144 - test loss: 15.6241 - train acc: 0.3366 - test acc: 0.3077 - 3m 36s\n",
      "batch: 1500/1563 - train loss: 14.5240 - test loss: 15.1369 - train acc: 0.3403 - test acc: 0.3247 - 3m 40s\n",
      "batch: 1563/1563 - train loss: 14.4984 - test loss: 15.7866 - train acc: 0.3293 - test acc: 0.3008 - 3m 44s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.1363 - test loss: 16.9215 - train acc: 0.3853 - test acc: 0.2873 - 3m 49s\n",
      "batch: 200/1563 - train loss: 13.1079 - test loss: 15.2841 - train acc: 0.3922 - test acc: 0.3340 - 3m 54s\n",
      "batch: 300/1563 - train loss: 13.2668 - test loss: 17.7272 - train acc: 0.3860 - test acc: 0.2731 - 4m 0s\n",
      "batch: 400/1563 - train loss: 13.3801 - test loss: 14.4976 - train acc: 0.3819 - test acc: 0.3569 - 4m 5s\n",
      "batch: 500/1563 - train loss: 13.2648 - test loss: 15.2007 - train acc: 0.3778 - test acc: 0.3347 - 4m 10s\n",
      "batch: 600/1563 - train loss: 13.2569 - test loss: 15.7001 - train acc: 0.3913 - test acc: 0.3074 - 4m 15s\n",
      "batch: 700/1563 - train loss: 13.5209 - test loss: 15.4275 - train acc: 0.3775 - test acc: 0.3398 - 4m 19s\n",
      "batch: 800/1563 - train loss: 13.3394 - test loss: 14.6377 - train acc: 0.3878 - test acc: 0.3505 - 4m 24s\n",
      "batch: 900/1563 - train loss: 13.6572 - test loss: 16.2521 - train acc: 0.3697 - test acc: 0.3059 - 4m 29s\n",
      "batch: 1000/1563 - train loss: 13.2281 - test loss: 14.1698 - train acc: 0.3863 - test acc: 0.3669 - 4m 34s\n",
      "batch: 1100/1563 - train loss: 13.2681 - test loss: 14.2144 - train acc: 0.3775 - test acc: 0.3607 - 4m 39s\n",
      "batch: 1200/1563 - train loss: 13.3429 - test loss: 14.6603 - train acc: 0.3863 - test acc: 0.3458 - 4m 43s\n",
      "batch: 1300/1563 - train loss: 13.0612 - test loss: 15.6604 - train acc: 0.3947 - test acc: 0.3223 - 4m 48s\n",
      "batch: 1400/1563 - train loss: 13.4212 - test loss: 14.7288 - train acc: 0.3735 - test acc: 0.3491 - 4m 53s\n",
      "batch: 1500/1563 - train loss: 13.3728 - test loss: 13.8874 - train acc: 0.3938 - test acc: 0.3777 - 4m 58s\n",
      "batch: 1563/1563 - train loss: 12.9983 - test loss: 13.8850 - train acc: 0.3894 - test acc: 0.3760 - 5m 2s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3484 - test loss: 13.9507 - train acc: 0.4569 - test acc: 0.3790 - 5m 7s\n",
      "batch: 200/1563 - train loss: 11.7566 - test loss: 14.5968 - train acc: 0.4435 - test acc: 0.3536 - 5m 11s\n",
      "batch: 300/1563 - train loss: 11.8569 - test loss: 14.1659 - train acc: 0.4369 - test acc: 0.3683 - 5m 16s\n",
      "batch: 400/1563 - train loss: 11.7231 - test loss: 14.0521 - train acc: 0.4460 - test acc: 0.3735 - 5m 21s\n",
      "batch: 500/1563 - train loss: 12.0160 - test loss: 14.8277 - train acc: 0.4425 - test acc: 0.3558 - 5m 26s\n",
      "batch: 600/1563 - train loss: 12.0993 - test loss: 14.2209 - train acc: 0.4332 - test acc: 0.3696 - 5m 31s\n",
      "batch: 700/1563 - train loss: 11.8520 - test loss: 14.9895 - train acc: 0.4504 - test acc: 0.3437 - 5m 36s\n",
      "batch: 800/1563 - train loss: 11.8627 - test loss: 14.7264 - train acc: 0.4340 - test acc: 0.3577 - 5m 41s\n",
      "batch: 900/1563 - train loss: 11.7272 - test loss: 14.9822 - train acc: 0.4347 - test acc: 0.3473 - 5m 45s\n",
      "batch: 1000/1563 - train loss: 12.1437 - test loss: 13.5998 - train acc: 0.4206 - test acc: 0.3872 - 5m 50s\n",
      "batch: 1100/1563 - train loss: 11.9656 - test loss: 13.5411 - train acc: 0.4416 - test acc: 0.3922 - 5m 55s\n",
      "batch: 1200/1563 - train loss: 12.4764 - test loss: 14.6046 - train acc: 0.4253 - test acc: 0.3701 - 5m 59s\n",
      "batch: 1300/1563 - train loss: 12.0778 - test loss: 13.7528 - train acc: 0.4381 - test acc: 0.3807 - 6m 4s\n",
      "batch: 1400/1563 - train loss: 11.9153 - test loss: 13.8993 - train acc: 0.4312 - test acc: 0.3836 - 6m 9s\n",
      "batch: 1500/1563 - train loss: 12.0585 - test loss: 14.2284 - train acc: 0.4394 - test acc: 0.3689 - 6m 14s\n",
      "batch: 1563/1563 - train loss: 11.6696 - test loss: 14.0567 - train acc: 0.4522 - test acc: 0.3723 - 6m 18s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.9121 - test loss: 15.3814 - train acc: 0.5150 - test acc: 0.3481 - 6m 23s\n",
      "batch: 200/1563 - train loss: 10.2763 - test loss: 13.9055 - train acc: 0.5012 - test acc: 0.3833 - 6m 27s\n",
      "batch: 300/1563 - train loss: 10.1938 - test loss: 14.6659 - train acc: 0.4947 - test acc: 0.3647 - 6m 32s\n",
      "batch: 400/1563 - train loss: 10.8234 - test loss: 14.0775 - train acc: 0.4691 - test acc: 0.3760 - 6m 37s\n",
      "batch: 500/1563 - train loss: 10.8272 - test loss: 14.9271 - train acc: 0.4809 - test acc: 0.3582 - 6m 42s\n",
      "batch: 600/1563 - train loss: 10.6224 - test loss: 13.6784 - train acc: 0.4803 - test acc: 0.3943 - 6m 47s\n",
      "batch: 700/1563 - train loss: 10.5289 - test loss: 14.1472 - train acc: 0.4885 - test acc: 0.3803 - 6m 52s\n",
      "batch: 800/1563 - train loss: 10.8098 - test loss: 14.3261 - train acc: 0.4774 - test acc: 0.3844 - 6m 56s\n",
      "batch: 900/1563 - train loss: 10.6034 - test loss: 13.3973 - train acc: 0.4847 - test acc: 0.4075 - 7m 1s\n",
      "batch: 1000/1563 - train loss: 11.0355 - test loss: 15.1106 - train acc: 0.4775 - test acc: 0.3516 - 7m 5s\n",
      "batch: 1100/1563 - train loss: 10.9143 - test loss: 13.5128 - train acc: 0.4841 - test acc: 0.3966 - 7m 11s\n",
      "batch: 1200/1563 - train loss: 10.5553 - test loss: 14.2904 - train acc: 0.4919 - test acc: 0.3718 - 7m 15s\n",
      "batch: 1300/1563 - train loss: 10.8473 - test loss: 13.6462 - train acc: 0.4866 - test acc: 0.3975 - 7m 20s\n",
      "batch: 1400/1563 - train loss: 10.7972 - test loss: 13.7234 - train acc: 0.4968 - test acc: 0.3948 - 7m 25s\n",
      "batch: 1500/1563 - train loss: 11.0455 - test loss: 13.8384 - train acc: 0.4855 - test acc: 0.3917 - 7m 30s\n",
      "batch: 1563/1563 - train loss: 10.9814 - test loss: 13.4413 - train acc: 0.4725 - test acc: 0.3954 - 7m 34s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.5537 - test loss: 14.7094 - train acc: 0.5750 - test acc: 0.3746 - 7m 39s\n",
      "batch: 200/1563 - train loss: 9.0341 - test loss: 14.1425 - train acc: 0.5440 - test acc: 0.3886 - 7m 44s\n",
      "batch: 300/1563 - train loss: 8.8732 - test loss: 13.7310 - train acc: 0.5609 - test acc: 0.4080 - 7m 49s\n",
      "batch: 400/1563 - train loss: 9.2860 - test loss: 15.2477 - train acc: 0.5512 - test acc: 0.3636 - 7m 53s\n",
      "batch: 500/1563 - train loss: 9.1573 - test loss: 14.2222 - train acc: 0.5496 - test acc: 0.3915 - 7m 58s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 9.6747 - test loss: 13.5474 - train acc: 0.5284 - test acc: 0.4057 - 8m 3s\n",
      "batch: 700/1563 - train loss: 9.7440 - test loss: 14.6558 - train acc: 0.5168 - test acc: 0.3814 - 8m 7s\n",
      "batch: 800/1563 - train loss: 9.5898 - test loss: 14.0397 - train acc: 0.5225 - test acc: 0.3918 - 8m 12s\n",
      "batch: 900/1563 - train loss: 9.6970 - test loss: 19.5851 - train acc: 0.5213 - test acc: 0.2809 - 8m 17s\n",
      "batch: 1000/1563 - train loss: 9.8432 - test loss: 14.2269 - train acc: 0.5185 - test acc: 0.3866 - 8m 22s\n",
      "batch: 1100/1563 - train loss: 9.7207 - test loss: 14.2599 - train acc: 0.5343 - test acc: 0.3798 - 8m 27s\n",
      "batch: 1200/1563 - train loss: 9.6413 - test loss: 13.2499 - train acc: 0.5247 - test acc: 0.4130 - 8m 32s\n",
      "batch: 1300/1563 - train loss: 9.5846 - test loss: 14.5653 - train acc: 0.5294 - test acc: 0.3806 - 8m 37s\n",
      "batch: 1400/1563 - train loss: 9.8692 - test loss: 13.3567 - train acc: 0.5250 - test acc: 0.4141 - 8m 41s\n",
      "batch: 1500/1563 - train loss: 9.7465 - test loss: 13.6177 - train acc: 0.5287 - test acc: 0.4052 - 8m 46s\n",
      "batch: 1563/1563 - train loss: 9.7082 - test loss: 13.6033 - train acc: 0.5159 - test acc: 0.4030 - 8m 50s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.3123 - test loss: 13.8586 - train acc: 0.6297 - test acc: 0.4095 - 8m 55s\n",
      "batch: 200/1563 - train loss: 7.6429 - test loss: 14.6801 - train acc: 0.6300 - test acc: 0.3971 - 9m 0s\n",
      "batch: 300/1563 - train loss: 7.8647 - test loss: 15.9851 - train acc: 0.5934 - test acc: 0.3576 - 9m 4s\n",
      "batch: 400/1563 - train loss: 8.1160 - test loss: 13.9026 - train acc: 0.5975 - test acc: 0.4091 - 9m 9s\n",
      "batch: 500/1563 - train loss: 8.1659 - test loss: 13.9945 - train acc: 0.5828 - test acc: 0.4018 - 9m 14s\n",
      "batch: 600/1563 - train loss: 8.1997 - test loss: 13.9490 - train acc: 0.5865 - test acc: 0.4059 - 9m 19s\n",
      "batch: 700/1563 - train loss: 8.2972 - test loss: 14.1500 - train acc: 0.5812 - test acc: 0.3993 - 9m 24s\n",
      "batch: 800/1563 - train loss: 8.6324 - test loss: 13.9518 - train acc: 0.5728 - test acc: 0.3981 - 9m 29s\n",
      "batch: 900/1563 - train loss: 8.6368 - test loss: 14.3807 - train acc: 0.5668 - test acc: 0.3923 - 9m 33s\n",
      "batch: 1000/1563 - train loss: 8.5547 - test loss: 16.1248 - train acc: 0.5750 - test acc: 0.3518 - 9m 38s\n",
      "batch: 1100/1563 - train loss: 8.6838 - test loss: 13.9917 - train acc: 0.5621 - test acc: 0.4001 - 9m 43s\n",
      "batch: 1200/1563 - train loss: 9.0345 - test loss: 15.1339 - train acc: 0.5434 - test acc: 0.3822 - 9m 48s\n",
      "batch: 1300/1563 - train loss: 8.9059 - test loss: 14.0686 - train acc: 0.5684 - test acc: 0.4071 - 9m 53s\n",
      "batch: 1400/1563 - train loss: 8.7984 - test loss: 14.1385 - train acc: 0.5587 - test acc: 0.3994 - 9m 58s\n",
      "batch: 1500/1563 - train loss: 8.6916 - test loss: 15.0368 - train acc: 0.5674 - test acc: 0.3807 - 10m 2s\n",
      "batch: 1563/1563 - train loss: 8.7842 - test loss: 13.6152 - train acc: 0.5691 - test acc: 0.4045 - 10m 7s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4173 - test loss: 14.4080 - train acc: 0.6697 - test acc: 0.4046 - 10m 11s\n",
      "batch: 200/1563 - train loss: 6.3735 - test loss: 14.4136 - train acc: 0.6654 - test acc: 0.4094 - 10m 16s\n",
      "batch: 300/1563 - train loss: 6.1628 - test loss: 14.6658 - train acc: 0.6781 - test acc: 0.4065 - 10m 21s\n",
      "batch: 400/1563 - train loss: 7.0859 - test loss: 14.7631 - train acc: 0.6331 - test acc: 0.3952 - 10m 26s\n",
      "batch: 500/1563 - train loss: 7.0029 - test loss: 15.8602 - train acc: 0.6372 - test acc: 0.3795 - 10m 31s\n",
      "batch: 600/1563 - train loss: 7.1975 - test loss: 14.5061 - train acc: 0.6403 - test acc: 0.4092 - 10m 35s\n",
      "batch: 700/1563 - train loss: 7.1879 - test loss: 14.9588 - train acc: 0.6206 - test acc: 0.3957 - 10m 40s\n",
      "batch: 800/1563 - train loss: 7.4212 - test loss: 14.1648 - train acc: 0.6159 - test acc: 0.4064 - 10m 45s\n",
      "batch: 900/1563 - train loss: 7.7311 - test loss: 14.4117 - train acc: 0.6122 - test acc: 0.4086 - 10m 50s\n",
      "batch: 1000/1563 - train loss: 7.7570 - test loss: 14.7637 - train acc: 0.5994 - test acc: 0.3990 - 10m 56s\n",
      "batch: 1100/1563 - train loss: 7.3199 - test loss: 14.2153 - train acc: 0.6228 - test acc: 0.4129 - 11m 1s\n",
      "batch: 1200/1563 - train loss: 7.7155 - test loss: 14.3386 - train acc: 0.6094 - test acc: 0.4130 - 11m 5s\n",
      "batch: 1300/1563 - train loss: 7.7125 - test loss: 14.7789 - train acc: 0.6025 - test acc: 0.4014 - 11m 10s\n",
      "batch: 1400/1563 - train loss: 7.7999 - test loss: 14.4416 - train acc: 0.6134 - test acc: 0.4086 - 11m 15s\n",
      "batch: 1500/1563 - train loss: 8.1217 - test loss: 14.8296 - train acc: 0.5828 - test acc: 0.3894 - 11m 20s\n",
      "batch: 1563/1563 - train loss: 7.9516 - test loss: 14.0442 - train acc: 0.5981 - test acc: 0.4149 - 11m 24s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.0378 - test loss: 15.0125 - train acc: 0.7394 - test acc: 0.4010 - 11m 29s\n",
      "batch: 200/1563 - train loss: 5.3303 - test loss: 14.6423 - train acc: 0.7169 - test acc: 0.4148 - 11m 34s\n",
      "batch: 300/1563 - train loss: 5.5585 - test loss: 15.5150 - train acc: 0.7000 - test acc: 0.3954 - 11m 39s\n",
      "batch: 400/1563 - train loss: 5.6784 - test loss: 15.3799 - train acc: 0.7063 - test acc: 0.4023 - 11m 44s\n",
      "batch: 500/1563 - train loss: 5.6646 - test loss: 15.0700 - train acc: 0.7075 - test acc: 0.4058 - 11m 48s\n",
      "batch: 600/1563 - train loss: 6.2325 - test loss: 15.6748 - train acc: 0.6800 - test acc: 0.3924 - 11m 54s\n",
      "batch: 700/1563 - train loss: 6.3983 - test loss: 14.6552 - train acc: 0.6641 - test acc: 0.4001 - 11m 59s\n",
      "batch: 800/1563 - train loss: 6.4769 - test loss: 14.5643 - train acc: 0.6600 - test acc: 0.4155 - 12m 4s\n",
      "batch: 900/1563 - train loss: 6.3404 - test loss: 15.3928 - train acc: 0.6678 - test acc: 0.4031 - 12m 10s\n",
      "batch: 1000/1563 - train loss: 6.4014 - test loss: 15.2101 - train acc: 0.6691 - test acc: 0.3973 - 12m 14s\n",
      "batch: 1100/1563 - train loss: 6.6616 - test loss: 15.4599 - train acc: 0.6613 - test acc: 0.4060 - 12m 20s\n",
      "batch: 1200/1563 - train loss: 6.5624 - test loss: 16.5517 - train acc: 0.6698 - test acc: 0.3738 - 12m 24s\n",
      "batch: 1300/1563 - train loss: 7.1258 - test loss: 14.9458 - train acc: 0.6375 - test acc: 0.4073 - 12m 30s\n",
      "batch: 1400/1563 - train loss: 6.8567 - test loss: 14.4535 - train acc: 0.6534 - test acc: 0.4056 - 12m 34s\n",
      "batch: 1500/1563 - train loss: 6.9888 - test loss: 14.6239 - train acc: 0.6413 - test acc: 0.4109 - 12m 39s\n",
      "batch: 1563/1563 - train loss: 7.1536 - test loss: 14.1293 - train acc: 0.6363 - test acc: 0.4270 - 12m 43s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.3635 - test loss: 14.7216 - train acc: 0.7665 - test acc: 0.4280 - 12m 48s\n",
      "batch: 200/1563 - train loss: 4.4424 - test loss: 15.3060 - train acc: 0.7584 - test acc: 0.4168 - 12m 53s\n",
      "batch: 300/1563 - train loss: 4.5101 - test loss: 18.5353 - train acc: 0.7593 - test acc: 0.3635 - 12m 58s\n",
      "batch: 400/1563 - train loss: 5.0717 - test loss: 16.1757 - train acc: 0.7350 - test acc: 0.3919 - 13m 4s\n",
      "batch: 500/1563 - train loss: 5.0208 - test loss: 15.4971 - train acc: 0.7232 - test acc: 0.4119 - 13m 8s\n",
      "batch: 600/1563 - train loss: 5.1355 - test loss: 16.0637 - train acc: 0.7244 - test acc: 0.3956 - 13m 14s\n",
      "batch: 700/1563 - train loss: 5.4114 - test loss: 16.2770 - train acc: 0.7085 - test acc: 0.3944 - 13m 19s\n",
      "batch: 800/1563 - train loss: 5.3621 - test loss: 15.8710 - train acc: 0.7135 - test acc: 0.4079 - 13m 23s\n",
      "batch: 900/1563 - train loss: 5.4944 - test loss: 16.0102 - train acc: 0.7091 - test acc: 0.4086 - 13m 28s\n",
      "batch: 1000/1563 - train loss: 5.8841 - test loss: 15.1365 - train acc: 0.6901 - test acc: 0.4139 - 13m 33s\n",
      "batch: 1100/1563 - train loss: 5.9438 - test loss: 14.9258 - train acc: 0.6947 - test acc: 0.4221 - 13m 38s\n",
      "batch: 1200/1563 - train loss: 5.9079 - test loss: 15.0489 - train acc: 0.6879 - test acc: 0.4207 - 13m 43s\n",
      "batch: 1300/1563 - train loss: 5.8066 - test loss: 16.5187 - train acc: 0.6947 - test acc: 0.3896 - 13m 47s\n",
      "batch: 1400/1563 - train loss: 5.7963 - test loss: 15.0614 - train acc: 0.6900 - test acc: 0.4165 - 13m 52s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 6.1555 - test loss: 15.1252 - train acc: 0.6750 - test acc: 0.4201 - 13m 57s\n",
      "batch: 1563/1563 - train loss: 6.0714 - test loss: 15.6622 - train acc: 0.6738 - test acc: 0.4047 - 14m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.5342 - test loss: 15.8188 - train acc: 0.8106 - test acc: 0.4111 - 14m 6s\n",
      "batch: 200/1563 - train loss: 3.5778 - test loss: 15.9947 - train acc: 0.8041 - test acc: 0.4138 - 14m 11s\n",
      "batch: 300/1563 - train loss: 3.8025 - test loss: 16.2076 - train acc: 0.7856 - test acc: 0.4090 - 14m 16s\n",
      "batch: 400/1563 - train loss: 4.0508 - test loss: 15.4642 - train acc: 0.7828 - test acc: 0.4238 - 14m 21s\n",
      "batch: 500/1563 - train loss: 3.8725 - test loss: 16.0506 - train acc: 0.7871 - test acc: 0.4183 - 14m 26s\n",
      "batch: 600/1563 - train loss: 4.1937 - test loss: 16.5637 - train acc: 0.7609 - test acc: 0.3951 - 14m 30s\n",
      "batch: 700/1563 - train loss: 4.3481 - test loss: 16.4833 - train acc: 0.7637 - test acc: 0.4122 - 14m 35s\n",
      "batch: 800/1563 - train loss: 4.5609 - test loss: 15.9369 - train acc: 0.7587 - test acc: 0.4153 - 14m 40s\n",
      "batch: 900/1563 - train loss: 4.8119 - test loss: 15.8032 - train acc: 0.7428 - test acc: 0.4128 - 14m 45s\n",
      "batch: 1000/1563 - train loss: 4.8515 - test loss: 16.1089 - train acc: 0.7315 - test acc: 0.4022 - 14m 50s\n",
      "batch: 1100/1563 - train loss: 4.8764 - test loss: 16.4345 - train acc: 0.7325 - test acc: 0.4051 - 14m 55s\n",
      "batch: 1200/1563 - train loss: 4.9876 - test loss: 15.6875 - train acc: 0.7366 - test acc: 0.4267 - 15m 0s\n",
      "batch: 1300/1563 - train loss: 4.9042 - test loss: 15.8664 - train acc: 0.7335 - test acc: 0.4165 - 15m 5s\n",
      "batch: 1400/1563 - train loss: 5.2465 - test loss: 15.6827 - train acc: 0.7163 - test acc: 0.4145 - 15m 10s\n",
      "batch: 1500/1563 - train loss: 5.1318 - test loss: 15.7914 - train acc: 0.7259 - test acc: 0.4135 - 15m 14s\n",
      "batch: 1563/1563 - train loss: 5.3601 - test loss: 16.3481 - train acc: 0.7110 - test acc: 0.4039 - 15m 18s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 2.9900 - test loss: 15.9310 - train acc: 0.8405 - test acc: 0.4255 - 15m 24s\n",
      "batch: 200/1563 - train loss: 3.0287 - test loss: 16.5357 - train acc: 0.8331 - test acc: 0.4179 - 15m 28s\n",
      "batch: 300/1563 - train loss: 2.9812 - test loss: 17.2508 - train acc: 0.8312 - test acc: 0.4195 - 15m 33s\n",
      "batch: 400/1563 - train loss: 3.3680 - test loss: 16.4992 - train acc: 0.8162 - test acc: 0.4217 - 15m 38s\n",
      "batch: 500/1563 - train loss: 3.2295 - test loss: 16.4963 - train acc: 0.8225 - test acc: 0.4276 - 15m 43s\n",
      "batch: 600/1563 - train loss: 3.2695 - test loss: 16.5523 - train acc: 0.8247 - test acc: 0.4189 - 15m 48s\n",
      "batch: 700/1563 - train loss: 3.6105 - test loss: 17.3730 - train acc: 0.7981 - test acc: 0.4122 - 15m 53s\n",
      "batch: 800/1563 - train loss: 3.6302 - test loss: 16.7446 - train acc: 0.7956 - test acc: 0.4178 - 15m 58s\n",
      "batch: 900/1563 - train loss: 3.8511 - test loss: 17.1370 - train acc: 0.7841 - test acc: 0.4163 - 16m 2s\n",
      "batch: 1000/1563 - train loss: 3.9938 - test loss: 16.7014 - train acc: 0.7790 - test acc: 0.4097 - 16m 7s\n",
      "batch: 1100/1563 - train loss: 4.1747 - test loss: 16.8972 - train acc: 0.7715 - test acc: 0.4127 - 16m 12s\n",
      "batch: 1200/1563 - train loss: 4.1016 - test loss: 17.0561 - train acc: 0.7824 - test acc: 0.4090 - 16m 17s\n",
      "batch: 1300/1563 - train loss: 4.1567 - test loss: 16.2636 - train acc: 0.7741 - test acc: 0.4123 - 16m 22s\n",
      "batch: 1400/1563 - train loss: 4.2970 - test loss: 16.4710 - train acc: 0.7594 - test acc: 0.4153 - 16m 26s\n",
      "batch: 1500/1563 - train loss: 4.5300 - test loss: 16.4950 - train acc: 0.7581 - test acc: 0.4205 - 16m 31s\n",
      "batch: 1563/1563 - train loss: 4.7454 - test loss: 17.0516 - train acc: 0.7347 - test acc: 0.4084 - 16m 35s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.6654 - test loss: 16.6519 - train acc: 0.8587 - test acc: 0.4170 - 16m 40s\n",
      "batch: 200/1563 - train loss: 2.4825 - test loss: 16.8459 - train acc: 0.8662 - test acc: 0.4223 - 16m 45s\n",
      "batch: 300/1563 - train loss: 2.4532 - test loss: 16.8680 - train acc: 0.8622 - test acc: 0.4297 - 16m 50s\n",
      "batch: 400/1563 - train loss: 2.5774 - test loss: 17.6583 - train acc: 0.8565 - test acc: 0.4130 - 16m 55s\n",
      "batch: 500/1563 - train loss: 2.6811 - test loss: 17.8182 - train acc: 0.8549 - test acc: 0.4134 - 17m 0s\n",
      "batch: 600/1563 - train loss: 2.8867 - test loss: 17.5842 - train acc: 0.8384 - test acc: 0.4250 - 17m 6s\n",
      "batch: 700/1563 - train loss: 2.9598 - test loss: 17.4674 - train acc: 0.8409 - test acc: 0.4278 - 17m 10s\n",
      "batch: 800/1563 - train loss: 3.1876 - test loss: 17.3757 - train acc: 0.8240 - test acc: 0.4247 - 17m 15s\n",
      "batch: 900/1563 - train loss: 3.3523 - test loss: 17.2264 - train acc: 0.8159 - test acc: 0.4246 - 17m 20s\n",
      "batch: 1000/1563 - train loss: 3.2790 - test loss: 17.9972 - train acc: 0.8144 - test acc: 0.4069 - 17m 25s\n",
      "batch: 1100/1563 - train loss: 3.3292 - test loss: 18.0611 - train acc: 0.8125 - test acc: 0.4097 - 17m 31s\n",
      "batch: 1200/1563 - train loss: 3.6452 - test loss: 17.2023 - train acc: 0.7974 - test acc: 0.4236 - 17m 36s\n",
      "batch: 1300/1563 - train loss: 3.5291 - test loss: 17.4364 - train acc: 0.8046 - test acc: 0.4221 - 17m 41s\n",
      "batch: 1400/1563 - train loss: 3.8185 - test loss: 17.6734 - train acc: 0.7837 - test acc: 0.4123 - 17m 46s\n",
      "batch: 1500/1563 - train loss: 3.7256 - test loss: 17.0304 - train acc: 0.7878 - test acc: 0.4149 - 17m 51s\n",
      "batch: 1563/1563 - train loss: 3.6263 - test loss: 17.2812 - train acc: 0.7950 - test acc: 0.4202 - 17m 55s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.0321 - test loss: 17.2265 - train acc: 0.8919 - test acc: 0.4320 - 18m 0s\n",
      "batch: 200/1563 - train loss: 1.8771 - test loss: 17.6401 - train acc: 0.8969 - test acc: 0.4228 - 18m 5s\n",
      "batch: 300/1563 - train loss: 2.0504 - test loss: 17.3254 - train acc: 0.8866 - test acc: 0.4347 - 18m 10s\n",
      "batch: 400/1563 - train loss: 2.0416 - test loss: 17.8618 - train acc: 0.8882 - test acc: 0.4209 - 18m 15s\n",
      "batch: 500/1563 - train loss: 2.1738 - test loss: 18.6321 - train acc: 0.8766 - test acc: 0.4113 - 18m 20s\n",
      "batch: 600/1563 - train loss: 2.3544 - test loss: 18.3750 - train acc: 0.8706 - test acc: 0.4158 - 18m 25s\n",
      "batch: 700/1563 - train loss: 2.3500 - test loss: 18.2335 - train acc: 0.8694 - test acc: 0.4240 - 18m 30s\n",
      "batch: 800/1563 - train loss: 2.6140 - test loss: 18.1462 - train acc: 0.8481 - test acc: 0.4157 - 18m 35s\n",
      "batch: 900/1563 - train loss: 2.6665 - test loss: 17.4310 - train acc: 0.8541 - test acc: 0.4213 - 18m 39s\n",
      "batch: 1000/1563 - train loss: 2.8023 - test loss: 18.0961 - train acc: 0.8422 - test acc: 0.4183 - 18m 44s\n",
      "batch: 1100/1563 - train loss: 2.9177 - test loss: 17.7568 - train acc: 0.8409 - test acc: 0.4202 - 18m 49s\n",
      "batch: 1200/1563 - train loss: 3.0329 - test loss: 18.3396 - train acc: 0.8274 - test acc: 0.4149 - 18m 54s\n",
      "batch: 1300/1563 - train loss: 3.0636 - test loss: 17.7842 - train acc: 0.8358 - test acc: 0.4200 - 18m 59s\n",
      "batch: 1400/1563 - train loss: 2.6656 - test loss: 17.9878 - train acc: 0.8490 - test acc: 0.4245 - 19m 4s\n",
      "batch: 1500/1563 - train loss: 3.1532 - test loss: 18.0578 - train acc: 0.8228 - test acc: 0.4183 - 19m 9s\n",
      "batch: 1563/1563 - train loss: 3.0920 - test loss: 17.9115 - train acc: 0.8253 - test acc: 0.4223 - 19m 13s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.7859 - test loss: 17.9682 - train acc: 0.9041 - test acc: 0.4270 - 19m 18s\n",
      "batch: 200/1563 - train loss: 1.6941 - test loss: 18.2432 - train acc: 0.9101 - test acc: 0.4253 - 19m 23s\n",
      "batch: 300/1563 - train loss: 1.6752 - test loss: 18.4669 - train acc: 0.9008 - test acc: 0.4227 - 19m 28s\n",
      "batch: 400/1563 - train loss: 1.7751 - test loss: 18.7246 - train acc: 0.8995 - test acc: 0.4258 - 19m 33s\n",
      "batch: 500/1563 - train loss: 2.0904 - test loss: 18.7080 - train acc: 0.8822 - test acc: 0.4253 - 19m 37s\n",
      "batch: 600/1563 - train loss: 1.8838 - test loss: 18.5504 - train acc: 0.8923 - test acc: 0.4247 - 19m 42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 700/1563 - train loss: 1.9249 - test loss: 18.7599 - train acc: 0.8973 - test acc: 0.4172 - 19m 47s\n",
      "batch: 800/1563 - train loss: 2.1370 - test loss: 19.2206 - train acc: 0.8775 - test acc: 0.4088 - 19m 52s\n",
      "batch: 900/1563 - train loss: 2.0806 - test loss: 18.9534 - train acc: 0.8829 - test acc: 0.4244 - 19m 57s\n",
      "time is up! finishing training\n",
      "batch: 904/1563 - train loss: 2.1181 - test loss: 18.8514 - train acc: 0.8810 - test acc: 0.4282 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1548 - test loss: 24.7046 - train acc: 0.0367 - test acc: 0.0560 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.0055 - test loss: 23.3158 - train acc: 0.0646 - test acc: 0.0744 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.6068 - test loss: 22.8989 - train acc: 0.0770 - test acc: 0.0900 - 0m 10s\n",
      "batch: 400/1563 - train loss: 22.5156 - test loss: 21.8077 - train acc: 0.0821 - test acc: 0.1131 - 0m 15s\n",
      "batch: 500/1563 - train loss: 22.1984 - test loss: 22.3181 - train acc: 0.1056 - test acc: 0.1007 - 0m 20s\n",
      "batch: 600/1563 - train loss: 21.4673 - test loss: 21.3137 - train acc: 0.1140 - test acc: 0.1275 - 0m 25s\n",
      "batch: 700/1563 - train loss: 20.9592 - test loss: 20.5832 - train acc: 0.1359 - test acc: 0.1504 - 0m 29s\n",
      "batch: 800/1563 - train loss: 20.8360 - test loss: 21.0665 - train acc: 0.1394 - test acc: 0.1367 - 0m 34s\n",
      "batch: 900/1563 - train loss: 20.5693 - test loss: 20.6158 - train acc: 0.1419 - test acc: 0.1643 - 0m 39s\n",
      "batch: 1000/1563 - train loss: 19.7993 - test loss: 19.8629 - train acc: 0.1713 - test acc: 0.1642 - 0m 43s\n",
      "batch: 1100/1563 - train loss: 19.4417 - test loss: 20.7858 - train acc: 0.1776 - test acc: 0.1577 - 0m 48s\n",
      "batch: 1200/1563 - train loss: 19.4787 - test loss: 20.3020 - train acc: 0.1763 - test acc: 0.1612 - 0m 53s\n",
      "batch: 1300/1563 - train loss: 19.2792 - test loss: 19.8297 - train acc: 0.1816 - test acc: 0.1750 - 0m 57s\n",
      "batch: 1400/1563 - train loss: 19.3255 - test loss: 19.1295 - train acc: 0.1850 - test acc: 0.1925 - 1m 2s\n",
      "batch: 1500/1563 - train loss: 18.7425 - test loss: 19.3654 - train acc: 0.1903 - test acc: 0.1865 - 1m 7s\n",
      "batch: 1563/1563 - train loss: 18.5783 - test loss: 18.1495 - train acc: 0.1978 - test acc: 0.2148 - 1m 10s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.9463 - test loss: 18.2681 - train acc: 0.2125 - test acc: 0.2093 - 1m 15s\n",
      "batch: 200/1563 - train loss: 17.9104 - test loss: 19.1542 - train acc: 0.2244 - test acc: 0.1920 - 1m 20s\n",
      "batch: 300/1563 - train loss: 17.9371 - test loss: 19.0471 - train acc: 0.2247 - test acc: 0.1939 - 1m 24s\n",
      "batch: 400/1563 - train loss: 17.5455 - test loss: 18.5227 - train acc: 0.2469 - test acc: 0.2182 - 1m 29s\n",
      "batch: 500/1563 - train loss: 17.6247 - test loss: 19.5591 - train acc: 0.2438 - test acc: 0.1977 - 1m 34s\n",
      "batch: 600/1563 - train loss: 17.4105 - test loss: 17.7844 - train acc: 0.2400 - test acc: 0.2349 - 1m 39s\n",
      "batch: 700/1563 - train loss: 17.2201 - test loss: 18.0425 - train acc: 0.2372 - test acc: 0.2284 - 1m 43s\n",
      "batch: 800/1563 - train loss: 17.1059 - test loss: 17.2588 - train acc: 0.2491 - test acc: 0.2439 - 1m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 900/1563 - train loss: 16.8669 - test loss: 17.3335 - train acc: 0.2550 - test acc: 0.2511 - 1m 53s\n",
      "batch: 1000/1563 - train loss: 16.9492 - test loss: 17.2998 - train acc: 0.2644 - test acc: 0.2505 - 1m 58s\n",
      "batch: 1100/1563 - train loss: 16.7208 - test loss: 16.8097 - train acc: 0.2488 - test acc: 0.2617 - 2m 3s\n",
      "batch: 1200/1563 - train loss: 16.6467 - test loss: 18.3612 - train acc: 0.2629 - test acc: 0.2268 - 2m 7s\n",
      "batch: 1300/1563 - train loss: 16.4613 - test loss: 16.7528 - train acc: 0.2675 - test acc: 0.2580 - 2m 12s\n",
      "batch: 1400/1563 - train loss: 15.9221 - test loss: 19.0687 - train acc: 0.2840 - test acc: 0.2115 - 2m 17s\n",
      "batch: 1500/1563 - train loss: 16.4017 - test loss: 16.0180 - train acc: 0.2706 - test acc: 0.2971 - 2m 22s\n",
      "batch: 1563/1563 - train loss: 16.0499 - test loss: 16.5127 - train acc: 0.2897 - test acc: 0.2815 - 2m 26s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.9469 - test loss: 19.9533 - train acc: 0.3193 - test acc: 0.2162 - 2m 30s\n",
      "batch: 200/1563 - train loss: 14.7459 - test loss: 16.2343 - train acc: 0.3272 - test acc: 0.2947 - 2m 35s\n",
      "batch: 300/1563 - train loss: 15.1402 - test loss: 16.0368 - train acc: 0.3171 - test acc: 0.2972 - 2m 40s\n",
      "batch: 400/1563 - train loss: 15.0425 - test loss: 16.5986 - train acc: 0.3231 - test acc: 0.2773 - 2m 44s\n",
      "batch: 500/1563 - train loss: 14.9212 - test loss: 16.5274 - train acc: 0.3138 - test acc: 0.2769 - 2m 49s\n",
      "batch: 600/1563 - train loss: 14.9203 - test loss: 16.6572 - train acc: 0.3227 - test acc: 0.2783 - 2m 54s\n",
      "batch: 700/1563 - train loss: 14.6653 - test loss: 15.4921 - train acc: 0.3418 - test acc: 0.3066 - 2m 59s\n",
      "batch: 800/1563 - train loss: 14.9178 - test loss: 16.5295 - train acc: 0.3219 - test acc: 0.2692 - 3m 3s\n",
      "batch: 900/1563 - train loss: 14.9310 - test loss: 16.8952 - train acc: 0.3275 - test acc: 0.2743 - 3m 8s\n",
      "batch: 1000/1563 - train loss: 14.7816 - test loss: 15.2786 - train acc: 0.3312 - test acc: 0.3232 - 3m 13s\n",
      "batch: 1100/1563 - train loss: 14.4641 - test loss: 15.2117 - train acc: 0.3459 - test acc: 0.3242 - 3m 17s\n",
      "batch: 1200/1563 - train loss: 14.6349 - test loss: 14.9327 - train acc: 0.3356 - test acc: 0.3303 - 3m 22s\n",
      "batch: 1300/1563 - train loss: 14.5300 - test loss: 16.0478 - train acc: 0.3406 - test acc: 0.2996 - 3m 27s\n",
      "batch: 1400/1563 - train loss: 14.6438 - test loss: 14.8007 - train acc: 0.3353 - test acc: 0.3349 - 3m 31s\n",
      "batch: 1500/1563 - train loss: 14.4933 - test loss: 15.6911 - train acc: 0.3422 - test acc: 0.3164 - 3m 36s\n",
      "batch: 1563/1563 - train loss: 14.2224 - test loss: 14.5201 - train acc: 0.3574 - test acc: 0.3496 - 3m 40s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.9648 - test loss: 14.9856 - train acc: 0.4028 - test acc: 0.3366 - 3m 45s\n",
      "batch: 200/1563 - train loss: 13.3905 - test loss: 15.9120 - train acc: 0.3835 - test acc: 0.3096 - 3m 49s\n",
      "batch: 300/1563 - train loss: 12.9275 - test loss: 15.1944 - train acc: 0.3873 - test acc: 0.3197 - 3m 54s\n",
      "batch: 400/1563 - train loss: 12.9943 - test loss: 16.0961 - train acc: 0.4025 - test acc: 0.2995 - 3m 59s\n",
      "batch: 500/1563 - train loss: 12.8944 - test loss: 15.9666 - train acc: 0.3997 - test acc: 0.3139 - 4m 4s\n",
      "batch: 600/1563 - train loss: 13.2797 - test loss: 15.7434 - train acc: 0.3778 - test acc: 0.3136 - 4m 8s\n",
      "batch: 700/1563 - train loss: 13.2347 - test loss: 14.9522 - train acc: 0.3841 - test acc: 0.3365 - 4m 13s\n",
      "batch: 800/1563 - train loss: 12.9785 - test loss: 15.9448 - train acc: 0.3972 - test acc: 0.3099 - 4m 17s\n",
      "batch: 900/1563 - train loss: 13.4359 - test loss: 16.5732 - train acc: 0.3862 - test acc: 0.2952 - 4m 22s\n",
      "batch: 1000/1563 - train loss: 13.3377 - test loss: 17.4603 - train acc: 0.3784 - test acc: 0.2783 - 4m 27s\n",
      "batch: 1100/1563 - train loss: 12.8536 - test loss: 16.5987 - train acc: 0.4028 - test acc: 0.3008 - 4m 32s\n",
      "batch: 1200/1563 - train loss: 13.2946 - test loss: 14.4642 - train acc: 0.3947 - test acc: 0.3549 - 4m 36s\n",
      "batch: 1300/1563 - train loss: 13.2760 - test loss: 14.5154 - train acc: 0.3885 - test acc: 0.3558 - 4m 41s\n",
      "batch: 1400/1563 - train loss: 13.0652 - test loss: 15.0298 - train acc: 0.3966 - test acc: 0.3334 - 4m 46s\n",
      "batch: 1500/1563 - train loss: 13.0117 - test loss: 13.7903 - train acc: 0.3953 - test acc: 0.3788 - 4m 50s\n",
      "batch: 1563/1563 - train loss: 12.9748 - test loss: 14.4025 - train acc: 0.3984 - test acc: 0.3575 - 4m 54s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3233 - test loss: 14.0636 - train acc: 0.4594 - test acc: 0.3709 - 4m 59s\n",
      "batch: 200/1563 - train loss: 11.6113 - test loss: 14.0743 - train acc: 0.4575 - test acc: 0.3726 - 5m 4s\n",
      "batch: 300/1563 - train loss: 11.2825 - test loss: 14.0877 - train acc: 0.4572 - test acc: 0.3673 - 5m 8s\n",
      "batch: 400/1563 - train loss: 11.1950 - test loss: 16.4131 - train acc: 0.4750 - test acc: 0.3207 - 5m 13s\n",
      "batch: 500/1563 - train loss: 11.8443 - test loss: 15.2990 - train acc: 0.4488 - test acc: 0.3362 - 5m 18s\n",
      "batch: 600/1563 - train loss: 11.8075 - test loss: 14.0248 - train acc: 0.4394 - test acc: 0.3781 - 5m 22s\n",
      "batch: 700/1563 - train loss: 11.9967 - test loss: 14.9166 - train acc: 0.4397 - test acc: 0.3503 - 5m 27s\n",
      "batch: 800/1563 - train loss: 12.0133 - test loss: 15.1054 - train acc: 0.4316 - test acc: 0.3384 - 5m 32s\n",
      "batch: 900/1563 - train loss: 11.9536 - test loss: 14.2043 - train acc: 0.4300 - test acc: 0.3683 - 5m 37s\n",
      "batch: 1000/1563 - train loss: 11.8191 - test loss: 15.2314 - train acc: 0.4391 - test acc: 0.3443 - 5m 41s\n",
      "batch: 1100/1563 - train loss: 11.7064 - test loss: 14.5027 - train acc: 0.4422 - test acc: 0.3599 - 5m 46s\n",
      "batch: 1200/1563 - train loss: 12.1122 - test loss: 14.6849 - train acc: 0.4316 - test acc: 0.3599 - 5m 51s\n",
      "batch: 1300/1563 - train loss: 11.8401 - test loss: 13.8559 - train acc: 0.4366 - test acc: 0.3890 - 5m 55s\n",
      "batch: 1400/1563 - train loss: 12.0149 - test loss: 14.8531 - train acc: 0.4357 - test acc: 0.3559 - 6m 0s\n",
      "batch: 1500/1563 - train loss: 11.8730 - test loss: 13.9322 - train acc: 0.4534 - test acc: 0.3730 - 6m 5s\n",
      "batch: 1563/1563 - train loss: 11.7599 - test loss: 14.3507 - train acc: 0.4459 - test acc: 0.3644 - 6m 9s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.7913 - test loss: 13.9300 - train acc: 0.5259 - test acc: 0.3833 - 6m 14s\n",
      "batch: 200/1563 - train loss: 9.9146 - test loss: 13.7538 - train acc: 0.5103 - test acc: 0.3833 - 6m 18s\n",
      "batch: 300/1563 - train loss: 10.1115 - test loss: 14.1035 - train acc: 0.5190 - test acc: 0.3879 - 6m 23s\n",
      "batch: 400/1563 - train loss: 10.4108 - test loss: 14.4643 - train acc: 0.4919 - test acc: 0.3613 - 6m 27s\n",
      "batch: 500/1563 - train loss: 10.4645 - test loss: 14.4517 - train acc: 0.4938 - test acc: 0.3705 - 6m 32s\n",
      "batch: 600/1563 - train loss: 10.4382 - test loss: 13.4319 - train acc: 0.4990 - test acc: 0.4038 - 6m 37s\n",
      "batch: 700/1563 - train loss: 10.9138 - test loss: 13.4213 - train acc: 0.4797 - test acc: 0.3980 - 6m 42s\n",
      "batch: 800/1563 - train loss: 10.6358 - test loss: 14.3599 - train acc: 0.4938 - test acc: 0.3728 - 6m 46s\n",
      "batch: 900/1563 - train loss: 10.6128 - test loss: 14.0188 - train acc: 0.4840 - test acc: 0.3815 - 6m 51s\n",
      "batch: 1000/1563 - train loss: 10.6421 - test loss: 13.4367 - train acc: 0.4928 - test acc: 0.3944 - 6m 56s\n",
      "batch: 1100/1563 - train loss: 10.8317 - test loss: 13.5258 - train acc: 0.4747 - test acc: 0.4027 - 7m 0s\n",
      "batch: 1200/1563 - train loss: 10.6737 - test loss: 13.5358 - train acc: 0.4738 - test acc: 0.3939 - 7m 5s\n",
      "batch: 1300/1563 - train loss: 10.4157 - test loss: 13.2633 - train acc: 0.5050 - test acc: 0.4088 - 7m 10s\n",
      "batch: 1400/1563 - train loss: 10.6288 - test loss: 13.2969 - train acc: 0.4804 - test acc: 0.3999 - 7m 15s\n",
      "batch: 1500/1563 - train loss: 10.8543 - test loss: 14.0966 - train acc: 0.4719 - test acc: 0.3818 - 7m 20s\n",
      "batch: 1563/1563 - train loss: 11.1266 - test loss: 14.4137 - train acc: 0.4666 - test acc: 0.3687 - 7m 24s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 8.4287 - test loss: 13.4930 - train acc: 0.5913 - test acc: 0.4036 - 7m 29s\n",
      "batch: 200/1563 - train loss: 8.5792 - test loss: 14.0574 - train acc: 0.5809 - test acc: 0.3917 - 7m 34s\n",
      "batch: 300/1563 - train loss: 8.7673 - test loss: 14.0559 - train acc: 0.5622 - test acc: 0.3938 - 7m 38s\n",
      "batch: 400/1563 - train loss: 8.8352 - test loss: 13.9196 - train acc: 0.5593 - test acc: 0.3904 - 7m 44s\n",
      "batch: 500/1563 - train loss: 9.1918 - test loss: 14.3751 - train acc: 0.5465 - test acc: 0.3863 - 7m 48s\n",
      "batch: 600/1563 - train loss: 9.3123 - test loss: 14.3344 - train acc: 0.5434 - test acc: 0.3815 - 7m 53s\n",
      "batch: 700/1563 - train loss: 9.6878 - test loss: 13.5451 - train acc: 0.5175 - test acc: 0.4066 - 7m 58s\n",
      "batch: 800/1563 - train loss: 9.4501 - test loss: 15.9669 - train acc: 0.5347 - test acc: 0.3411 - 8m 3s\n",
      "batch: 900/1563 - train loss: 9.6771 - test loss: 14.8384 - train acc: 0.5243 - test acc: 0.3764 - 8m 8s\n",
      "batch: 1000/1563 - train loss: 9.4780 - test loss: 14.9550 - train acc: 0.5394 - test acc: 0.3694 - 8m 14s\n",
      "batch: 1100/1563 - train loss: 9.6445 - test loss: 13.6100 - train acc: 0.5231 - test acc: 0.4070 - 8m 19s\n",
      "batch: 1200/1563 - train loss: 9.8995 - test loss: 13.9118 - train acc: 0.5075 - test acc: 0.3925 - 8m 24s\n",
      "batch: 1300/1563 - train loss: 9.7306 - test loss: 14.2128 - train acc: 0.5247 - test acc: 0.3828 - 8m 28s\n",
      "batch: 1400/1563 - train loss: 9.3778 - test loss: 14.1382 - train acc: 0.5413 - test acc: 0.3933 - 8m 33s\n",
      "batch: 1500/1563 - train loss: 9.9337 - test loss: 13.5784 - train acc: 0.5141 - test acc: 0.4036 - 8m 38s\n",
      "batch: 1563/1563 - train loss: 9.7284 - test loss: 14.4607 - train acc: 0.5284 - test acc: 0.3841 - 8m 42s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.5682 - test loss: 14.2441 - train acc: 0.6187 - test acc: 0.3969 - 8m 47s\n",
      "batch: 200/1563 - train loss: 7.3594 - test loss: 14.3700 - train acc: 0.6212 - test acc: 0.3952 - 8m 52s\n",
      "batch: 300/1563 - train loss: 7.7167 - test loss: 14.0077 - train acc: 0.6065 - test acc: 0.4032 - 8m 57s\n",
      "batch: 400/1563 - train loss: 7.7378 - test loss: 14.1445 - train acc: 0.6047 - test acc: 0.3990 - 9m 2s\n",
      "batch: 500/1563 - train loss: 8.0160 - test loss: 14.1800 - train acc: 0.5891 - test acc: 0.4107 - 9m 7s\n",
      "batch: 600/1563 - train loss: 8.3360 - test loss: 14.0113 - train acc: 0.5878 - test acc: 0.4074 - 9m 12s\n",
      "batch: 700/1563 - train loss: 8.3827 - test loss: 15.2176 - train acc: 0.5737 - test acc: 0.3682 - 9m 16s\n",
      "batch: 800/1563 - train loss: 8.3809 - test loss: 13.8792 - train acc: 0.5781 - test acc: 0.4120 - 9m 21s\n",
      "batch: 900/1563 - train loss: 8.6680 - test loss: 13.7818 - train acc: 0.5656 - test acc: 0.4153 - 9m 26s\n",
      "batch: 1000/1563 - train loss: 8.3679 - test loss: 14.5687 - train acc: 0.5812 - test acc: 0.3978 - 9m 31s\n",
      "batch: 1100/1563 - train loss: 8.4960 - test loss: 14.3010 - train acc: 0.5728 - test acc: 0.3936 - 9m 35s\n",
      "batch: 1200/1563 - train loss: 8.8985 - test loss: 13.3989 - train acc: 0.5559 - test acc: 0.4178 - 9m 40s\n",
      "batch: 1300/1563 - train loss: 8.7119 - test loss: 16.2695 - train acc: 0.5593 - test acc: 0.3600 - 9m 45s\n",
      "batch: 1400/1563 - train loss: 8.4984 - test loss: 13.6545 - train acc: 0.5784 - test acc: 0.4186 - 9m 50s\n",
      "batch: 1500/1563 - train loss: 8.8148 - test loss: 14.1837 - train acc: 0.5628 - test acc: 0.3949 - 9m 55s\n",
      "batch: 1563/1563 - train loss: 8.9549 - test loss: 14.0489 - train acc: 0.5512 - test acc: 0.4059 - 9m 59s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 5.9353 - test loss: 13.8588 - train acc: 0.6897 - test acc: 0.4167 - 10m 3s\n",
      "batch: 200/1563 - train loss: 6.4907 - test loss: 14.1330 - train acc: 0.6703 - test acc: 0.4134 - 10m 8s\n",
      "batch: 300/1563 - train loss: 6.5862 - test loss: 14.3948 - train acc: 0.6525 - test acc: 0.4092 - 10m 13s\n",
      "batch: 400/1563 - train loss: 6.8257 - test loss: 14.4391 - train acc: 0.6481 - test acc: 0.4107 - 10m 18s\n",
      "batch: 500/1563 - train loss: 7.0153 - test loss: 14.5429 - train acc: 0.6465 - test acc: 0.4102 - 10m 23s\n",
      "batch: 600/1563 - train loss: 6.8194 - test loss: 14.2216 - train acc: 0.6512 - test acc: 0.4105 - 10m 27s\n",
      "batch: 700/1563 - train loss: 7.2228 - test loss: 14.6170 - train acc: 0.6281 - test acc: 0.4059 - 10m 32s\n",
      "batch: 800/1563 - train loss: 7.4997 - test loss: 14.7962 - train acc: 0.6209 - test acc: 0.4051 - 10m 37s\n",
      "batch: 900/1563 - train loss: 7.4264 - test loss: 14.5712 - train acc: 0.6118 - test acc: 0.4027 - 10m 41s\n",
      "batch: 1000/1563 - train loss: 7.3098 - test loss: 14.6689 - train acc: 0.6356 - test acc: 0.4031 - 10m 46s\n",
      "batch: 1100/1563 - train loss: 7.7975 - test loss: 13.9129 - train acc: 0.6103 - test acc: 0.4164 - 10m 51s\n",
      "batch: 1200/1563 - train loss: 7.6802 - test loss: 14.0083 - train acc: 0.6002 - test acc: 0.4002 - 10m 56s\n",
      "batch: 1300/1563 - train loss: 7.8841 - test loss: 14.2524 - train acc: 0.6013 - test acc: 0.4091 - 11m 1s\n",
      "batch: 1400/1563 - train loss: 7.5046 - test loss: 13.8933 - train acc: 0.6228 - test acc: 0.4232 - 11m 5s\n",
      "batch: 1500/1563 - train loss: 7.6014 - test loss: 14.2475 - train acc: 0.6153 - test acc: 0.4008 - 11m 10s\n",
      "batch: 1563/1563 - train loss: 7.6569 - test loss: 14.7112 - train acc: 0.6200 - test acc: 0.4015 - 11m 14s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.0960 - test loss: 14.2383 - train acc: 0.7344 - test acc: 0.4263 - 11m 19s\n",
      "batch: 200/1563 - train loss: 5.3167 - test loss: 14.7661 - train acc: 0.7175 - test acc: 0.4142 - 11m 24s\n",
      "batch: 300/1563 - train loss: 5.6431 - test loss: 15.0588 - train acc: 0.7041 - test acc: 0.4043 - 11m 29s\n",
      "batch: 400/1563 - train loss: 5.6524 - test loss: 15.7880 - train acc: 0.7057 - test acc: 0.3929 - 11m 34s\n",
      "batch: 500/1563 - train loss: 5.7641 - test loss: 14.7715 - train acc: 0.6950 - test acc: 0.4121 - 11m 39s\n",
      "batch: 600/1563 - train loss: 6.0500 - test loss: 14.9437 - train acc: 0.6813 - test acc: 0.4060 - 11m 43s\n",
      "batch: 700/1563 - train loss: 6.0078 - test loss: 15.2124 - train acc: 0.6807 - test acc: 0.4025 - 11m 48s\n",
      "batch: 800/1563 - train loss: 6.3754 - test loss: 14.5700 - train acc: 0.6672 - test acc: 0.4202 - 11m 53s\n",
      "batch: 900/1563 - train loss: 6.3315 - test loss: 15.1988 - train acc: 0.6700 - test acc: 0.4043 - 11m 58s\n",
      "batch: 1000/1563 - train loss: 6.5791 - test loss: 15.7657 - train acc: 0.6587 - test acc: 0.3930 - 12m 2s\n",
      "batch: 1100/1563 - train loss: 6.9154 - test loss: 14.7236 - train acc: 0.6306 - test acc: 0.4111 - 12m 7s\n",
      "batch: 1200/1563 - train loss: 6.6849 - test loss: 14.4943 - train acc: 0.6543 - test acc: 0.4187 - 12m 12s\n",
      "batch: 1300/1563 - train loss: 6.7805 - test loss: 14.4153 - train acc: 0.6418 - test acc: 0.4120 - 12m 17s\n",
      "batch: 1400/1563 - train loss: 6.9636 - test loss: 14.7896 - train acc: 0.6366 - test acc: 0.4064 - 12m 21s\n",
      "batch: 1500/1563 - train loss: 6.7238 - test loss: 14.6121 - train acc: 0.6556 - test acc: 0.4124 - 12m 27s\n",
      "batch: 1563/1563 - train loss: 6.7689 - test loss: 14.7401 - train acc: 0.6453 - test acc: 0.4076 - 12m 31s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.3155 - test loss: 15.2450 - train acc: 0.7734 - test acc: 0.4124 - 12m 35s\n",
      "batch: 200/1563 - train loss: 4.4014 - test loss: 15.2017 - train acc: 0.7703 - test acc: 0.4139 - 12m 40s\n",
      "batch: 300/1563 - train loss: 4.3077 - test loss: 14.8790 - train acc: 0.7710 - test acc: 0.4256 - 12m 45s\n",
      "batch: 400/1563 - train loss: 4.7180 - test loss: 15.5951 - train acc: 0.7466 - test acc: 0.4099 - 12m 50s\n",
      "batch: 500/1563 - train loss: 5.0352 - test loss: 15.5348 - train acc: 0.7262 - test acc: 0.4049 - 12m 55s\n",
      "batch: 600/1563 - train loss: 5.1424 - test loss: 15.9040 - train acc: 0.7309 - test acc: 0.3965 - 13m 0s\n",
      "batch: 700/1563 - train loss: 5.3106 - test loss: 15.9383 - train acc: 0.7088 - test acc: 0.4066 - 13m 5s\n",
      "batch: 800/1563 - train loss: 5.2618 - test loss: 15.4964 - train acc: 0.7168 - test acc: 0.4065 - 13m 10s\n",
      "batch: 900/1563 - train loss: 5.5955 - test loss: 15.7960 - train acc: 0.7003 - test acc: 0.4099 - 13m 14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 5.4351 - test loss: 15.5298 - train acc: 0.7025 - test acc: 0.4056 - 13m 19s\n",
      "batch: 1100/1563 - train loss: 5.4887 - test loss: 17.0953 - train acc: 0.7116 - test acc: 0.3849 - 13m 24s\n",
      "batch: 1200/1563 - train loss: 5.7581 - test loss: 14.8228 - train acc: 0.7000 - test acc: 0.4213 - 13m 29s\n",
      "batch: 1300/1563 - train loss: 5.8268 - test loss: 15.8895 - train acc: 0.6822 - test acc: 0.3927 - 13m 34s\n",
      "batch: 1400/1563 - train loss: 5.8961 - test loss: 15.5404 - train acc: 0.6800 - test acc: 0.4118 - 13m 39s\n",
      "batch: 1500/1563 - train loss: 5.9468 - test loss: 15.3945 - train acc: 0.6800 - test acc: 0.4137 - 13m 44s\n",
      "batch: 1563/1563 - train loss: 5.8506 - test loss: 15.4054 - train acc: 0.6866 - test acc: 0.4113 - 13m 48s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.5342 - test loss: 15.9912 - train acc: 0.8096 - test acc: 0.4180 - 13m 53s\n",
      "batch: 200/1563 - train loss: 3.5956 - test loss: 16.6494 - train acc: 0.8037 - test acc: 0.3953 - 13m 58s\n",
      "batch: 300/1563 - train loss: 3.6215 - test loss: 16.2567 - train acc: 0.8012 - test acc: 0.4107 - 14m 3s\n",
      "batch: 400/1563 - train loss: 4.0657 - test loss: 16.7293 - train acc: 0.7769 - test acc: 0.4008 - 14m 8s\n",
      "batch: 500/1563 - train loss: 4.0607 - test loss: 16.9078 - train acc: 0.7788 - test acc: 0.4078 - 14m 13s\n",
      "batch: 600/1563 - train loss: 4.3929 - test loss: 16.8200 - train acc: 0.7631 - test acc: 0.4019 - 14m 17s\n",
      "batch: 700/1563 - train loss: 4.5223 - test loss: 16.8461 - train acc: 0.7556 - test acc: 0.3936 - 14m 22s\n",
      "batch: 800/1563 - train loss: 4.2536 - test loss: 16.2311 - train acc: 0.7709 - test acc: 0.4112 - 14m 27s\n",
      "batch: 900/1563 - train loss: 4.5915 - test loss: 16.6112 - train acc: 0.7475 - test acc: 0.4071 - 14m 32s\n",
      "batch: 1000/1563 - train loss: 4.7092 - test loss: 16.6776 - train acc: 0.7362 - test acc: 0.3934 - 14m 37s\n",
      "batch: 1100/1563 - train loss: 4.8645 - test loss: 15.8689 - train acc: 0.7294 - test acc: 0.4198 - 14m 42s\n",
      "batch: 1200/1563 - train loss: 5.0442 - test loss: 16.6223 - train acc: 0.7290 - test acc: 0.3996 - 14m 47s\n",
      "batch: 1300/1563 - train loss: 5.1687 - test loss: 15.6958 - train acc: 0.7269 - test acc: 0.4181 - 14m 52s\n",
      "batch: 1400/1563 - train loss: 4.9508 - test loss: 19.3768 - train acc: 0.7266 - test acc: 0.3550 - 14m 57s\n",
      "batch: 1500/1563 - train loss: 5.2565 - test loss: 16.2024 - train acc: 0.7084 - test acc: 0.4088 - 15m 2s\n",
      "batch: 1563/1563 - train loss: 5.3296 - test loss: 15.5582 - train acc: 0.7091 - test acc: 0.4095 - 15m 6s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 2.9567 - test loss: 15.6079 - train acc: 0.8387 - test acc: 0.4290 - 15m 11s\n",
      "batch: 200/1563 - train loss: 3.1294 - test loss: 16.4778 - train acc: 0.8356 - test acc: 0.4152 - 15m 16s\n",
      "batch: 300/1563 - train loss: 3.0856 - test loss: 16.9180 - train acc: 0.8262 - test acc: 0.4201 - 15m 21s\n",
      "batch: 400/1563 - train loss: 3.1460 - test loss: 16.2750 - train acc: 0.8249 - test acc: 0.4242 - 15m 26s\n",
      "batch: 500/1563 - train loss: 3.1977 - test loss: 16.9107 - train acc: 0.8237 - test acc: 0.4226 - 15m 31s\n",
      "batch: 600/1563 - train loss: 3.2534 - test loss: 16.8619 - train acc: 0.8215 - test acc: 0.4187 - 15m 36s\n",
      "batch: 700/1563 - train loss: 3.5645 - test loss: 17.7326 - train acc: 0.7984 - test acc: 0.3989 - 15m 41s\n",
      "batch: 800/1563 - train loss: 3.8362 - test loss: 17.5932 - train acc: 0.7835 - test acc: 0.3930 - 15m 46s\n",
      "batch: 900/1563 - train loss: 3.8394 - test loss: 17.3663 - train acc: 0.7852 - test acc: 0.4056 - 15m 51s\n",
      "batch: 1000/1563 - train loss: 3.9711 - test loss: 16.5635 - train acc: 0.7784 - test acc: 0.4104 - 15m 56s\n",
      "batch: 1100/1563 - train loss: 4.1677 - test loss: 16.3822 - train acc: 0.7712 - test acc: 0.4219 - 16m 1s\n",
      "batch: 1200/1563 - train loss: 4.2269 - test loss: 18.1766 - train acc: 0.7656 - test acc: 0.3874 - 16m 6s\n",
      "batch: 1300/1563 - train loss: 4.2657 - test loss: 17.0041 - train acc: 0.7712 - test acc: 0.4051 - 16m 10s\n",
      "batch: 1400/1563 - train loss: 4.5029 - test loss: 16.3108 - train acc: 0.7571 - test acc: 0.4158 - 16m 15s\n",
      "batch: 1500/1563 - train loss: 4.2512 - test loss: 16.5567 - train acc: 0.7653 - test acc: 0.4126 - 16m 20s\n",
      "batch: 1563/1563 - train loss: 4.3557 - test loss: 16.7538 - train acc: 0.7546 - test acc: 0.4083 - 16m 24s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.5850 - test loss: 16.6076 - train acc: 0.8566 - test acc: 0.4244 - 16m 29s\n",
      "batch: 200/1563 - train loss: 2.3582 - test loss: 17.0779 - train acc: 0.8613 - test acc: 0.4208 - 16m 34s\n",
      "batch: 300/1563 - train loss: 2.3823 - test loss: 17.4009 - train acc: 0.8672 - test acc: 0.4179 - 16m 39s\n",
      "batch: 400/1563 - train loss: 2.6579 - test loss: 19.2014 - train acc: 0.8512 - test acc: 0.3808 - 16m 44s\n",
      "batch: 500/1563 - train loss: 3.0331 - test loss: 17.8381 - train acc: 0.8265 - test acc: 0.4050 - 16m 49s\n",
      "batch: 600/1563 - train loss: 2.8120 - test loss: 17.7848 - train acc: 0.8406 - test acc: 0.4092 - 16m 53s\n",
      "batch: 700/1563 - train loss: 3.0528 - test loss: 17.6414 - train acc: 0.8263 - test acc: 0.4020 - 16m 58s\n",
      "batch: 800/1563 - train loss: 2.9351 - test loss: 17.6762 - train acc: 0.8281 - test acc: 0.4089 - 17m 3s\n",
      "batch: 900/1563 - train loss: 3.2867 - test loss: 17.4765 - train acc: 0.8231 - test acc: 0.4164 - 17m 8s\n",
      "batch: 1000/1563 - train loss: 3.2489 - test loss: 17.7976 - train acc: 0.8243 - test acc: 0.4158 - 17m 12s\n",
      "batch: 1100/1563 - train loss: 3.4418 - test loss: 17.5559 - train acc: 0.8177 - test acc: 0.4143 - 17m 18s\n",
      "batch: 1200/1563 - train loss: 3.4578 - test loss: 17.9792 - train acc: 0.8090 - test acc: 0.3974 - 17m 22s\n",
      "batch: 1300/1563 - train loss: 3.7433 - test loss: 17.1002 - train acc: 0.7943 - test acc: 0.4214 - 17m 27s\n",
      "batch: 1400/1563 - train loss: 3.4510 - test loss: 17.5292 - train acc: 0.8065 - test acc: 0.4139 - 17m 32s\n",
      "batch: 1500/1563 - train loss: 3.5905 - test loss: 17.7050 - train acc: 0.7980 - test acc: 0.4120 - 17m 37s\n",
      "batch: 1563/1563 - train loss: 3.6166 - test loss: 18.5445 - train acc: 0.7897 - test acc: 0.3948 - 17m 41s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.0831 - test loss: 17.8623 - train acc: 0.8847 - test acc: 0.4168 - 17m 47s\n",
      "batch: 200/1563 - train loss: 2.0790 - test loss: 17.8523 - train acc: 0.8850 - test acc: 0.4205 - 17m 51s\n",
      "batch: 300/1563 - train loss: 2.2008 - test loss: 18.5894 - train acc: 0.8732 - test acc: 0.4197 - 17m 56s\n",
      "batch: 400/1563 - train loss: 2.1895 - test loss: 17.6331 - train acc: 0.8804 - test acc: 0.4221 - 18m 1s\n",
      "batch: 500/1563 - train loss: 2.2980 - test loss: 17.9253 - train acc: 0.8697 - test acc: 0.4158 - 18m 6s\n",
      "batch: 600/1563 - train loss: 2.1457 - test loss: 18.1413 - train acc: 0.8778 - test acc: 0.4189 - 18m 11s\n",
      "batch: 700/1563 - train loss: 2.3914 - test loss: 18.0495 - train acc: 0.8687 - test acc: 0.4286 - 18m 16s\n",
      "batch: 800/1563 - train loss: 2.5039 - test loss: 18.9093 - train acc: 0.8531 - test acc: 0.4127 - 18m 21s\n",
      "batch: 900/1563 - train loss: 2.7418 - test loss: 18.9133 - train acc: 0.8468 - test acc: 0.4087 - 18m 26s\n",
      "batch: 1000/1563 - train loss: 2.7747 - test loss: 18.6032 - train acc: 0.8440 - test acc: 0.4063 - 18m 31s\n",
      "batch: 1100/1563 - train loss: 2.7834 - test loss: 17.8208 - train acc: 0.8459 - test acc: 0.4200 - 18m 35s\n",
      "batch: 1200/1563 - train loss: 2.8765 - test loss: 18.9043 - train acc: 0.8312 - test acc: 0.3998 - 18m 40s\n",
      "batch: 1300/1563 - train loss: 2.9583 - test loss: 18.6188 - train acc: 0.8303 - test acc: 0.4122 - 18m 45s\n",
      "batch: 1400/1563 - train loss: 3.0404 - test loss: 18.0318 - train acc: 0.8262 - test acc: 0.4148 - 18m 51s\n",
      "batch: 1500/1563 - train loss: 3.1819 - test loss: 18.0215 - train acc: 0.8197 - test acc: 0.4167 - 18m 55s\n",
      "batch: 1563/1563 - train loss: 3.1267 - test loss: 18.3730 - train acc: 0.8293 - test acc: 0.4161 - 18m 59s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.8184 - test loss: 18.0263 - train acc: 0.8954 - test acc: 0.4248 - 19m 4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 1.6074 - test loss: 18.9198 - train acc: 0.9117 - test acc: 0.4178 - 19m 9s\n",
      "batch: 300/1563 - train loss: 1.6781 - test loss: 18.2176 - train acc: 0.9066 - test acc: 0.4267 - 19m 14s\n",
      "batch: 400/1563 - train loss: 1.7017 - test loss: 18.3185 - train acc: 0.9067 - test acc: 0.4291 - 19m 18s\n",
      "batch: 500/1563 - train loss: 1.8189 - test loss: 18.4111 - train acc: 0.9045 - test acc: 0.4234 - 19m 24s\n",
      "batch: 600/1563 - train loss: 1.8477 - test loss: 19.2300 - train acc: 0.8954 - test acc: 0.4123 - 19m 28s\n",
      "batch: 700/1563 - train loss: 1.9854 - test loss: 19.3716 - train acc: 0.8869 - test acc: 0.4187 - 19m 33s\n",
      "batch: 800/1563 - train loss: 2.2052 - test loss: 18.6383 - train acc: 0.8750 - test acc: 0.4226 - 19m 38s\n",
      "batch: 900/1563 - train loss: 2.2969 - test loss: 19.1953 - train acc: 0.8734 - test acc: 0.4085 - 19m 43s\n",
      "batch: 1000/1563 - train loss: 2.4052 - test loss: 18.9707 - train acc: 0.8640 - test acc: 0.4149 - 19m 48s\n",
      "batch: 1100/1563 - train loss: 2.2836 - test loss: 18.7544 - train acc: 0.8659 - test acc: 0.4197 - 19m 53s\n",
      "batch: 1200/1563 - train loss: 2.3195 - test loss: 18.1878 - train acc: 0.8750 - test acc: 0.4243 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 1201/1563 - train loss: 2.3254 - test loss: 18.2709 - train acc: 0.8747 - test acc: 0.4218 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0105 - test loss: 24.7940 - train acc: 0.0361 - test acc: 0.0541 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.4250 - test loss: 23.9717 - train acc: 0.0586 - test acc: 0.0710 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.1661 - test loss: 22.4733 - train acc: 0.0808 - test acc: 0.0963 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.6394 - test loss: 22.1648 - train acc: 0.0834 - test acc: 0.1117 - 0m 17s\n",
      "batch: 500/1563 - train loss: 22.1379 - test loss: 21.4049 - train acc: 0.1068 - test acc: 0.1305 - 0m 22s\n",
      "batch: 600/1563 - train loss: 21.8517 - test loss: 21.4216 - train acc: 0.1031 - test acc: 0.1214 - 0m 26s\n",
      "batch: 700/1563 - train loss: 21.0677 - test loss: 21.2531 - train acc: 0.1246 - test acc: 0.1162 - 0m 31s\n",
      "batch: 800/1563 - train loss: 20.7246 - test loss: 20.4472 - train acc: 0.1381 - test acc: 0.1534 - 0m 36s\n",
      "batch: 900/1563 - train loss: 20.6092 - test loss: 20.4939 - train acc: 0.1388 - test acc: 0.1483 - 0m 41s\n",
      "batch: 1000/1563 - train loss: 20.1920 - test loss: 19.6144 - train acc: 0.1551 - test acc: 0.1735 - 0m 47s\n",
      "batch: 1100/1563 - train loss: 19.6252 - test loss: 19.6705 - train acc: 0.1779 - test acc: 0.1723 - 0m 51s\n",
      "batch: 1200/1563 - train loss: 19.6358 - test loss: 18.7346 - train acc: 0.1773 - test acc: 0.1972 - 0m 56s\n",
      "batch: 1300/1563 - train loss: 19.3233 - test loss: 19.8688 - train acc: 0.1832 - test acc: 0.1690 - 1m 1s\n",
      "batch: 1400/1563 - train loss: 19.2489 - test loss: 18.6886 - train acc: 0.1778 - test acc: 0.2077 - 1m 6s\n",
      "batch: 1500/1563 - train loss: 18.9673 - test loss: 18.8855 - train acc: 0.1982 - test acc: 0.1933 - 1m 11s\n",
      "batch: 1563/1563 - train loss: 18.6676 - test loss: 18.3248 - train acc: 0.1988 - test acc: 0.2147 - 1m 15s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 17.9058 - test loss: 19.5544 - train acc: 0.2325 - test acc: 0.1881 - 1m 21s\n",
      "batch: 200/1563 - train loss: 18.0889 - test loss: 18.4283 - train acc: 0.2173 - test acc: 0.2177 - 1m 26s\n",
      "batch: 300/1563 - train loss: 17.6947 - test loss: 18.1962 - train acc: 0.2291 - test acc: 0.2147 - 1m 30s\n",
      "batch: 400/1563 - train loss: 17.7159 - test loss: 19.2379 - train acc: 0.2275 - test acc: 0.1891 - 1m 35s\n",
      "batch: 500/1563 - train loss: 17.8140 - test loss: 17.9209 - train acc: 0.2284 - test acc: 0.2304 - 1m 40s\n",
      "batch: 600/1563 - train loss: 17.5799 - test loss: 18.1186 - train acc: 0.2353 - test acc: 0.2174 - 1m 45s\n",
      "batch: 700/1563 - train loss: 17.1568 - test loss: 17.5427 - train acc: 0.2525 - test acc: 0.2404 - 1m 50s\n",
      "batch: 800/1563 - train loss: 17.0778 - test loss: 18.5728 - train acc: 0.2528 - test acc: 0.2142 - 1m 55s\n",
      "batch: 900/1563 - train loss: 16.9982 - test loss: 18.2012 - train acc: 0.2469 - test acc: 0.2232 - 2m 0s\n",
      "batch: 1000/1563 - train loss: 17.0117 - test loss: 16.9544 - train acc: 0.2538 - test acc: 0.2565 - 2m 5s\n",
      "batch: 1100/1563 - train loss: 16.7699 - test loss: 17.2827 - train acc: 0.2612 - test acc: 0.2539 - 2m 10s\n",
      "batch: 1200/1563 - train loss: 16.6276 - test loss: 16.7770 - train acc: 0.2606 - test acc: 0.2637 - 2m 15s\n",
      "batch: 1300/1563 - train loss: 16.5407 - test loss: 16.3869 - train acc: 0.2678 - test acc: 0.2811 - 2m 20s\n",
      "batch: 1400/1563 - train loss: 16.5452 - test loss: 17.1318 - train acc: 0.2712 - test acc: 0.2563 - 2m 25s\n",
      "batch: 1500/1563 - train loss: 16.4705 - test loss: 17.0546 - train acc: 0.2684 - test acc: 0.2639 - 2m 30s\n",
      "batch: 1563/1563 - train loss: 16.4001 - test loss: 16.1923 - train acc: 0.2787 - test acc: 0.2873 - 2m 34s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.3364 - test loss: 16.3937 - train acc: 0.3097 - test acc: 0.2831 - 2m 39s\n",
      "batch: 200/1563 - train loss: 14.9032 - test loss: 16.0458 - train acc: 0.3206 - test acc: 0.2941 - 2m 44s\n",
      "batch: 300/1563 - train loss: 15.1681 - test loss: 17.1875 - train acc: 0.3134 - test acc: 0.2506 - 2m 49s\n",
      "batch: 400/1563 - train loss: 15.2918 - test loss: 15.7412 - train acc: 0.3215 - test acc: 0.3078 - 2m 54s\n",
      "batch: 500/1563 - train loss: 15.1034 - test loss: 16.0571 - train acc: 0.3203 - test acc: 0.2948 - 2m 59s\n",
      "batch: 600/1563 - train loss: 15.2527 - test loss: 16.8548 - train acc: 0.3134 - test acc: 0.2704 - 3m 4s\n",
      "batch: 700/1563 - train loss: 14.8441 - test loss: 16.1797 - train acc: 0.3328 - test acc: 0.2908 - 3m 9s\n",
      "batch: 800/1563 - train loss: 14.9894 - test loss: 15.9879 - train acc: 0.3246 - test acc: 0.2938 - 3m 15s\n",
      "batch: 900/1563 - train loss: 14.7812 - test loss: 16.6678 - train acc: 0.3328 - test acc: 0.2778 - 3m 20s\n",
      "batch: 1000/1563 - train loss: 15.0684 - test loss: 15.6821 - train acc: 0.3103 - test acc: 0.3041 - 3m 25s\n",
      "batch: 1100/1563 - train loss: 14.7478 - test loss: 16.0560 - train acc: 0.3322 - test acc: 0.2990 - 3m 30s\n",
      "batch: 1200/1563 - train loss: 14.8537 - test loss: 15.7091 - train acc: 0.3272 - test acc: 0.3021 - 3m 35s\n",
      "batch: 1300/1563 - train loss: 14.9616 - test loss: 16.8389 - train acc: 0.3227 - test acc: 0.2713 - 3m 41s\n",
      "batch: 1400/1563 - train loss: 14.7805 - test loss: 15.0484 - train acc: 0.3306 - test acc: 0.3280 - 3m 46s\n",
      "batch: 1500/1563 - train loss: 14.7082 - test loss: 15.1966 - train acc: 0.3331 - test acc: 0.3207 - 3m 51s\n",
      "batch: 1563/1563 - train loss: 14.6002 - test loss: 15.8455 - train acc: 0.3497 - test acc: 0.3002 - 3m 55s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.9872 - test loss: 14.8543 - train acc: 0.3972 - test acc: 0.3459 - 4m 0s\n",
      "batch: 200/1563 - train loss: 12.7128 - test loss: 19.5829 - train acc: 0.4000 - test acc: 0.2404 - 4m 5s\n",
      "batch: 300/1563 - train loss: 13.0730 - test loss: 15.0475 - train acc: 0.3863 - test acc: 0.3368 - 4m 11s\n",
      "batch: 400/1563 - train loss: 13.4395 - test loss: 14.3312 - train acc: 0.3800 - test acc: 0.3580 - 4m 16s\n",
      "batch: 500/1563 - train loss: 13.3165 - test loss: 15.1134 - train acc: 0.3877 - test acc: 0.3308 - 4m 21s\n",
      "batch: 600/1563 - train loss: 13.4054 - test loss: 15.7738 - train acc: 0.3863 - test acc: 0.3181 - 4m 26s\n",
      "batch: 700/1563 - train loss: 13.2611 - test loss: 15.3444 - train acc: 0.3819 - test acc: 0.3251 - 4m 31s\n",
      "batch: 800/1563 - train loss: 13.5617 - test loss: 15.5249 - train acc: 0.3610 - test acc: 0.3176 - 4m 36s\n",
      "batch: 900/1563 - train loss: 13.4073 - test loss: 16.9621 - train acc: 0.3784 - test acc: 0.2946 - 4m 42s\n",
      "batch: 1000/1563 - train loss: 13.3580 - test loss: 15.0770 - train acc: 0.3806 - test acc: 0.3339 - 4m 47s\n",
      "batch: 1100/1563 - train loss: 13.1158 - test loss: 14.9803 - train acc: 0.3922 - test acc: 0.3390 - 4m 52s\n",
      "batch: 1200/1563 - train loss: 13.3408 - test loss: 14.1964 - train acc: 0.3906 - test acc: 0.3630 - 4m 57s\n",
      "batch: 1300/1563 - train loss: 13.4991 - test loss: 15.1775 - train acc: 0.3838 - test acc: 0.3259 - 5m 2s\n",
      "batch: 1400/1563 - train loss: 13.1730 - test loss: 14.6543 - train acc: 0.3950 - test acc: 0.3467 - 5m 7s\n",
      "batch: 1500/1563 - train loss: 12.8680 - test loss: 14.3550 - train acc: 0.3959 - test acc: 0.3613 - 5m 12s\n",
      "batch: 1563/1563 - train loss: 13.0434 - test loss: 16.0504 - train acc: 0.3850 - test acc: 0.3108 - 5m 16s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.4158 - test loss: 14.1946 - train acc: 0.4616 - test acc: 0.3692 - 5m 21s\n",
      "batch: 200/1563 - train loss: 11.3986 - test loss: 15.3366 - train acc: 0.4619 - test acc: 0.3320 - 5m 26s\n",
      "batch: 300/1563 - train loss: 11.8539 - test loss: 14.0836 - train acc: 0.4357 - test acc: 0.3661 - 5m 32s\n",
      "batch: 400/1563 - train loss: 11.5136 - test loss: 14.6035 - train acc: 0.4482 - test acc: 0.3560 - 5m 37s\n",
      "batch: 500/1563 - train loss: 11.9958 - test loss: 13.7067 - train acc: 0.4303 - test acc: 0.3794 - 5m 41s\n",
      "batch: 600/1563 - train loss: 11.8313 - test loss: 14.0199 - train acc: 0.4382 - test acc: 0.3705 - 5m 46s\n",
      "batch: 700/1563 - train loss: 11.8993 - test loss: 14.1070 - train acc: 0.4338 - test acc: 0.3701 - 5m 51s\n",
      "batch: 800/1563 - train loss: 11.6591 - test loss: 14.5939 - train acc: 0.4412 - test acc: 0.3597 - 5m 56s\n",
      "batch: 900/1563 - train loss: 12.1511 - test loss: 14.0590 - train acc: 0.4257 - test acc: 0.3725 - 6m 1s\n",
      "batch: 1000/1563 - train loss: 11.6195 - test loss: 13.8945 - train acc: 0.4457 - test acc: 0.3802 - 6m 6s\n",
      "batch: 1100/1563 - train loss: 11.9294 - test loss: 14.1713 - train acc: 0.4251 - test acc: 0.3693 - 6m 11s\n",
      "batch: 1200/1563 - train loss: 11.7146 - test loss: 14.2160 - train acc: 0.4516 - test acc: 0.3710 - 6m 16s\n",
      "batch: 1300/1563 - train loss: 12.1891 - test loss: 13.6521 - train acc: 0.4307 - test acc: 0.3875 - 6m 21s\n",
      "batch: 1400/1563 - train loss: 11.9402 - test loss: 13.9834 - train acc: 0.4200 - test acc: 0.3730 - 6m 25s\n",
      "batch: 1500/1563 - train loss: 11.8866 - test loss: 15.0946 - train acc: 0.4419 - test acc: 0.3445 - 6m 30s\n",
      "batch: 1563/1563 - train loss: 12.0115 - test loss: 13.9359 - train acc: 0.4356 - test acc: 0.3784 - 6m 34s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.8589 - test loss: 14.3941 - train acc: 0.5066 - test acc: 0.3738 - 6m 39s\n",
      "batch: 200/1563 - train loss: 10.2577 - test loss: 14.0642 - train acc: 0.5009 - test acc: 0.3825 - 6m 44s\n",
      "batch: 300/1563 - train loss: 10.2085 - test loss: 13.8258 - train acc: 0.5078 - test acc: 0.3843 - 6m 49s\n",
      "batch: 400/1563 - train loss: 10.4921 - test loss: 13.7815 - train acc: 0.4813 - test acc: 0.3835 - 6m 54s\n",
      "batch: 500/1563 - train loss: 10.2898 - test loss: 13.6228 - train acc: 0.5018 - test acc: 0.3922 - 6m 58s\n",
      "batch: 600/1563 - train loss: 10.5013 - test loss: 13.7988 - train acc: 0.5016 - test acc: 0.3861 - 7m 3s\n",
      "batch: 700/1563 - train loss: 10.3994 - test loss: 14.2624 - train acc: 0.4991 - test acc: 0.3744 - 7m 8s\n",
      "batch: 800/1563 - train loss: 10.9318 - test loss: 13.8223 - train acc: 0.4725 - test acc: 0.3842 - 7m 13s\n",
      "batch: 900/1563 - train loss: 10.6252 - test loss: 14.0404 - train acc: 0.4959 - test acc: 0.3769 - 7m 18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 10.8673 - test loss: 13.7312 - train acc: 0.4762 - test acc: 0.3892 - 7m 23s\n",
      "batch: 1100/1563 - train loss: 10.7140 - test loss: 14.0320 - train acc: 0.4810 - test acc: 0.3792 - 7m 28s\n",
      "batch: 1200/1563 - train loss: 10.6735 - test loss: 13.2459 - train acc: 0.4934 - test acc: 0.4052 - 7m 33s\n",
      "batch: 1300/1563 - train loss: 10.8040 - test loss: 13.5924 - train acc: 0.4775 - test acc: 0.3949 - 7m 37s\n",
      "batch: 1400/1563 - train loss: 11.0192 - test loss: 13.4075 - train acc: 0.4613 - test acc: 0.3972 - 7m 42s\n",
      "batch: 1500/1563 - train loss: 10.8686 - test loss: 14.1508 - train acc: 0.4900 - test acc: 0.3876 - 7m 47s\n",
      "batch: 1563/1563 - train loss: 10.7482 - test loss: 13.5866 - train acc: 0.4822 - test acc: 0.3915 - 7m 51s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.7132 - test loss: 14.8975 - train acc: 0.5725 - test acc: 0.3723 - 7m 56s\n",
      "batch: 200/1563 - train loss: 8.7082 - test loss: 13.8952 - train acc: 0.5575 - test acc: 0.3994 - 8m 1s\n",
      "batch: 300/1563 - train loss: 8.8179 - test loss: 13.8079 - train acc: 0.5550 - test acc: 0.4026 - 8m 6s\n",
      "batch: 400/1563 - train loss: 8.9604 - test loss: 13.8259 - train acc: 0.5632 - test acc: 0.3971 - 8m 12s\n",
      "batch: 500/1563 - train loss: 9.3966 - test loss: 13.8996 - train acc: 0.5440 - test acc: 0.3918 - 8m 16s\n",
      "batch: 600/1563 - train loss: 9.2244 - test loss: 14.4297 - train acc: 0.5338 - test acc: 0.3791 - 8m 21s\n",
      "batch: 700/1563 - train loss: 9.4044 - test loss: 13.9630 - train acc: 0.5340 - test acc: 0.3932 - 8m 26s\n",
      "batch: 800/1563 - train loss: 9.4511 - test loss: 13.9396 - train acc: 0.5250 - test acc: 0.3921 - 8m 31s\n",
      "batch: 900/1563 - train loss: 9.5261 - test loss: 14.8269 - train acc: 0.5284 - test acc: 0.3716 - 8m 36s\n",
      "batch: 1000/1563 - train loss: 9.4006 - test loss: 13.9034 - train acc: 0.5425 - test acc: 0.3925 - 8m 40s\n",
      "batch: 1100/1563 - train loss: 9.4082 - test loss: 13.7735 - train acc: 0.5438 - test acc: 0.4005 - 8m 46s\n",
      "batch: 1200/1563 - train loss: 9.5394 - test loss: 14.6798 - train acc: 0.5306 - test acc: 0.3754 - 8m 51s\n",
      "batch: 1300/1563 - train loss: 9.6852 - test loss: 14.2001 - train acc: 0.5322 - test acc: 0.3844 - 8m 56s\n",
      "batch: 1400/1563 - train loss: 10.1586 - test loss: 13.9465 - train acc: 0.5037 - test acc: 0.3891 - 9m 1s\n",
      "batch: 1500/1563 - train loss: 9.7226 - test loss: 14.0552 - train acc: 0.5424 - test acc: 0.3868 - 9m 6s\n",
      "batch: 1563/1563 - train loss: 9.7108 - test loss: 13.2898 - train acc: 0.5369 - test acc: 0.4080 - 9m 11s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.1058 - test loss: 14.3967 - train acc: 0.6378 - test acc: 0.3919 - 9m 17s\n",
      "batch: 200/1563 - train loss: 7.6553 - test loss: 14.0075 - train acc: 0.6212 - test acc: 0.4095 - 9m 22s\n",
      "batch: 300/1563 - train loss: 7.6599 - test loss: 14.7580 - train acc: 0.6194 - test acc: 0.3821 - 9m 28s\n",
      "batch: 400/1563 - train loss: 7.9942 - test loss: 14.7977 - train acc: 0.5994 - test acc: 0.3834 - 9m 33s\n",
      "batch: 500/1563 - train loss: 8.1462 - test loss: 14.1344 - train acc: 0.5759 - test acc: 0.3990 - 9m 38s\n",
      "batch: 600/1563 - train loss: 8.1947 - test loss: 14.6162 - train acc: 0.5846 - test acc: 0.3919 - 9m 44s\n",
      "batch: 700/1563 - train loss: 8.0537 - test loss: 13.9374 - train acc: 0.5919 - test acc: 0.4053 - 9m 50s\n",
      "batch: 800/1563 - train loss: 8.4087 - test loss: 15.2286 - train acc: 0.5747 - test acc: 0.3764 - 9m 55s\n",
      "batch: 900/1563 - train loss: 8.7284 - test loss: 13.9799 - train acc: 0.5669 - test acc: 0.4027 - 10m 0s\n",
      "batch: 1000/1563 - train loss: 8.1822 - test loss: 13.7485 - train acc: 0.5974 - test acc: 0.4096 - 10m 5s\n",
      "batch: 1100/1563 - train loss: 8.6257 - test loss: 14.0060 - train acc: 0.5862 - test acc: 0.4063 - 10m 10s\n",
      "batch: 1200/1563 - train loss: 8.3143 - test loss: 14.0586 - train acc: 0.5957 - test acc: 0.4001 - 10m 16s\n",
      "batch: 1300/1563 - train loss: 8.8645 - test loss: 13.7278 - train acc: 0.5575 - test acc: 0.4128 - 10m 22s\n",
      "batch: 1400/1563 - train loss: 8.7602 - test loss: 13.5191 - train acc: 0.5637 - test acc: 0.4156 - 10m 27s\n",
      "batch: 1500/1563 - train loss: 8.7616 - test loss: 13.6513 - train acc: 0.5659 - test acc: 0.4116 - 10m 32s\n",
      "batch: 1563/1563 - train loss: 8.7717 - test loss: 14.0338 - train acc: 0.5694 - test acc: 0.3981 - 10m 37s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.5941 - test loss: 13.9159 - train acc: 0.6615 - test acc: 0.4216 - 10m 42s\n",
      "batch: 200/1563 - train loss: 6.3687 - test loss: 14.0461 - train acc: 0.6685 - test acc: 0.4142 - 10m 48s\n",
      "batch: 300/1563 - train loss: 6.3433 - test loss: 14.2948 - train acc: 0.6693 - test acc: 0.4090 - 10m 53s\n",
      "batch: 400/1563 - train loss: 6.6098 - test loss: 14.8615 - train acc: 0.6572 - test acc: 0.4062 - 10m 59s\n",
      "batch: 500/1563 - train loss: 6.7222 - test loss: 14.4108 - train acc: 0.6525 - test acc: 0.4055 - 11m 4s\n",
      "batch: 600/1563 - train loss: 6.8518 - test loss: 14.0773 - train acc: 0.6538 - test acc: 0.4131 - 11m 10s\n",
      "batch: 700/1563 - train loss: 7.3934 - test loss: 16.7002 - train acc: 0.6187 - test acc: 0.3608 - 11m 15s\n",
      "batch: 800/1563 - train loss: 6.9232 - test loss: 15.3069 - train acc: 0.6463 - test acc: 0.3925 - 11m 20s\n",
      "batch: 900/1563 - train loss: 7.5038 - test loss: 15.2483 - train acc: 0.6213 - test acc: 0.3896 - 11m 26s\n",
      "batch: 1000/1563 - train loss: 7.2802 - test loss: 14.3840 - train acc: 0.6269 - test acc: 0.4077 - 11m 31s\n",
      "batch: 1100/1563 - train loss: 7.4199 - test loss: 14.1019 - train acc: 0.6197 - test acc: 0.4123 - 11m 37s\n",
      "batch: 1200/1563 - train loss: 7.4588 - test loss: 14.7691 - train acc: 0.6160 - test acc: 0.4028 - 11m 42s\n",
      "batch: 1300/1563 - train loss: 7.8564 - test loss: 15.7786 - train acc: 0.5959 - test acc: 0.3785 - 11m 47s\n",
      "batch: 1400/1563 - train loss: 8.1475 - test loss: 14.7489 - train acc: 0.5962 - test acc: 0.3886 - 11m 53s\n",
      "batch: 1500/1563 - train loss: 7.4974 - test loss: 13.9160 - train acc: 0.6263 - test acc: 0.4212 - 11m 58s\n",
      "batch: 1563/1563 - train loss: 7.4381 - test loss: 14.0089 - train acc: 0.6200 - test acc: 0.4185 - 12m 3s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.2957 - test loss: 14.4337 - train acc: 0.7194 - test acc: 0.4196 - 12m 8s\n",
      "batch: 200/1563 - train loss: 5.1247 - test loss: 14.5773 - train acc: 0.7350 - test acc: 0.4245 - 12m 13s\n",
      "batch: 300/1563 - train loss: 5.2443 - test loss: 15.1159 - train acc: 0.7088 - test acc: 0.4091 - 12m 18s\n",
      "batch: 400/1563 - train loss: 5.4914 - test loss: 14.8768 - train acc: 0.7057 - test acc: 0.4163 - 12m 23s\n",
      "batch: 500/1563 - train loss: 5.8216 - test loss: 15.0072 - train acc: 0.6972 - test acc: 0.4158 - 12m 29s\n",
      "batch: 600/1563 - train loss: 6.0202 - test loss: 14.8938 - train acc: 0.6835 - test acc: 0.4084 - 12m 35s\n",
      "batch: 700/1563 - train loss: 6.0102 - test loss: 15.6141 - train acc: 0.6869 - test acc: 0.3982 - 12m 40s\n",
      "batch: 800/1563 - train loss: 6.3905 - test loss: 14.3573 - train acc: 0.6685 - test acc: 0.4259 - 12m 45s\n",
      "batch: 900/1563 - train loss: 6.4818 - test loss: 14.5090 - train acc: 0.6684 - test acc: 0.4173 - 12m 50s\n",
      "batch: 1000/1563 - train loss: 6.8479 - test loss: 15.1785 - train acc: 0.6497 - test acc: 0.4020 - 12m 55s\n",
      "batch: 1100/1563 - train loss: 6.2643 - test loss: 14.8692 - train acc: 0.6738 - test acc: 0.4097 - 13m 1s\n",
      "batch: 1200/1563 - train loss: 6.6338 - test loss: 14.6734 - train acc: 0.6578 - test acc: 0.4148 - 13m 6s\n",
      "batch: 1300/1563 - train loss: 6.6138 - test loss: 14.7014 - train acc: 0.6481 - test acc: 0.4166 - 13m 11s\n",
      "batch: 1400/1563 - train loss: 6.5185 - test loss: 14.3437 - train acc: 0.6694 - test acc: 0.4241 - 13m 17s\n",
      "batch: 1500/1563 - train loss: 6.9554 - test loss: 15.2889 - train acc: 0.6456 - test acc: 0.3992 - 13m 22s\n",
      "batch: 1563/1563 - train loss: 7.0509 - test loss: 15.6997 - train acc: 0.6406 - test acc: 0.3897 - 13m 26s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.3018 - test loss: 15.0197 - train acc: 0.7694 - test acc: 0.4184 - 13m 32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 4.0293 - test loss: 16.2892 - train acc: 0.7775 - test acc: 0.3939 - 13m 37s\n",
      "batch: 300/1563 - train loss: 4.5684 - test loss: 15.6455 - train acc: 0.7503 - test acc: 0.4030 - 13m 43s\n",
      "batch: 400/1563 - train loss: 4.4297 - test loss: 15.4152 - train acc: 0.7690 - test acc: 0.4222 - 13m 48s\n",
      "batch: 500/1563 - train loss: 4.8252 - test loss: 15.7031 - train acc: 0.7366 - test acc: 0.4149 - 13m 53s\n",
      "batch: 600/1563 - train loss: 4.9402 - test loss: 15.5816 - train acc: 0.7406 - test acc: 0.4063 - 13m 59s\n",
      "batch: 700/1563 - train loss: 5.1578 - test loss: 15.8097 - train acc: 0.7237 - test acc: 0.4089 - 14m 4s\n",
      "batch: 800/1563 - train loss: 5.2686 - test loss: 15.7701 - train acc: 0.7197 - test acc: 0.4028 - 14m 10s\n",
      "batch: 900/1563 - train loss: 5.4631 - test loss: 15.8871 - train acc: 0.7066 - test acc: 0.4042 - 14m 15s\n",
      "batch: 1000/1563 - train loss: 5.6052 - test loss: 16.5322 - train acc: 0.6978 - test acc: 0.3888 - 14m 20s\n",
      "batch: 1100/1563 - train loss: 5.4888 - test loss: 15.3068 - train acc: 0.7122 - test acc: 0.4163 - 14m 26s\n",
      "batch: 1200/1563 - train loss: 5.7224 - test loss: 15.4172 - train acc: 0.6997 - test acc: 0.4150 - 14m 31s\n",
      "batch: 1300/1563 - train loss: 5.7505 - test loss: 14.9773 - train acc: 0.6922 - test acc: 0.4226 - 14m 37s\n",
      "batch: 1400/1563 - train loss: 5.6973 - test loss: 15.1612 - train acc: 0.7026 - test acc: 0.4180 - 14m 42s\n",
      "batch: 1500/1563 - train loss: 5.7080 - test loss: 15.4461 - train acc: 0.6941 - test acc: 0.4084 - 14m 47s\n",
      "batch: 1563/1563 - train loss: 6.0040 - test loss: 15.7440 - train acc: 0.6804 - test acc: 0.4009 - 14m 52s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.4508 - test loss: 15.7049 - train acc: 0.8233 - test acc: 0.4203 - 14m 57s\n",
      "batch: 200/1563 - train loss: 3.3972 - test loss: 15.7159 - train acc: 0.8181 - test acc: 0.4176 - 15m 3s\n",
      "batch: 300/1563 - train loss: 3.7304 - test loss: 16.2726 - train acc: 0.8006 - test acc: 0.4150 - 15m 9s\n",
      "batch: 400/1563 - train loss: 3.5970 - test loss: 16.1565 - train acc: 0.8034 - test acc: 0.4215 - 15m 14s\n",
      "batch: 500/1563 - train loss: 4.0905 - test loss: 16.2391 - train acc: 0.7727 - test acc: 0.4173 - 15m 20s\n",
      "batch: 600/1563 - train loss: 4.0724 - test loss: 16.0909 - train acc: 0.7800 - test acc: 0.4202 - 15m 25s\n",
      "batch: 700/1563 - train loss: 4.2027 - test loss: 15.9624 - train acc: 0.7665 - test acc: 0.4159 - 15m 30s\n",
      "batch: 800/1563 - train loss: 4.3446 - test loss: 16.7060 - train acc: 0.7656 - test acc: 0.4034 - 15m 36s\n",
      "batch: 900/1563 - train loss: 4.6870 - test loss: 15.9913 - train acc: 0.7591 - test acc: 0.4137 - 15m 42s\n",
      "batch: 1000/1563 - train loss: 4.4863 - test loss: 16.0967 - train acc: 0.7563 - test acc: 0.4177 - 15m 47s\n",
      "batch: 1100/1563 - train loss: 4.5523 - test loss: 15.9309 - train acc: 0.7513 - test acc: 0.4213 - 15m 52s\n",
      "batch: 1200/1563 - train loss: 4.6425 - test loss: 16.3962 - train acc: 0.7497 - test acc: 0.4084 - 15m 58s\n",
      "batch: 1300/1563 - train loss: 5.1840 - test loss: 16.2398 - train acc: 0.7266 - test acc: 0.4101 - 16m 3s\n",
      "batch: 1400/1563 - train loss: 5.2380 - test loss: 15.5156 - train acc: 0.7156 - test acc: 0.4182 - 16m 8s\n",
      "batch: 1500/1563 - train loss: 5.1768 - test loss: 17.0419 - train acc: 0.7169 - test acc: 0.3805 - 16m 14s\n",
      "batch: 1563/1563 - train loss: 5.1382 - test loss: 15.8608 - train acc: 0.7203 - test acc: 0.4214 - 16m 19s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.0776 - test loss: 15.8878 - train acc: 0.8281 - test acc: 0.4284 - 16m 24s\n",
      "batch: 200/1563 - train loss: 2.8997 - test loss: 16.5827 - train acc: 0.8435 - test acc: 0.4146 - 16m 30s\n",
      "batch: 300/1563 - train loss: 3.1214 - test loss: 16.4517 - train acc: 0.8321 - test acc: 0.4206 - 16m 35s\n",
      "batch: 400/1563 - train loss: 3.2472 - test loss: 16.9455 - train acc: 0.8084 - test acc: 0.4168 - 16m 40s\n",
      "batch: 500/1563 - train loss: 3.6063 - test loss: 17.3136 - train acc: 0.7947 - test acc: 0.4056 - 16m 45s\n",
      "batch: 600/1563 - train loss: 3.4726 - test loss: 17.4467 - train acc: 0.8027 - test acc: 0.4088 - 16m 50s\n",
      "batch: 700/1563 - train loss: 3.5071 - test loss: 16.5680 - train acc: 0.8043 - test acc: 0.4210 - 16m 55s\n",
      "batch: 800/1563 - train loss: 3.5521 - test loss: 17.1154 - train acc: 0.8093 - test acc: 0.4130 - 17m 0s\n",
      "batch: 900/1563 - train loss: 3.7427 - test loss: 16.8458 - train acc: 0.7887 - test acc: 0.4144 - 17m 5s\n",
      "batch: 1000/1563 - train loss: 3.7286 - test loss: 17.3404 - train acc: 0.7915 - test acc: 0.4121 - 17m 10s\n",
      "batch: 1100/1563 - train loss: 4.0140 - test loss: 16.4724 - train acc: 0.7874 - test acc: 0.4212 - 17m 16s\n",
      "batch: 1200/1563 - train loss: 4.1248 - test loss: 16.8451 - train acc: 0.7700 - test acc: 0.4111 - 17m 21s\n",
      "batch: 1300/1563 - train loss: 4.2168 - test loss: 16.9175 - train acc: 0.7609 - test acc: 0.4129 - 17m 25s\n",
      "batch: 1400/1563 - train loss: 4.2891 - test loss: 16.7281 - train acc: 0.7647 - test acc: 0.4168 - 17m 30s\n",
      "batch: 1500/1563 - train loss: 4.3054 - test loss: 16.6851 - train acc: 0.7628 - test acc: 0.4171 - 17m 35s\n",
      "batch: 1563/1563 - train loss: 4.6725 - test loss: 16.7511 - train acc: 0.7375 - test acc: 0.4170 - 17m 39s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.4286 - test loss: 16.6690 - train acc: 0.8734 - test acc: 0.4264 - 17m 44s\n",
      "batch: 200/1563 - train loss: 2.5200 - test loss: 17.8034 - train acc: 0.8644 - test acc: 0.4139 - 17m 50s\n",
      "batch: 300/1563 - train loss: 2.7181 - test loss: 17.0659 - train acc: 0.8559 - test acc: 0.4247 - 17m 54s\n",
      "batch: 400/1563 - train loss: 2.4108 - test loss: 17.1985 - train acc: 0.8635 - test acc: 0.4258 - 17m 59s\n",
      "batch: 500/1563 - train loss: 2.5456 - test loss: 17.5123 - train acc: 0.8572 - test acc: 0.4252 - 18m 4s\n",
      "batch: 600/1563 - train loss: 2.7992 - test loss: 17.4237 - train acc: 0.8412 - test acc: 0.4156 - 18m 9s\n",
      "batch: 700/1563 - train loss: 2.8748 - test loss: 17.2425 - train acc: 0.8444 - test acc: 0.4270 - 18m 14s\n",
      "batch: 800/1563 - train loss: 3.0318 - test loss: 17.5238 - train acc: 0.8309 - test acc: 0.4203 - 18m 19s\n",
      "batch: 900/1563 - train loss: 2.8972 - test loss: 17.6197 - train acc: 0.8371 - test acc: 0.4139 - 18m 24s\n",
      "batch: 1000/1563 - train loss: 3.3968 - test loss: 17.2209 - train acc: 0.8127 - test acc: 0.4236 - 18m 29s\n",
      "batch: 1100/1563 - train loss: 3.3344 - test loss: 17.8471 - train acc: 0.8103 - test acc: 0.4103 - 18m 34s\n",
      "batch: 1200/1563 - train loss: 3.5295 - test loss: 17.2894 - train acc: 0.8075 - test acc: 0.4242 - 18m 39s\n",
      "batch: 1300/1563 - train loss: 3.5233 - test loss: 16.8975 - train acc: 0.8028 - test acc: 0.4215 - 18m 44s\n",
      "batch: 1400/1563 - train loss: 3.4569 - test loss: 17.2451 - train acc: 0.8140 - test acc: 0.4199 - 18m 48s\n",
      "batch: 1500/1563 - train loss: 3.3285 - test loss: 18.0677 - train acc: 0.8122 - test acc: 0.4131 - 18m 53s\n",
      "batch: 1563/1563 - train loss: 3.5102 - test loss: 17.7754 - train acc: 0.8024 - test acc: 0.4077 - 18m 58s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 1.9748 - test loss: 17.3565 - train acc: 0.8916 - test acc: 0.4210 - 19m 2s\n",
      "batch: 200/1563 - train loss: 1.8432 - test loss: 18.0815 - train acc: 0.8947 - test acc: 0.4174 - 19m 7s\n",
      "batch: 300/1563 - train loss: 1.8938 - test loss: 18.3561 - train acc: 0.8882 - test acc: 0.4148 - 19m 12s\n",
      "batch: 400/1563 - train loss: 1.8865 - test loss: 17.8897 - train acc: 0.8935 - test acc: 0.4297 - 19m 17s\n",
      "batch: 500/1563 - train loss: 1.9960 - test loss: 18.3177 - train acc: 0.8878 - test acc: 0.4199 - 19m 22s\n",
      "batch: 600/1563 - train loss: 2.3307 - test loss: 18.5156 - train acc: 0.8618 - test acc: 0.4127 - 19m 27s\n",
      "batch: 700/1563 - train loss: 2.4351 - test loss: 18.3574 - train acc: 0.8638 - test acc: 0.4190 - 19m 32s\n",
      "batch: 800/1563 - train loss: 2.4540 - test loss: 18.8239 - train acc: 0.8568 - test acc: 0.4101 - 19m 37s\n",
      "batch: 900/1563 - train loss: 2.4908 - test loss: 18.0501 - train acc: 0.8597 - test acc: 0.4154 - 19m 42s\n",
      "batch: 1000/1563 - train loss: 2.7170 - test loss: 18.7887 - train acc: 0.8409 - test acc: 0.4104 - 19m 46s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 2.6001 - test loss: 18.2741 - train acc: 0.8484 - test acc: 0.4213 - 19m 51s\n",
      "batch: 1200/1563 - train loss: 2.9164 - test loss: 19.2842 - train acc: 0.8415 - test acc: 0.3981 - 19m 56s\n",
      "time is up! finishing training\n",
      "batch: 1273/1563 - train loss: 3.1680 - test loss: 19.0131 - train acc: 0.8224 - test acc: 0.4036 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1026 - test loss: 26.3338 - train acc: 0.0429 - test acc: 0.0479 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.1849 - test loss: 23.8099 - train acc: 0.0621 - test acc: 0.0655 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.2594 - test loss: 23.5248 - train acc: 0.0783 - test acc: 0.0814 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.4667 - test loss: 22.0688 - train acc: 0.0972 - test acc: 0.1039 - 0m 16s\n",
      "batch: 500/1563 - train loss: 22.0937 - test loss: 21.3937 - train acc: 0.1031 - test acc: 0.1256 - 0m 21s\n",
      "batch: 600/1563 - train loss: 21.9081 - test loss: 21.4554 - train acc: 0.1081 - test acc: 0.1318 - 0m 26s\n",
      "batch: 700/1563 - train loss: 21.3323 - test loss: 21.3604 - train acc: 0.1369 - test acc: 0.1257 - 0m 30s\n",
      "batch: 800/1563 - train loss: 20.7665 - test loss: 21.1832 - train acc: 0.1369 - test acc: 0.1259 - 0m 35s\n",
      "batch: 900/1563 - train loss: 20.2815 - test loss: 20.1432 - train acc: 0.1525 - test acc: 0.1551 - 0m 40s\n",
      "batch: 1000/1563 - train loss: 20.1323 - test loss: 20.1245 - train acc: 0.1603 - test acc: 0.1540 - 0m 44s\n",
      "batch: 1100/1563 - train loss: 20.0742 - test loss: 19.5324 - train acc: 0.1620 - test acc: 0.1821 - 0m 49s\n",
      "batch: 1200/1563 - train loss: 19.5754 - test loss: 19.2831 - train acc: 0.1763 - test acc: 0.1835 - 0m 54s\n",
      "batch: 1300/1563 - train loss: 19.4313 - test loss: 18.6242 - train acc: 0.1847 - test acc: 0.2019 - 0m 59s\n",
      "batch: 1400/1563 - train loss: 19.3262 - test loss: 19.8761 - train acc: 0.1835 - test acc: 0.1740 - 1m 4s\n",
      "batch: 1500/1563 - train loss: 18.9882 - test loss: 18.5594 - train acc: 0.1941 - test acc: 0.2072 - 1m 8s\n",
      "batch: 1563/1563 - train loss: 18.8348 - test loss: 18.7669 - train acc: 0.1960 - test acc: 0.1999 - 1m 13s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 18.0869 - test loss: 19.8917 - train acc: 0.2210 - test acc: 0.1686 - 1m 18s\n",
      "batch: 200/1563 - train loss: 17.9386 - test loss: 18.9739 - train acc: 0.2191 - test acc: 0.1876 - 1m 23s\n",
      "batch: 300/1563 - train loss: 17.4656 - test loss: 17.6773 - train acc: 0.2362 - test acc: 0.2354 - 1m 27s\n",
      "batch: 400/1563 - train loss: 17.9787 - test loss: 18.4412 - train acc: 0.2170 - test acc: 0.2142 - 1m 32s\n",
      "batch: 500/1563 - train loss: 17.7683 - test loss: 17.7962 - train acc: 0.2323 - test acc: 0.2266 - 1m 37s\n",
      "batch: 600/1563 - train loss: 17.3220 - test loss: 20.3945 - train acc: 0.2431 - test acc: 0.1768 - 1m 42s\n",
      "batch: 700/1563 - train loss: 17.3758 - test loss: 18.1077 - train acc: 0.2341 - test acc: 0.2236 - 1m 47s\n",
      "batch: 800/1563 - train loss: 17.6047 - test loss: 17.0284 - train acc: 0.2269 - test acc: 0.2597 - 1m 52s\n",
      "batch: 900/1563 - train loss: 17.0465 - test loss: 18.3111 - train acc: 0.2531 - test acc: 0.2194 - 1m 57s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 17.1044 - test loss: 17.9342 - train acc: 0.2584 - test acc: 0.2274 - 2m 2s\n",
      "batch: 1100/1563 - train loss: 16.8692 - test loss: 16.6960 - train acc: 0.2631 - test acc: 0.2627 - 2m 7s\n",
      "batch: 1200/1563 - train loss: 16.2386 - test loss: 16.9022 - train acc: 0.2812 - test acc: 0.2680 - 2m 11s\n",
      "batch: 1300/1563 - train loss: 16.9368 - test loss: 16.7958 - train acc: 0.2475 - test acc: 0.2616 - 2m 16s\n",
      "batch: 1400/1563 - train loss: 16.1336 - test loss: 16.1000 - train acc: 0.2887 - test acc: 0.2930 - 2m 22s\n",
      "batch: 1500/1563 - train loss: 16.4228 - test loss: 16.3187 - train acc: 0.2793 - test acc: 0.2800 - 2m 27s\n",
      "batch: 1563/1563 - train loss: 16.3786 - test loss: 16.7437 - train acc: 0.2800 - test acc: 0.2691 - 2m 31s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.3585 - test loss: 16.7392 - train acc: 0.3146 - test acc: 0.2732 - 2m 35s\n",
      "batch: 200/1563 - train loss: 15.1556 - test loss: 16.5208 - train acc: 0.3127 - test acc: 0.2818 - 2m 40s\n",
      "batch: 300/1563 - train loss: 15.3022 - test loss: 17.7155 - train acc: 0.3033 - test acc: 0.2401 - 2m 45s\n",
      "batch: 400/1563 - train loss: 14.8009 - test loss: 16.5809 - train acc: 0.3272 - test acc: 0.2819 - 2m 50s\n",
      "batch: 500/1563 - train loss: 15.1285 - test loss: 15.7901 - train acc: 0.3240 - test acc: 0.2925 - 2m 55s\n",
      "batch: 600/1563 - train loss: 15.1430 - test loss: 16.3844 - train acc: 0.3243 - test acc: 0.2844 - 3m 0s\n",
      "batch: 700/1563 - train loss: 15.0535 - test loss: 15.7955 - train acc: 0.3306 - test acc: 0.3010 - 3m 5s\n",
      "batch: 800/1563 - train loss: 14.8965 - test loss: 16.1575 - train acc: 0.3266 - test acc: 0.2900 - 3m 10s\n",
      "batch: 900/1563 - train loss: 14.7479 - test loss: 15.3159 - train acc: 0.3387 - test acc: 0.3115 - 3m 15s\n",
      "batch: 1000/1563 - train loss: 14.9353 - test loss: 16.5445 - train acc: 0.3365 - test acc: 0.2803 - 3m 20s\n",
      "batch: 1100/1563 - train loss: 14.8181 - test loss: 16.8449 - train acc: 0.3340 - test acc: 0.2700 - 3m 25s\n",
      "batch: 1200/1563 - train loss: 14.7092 - test loss: 16.4344 - train acc: 0.3378 - test acc: 0.2887 - 3m 30s\n",
      "batch: 1300/1563 - train loss: 15.1284 - test loss: 14.7472 - train acc: 0.3085 - test acc: 0.3356 - 3m 35s\n",
      "batch: 1400/1563 - train loss: 14.2525 - test loss: 15.8056 - train acc: 0.3516 - test acc: 0.3037 - 3m 40s\n",
      "batch: 1500/1563 - train loss: 14.4944 - test loss: 15.0035 - train acc: 0.3415 - test acc: 0.3265 - 3m 45s\n",
      "batch: 1563/1563 - train loss: 14.3155 - test loss: 16.0824 - train acc: 0.3550 - test acc: 0.2977 - 3m 49s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 12.9645 - test loss: 15.4774 - train acc: 0.3966 - test acc: 0.3201 - 3m 54s\n",
      "batch: 200/1563 - train loss: 13.1734 - test loss: 17.6821 - train acc: 0.3912 - test acc: 0.2639 - 3m 59s\n",
      "batch: 300/1563 - train loss: 13.2907 - test loss: 16.4926 - train acc: 0.3916 - test acc: 0.2980 - 4m 4s\n",
      "batch: 400/1563 - train loss: 13.4191 - test loss: 15.9401 - train acc: 0.3743 - test acc: 0.3099 - 4m 9s\n",
      "batch: 500/1563 - train loss: 13.1785 - test loss: 16.9757 - train acc: 0.3859 - test acc: 0.2863 - 4m 13s\n",
      "batch: 600/1563 - train loss: 13.4269 - test loss: 14.7469 - train acc: 0.3725 - test acc: 0.3444 - 4m 18s\n",
      "batch: 700/1563 - train loss: 13.3533 - test loss: 15.1979 - train acc: 0.3966 - test acc: 0.3285 - 4m 23s\n",
      "batch: 800/1563 - train loss: 13.3829 - test loss: 16.5030 - train acc: 0.3760 - test acc: 0.2938 - 4m 29s\n",
      "batch: 900/1563 - train loss: 13.4173 - test loss: 16.1927 - train acc: 0.3706 - test acc: 0.2994 - 4m 33s\n",
      "batch: 1000/1563 - train loss: 13.2941 - test loss: 15.1436 - train acc: 0.3868 - test acc: 0.3309 - 4m 38s\n",
      "batch: 1100/1563 - train loss: 13.7193 - test loss: 15.7426 - train acc: 0.3615 - test acc: 0.3106 - 4m 43s\n",
      "batch: 1200/1563 - train loss: 13.5549 - test loss: 14.7558 - train acc: 0.3722 - test acc: 0.3326 - 4m 48s\n",
      "batch: 1300/1563 - train loss: 13.0348 - test loss: 14.3412 - train acc: 0.3959 - test acc: 0.3519 - 4m 53s\n",
      "batch: 1400/1563 - train loss: 13.2923 - test loss: 14.9848 - train acc: 0.3943 - test acc: 0.3387 - 4m 58s\n",
      "batch: 1500/1563 - train loss: 13.2619 - test loss: 15.5149 - train acc: 0.3931 - test acc: 0.3169 - 5m 3s\n",
      "batch: 1563/1563 - train loss: 13.0063 - test loss: 14.6636 - train acc: 0.4085 - test acc: 0.3467 - 5m 7s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.2730 - test loss: 15.6185 - train acc: 0.4673 - test acc: 0.3302 - 5m 12s\n",
      "batch: 200/1563 - train loss: 11.4367 - test loss: 14.5283 - train acc: 0.4435 - test acc: 0.3591 - 5m 17s\n",
      "batch: 300/1563 - train loss: 11.7595 - test loss: 15.3276 - train acc: 0.4439 - test acc: 0.3387 - 5m 22s\n",
      "batch: 400/1563 - train loss: 12.0805 - test loss: 14.4740 - train acc: 0.4319 - test acc: 0.3645 - 5m 26s\n",
      "batch: 500/1563 - train loss: 11.8444 - test loss: 14.5934 - train acc: 0.4428 - test acc: 0.3485 - 5m 31s\n",
      "batch: 600/1563 - train loss: 12.3002 - test loss: 14.0677 - train acc: 0.4325 - test acc: 0.3683 - 5m 37s\n",
      "batch: 700/1563 - train loss: 12.0110 - test loss: 14.3134 - train acc: 0.4440 - test acc: 0.3604 - 5m 41s\n",
      "batch: 800/1563 - train loss: 12.0573 - test loss: 14.2670 - train acc: 0.4206 - test acc: 0.3633 - 5m 46s\n",
      "batch: 900/1563 - train loss: 11.8007 - test loss: 14.7595 - train acc: 0.4354 - test acc: 0.3459 - 5m 51s\n",
      "batch: 1000/1563 - train loss: 12.1067 - test loss: 14.2269 - train acc: 0.4403 - test acc: 0.3664 - 5m 56s\n",
      "batch: 1100/1563 - train loss: 12.0469 - test loss: 14.0173 - train acc: 0.4338 - test acc: 0.3696 - 6m 1s\n",
      "batch: 1200/1563 - train loss: 12.2745 - test loss: 14.1497 - train acc: 0.4350 - test acc: 0.3650 - 6m 6s\n",
      "batch: 1300/1563 - train loss: 12.4646 - test loss: 14.7071 - train acc: 0.4301 - test acc: 0.3566 - 6m 11s\n",
      "batch: 1400/1563 - train loss: 12.0451 - test loss: 13.8490 - train acc: 0.4260 - test acc: 0.3750 - 6m 15s\n",
      "batch: 1500/1563 - train loss: 11.9752 - test loss: 15.0081 - train acc: 0.4366 - test acc: 0.3397 - 6m 20s\n",
      "batch: 1563/1563 - train loss: 11.9943 - test loss: 13.3431 - train acc: 0.4379 - test acc: 0.3917 - 6m 24s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.8255 - test loss: 14.1984 - train acc: 0.5243 - test acc: 0.3807 - 6m 29s\n",
      "batch: 200/1563 - train loss: 10.1161 - test loss: 14.4438 - train acc: 0.5100 - test acc: 0.3725 - 6m 34s\n",
      "batch: 300/1563 - train loss: 10.1298 - test loss: 14.4980 - train acc: 0.4953 - test acc: 0.3708 - 6m 39s\n",
      "batch: 400/1563 - train loss: 10.6551 - test loss: 14.0329 - train acc: 0.4794 - test acc: 0.3743 - 6m 44s\n",
      "batch: 500/1563 - train loss: 10.6267 - test loss: 14.0103 - train acc: 0.4850 - test acc: 0.3769 - 6m 49s\n",
      "batch: 600/1563 - train loss: 10.4039 - test loss: 13.9669 - train acc: 0.4944 - test acc: 0.3848 - 6m 54s\n",
      "batch: 700/1563 - train loss: 10.9337 - test loss: 14.3932 - train acc: 0.4710 - test acc: 0.3708 - 6m 59s\n",
      "batch: 800/1563 - train loss: 10.9259 - test loss: 13.5516 - train acc: 0.4785 - test acc: 0.3994 - 7m 4s\n",
      "batch: 900/1563 - train loss: 11.0135 - test loss: 14.1797 - train acc: 0.4841 - test acc: 0.3772 - 7m 9s\n",
      "batch: 1000/1563 - train loss: 10.7832 - test loss: 14.1769 - train acc: 0.4853 - test acc: 0.3813 - 7m 15s\n",
      "batch: 1100/1563 - train loss: 10.9209 - test loss: 15.3055 - train acc: 0.4844 - test acc: 0.3397 - 7m 19s\n",
      "batch: 1200/1563 - train loss: 11.1407 - test loss: 14.0115 - train acc: 0.4709 - test acc: 0.3847 - 7m 24s\n",
      "batch: 1300/1563 - train loss: 10.9223 - test loss: 13.6763 - train acc: 0.4807 - test acc: 0.3874 - 7m 29s\n",
      "batch: 1400/1563 - train loss: 11.0243 - test loss: 13.5307 - train acc: 0.4722 - test acc: 0.3996 - 7m 34s\n",
      "batch: 1500/1563 - train loss: 10.8661 - test loss: 13.5473 - train acc: 0.4769 - test acc: 0.3974 - 7m 39s\n",
      "batch: 1563/1563 - train loss: 10.8501 - test loss: 14.6514 - train acc: 0.4716 - test acc: 0.3680 - 7m 43s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.8120 - test loss: 14.3598 - train acc: 0.5597 - test acc: 0.3799 - 7m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 8.9073 - test loss: 13.7953 - train acc: 0.5569 - test acc: 0.3904 - 7m 53s\n",
      "batch: 300/1563 - train loss: 9.1580 - test loss: 14.1005 - train acc: 0.5484 - test acc: 0.3905 - 7m 58s\n",
      "batch: 400/1563 - train loss: 9.0046 - test loss: 13.9002 - train acc: 0.5581 - test acc: 0.3945 - 8m 3s\n",
      "batch: 500/1563 - train loss: 9.3459 - test loss: 14.0670 - train acc: 0.5500 - test acc: 0.3943 - 8m 8s\n",
      "batch: 600/1563 - train loss: 9.4159 - test loss: 13.6707 - train acc: 0.5381 - test acc: 0.3965 - 8m 13s\n",
      "batch: 700/1563 - train loss: 9.4514 - test loss: 13.6222 - train acc: 0.5287 - test acc: 0.4060 - 8m 18s\n",
      "batch: 800/1563 - train loss: 9.8953 - test loss: 14.1805 - train acc: 0.5300 - test acc: 0.3895 - 8m 23s\n",
      "batch: 900/1563 - train loss: 9.6265 - test loss: 13.5621 - train acc: 0.5362 - test acc: 0.4134 - 8m 27s\n",
      "batch: 1000/1563 - train loss: 9.6695 - test loss: 14.4418 - train acc: 0.5347 - test acc: 0.3827 - 8m 33s\n",
      "batch: 1100/1563 - train loss: 9.7727 - test loss: 14.4794 - train acc: 0.5188 - test acc: 0.3868 - 8m 37s\n",
      "batch: 1200/1563 - train loss: 9.7718 - test loss: 13.8548 - train acc: 0.5259 - test acc: 0.4049 - 8m 42s\n",
      "batch: 1300/1563 - train loss: 9.7414 - test loss: 14.7188 - train acc: 0.5169 - test acc: 0.3803 - 8m 47s\n",
      "batch: 1400/1563 - train loss: 9.9685 - test loss: 13.8708 - train acc: 0.5112 - test acc: 0.3965 - 8m 52s\n",
      "batch: 1500/1563 - train loss: 10.0897 - test loss: 13.6647 - train acc: 0.5047 - test acc: 0.3971 - 8m 57s\n",
      "batch: 1563/1563 - train loss: 9.8068 - test loss: 13.7371 - train acc: 0.5200 - test acc: 0.4033 - 9m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.3245 - test loss: 14.5798 - train acc: 0.6347 - test acc: 0.3919 - 9m 6s\n",
      "batch: 200/1563 - train loss: 7.8089 - test loss: 14.2107 - train acc: 0.6071 - test acc: 0.3921 - 9m 10s\n",
      "batch: 300/1563 - train loss: 7.9523 - test loss: 14.6268 - train acc: 0.5978 - test acc: 0.3814 - 9m 16s\n",
      "batch: 400/1563 - train loss: 7.6860 - test loss: 14.0974 - train acc: 0.6041 - test acc: 0.4007 - 9m 20s\n",
      "batch: 500/1563 - train loss: 8.2592 - test loss: 14.9868 - train acc: 0.5840 - test acc: 0.3832 - 9m 25s\n",
      "batch: 600/1563 - train loss: 8.4921 - test loss: 13.9950 - train acc: 0.5697 - test acc: 0.4111 - 9m 30s\n",
      "batch: 700/1563 - train loss: 8.1535 - test loss: 14.4056 - train acc: 0.5900 - test acc: 0.3993 - 9m 35s\n",
      "batch: 800/1563 - train loss: 8.2427 - test loss: 14.0674 - train acc: 0.5881 - test acc: 0.4118 - 9m 40s\n",
      "batch: 900/1563 - train loss: 8.8857 - test loss: 14.5506 - train acc: 0.5572 - test acc: 0.3827 - 9m 44s\n",
      "batch: 1000/1563 - train loss: 8.8102 - test loss: 14.0500 - train acc: 0.5622 - test acc: 0.3991 - 9m 50s\n",
      "batch: 1100/1563 - train loss: 8.5790 - test loss: 13.9853 - train acc: 0.5725 - test acc: 0.4047 - 9m 55s\n",
      "batch: 1200/1563 - train loss: 8.6551 - test loss: 14.1199 - train acc: 0.5637 - test acc: 0.3970 - 9m 59s\n",
      "batch: 1300/1563 - train loss: 8.9840 - test loss: 13.8681 - train acc: 0.5513 - test acc: 0.4085 - 10m 4s\n",
      "batch: 1400/1563 - train loss: 9.0864 - test loss: 15.2678 - train acc: 0.5490 - test acc: 0.3669 - 10m 9s\n",
      "batch: 1500/1563 - train loss: 8.8288 - test loss: 13.8326 - train acc: 0.5584 - test acc: 0.4146 - 10m 14s\n",
      "batch: 1563/1563 - train loss: 8.9210 - test loss: 14.0154 - train acc: 0.5538 - test acc: 0.4033 - 10m 18s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4515 - test loss: 13.6807 - train acc: 0.6750 - test acc: 0.4227 - 10m 23s\n",
      "batch: 200/1563 - train loss: 6.3318 - test loss: 14.0921 - train acc: 0.6728 - test acc: 0.4175 - 10m 28s\n",
      "batch: 300/1563 - train loss: 6.6834 - test loss: 15.0113 - train acc: 0.6607 - test acc: 0.3925 - 10m 33s\n",
      "batch: 400/1563 - train loss: 6.7725 - test loss: 14.3131 - train acc: 0.6475 - test acc: 0.4146 - 10m 38s\n",
      "batch: 500/1563 - train loss: 7.1853 - test loss: 14.2341 - train acc: 0.6281 - test acc: 0.4166 - 10m 42s\n",
      "batch: 600/1563 - train loss: 7.1254 - test loss: 15.0456 - train acc: 0.6312 - test acc: 0.3940 - 10m 47s\n",
      "batch: 700/1563 - train loss: 7.1392 - test loss: 15.9042 - train acc: 0.6328 - test acc: 0.3639 - 10m 52s\n",
      "batch: 800/1563 - train loss: 7.1757 - test loss: 15.0442 - train acc: 0.6410 - test acc: 0.3916 - 10m 57s\n",
      "batch: 900/1563 - train loss: 7.4296 - test loss: 14.4887 - train acc: 0.6156 - test acc: 0.4010 - 11m 2s\n",
      "batch: 1000/1563 - train loss: 7.6102 - test loss: 14.2990 - train acc: 0.6159 - test acc: 0.4132 - 11m 7s\n",
      "batch: 1100/1563 - train loss: 7.7876 - test loss: 14.8606 - train acc: 0.5956 - test acc: 0.3965 - 11m 12s\n",
      "batch: 1200/1563 - train loss: 8.0794 - test loss: 14.0211 - train acc: 0.5931 - test acc: 0.4080 - 11m 17s\n",
      "batch: 1300/1563 - train loss: 7.9202 - test loss: 14.3106 - train acc: 0.5928 - test acc: 0.4068 - 11m 21s\n",
      "batch: 1400/1563 - train loss: 7.9025 - test loss: 14.3948 - train acc: 0.6078 - test acc: 0.3985 - 11m 27s\n",
      "batch: 1500/1563 - train loss: 8.1562 - test loss: 15.3644 - train acc: 0.5837 - test acc: 0.3752 - 11m 31s\n",
      "batch: 1563/1563 - train loss: 8.0045 - test loss: 15.1869 - train acc: 0.5969 - test acc: 0.3884 - 11m 36s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.4007 - test loss: 14.8618 - train acc: 0.7163 - test acc: 0.4141 - 11m 40s\n",
      "batch: 200/1563 - train loss: 5.6541 - test loss: 15.1858 - train acc: 0.7046 - test acc: 0.4026 - 11m 45s\n",
      "batch: 300/1563 - train loss: 5.2958 - test loss: 14.5829 - train acc: 0.7225 - test acc: 0.4174 - 11m 50s\n",
      "batch: 400/1563 - train loss: 5.7100 - test loss: 15.2280 - train acc: 0.7032 - test acc: 0.4038 - 11m 55s\n",
      "batch: 500/1563 - train loss: 6.0754 - test loss: 15.4742 - train acc: 0.6788 - test acc: 0.4006 - 12m 0s\n",
      "batch: 600/1563 - train loss: 6.2557 - test loss: 14.9069 - train acc: 0.6729 - test acc: 0.4044 - 12m 5s\n",
      "batch: 700/1563 - train loss: 6.0993 - test loss: 14.7753 - train acc: 0.6722 - test acc: 0.4145 - 12m 10s\n",
      "batch: 800/1563 - train loss: 6.3608 - test loss: 15.1564 - train acc: 0.6669 - test acc: 0.3991 - 12m 15s\n",
      "batch: 900/1563 - train loss: 6.1761 - test loss: 15.0484 - train acc: 0.6756 - test acc: 0.4111 - 12m 20s\n",
      "batch: 1000/1563 - train loss: 6.6370 - test loss: 15.5102 - train acc: 0.6569 - test acc: 0.3976 - 12m 25s\n",
      "batch: 1100/1563 - train loss: 6.8122 - test loss: 15.1663 - train acc: 0.6447 - test acc: 0.4013 - 12m 30s\n",
      "batch: 1200/1563 - train loss: 6.7231 - test loss: 15.0665 - train acc: 0.6506 - test acc: 0.4052 - 12m 35s\n",
      "batch: 1300/1563 - train loss: 6.9026 - test loss: 15.2629 - train acc: 0.6422 - test acc: 0.3950 - 12m 40s\n",
      "batch: 1400/1563 - train loss: 6.9539 - test loss: 14.8498 - train acc: 0.6269 - test acc: 0.4089 - 12m 45s\n",
      "batch: 1500/1563 - train loss: 7.0379 - test loss: 14.7282 - train acc: 0.6294 - test acc: 0.4101 - 12m 50s\n",
      "batch: 1563/1563 - train loss: 6.8392 - test loss: 15.7015 - train acc: 0.6447 - test acc: 0.3882 - 12m 55s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.5366 - test loss: 14.7056 - train acc: 0.7619 - test acc: 0.4203 - 13m 0s\n",
      "batch: 200/1563 - train loss: 4.5489 - test loss: 14.9674 - train acc: 0.7609 - test acc: 0.4158 - 13m 5s\n",
      "batch: 300/1563 - train loss: 4.4025 - test loss: 15.9341 - train acc: 0.7699 - test acc: 0.4135 - 13m 10s\n",
      "batch: 400/1563 - train loss: 4.7152 - test loss: 14.9402 - train acc: 0.7506 - test acc: 0.4224 - 13m 15s\n",
      "batch: 500/1563 - train loss: 4.9386 - test loss: 16.5996 - train acc: 0.7309 - test acc: 0.3919 - 13m 20s\n",
      "batch: 600/1563 - train loss: 5.0876 - test loss: 15.8375 - train acc: 0.7256 - test acc: 0.4021 - 13m 25s\n",
      "batch: 700/1563 - train loss: 5.1263 - test loss: 16.4002 - train acc: 0.7337 - test acc: 0.3883 - 13m 31s\n",
      "batch: 800/1563 - train loss: 5.6580 - test loss: 15.8148 - train acc: 0.7028 - test acc: 0.4035 - 13m 36s\n",
      "batch: 900/1563 - train loss: 5.4035 - test loss: 15.4612 - train acc: 0.7131 - test acc: 0.4138 - 13m 41s\n",
      "batch: 1000/1563 - train loss: 5.5775 - test loss: 16.6135 - train acc: 0.6972 - test acc: 0.3895 - 13m 46s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 5.7734 - test loss: 16.4807 - train acc: 0.6984 - test acc: 0.3889 - 13m 51s\n",
      "batch: 1200/1563 - train loss: 6.0952 - test loss: 15.2885 - train acc: 0.6844 - test acc: 0.4088 - 13m 56s\n",
      "batch: 1300/1563 - train loss: 5.8520 - test loss: 15.5170 - train acc: 0.6887 - test acc: 0.4038 - 14m 1s\n",
      "batch: 1400/1563 - train loss: 5.8045 - test loss: 15.1164 - train acc: 0.6994 - test acc: 0.4121 - 14m 6s\n",
      "batch: 1500/1563 - train loss: 6.1911 - test loss: 15.8156 - train acc: 0.6744 - test acc: 0.4107 - 14m 11s\n",
      "batch: 1563/1563 - train loss: 6.0832 - test loss: 16.2307 - train acc: 0.6747 - test acc: 0.4009 - 14m 16s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.4555 - test loss: 16.1088 - train acc: 0.8183 - test acc: 0.4245 - 14m 21s\n",
      "batch: 200/1563 - train loss: 3.7037 - test loss: 15.8625 - train acc: 0.8031 - test acc: 0.4224 - 14m 26s\n",
      "batch: 300/1563 - train loss: 3.8037 - test loss: 16.0303 - train acc: 0.7924 - test acc: 0.4204 - 14m 31s\n",
      "batch: 400/1563 - train loss: 3.9361 - test loss: 16.3267 - train acc: 0.7815 - test acc: 0.4168 - 14m 37s\n",
      "batch: 500/1563 - train loss: 4.0037 - test loss: 16.0255 - train acc: 0.7769 - test acc: 0.4205 - 14m 42s\n",
      "batch: 600/1563 - train loss: 4.3209 - test loss: 16.5674 - train acc: 0.7659 - test acc: 0.4099 - 14m 48s\n",
      "batch: 700/1563 - train loss: 4.5232 - test loss: 16.2249 - train acc: 0.7484 - test acc: 0.4037 - 14m 53s\n",
      "batch: 800/1563 - train loss: 4.4331 - test loss: 16.4697 - train acc: 0.7547 - test acc: 0.4059 - 14m 58s\n",
      "batch: 900/1563 - train loss: 4.7695 - test loss: 16.5324 - train acc: 0.7478 - test acc: 0.4056 - 15m 2s\n",
      "batch: 1000/1563 - train loss: 4.7721 - test loss: 16.4648 - train acc: 0.7463 - test acc: 0.4120 - 15m 8s\n",
      "batch: 1100/1563 - train loss: 4.8745 - test loss: 18.0362 - train acc: 0.7394 - test acc: 0.3763 - 15m 13s\n",
      "batch: 1200/1563 - train loss: 5.0369 - test loss: 15.8875 - train acc: 0.7300 - test acc: 0.4066 - 15m 18s\n",
      "batch: 1300/1563 - train loss: 4.9056 - test loss: 16.2587 - train acc: 0.7366 - test acc: 0.4065 - 15m 23s\n",
      "batch: 1400/1563 - train loss: 5.1297 - test loss: 15.9706 - train acc: 0.7159 - test acc: 0.4099 - 15m 28s\n",
      "batch: 1500/1563 - train loss: 5.0947 - test loss: 16.1914 - train acc: 0.7244 - test acc: 0.4130 - 15m 33s\n",
      "batch: 1563/1563 - train loss: 5.1977 - test loss: 16.2131 - train acc: 0.7181 - test acc: 0.4096 - 15m 37s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.0303 - test loss: 19.2283 - train acc: 0.8378 - test acc: 0.3706 - 15m 42s\n",
      "batch: 200/1563 - train loss: 2.9220 - test loss: 16.8416 - train acc: 0.8378 - test acc: 0.4184 - 15m 47s\n",
      "batch: 300/1563 - train loss: 2.7786 - test loss: 16.5546 - train acc: 0.8578 - test acc: 0.4229 - 15m 52s\n",
      "batch: 400/1563 - train loss: 3.2678 - test loss: 21.2258 - train acc: 0.8174 - test acc: 0.3594 - 15m 57s\n",
      "batch: 500/1563 - train loss: 3.3063 - test loss: 17.8766 - train acc: 0.8187 - test acc: 0.4025 - 16m 2s\n",
      "batch: 600/1563 - train loss: 3.5284 - test loss: 17.2196 - train acc: 0.7993 - test acc: 0.4137 - 16m 7s\n",
      "batch: 700/1563 - train loss: 3.5255 - test loss: 17.8932 - train acc: 0.8052 - test acc: 0.3979 - 16m 12s\n",
      "batch: 800/1563 - train loss: 3.8824 - test loss: 16.7647 - train acc: 0.7896 - test acc: 0.4144 - 16m 17s\n",
      "batch: 900/1563 - train loss: 4.2224 - test loss: 17.1396 - train acc: 0.7737 - test acc: 0.4021 - 16m 22s\n",
      "batch: 1000/1563 - train loss: 3.9245 - test loss: 16.5895 - train acc: 0.7856 - test acc: 0.4202 - 16m 27s\n",
      "batch: 1100/1563 - train loss: 4.0944 - test loss: 16.9079 - train acc: 0.7709 - test acc: 0.4112 - 16m 32s\n",
      "batch: 1200/1563 - train loss: 3.9588 - test loss: 16.9205 - train acc: 0.7859 - test acc: 0.4083 - 16m 37s\n",
      "batch: 1300/1563 - train loss: 4.1090 - test loss: 16.7564 - train acc: 0.7812 - test acc: 0.4139 - 16m 42s\n",
      "batch: 1400/1563 - train loss: 4.1577 - test loss: 17.0770 - train acc: 0.7719 - test acc: 0.4024 - 16m 47s\n",
      "batch: 1500/1563 - train loss: 4.4352 - test loss: 16.8881 - train acc: 0.7575 - test acc: 0.4122 - 16m 52s\n",
      "batch: 1563/1563 - train loss: 4.2802 - test loss: 16.6719 - train acc: 0.7609 - test acc: 0.4128 - 16m 56s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.6149 - test loss: 16.5530 - train acc: 0.8540 - test acc: 0.4254 - 17m 1s\n",
      "batch: 200/1563 - train loss: 2.5166 - test loss: 16.8983 - train acc: 0.8663 - test acc: 0.4227 - 17m 6s\n",
      "batch: 300/1563 - train loss: 2.7303 - test loss: 17.1753 - train acc: 0.8472 - test acc: 0.4231 - 17m 11s\n",
      "batch: 400/1563 - train loss: 2.6310 - test loss: 17.2057 - train acc: 0.8462 - test acc: 0.4195 - 17m 17s\n",
      "batch: 500/1563 - train loss: 2.5297 - test loss: 17.2636 - train acc: 0.8543 - test acc: 0.4262 - 17m 22s\n",
      "batch: 600/1563 - train loss: 2.6889 - test loss: 17.7464 - train acc: 0.8506 - test acc: 0.4095 - 17m 27s\n",
      "batch: 700/1563 - train loss: 2.9418 - test loss: 17.7932 - train acc: 0.8340 - test acc: 0.4083 - 17m 32s\n",
      "batch: 800/1563 - train loss: 3.2518 - test loss: 17.7619 - train acc: 0.8153 - test acc: 0.4098 - 17m 37s\n",
      "batch: 900/1563 - train loss: 3.2101 - test loss: 18.2185 - train acc: 0.8153 - test acc: 0.4070 - 17m 42s\n",
      "batch: 1000/1563 - train loss: 3.4198 - test loss: 17.5456 - train acc: 0.8099 - test acc: 0.4146 - 17m 47s\n",
      "batch: 1100/1563 - train loss: 3.3028 - test loss: 17.2000 - train acc: 0.8196 - test acc: 0.4223 - 17m 53s\n",
      "batch: 1200/1563 - train loss: 3.5223 - test loss: 16.9862 - train acc: 0.8053 - test acc: 0.4232 - 17m 58s\n",
      "batch: 1300/1563 - train loss: 3.6496 - test loss: 16.8335 - train acc: 0.8028 - test acc: 0.4221 - 18m 3s\n",
      "batch: 1400/1563 - train loss: 3.4652 - test loss: 17.4734 - train acc: 0.8153 - test acc: 0.4152 - 18m 8s\n",
      "batch: 1500/1563 - train loss: 3.6064 - test loss: 17.5435 - train acc: 0.7987 - test acc: 0.4134 - 18m 13s\n",
      "batch: 1563/1563 - train loss: 3.6684 - test loss: 17.2616 - train acc: 0.7999 - test acc: 0.4169 - 18m 17s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.0256 - test loss: 17.8262 - train acc: 0.8919 - test acc: 0.4186 - 18m 22s\n",
      "batch: 200/1563 - train loss: 1.8692 - test loss: 17.9269 - train acc: 0.8932 - test acc: 0.4183 - 18m 27s\n",
      "batch: 300/1563 - train loss: 1.9605 - test loss: 17.8212 - train acc: 0.8920 - test acc: 0.4266 - 18m 32s\n",
      "batch: 400/1563 - train loss: 2.1421 - test loss: 18.0649 - train acc: 0.8807 - test acc: 0.4210 - 18m 37s\n",
      "batch: 500/1563 - train loss: 2.1666 - test loss: 17.9728 - train acc: 0.8778 - test acc: 0.4165 - 18m 42s\n",
      "batch: 600/1563 - train loss: 2.3400 - test loss: 18.2154 - train acc: 0.8654 - test acc: 0.4150 - 18m 47s\n",
      "batch: 700/1563 - train loss: 2.4092 - test loss: 18.7348 - train acc: 0.8600 - test acc: 0.4153 - 18m 53s\n",
      "batch: 800/1563 - train loss: 2.4725 - test loss: 18.0456 - train acc: 0.8594 - test acc: 0.4184 - 18m 58s\n",
      "batch: 900/1563 - train loss: 2.4553 - test loss: 18.4986 - train acc: 0.8556 - test acc: 0.4119 - 19m 2s\n",
      "batch: 1000/1563 - train loss: 2.7190 - test loss: 18.6169 - train acc: 0.8465 - test acc: 0.3994 - 19m 8s\n",
      "batch: 1100/1563 - train loss: 2.8482 - test loss: 19.3743 - train acc: 0.8412 - test acc: 0.3994 - 19m 13s\n",
      "batch: 1200/1563 - train loss: 2.6440 - test loss: 18.0913 - train acc: 0.8468 - test acc: 0.4208 - 19m 18s\n",
      "batch: 1300/1563 - train loss: 2.9134 - test loss: 18.8601 - train acc: 0.8434 - test acc: 0.4033 - 19m 23s\n",
      "batch: 1400/1563 - train loss: 3.1138 - test loss: 18.3573 - train acc: 0.8265 - test acc: 0.4100 - 19m 28s\n",
      "batch: 1500/1563 - train loss: 3.2582 - test loss: 18.3865 - train acc: 0.8146 - test acc: 0.4149 - 19m 33s\n",
      "batch: 1563/1563 - train loss: 3.2318 - test loss: 19.2285 - train acc: 0.8187 - test acc: 0.3966 - 19m 37s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.7855 - test loss: 18.1195 - train acc: 0.9013 - test acc: 0.4256 - 19m 43s\n",
      "batch: 200/1563 - train loss: 1.6454 - test loss: 18.3049 - train acc: 0.9114 - test acc: 0.4258 - 19m 47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 300/1563 - train loss: 1.7056 - test loss: 19.0010 - train acc: 0.9017 - test acc: 0.4131 - 19m 53s\n",
      "batch: 400/1563 - train loss: 1.8624 - test loss: 19.1594 - train acc: 0.8907 - test acc: 0.4133 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 401/1563 - train loss: 1.8711 - test loss: 18.9203 - train acc: 0.8894 - test acc: 0.4187 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.0189 - test loss: 24.9196 - train acc: 0.0392 - test acc: 0.0439 - 0m 2s\n",
      "batch: 200/1563 - train loss: 24.2459 - test loss: 23.1231 - train acc: 0.0639 - test acc: 0.0775 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.3914 - test loss: 22.9961 - train acc: 0.0727 - test acc: 0.0814 - 0m 12s\n",
      "batch: 400/1563 - train loss: 22.8711 - test loss: 22.7789 - train acc: 0.0871 - test acc: 0.0832 - 0m 17s\n",
      "batch: 500/1563 - train loss: 22.0387 - test loss: 22.7160 - train acc: 0.0986 - test acc: 0.0924 - 0m 22s\n",
      "batch: 600/1563 - train loss: 21.7229 - test loss: 21.6090 - train acc: 0.1140 - test acc: 0.1227 - 0m 27s\n",
      "batch: 700/1563 - train loss: 21.3512 - test loss: 21.4047 - train acc: 0.1244 - test acc: 0.1310 - 0m 32s\n",
      "batch: 800/1563 - train loss: 20.7796 - test loss: 20.9692 - train acc: 0.1444 - test acc: 0.1356 - 0m 36s\n",
      "batch: 900/1563 - train loss: 20.7798 - test loss: 20.7186 - train acc: 0.1312 - test acc: 0.1418 - 0m 41s\n",
      "batch: 1000/1563 - train loss: 20.0414 - test loss: 20.4640 - train acc: 0.1544 - test acc: 0.1573 - 0m 47s\n",
      "batch: 1100/1563 - train loss: 19.7431 - test loss: 19.7081 - train acc: 0.1754 - test acc: 0.1741 - 0m 52s\n",
      "batch: 1200/1563 - train loss: 19.6804 - test loss: 20.0443 - train acc: 0.1657 - test acc: 0.1724 - 0m 57s\n",
      "batch: 1300/1563 - train loss: 19.3480 - test loss: 21.0704 - train acc: 0.1820 - test acc: 0.1328 - 1m 2s\n",
      "batch: 1400/1563 - train loss: 18.9142 - test loss: 18.7338 - train acc: 0.2000 - test acc: 0.2019 - 1m 7s\n",
      "batch: 1500/1563 - train loss: 18.9636 - test loss: 18.7087 - train acc: 0.1923 - test acc: 0.2119 - 1m 12s\n",
      "batch: 1563/1563 - train loss: 19.0718 - test loss: 18.4733 - train acc: 0.1932 - test acc: 0.2177 - 1m 17s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 18.2169 - test loss: 18.8017 - train acc: 0.2091 - test acc: 0.2025 - 1m 21s\n",
      "batch: 200/1563 - train loss: 17.6680 - test loss: 18.9136 - train acc: 0.2247 - test acc: 0.2042 - 1m 26s\n",
      "batch: 300/1563 - train loss: 18.1804 - test loss: 18.7260 - train acc: 0.2110 - test acc: 0.1979 - 1m 32s\n",
      "batch: 400/1563 - train loss: 18.0855 - test loss: 18.5409 - train acc: 0.2190 - test acc: 0.2126 - 1m 37s\n",
      "batch: 500/1563 - train loss: 17.6249 - test loss: 18.0680 - train acc: 0.2303 - test acc: 0.2264 - 1m 42s\n",
      "batch: 600/1563 - train loss: 17.2765 - test loss: 17.6130 - train acc: 0.2410 - test acc: 0.2359 - 1m 47s\n",
      "batch: 700/1563 - train loss: 17.9525 - test loss: 17.8138 - train acc: 0.2322 - test acc: 0.2353 - 1m 52s\n",
      "batch: 800/1563 - train loss: 17.1236 - test loss: 17.0133 - train acc: 0.2481 - test acc: 0.2591 - 1m 57s\n",
      "batch: 900/1563 - train loss: 16.8754 - test loss: 18.2405 - train acc: 0.2531 - test acc: 0.2254 - 2m 2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 17.0287 - test loss: 17.5684 - train acc: 0.2566 - test acc: 0.2438 - 2m 7s\n",
      "batch: 1100/1563 - train loss: 16.8613 - test loss: 17.6764 - train acc: 0.2609 - test acc: 0.2381 - 2m 12s\n",
      "batch: 1200/1563 - train loss: 16.9558 - test loss: 16.9006 - train acc: 0.2559 - test acc: 0.2606 - 2m 17s\n",
      "batch: 1300/1563 - train loss: 16.5672 - test loss: 16.6117 - train acc: 0.2660 - test acc: 0.2731 - 2m 22s\n",
      "batch: 1400/1563 - train loss: 16.7932 - test loss: 17.1023 - train acc: 0.2597 - test acc: 0.2505 - 2m 27s\n",
      "batch: 1500/1563 - train loss: 16.3768 - test loss: 16.6821 - train acc: 0.2772 - test acc: 0.2707 - 2m 32s\n",
      "batch: 1563/1563 - train loss: 16.3743 - test loss: 16.8065 - train acc: 0.2831 - test acc: 0.2700 - 2m 36s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.3793 - test loss: 15.9374 - train acc: 0.3134 - test acc: 0.2927 - 2m 41s\n",
      "batch: 200/1563 - train loss: 15.3499 - test loss: 16.1716 - train acc: 0.3112 - test acc: 0.2899 - 2m 46s\n",
      "batch: 300/1563 - train loss: 15.5009 - test loss: 17.4118 - train acc: 0.3006 - test acc: 0.2472 - 2m 51s\n",
      "batch: 400/1563 - train loss: 15.4726 - test loss: 16.3734 - train acc: 0.3056 - test acc: 0.2812 - 2m 56s\n",
      "batch: 500/1563 - train loss: 15.1584 - test loss: 16.5893 - train acc: 0.3222 - test acc: 0.2712 - 3m 1s\n",
      "batch: 600/1563 - train loss: 15.1166 - test loss: 15.6021 - train acc: 0.3215 - test acc: 0.3009 - 3m 6s\n",
      "batch: 700/1563 - train loss: 14.8650 - test loss: 15.4892 - train acc: 0.3318 - test acc: 0.3069 - 3m 11s\n",
      "batch: 800/1563 - train loss: 14.9664 - test loss: 17.3968 - train acc: 0.3187 - test acc: 0.2647 - 3m 16s\n",
      "batch: 900/1563 - train loss: 15.0768 - test loss: 16.1667 - train acc: 0.3125 - test acc: 0.2934 - 3m 21s\n",
      "batch: 1000/1563 - train loss: 14.8301 - test loss: 15.5936 - train acc: 0.3381 - test acc: 0.3043 - 3m 26s\n",
      "batch: 1100/1563 - train loss: 15.0600 - test loss: 15.4717 - train acc: 0.3293 - test acc: 0.3082 - 3m 31s\n",
      "batch: 1200/1563 - train loss: 14.8215 - test loss: 15.3544 - train acc: 0.3304 - test acc: 0.3198 - 3m 36s\n",
      "batch: 1300/1563 - train loss: 14.7283 - test loss: 15.3644 - train acc: 0.3306 - test acc: 0.3176 - 3m 41s\n",
      "batch: 1400/1563 - train loss: 14.4004 - test loss: 15.6518 - train acc: 0.3447 - test acc: 0.3103 - 3m 46s\n",
      "batch: 1500/1563 - train loss: 14.3932 - test loss: 15.2221 - train acc: 0.3363 - test acc: 0.3277 - 3m 51s\n",
      "batch: 1563/1563 - train loss: 14.4988 - test loss: 16.2780 - train acc: 0.3341 - test acc: 0.2979 - 3m 55s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.0674 - test loss: 15.6030 - train acc: 0.3929 - test acc: 0.3152 - 4m 0s\n",
      "batch: 200/1563 - train loss: 13.6316 - test loss: 14.4743 - train acc: 0.3719 - test acc: 0.3551 - 4m 5s\n",
      "batch: 300/1563 - train loss: 13.5312 - test loss: 15.6606 - train acc: 0.3762 - test acc: 0.3172 - 4m 10s\n",
      "batch: 400/1563 - train loss: 13.5314 - test loss: 16.3162 - train acc: 0.3869 - test acc: 0.3004 - 4m 15s\n",
      "batch: 500/1563 - train loss: 13.5155 - test loss: 15.0701 - train acc: 0.3713 - test acc: 0.3271 - 4m 20s\n",
      "batch: 600/1563 - train loss: 13.6075 - test loss: 15.0763 - train acc: 0.3834 - test acc: 0.3319 - 4m 25s\n",
      "batch: 700/1563 - train loss: 13.4002 - test loss: 14.9749 - train acc: 0.3819 - test acc: 0.3402 - 4m 30s\n",
      "batch: 800/1563 - train loss: 13.3226 - test loss: 15.0497 - train acc: 0.3756 - test acc: 0.3426 - 4m 35s\n",
      "batch: 900/1563 - train loss: 13.2555 - test loss: 15.0718 - train acc: 0.3793 - test acc: 0.3408 - 4m 40s\n",
      "batch: 1000/1563 - train loss: 13.5333 - test loss: 14.2905 - train acc: 0.3831 - test acc: 0.3539 - 4m 44s\n",
      "batch: 1100/1563 - train loss: 13.3870 - test loss: 14.3676 - train acc: 0.3869 - test acc: 0.3523 - 4m 49s\n",
      "batch: 1200/1563 - train loss: 13.4684 - test loss: 15.3127 - train acc: 0.3847 - test acc: 0.3210 - 4m 54s\n",
      "batch: 1300/1563 - train loss: 13.2797 - test loss: 14.6720 - train acc: 0.3881 - test acc: 0.3497 - 5m 0s\n",
      "batch: 1400/1563 - train loss: 13.0798 - test loss: 14.9787 - train acc: 0.4028 - test acc: 0.3374 - 5m 4s\n",
      "batch: 1500/1563 - train loss: 13.0165 - test loss: 14.6797 - train acc: 0.4022 - test acc: 0.3397 - 5m 10s\n",
      "batch: 1563/1563 - train loss: 12.9862 - test loss: 14.6929 - train acc: 0.3972 - test acc: 0.3482 - 5m 14s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3940 - test loss: 14.2393 - train acc: 0.4616 - test acc: 0.3681 - 5m 19s\n",
      "batch: 200/1563 - train loss: 11.4412 - test loss: 14.1579 - train acc: 0.4491 - test acc: 0.3687 - 5m 23s\n",
      "batch: 300/1563 - train loss: 11.8700 - test loss: 14.3497 - train acc: 0.4415 - test acc: 0.3612 - 5m 28s\n",
      "batch: 400/1563 - train loss: 11.9431 - test loss: 14.2758 - train acc: 0.4363 - test acc: 0.3595 - 5m 34s\n",
      "batch: 500/1563 - train loss: 11.9691 - test loss: 15.2498 - train acc: 0.4353 - test acc: 0.3410 - 5m 39s\n",
      "batch: 600/1563 - train loss: 12.1892 - test loss: 15.0536 - train acc: 0.4432 - test acc: 0.3409 - 5m 44s\n",
      "batch: 700/1563 - train loss: 12.1911 - test loss: 15.2172 - train acc: 0.4328 - test acc: 0.3437 - 5m 49s\n",
      "batch: 800/1563 - train loss: 12.0856 - test loss: 14.4584 - train acc: 0.4369 - test acc: 0.3614 - 5m 53s\n",
      "batch: 900/1563 - train loss: 12.0645 - test loss: 15.1422 - train acc: 0.4260 - test acc: 0.3436 - 5m 58s\n",
      "batch: 1000/1563 - train loss: 12.0071 - test loss: 16.1172 - train acc: 0.4309 - test acc: 0.3283 - 6m 4s\n",
      "batch: 1100/1563 - train loss: 12.0738 - test loss: 13.7726 - train acc: 0.4344 - test acc: 0.3888 - 6m 9s\n",
      "batch: 1200/1563 - train loss: 11.9861 - test loss: 14.6554 - train acc: 0.4504 - test acc: 0.3600 - 6m 13s\n",
      "batch: 1300/1563 - train loss: 12.1716 - test loss: 13.6446 - train acc: 0.4459 - test acc: 0.3838 - 6m 19s\n",
      "batch: 1400/1563 - train loss: 11.9732 - test loss: 13.8972 - train acc: 0.4391 - test acc: 0.3705 - 6m 23s\n",
      "batch: 1500/1563 - train loss: 12.1023 - test loss: 13.4272 - train acc: 0.4316 - test acc: 0.3888 - 6m 28s\n",
      "batch: 1563/1563 - train loss: 11.8649 - test loss: 14.1178 - train acc: 0.4419 - test acc: 0.3717 - 6m 33s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0213 - test loss: 13.9388 - train acc: 0.5097 - test acc: 0.3804 - 6m 38s\n",
      "batch: 200/1563 - train loss: 10.4261 - test loss: 14.4901 - train acc: 0.4963 - test acc: 0.3653 - 6m 43s\n",
      "batch: 300/1563 - train loss: 10.2681 - test loss: 13.5699 - train acc: 0.4993 - test acc: 0.3976 - 6m 48s\n",
      "batch: 400/1563 - train loss: 10.6114 - test loss: 13.9090 - train acc: 0.4860 - test acc: 0.3822 - 6m 53s\n",
      "batch: 500/1563 - train loss: 10.3124 - test loss: 14.5610 - train acc: 0.5056 - test acc: 0.3723 - 6m 58s\n",
      "batch: 600/1563 - train loss: 10.6252 - test loss: 15.0927 - train acc: 0.4881 - test acc: 0.3527 - 7m 2s\n",
      "batch: 700/1563 - train loss: 10.7503 - test loss: 13.6171 - train acc: 0.4839 - test acc: 0.3880 - 7m 8s\n",
      "batch: 800/1563 - train loss: 10.8031 - test loss: 14.1756 - train acc: 0.4788 - test acc: 0.3715 - 7m 13s\n",
      "batch: 900/1563 - train loss: 10.6348 - test loss: 13.9285 - train acc: 0.4937 - test acc: 0.3853 - 7m 17s\n",
      "batch: 1000/1563 - train loss: 10.7316 - test loss: 13.9533 - train acc: 0.4809 - test acc: 0.3800 - 7m 22s\n",
      "batch: 1100/1563 - train loss: 10.7028 - test loss: 14.1827 - train acc: 0.4854 - test acc: 0.3827 - 7m 27s\n",
      "batch: 1200/1563 - train loss: 11.2141 - test loss: 14.9776 - train acc: 0.4619 - test acc: 0.3618 - 7m 33s\n",
      "batch: 1300/1563 - train loss: 10.8360 - test loss: 14.7999 - train acc: 0.4800 - test acc: 0.3575 - 7m 38s\n",
      "batch: 1400/1563 - train loss: 10.8863 - test loss: 13.6942 - train acc: 0.4790 - test acc: 0.3981 - 7m 44s\n",
      "batch: 1500/1563 - train loss: 11.1463 - test loss: 13.8470 - train acc: 0.4722 - test acc: 0.3960 - 7m 49s\n",
      "batch: 1563/1563 - train loss: 11.1652 - test loss: 13.1945 - train acc: 0.4647 - test acc: 0.4118 - 7m 53s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 9.0063 - test loss: 13.4484 - train acc: 0.5600 - test acc: 0.4044 - 7m 59s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 8.8758 - test loss: 13.5283 - train acc: 0.5531 - test acc: 0.4050 - 8m 4s\n",
      "batch: 300/1563 - train loss: 8.9308 - test loss: 13.9350 - train acc: 0.5550 - test acc: 0.3963 - 8m 10s\n",
      "batch: 400/1563 - train loss: 9.0564 - test loss: 14.1407 - train acc: 0.5572 - test acc: 0.3845 - 8m 15s\n",
      "batch: 500/1563 - train loss: 9.3378 - test loss: 14.9629 - train acc: 0.5343 - test acc: 0.3662 - 8m 20s\n",
      "batch: 600/1563 - train loss: 9.7339 - test loss: 13.5290 - train acc: 0.5250 - test acc: 0.4117 - 8m 25s\n",
      "batch: 700/1563 - train loss: 9.3177 - test loss: 13.6490 - train acc: 0.5431 - test acc: 0.4179 - 8m 30s\n",
      "batch: 800/1563 - train loss: 9.8339 - test loss: 14.5507 - train acc: 0.5240 - test acc: 0.3865 - 8m 35s\n",
      "batch: 900/1563 - train loss: 9.7029 - test loss: 13.6415 - train acc: 0.5347 - test acc: 0.4044 - 8m 40s\n",
      "batch: 1000/1563 - train loss: 9.8735 - test loss: 13.9574 - train acc: 0.5146 - test acc: 0.3947 - 8m 45s\n",
      "batch: 1100/1563 - train loss: 9.7966 - test loss: 13.8022 - train acc: 0.5266 - test acc: 0.3989 - 8m 51s\n",
      "batch: 1200/1563 - train loss: 9.7564 - test loss: 14.1989 - train acc: 0.5263 - test acc: 0.3873 - 8m 56s\n",
      "batch: 1300/1563 - train loss: 9.7436 - test loss: 14.7720 - train acc: 0.5290 - test acc: 0.3712 - 9m 1s\n",
      "batch: 1400/1563 - train loss: 9.7386 - test loss: 13.9695 - train acc: 0.5315 - test acc: 0.3898 - 9m 6s\n",
      "batch: 1500/1563 - train loss: 9.7504 - test loss: 14.5352 - train acc: 0.5228 - test acc: 0.3742 - 9m 12s\n",
      "batch: 1563/1563 - train loss: 10.0235 - test loss: 13.9311 - train acc: 0.5216 - test acc: 0.3953 - 9m 16s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.6315 - test loss: 13.9021 - train acc: 0.6178 - test acc: 0.4114 - 9m 21s\n",
      "batch: 200/1563 - train loss: 7.6737 - test loss: 14.7053 - train acc: 0.6197 - test acc: 0.3786 - 9m 26s\n",
      "batch: 300/1563 - train loss: 7.7824 - test loss: 14.1297 - train acc: 0.6103 - test acc: 0.4014 - 9m 31s\n",
      "batch: 400/1563 - train loss: 8.3422 - test loss: 14.4731 - train acc: 0.5775 - test acc: 0.3837 - 9m 36s\n",
      "batch: 500/1563 - train loss: 8.0955 - test loss: 14.0979 - train acc: 0.5984 - test acc: 0.3936 - 9m 41s\n",
      "batch: 600/1563 - train loss: 8.1297 - test loss: 14.5664 - train acc: 0.6022 - test acc: 0.3864 - 9m 47s\n",
      "batch: 700/1563 - train loss: 8.2225 - test loss: 13.7930 - train acc: 0.5890 - test acc: 0.4045 - 9m 52s\n",
      "batch: 800/1563 - train loss: 8.3472 - test loss: 14.1343 - train acc: 0.5825 - test acc: 0.4014 - 9m 57s\n",
      "batch: 900/1563 - train loss: 8.6125 - test loss: 15.7179 - train acc: 0.5763 - test acc: 0.3551 - 10m 2s\n",
      "batch: 1000/1563 - train loss: 8.8299 - test loss: 13.6153 - train acc: 0.5534 - test acc: 0.4160 - 10m 7s\n",
      "batch: 1100/1563 - train loss: 8.6864 - test loss: 14.1211 - train acc: 0.5728 - test acc: 0.3982 - 10m 12s\n",
      "batch: 1200/1563 - train loss: 8.6904 - test loss: 14.1228 - train acc: 0.5532 - test acc: 0.4032 - 10m 18s\n",
      "batch: 1300/1563 - train loss: 8.5593 - test loss: 13.8673 - train acc: 0.5790 - test acc: 0.4094 - 10m 23s\n",
      "batch: 1400/1563 - train loss: 8.9872 - test loss: 13.6042 - train acc: 0.5547 - test acc: 0.4165 - 10m 28s\n",
      "batch: 1500/1563 - train loss: 8.9224 - test loss: 13.7105 - train acc: 0.5497 - test acc: 0.4215 - 10m 33s\n",
      "batch: 1563/1563 - train loss: 8.8123 - test loss: 14.1467 - train acc: 0.5491 - test acc: 0.4000 - 10m 37s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.1742 - test loss: 14.0663 - train acc: 0.6885 - test acc: 0.4179 - 10m 43s\n",
      "batch: 200/1563 - train loss: 6.1514 - test loss: 13.9351 - train acc: 0.6903 - test acc: 0.4194 - 10m 47s\n",
      "batch: 300/1563 - train loss: 6.6628 - test loss: 14.4790 - train acc: 0.6534 - test acc: 0.4065 - 10m 53s\n",
      "batch: 400/1563 - train loss: 6.7147 - test loss: 14.7613 - train acc: 0.6509 - test acc: 0.3988 - 10m 58s\n",
      "batch: 500/1563 - train loss: 6.8808 - test loss: 15.0715 - train acc: 0.6469 - test acc: 0.4010 - 11m 3s\n",
      "batch: 600/1563 - train loss: 7.2987 - test loss: 14.7473 - train acc: 0.6256 - test acc: 0.3981 - 11m 8s\n",
      "batch: 700/1563 - train loss: 7.2486 - test loss: 14.0735 - train acc: 0.6350 - test acc: 0.4115 - 11m 13s\n",
      "batch: 800/1563 - train loss: 7.3845 - test loss: 14.1701 - train acc: 0.6222 - test acc: 0.4105 - 11m 18s\n",
      "batch: 900/1563 - train loss: 7.6584 - test loss: 14.2830 - train acc: 0.6066 - test acc: 0.4091 - 11m 23s\n",
      "batch: 1000/1563 - train loss: 7.6069 - test loss: 14.3168 - train acc: 0.6059 - test acc: 0.4056 - 11m 28s\n",
      "batch: 1100/1563 - train loss: 7.3882 - test loss: 14.6778 - train acc: 0.6291 - test acc: 0.4011 - 11m 33s\n",
      "batch: 1200/1563 - train loss: 7.5801 - test loss: 14.6696 - train acc: 0.6175 - test acc: 0.4052 - 11m 38s\n",
      "batch: 1300/1563 - train loss: 7.7236 - test loss: 14.2955 - train acc: 0.6053 - test acc: 0.4058 - 11m 43s\n",
      "batch: 1400/1563 - train loss: 7.4718 - test loss: 14.8306 - train acc: 0.6091 - test acc: 0.3963 - 11m 48s\n",
      "batch: 1500/1563 - train loss: 8.0471 - test loss: 14.3478 - train acc: 0.5894 - test acc: 0.4057 - 11m 53s\n",
      "batch: 1563/1563 - train loss: 7.9610 - test loss: 14.2613 - train acc: 0.5932 - test acc: 0.4140 - 11m 57s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.1761 - test loss: 14.3382 - train acc: 0.7331 - test acc: 0.4214 - 12m 2s\n",
      "batch: 200/1563 - train loss: 5.4487 - test loss: 14.4932 - train acc: 0.7147 - test acc: 0.4187 - 12m 7s\n",
      "batch: 300/1563 - train loss: 5.4573 - test loss: 15.1479 - train acc: 0.7135 - test acc: 0.4049 - 12m 12s\n",
      "batch: 400/1563 - train loss: 5.9999 - test loss: 15.1463 - train acc: 0.6919 - test acc: 0.4049 - 12m 17s\n",
      "batch: 500/1563 - train loss: 5.7887 - test loss: 16.1273 - train acc: 0.6953 - test acc: 0.3888 - 12m 22s\n",
      "batch: 600/1563 - train loss: 5.9688 - test loss: 15.4911 - train acc: 0.6881 - test acc: 0.3960 - 12m 27s\n",
      "batch: 700/1563 - train loss: 6.1064 - test loss: 15.0044 - train acc: 0.6740 - test acc: 0.4071 - 12m 32s\n",
      "batch: 800/1563 - train loss: 6.5904 - test loss: 14.8166 - train acc: 0.6613 - test acc: 0.4063 - 12m 37s\n",
      "batch: 900/1563 - train loss: 6.4711 - test loss: 15.1358 - train acc: 0.6716 - test acc: 0.4005 - 12m 42s\n",
      "batch: 1000/1563 - train loss: 6.6013 - test loss: 14.5492 - train acc: 0.6619 - test acc: 0.4198 - 12m 46s\n",
      "batch: 1100/1563 - train loss: 6.5017 - test loss: 14.9325 - train acc: 0.6625 - test acc: 0.4054 - 12m 52s\n",
      "batch: 1200/1563 - train loss: 6.6000 - test loss: 14.2573 - train acc: 0.6513 - test acc: 0.4256 - 12m 57s\n",
      "batch: 1300/1563 - train loss: 6.6455 - test loss: 14.4367 - train acc: 0.6581 - test acc: 0.4230 - 13m 2s\n",
      "batch: 1400/1563 - train loss: 6.6589 - test loss: 15.2376 - train acc: 0.6569 - test acc: 0.4047 - 13m 6s\n",
      "batch: 1500/1563 - train loss: 7.1461 - test loss: 14.8494 - train acc: 0.6269 - test acc: 0.4098 - 13m 11s\n",
      "batch: 1563/1563 - train loss: 7.0971 - test loss: 14.5281 - train acc: 0.6338 - test acc: 0.4155 - 13m 16s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.4568 - test loss: 15.7142 - train acc: 0.7634 - test acc: 0.4066 - 13m 20s\n",
      "batch: 200/1563 - train loss: 4.2058 - test loss: 15.2448 - train acc: 0.7790 - test acc: 0.4215 - 13m 25s\n",
      "batch: 300/1563 - train loss: 4.4799 - test loss: 15.8237 - train acc: 0.7618 - test acc: 0.3994 - 13m 31s\n",
      "batch: 400/1563 - train loss: 4.6650 - test loss: 16.2481 - train acc: 0.7537 - test acc: 0.3973 - 13m 36s\n",
      "batch: 500/1563 - train loss: 4.8742 - test loss: 15.4806 - train acc: 0.7375 - test acc: 0.4111 - 13m 40s\n",
      "batch: 600/1563 - train loss: 4.9090 - test loss: 15.6240 - train acc: 0.7412 - test acc: 0.4175 - 13m 45s\n",
      "batch: 700/1563 - train loss: 5.2385 - test loss: 16.4942 - train acc: 0.7232 - test acc: 0.4009 - 13m 50s\n",
      "batch: 800/1563 - train loss: 4.9991 - test loss: 16.2907 - train acc: 0.7319 - test acc: 0.3944 - 13m 55s\n",
      "batch: 900/1563 - train loss: 5.4230 - test loss: 15.4037 - train acc: 0.7160 - test acc: 0.4204 - 14m 0s\n",
      "batch: 1000/1563 - train loss: 5.7706 - test loss: 15.5924 - train acc: 0.6940 - test acc: 0.4060 - 14m 5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 5.7124 - test loss: 15.5956 - train acc: 0.6916 - test acc: 0.4050 - 14m 10s\n",
      "batch: 1200/1563 - train loss: 5.7660 - test loss: 15.1385 - train acc: 0.6875 - test acc: 0.4226 - 14m 15s\n",
      "batch: 1300/1563 - train loss: 6.0759 - test loss: 14.8302 - train acc: 0.6851 - test acc: 0.4144 - 14m 20s\n",
      "batch: 1400/1563 - train loss: 6.0452 - test loss: 15.9936 - train acc: 0.6766 - test acc: 0.3970 - 14m 25s\n",
      "batch: 1500/1563 - train loss: 6.0656 - test loss: 15.7276 - train acc: 0.6737 - test acc: 0.4155 - 14m 30s\n",
      "batch: 1563/1563 - train loss: 6.1378 - test loss: 15.0441 - train acc: 0.6757 - test acc: 0.4197 - 14m 35s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.6596 - test loss: 15.2490 - train acc: 0.8006 - test acc: 0.4249 - 14m 40s\n",
      "batch: 200/1563 - train loss: 3.6513 - test loss: 15.8413 - train acc: 0.8056 - test acc: 0.4189 - 14m 45s\n",
      "batch: 300/1563 - train loss: 3.7012 - test loss: 15.2805 - train acc: 0.7975 - test acc: 0.4311 - 14m 49s\n",
      "batch: 400/1563 - train loss: 3.8715 - test loss: 16.2599 - train acc: 0.7831 - test acc: 0.4142 - 14m 55s\n",
      "batch: 500/1563 - train loss: 3.9808 - test loss: 16.4146 - train acc: 0.7825 - test acc: 0.4083 - 14m 59s\n",
      "batch: 600/1563 - train loss: 4.5708 - test loss: 15.6776 - train acc: 0.7559 - test acc: 0.4150 - 15m 5s\n",
      "batch: 700/1563 - train loss: 4.2156 - test loss: 16.0005 - train acc: 0.7706 - test acc: 0.4123 - 15m 10s\n",
      "batch: 800/1563 - train loss: 4.4052 - test loss: 15.9554 - train acc: 0.7646 - test acc: 0.4185 - 15m 15s\n",
      "batch: 900/1563 - train loss: 4.4435 - test loss: 16.5146 - train acc: 0.7559 - test acc: 0.4126 - 15m 20s\n",
      "batch: 1000/1563 - train loss: 4.8709 - test loss: 16.0662 - train acc: 0.7322 - test acc: 0.4090 - 15m 25s\n",
      "batch: 1100/1563 - train loss: 4.7394 - test loss: 16.5684 - train acc: 0.7447 - test acc: 0.4053 - 15m 30s\n",
      "batch: 1200/1563 - train loss: 5.0207 - test loss: 16.9600 - train acc: 0.7322 - test acc: 0.3970 - 15m 35s\n",
      "batch: 1300/1563 - train loss: 5.1097 - test loss: 16.0184 - train acc: 0.7199 - test acc: 0.4104 - 15m 40s\n",
      "batch: 1400/1563 - train loss: 5.0899 - test loss: 16.1079 - train acc: 0.7272 - test acc: 0.4091 - 15m 45s\n",
      "batch: 1500/1563 - train loss: 5.0144 - test loss: 16.3986 - train acc: 0.7328 - test acc: 0.4090 - 15m 50s\n",
      "batch: 1563/1563 - train loss: 5.1853 - test loss: 15.4887 - train acc: 0.7175 - test acc: 0.4195 - 15m 54s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 2.8678 - test loss: 16.1467 - train acc: 0.8475 - test acc: 0.4203 - 15m 59s\n",
      "batch: 200/1563 - train loss: 3.0917 - test loss: 16.5425 - train acc: 0.8231 - test acc: 0.4189 - 16m 4s\n",
      "batch: 300/1563 - train loss: 3.1321 - test loss: 16.5011 - train acc: 0.8262 - test acc: 0.4187 - 16m 9s\n",
      "batch: 400/1563 - train loss: 3.1262 - test loss: 16.6715 - train acc: 0.8249 - test acc: 0.4188 - 16m 14s\n",
      "batch: 500/1563 - train loss: 3.0525 - test loss: 16.8113 - train acc: 0.8300 - test acc: 0.4235 - 16m 19s\n",
      "batch: 600/1563 - train loss: 3.1831 - test loss: 16.9955 - train acc: 0.8290 - test acc: 0.4093 - 16m 25s\n",
      "batch: 700/1563 - train loss: 3.3087 - test loss: 16.6625 - train acc: 0.8187 - test acc: 0.4231 - 16m 29s\n",
      "batch: 800/1563 - train loss: 3.7419 - test loss: 16.6062 - train acc: 0.7934 - test acc: 0.4192 - 16m 34s\n",
      "batch: 900/1563 - train loss: 3.6645 - test loss: 16.6608 - train acc: 0.7931 - test acc: 0.4217 - 16m 40s\n",
      "batch: 1000/1563 - train loss: 3.9357 - test loss: 17.3406 - train acc: 0.7791 - test acc: 0.4036 - 16m 45s\n",
      "batch: 1100/1563 - train loss: 3.8719 - test loss: 17.8131 - train acc: 0.7897 - test acc: 0.3976 - 16m 49s\n",
      "batch: 1200/1563 - train loss: 3.9962 - test loss: 16.7557 - train acc: 0.7837 - test acc: 0.4180 - 16m 55s\n",
      "batch: 1300/1563 - train loss: 3.9436 - test loss: 16.5119 - train acc: 0.7822 - test acc: 0.4191 - 17m 0s\n",
      "batch: 1400/1563 - train loss: 4.5012 - test loss: 16.9037 - train acc: 0.7647 - test acc: 0.4081 - 17m 5s\n",
      "batch: 1500/1563 - train loss: 4.2605 - test loss: 16.6489 - train acc: 0.7588 - test acc: 0.4034 - 17m 10s\n",
      "batch: 1563/1563 - train loss: 4.4770 - test loss: 16.5830 - train acc: 0.7503 - test acc: 0.4152 - 17m 15s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.5330 - test loss: 16.4822 - train acc: 0.8572 - test acc: 0.4331 - 17m 20s\n",
      "batch: 200/1563 - train loss: 2.3629 - test loss: 17.8086 - train acc: 0.8757 - test acc: 0.4060 - 17m 25s\n",
      "batch: 300/1563 - train loss: 2.4201 - test loss: 16.9238 - train acc: 0.8677 - test acc: 0.4212 - 17m 30s\n",
      "batch: 400/1563 - train loss: 2.6224 - test loss: 17.2148 - train acc: 0.8553 - test acc: 0.4181 - 17m 35s\n",
      "batch: 500/1563 - train loss: 2.6509 - test loss: 17.1614 - train acc: 0.8503 - test acc: 0.4135 - 17m 40s\n",
      "batch: 600/1563 - train loss: 2.7535 - test loss: 17.4499 - train acc: 0.8437 - test acc: 0.4185 - 17m 46s\n",
      "batch: 700/1563 - train loss: 2.8008 - test loss: 17.8053 - train acc: 0.8478 - test acc: 0.4123 - 17m 51s\n",
      "batch: 800/1563 - train loss: 3.1966 - test loss: 17.5885 - train acc: 0.8177 - test acc: 0.4099 - 17m 56s\n",
      "batch: 900/1563 - train loss: 3.1626 - test loss: 17.5135 - train acc: 0.8284 - test acc: 0.4123 - 18m 1s\n",
      "batch: 1000/1563 - train loss: 3.3704 - test loss: 17.1990 - train acc: 0.8106 - test acc: 0.4171 - 18m 6s\n",
      "batch: 1100/1563 - train loss: 3.2307 - test loss: 17.1099 - train acc: 0.8168 - test acc: 0.4212 - 18m 11s\n",
      "batch: 1200/1563 - train loss: 3.3262 - test loss: 16.9891 - train acc: 0.8165 - test acc: 0.4179 - 18m 17s\n",
      "batch: 1300/1563 - train loss: 3.3911 - test loss: 16.9783 - train acc: 0.8096 - test acc: 0.4277 - 18m 22s\n",
      "batch: 1400/1563 - train loss: 3.4676 - test loss: 17.1215 - train acc: 0.8046 - test acc: 0.4223 - 18m 27s\n",
      "batch: 1500/1563 - train loss: 3.5147 - test loss: 17.4696 - train acc: 0.8081 - test acc: 0.4169 - 18m 32s\n",
      "batch: 1563/1563 - train loss: 3.8360 - test loss: 17.2199 - train acc: 0.7872 - test acc: 0.4186 - 18m 36s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.1688 - test loss: 16.9765 - train acc: 0.8772 - test acc: 0.4300 - 18m 41s\n",
      "batch: 200/1563 - train loss: 1.9414 - test loss: 18.0757 - train acc: 0.8944 - test acc: 0.4186 - 18m 46s\n",
      "batch: 300/1563 - train loss: 2.0849 - test loss: 18.0001 - train acc: 0.8834 - test acc: 0.4184 - 18m 51s\n",
      "batch: 400/1563 - train loss: 2.0997 - test loss: 17.8170 - train acc: 0.8823 - test acc: 0.4264 - 18m 56s\n",
      "batch: 500/1563 - train loss: 2.2432 - test loss: 18.3664 - train acc: 0.8684 - test acc: 0.4265 - 19m 1s\n",
      "batch: 600/1563 - train loss: 2.1923 - test loss: 18.6577 - train acc: 0.8778 - test acc: 0.4099 - 19m 7s\n",
      "batch: 700/1563 - train loss: 2.4594 - test loss: 18.2516 - train acc: 0.8547 - test acc: 0.4212 - 19m 12s\n",
      "batch: 800/1563 - train loss: 2.5331 - test loss: 17.9343 - train acc: 0.8581 - test acc: 0.4257 - 19m 17s\n",
      "batch: 900/1563 - train loss: 2.4698 - test loss: 17.9584 - train acc: 0.8597 - test acc: 0.4188 - 19m 22s\n",
      "batch: 1000/1563 - train loss: 2.5331 - test loss: 18.5957 - train acc: 0.8578 - test acc: 0.4087 - 19m 27s\n",
      "batch: 1100/1563 - train loss: 2.7203 - test loss: 18.0806 - train acc: 0.8490 - test acc: 0.4063 - 19m 31s\n",
      "batch: 1200/1563 - train loss: 2.8468 - test loss: 18.1782 - train acc: 0.8390 - test acc: 0.4213 - 19m 36s\n",
      "batch: 1300/1563 - train loss: 2.9289 - test loss: 17.9751 - train acc: 0.8316 - test acc: 0.4150 - 19m 41s\n",
      "batch: 1400/1563 - train loss: 3.0719 - test loss: 18.1296 - train acc: 0.8218 - test acc: 0.4185 - 19m 47s\n",
      "batch: 1500/1563 - train loss: 3.1214 - test loss: 18.5199 - train acc: 0.8193 - test acc: 0.4106 - 19m 52s\n",
      "batch: 1563/1563 - train loss: 3.1764 - test loss: 18.4802 - train acc: 0.8184 - test acc: 0.4082 - 19m 56s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "time is up! finishing training\n",
      "batch: 37/1563 - train loss: 2.6299 - test loss: 18.0578 - train acc: 0.8531 - test acc: 0.4193 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n",
      "batch: 100/1563 - train loss: 26.1263 - test loss: 25.8553 - train acc: 0.0370 - test acc: 0.0507 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.2806 - test loss: 23.4620 - train acc: 0.0626 - test acc: 0.0746 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.3619 - test loss: 23.0494 - train acc: 0.0726 - test acc: 0.0826 - 0m 12s\n",
      "batch: 400/1563 - train loss: 22.4932 - test loss: 21.8612 - train acc: 0.0918 - test acc: 0.1067 - 0m 16s\n",
      "batch: 500/1563 - train loss: 21.9156 - test loss: 21.4449 - train acc: 0.1112 - test acc: 0.1230 - 0m 21s\n",
      "batch: 600/1563 - train loss: 21.7748 - test loss: 21.3234 - train acc: 0.1109 - test acc: 0.1296 - 0m 26s\n",
      "batch: 700/1563 - train loss: 21.1378 - test loss: 20.7607 - train acc: 0.1272 - test acc: 0.1423 - 0m 31s\n",
      "batch: 800/1563 - train loss: 20.9397 - test loss: 20.1959 - train acc: 0.1384 - test acc: 0.1494 - 0m 36s\n",
      "batch: 900/1563 - train loss: 20.5404 - test loss: 20.2780 - train acc: 0.1385 - test acc: 0.1501 - 0m 40s\n",
      "batch: 1000/1563 - train loss: 20.2329 - test loss: 19.5652 - train acc: 0.1531 - test acc: 0.1768 - 0m 46s\n",
      "batch: 1100/1563 - train loss: 19.6949 - test loss: 19.9109 - train acc: 0.1610 - test acc: 0.1754 - 0m 51s\n",
      "batch: 1200/1563 - train loss: 19.7334 - test loss: 19.6452 - train acc: 0.1683 - test acc: 0.1649 - 0m 55s\n",
      "batch: 1300/1563 - train loss: 19.5498 - test loss: 19.3516 - train acc: 0.1747 - test acc: 0.1820 - 1m 0s\n",
      "batch: 1400/1563 - train loss: 19.2400 - test loss: 19.2162 - train acc: 0.1888 - test acc: 0.1930 - 1m 5s\n",
      "batch: 1500/1563 - train loss: 18.7292 - test loss: 19.0249 - train acc: 0.2003 - test acc: 0.1940 - 1m 10s\n",
      "batch: 1563/1563 - train loss: 18.8302 - test loss: 19.0899 - train acc: 0.2031 - test acc: 0.1955 - 1m 14s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.9427 - test loss: 18.6501 - train acc: 0.2197 - test acc: 0.2094 - 1m 19s\n",
      "batch: 200/1563 - train loss: 17.9264 - test loss: 18.2109 - train acc: 0.2288 - test acc: 0.2206 - 1m 24s\n",
      "batch: 300/1563 - train loss: 18.0138 - test loss: 18.4634 - train acc: 0.2272 - test acc: 0.2098 - 1m 29s\n",
      "batch: 400/1563 - train loss: 17.6652 - test loss: 18.5066 - train acc: 0.2266 - test acc: 0.2125 - 1m 34s\n",
      "batch: 500/1563 - train loss: 17.6561 - test loss: 17.5814 - train acc: 0.2225 - test acc: 0.2436 - 1m 38s\n",
      "batch: 600/1563 - train loss: 17.4730 - test loss: 18.1003 - train acc: 0.2450 - test acc: 0.2229 - 1m 43s\n",
      "batch: 700/1563 - train loss: 17.5459 - test loss: 17.7209 - train acc: 0.2503 - test acc: 0.2341 - 1m 49s\n",
      "batch: 800/1563 - train loss: 17.2473 - test loss: 17.5119 - train acc: 0.2481 - test acc: 0.2454 - 1m 54s\n",
      "batch: 900/1563 - train loss: 17.1042 - test loss: 17.9941 - train acc: 0.2534 - test acc: 0.2372 - 1m 58s\n",
      "batch: 1000/1563 - train loss: 16.8167 - test loss: 17.1661 - train acc: 0.2578 - test acc: 0.2570 - 2m 3s\n",
      "batch: 1100/1563 - train loss: 16.9878 - test loss: 17.6127 - train acc: 0.2550 - test acc: 0.2433 - 2m 8s\n",
      "batch: 1200/1563 - train loss: 16.8361 - test loss: 17.6293 - train acc: 0.2650 - test acc: 0.2491 - 2m 13s\n",
      "batch: 1300/1563 - train loss: 16.6239 - test loss: 16.9457 - train acc: 0.2635 - test acc: 0.2561 - 2m 18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1400/1563 - train loss: 16.7167 - test loss: 16.1021 - train acc: 0.2753 - test acc: 0.2921 - 2m 23s\n",
      "batch: 1500/1563 - train loss: 16.1845 - test loss: 17.1306 - train acc: 0.2843 - test acc: 0.2659 - 2m 28s\n",
      "batch: 1563/1563 - train loss: 16.2618 - test loss: 16.8042 - train acc: 0.2803 - test acc: 0.2725 - 2m 32s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.1243 - test loss: 18.7233 - train acc: 0.3221 - test acc: 0.2253 - 2m 37s\n",
      "batch: 200/1563 - train loss: 15.3545 - test loss: 16.1629 - train acc: 0.3068 - test acc: 0.3005 - 2m 42s\n",
      "batch: 300/1563 - train loss: 15.3607 - test loss: 16.9258 - train acc: 0.3071 - test acc: 0.2686 - 2m 47s\n",
      "batch: 400/1563 - train loss: 15.2661 - test loss: 16.8851 - train acc: 0.3081 - test acc: 0.2845 - 2m 52s\n",
      "batch: 500/1563 - train loss: 15.4954 - test loss: 16.2356 - train acc: 0.3140 - test acc: 0.2928 - 2m 57s\n",
      "batch: 600/1563 - train loss: 15.0388 - test loss: 16.8369 - train acc: 0.3216 - test acc: 0.2777 - 3m 2s\n",
      "batch: 700/1563 - train loss: 15.2440 - test loss: 16.8691 - train acc: 0.3193 - test acc: 0.2674 - 3m 7s\n",
      "batch: 800/1563 - train loss: 15.1454 - test loss: 15.9506 - train acc: 0.3175 - test acc: 0.2938 - 3m 12s\n",
      "batch: 900/1563 - train loss: 14.9767 - test loss: 15.4308 - train acc: 0.3290 - test acc: 0.3070 - 3m 17s\n",
      "batch: 1000/1563 - train loss: 14.9732 - test loss: 15.5494 - train acc: 0.3253 - test acc: 0.3088 - 3m 22s\n",
      "batch: 1100/1563 - train loss: 14.9312 - test loss: 15.5312 - train acc: 0.3337 - test acc: 0.3089 - 3m 27s\n",
      "batch: 1200/1563 - train loss: 14.6332 - test loss: 16.1646 - train acc: 0.3371 - test acc: 0.2956 - 3m 32s\n",
      "batch: 1300/1563 - train loss: 14.5889 - test loss: 15.2532 - train acc: 0.3362 - test acc: 0.3164 - 3m 37s\n",
      "batch: 1400/1563 - train loss: 14.9752 - test loss: 15.5654 - train acc: 0.3359 - test acc: 0.3060 - 3m 42s\n",
      "batch: 1500/1563 - train loss: 14.4555 - test loss: 15.2136 - train acc: 0.3462 - test acc: 0.3183 - 3m 47s\n",
      "batch: 1563/1563 - train loss: 14.4809 - test loss: 15.4659 - train acc: 0.3421 - test acc: 0.3159 - 3m 51s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.0915 - test loss: 15.3254 - train acc: 0.3956 - test acc: 0.3243 - 3m 56s\n",
      "batch: 200/1563 - train loss: 13.0157 - test loss: 15.1572 - train acc: 0.3969 - test acc: 0.3281 - 4m 2s\n",
      "batch: 300/1563 - train loss: 13.5349 - test loss: 14.6133 - train acc: 0.3725 - test acc: 0.3431 - 4m 7s\n",
      "batch: 400/1563 - train loss: 13.1701 - test loss: 16.1361 - train acc: 0.3772 - test acc: 0.2954 - 4m 12s\n",
      "batch: 500/1563 - train loss: 13.6105 - test loss: 14.7242 - train acc: 0.3690 - test acc: 0.3457 - 4m 17s\n",
      "batch: 600/1563 - train loss: 13.5937 - test loss: 16.3922 - train acc: 0.3738 - test acc: 0.3001 - 4m 21s\n",
      "batch: 700/1563 - train loss: 13.3176 - test loss: 14.5811 - train acc: 0.3894 - test acc: 0.3496 - 4m 27s\n",
      "batch: 800/1563 - train loss: 13.5874 - test loss: 16.2913 - train acc: 0.3791 - test acc: 0.2916 - 4m 32s\n",
      "batch: 900/1563 - train loss: 13.4232 - test loss: 14.6674 - train acc: 0.3731 - test acc: 0.3516 - 4m 37s\n",
      "batch: 1000/1563 - train loss: 13.2374 - test loss: 14.7442 - train acc: 0.3953 - test acc: 0.3437 - 4m 42s\n",
      "batch: 1100/1563 - train loss: 13.5142 - test loss: 17.5449 - train acc: 0.3793 - test acc: 0.2779 - 4m 47s\n",
      "batch: 1200/1563 - train loss: 13.1237 - test loss: 16.4188 - train acc: 0.3857 - test acc: 0.2987 - 4m 52s\n",
      "batch: 1300/1563 - train loss: 13.1439 - test loss: 14.5775 - train acc: 0.3925 - test acc: 0.3563 - 4m 57s\n",
      "batch: 1400/1563 - train loss: 13.3675 - test loss: 14.6183 - train acc: 0.3872 - test acc: 0.3493 - 5m 2s\n",
      "batch: 1500/1563 - train loss: 13.3607 - test loss: 13.8949 - train acc: 0.3884 - test acc: 0.3694 - 5m 7s\n",
      "batch: 1563/1563 - train loss: 13.1166 - test loss: 14.5623 - train acc: 0.3909 - test acc: 0.3583 - 5m 11s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.5462 - test loss: 14.3473 - train acc: 0.4456 - test acc: 0.3574 - 5m 16s\n",
      "batch: 200/1563 - train loss: 11.7519 - test loss: 16.3726 - train acc: 0.4406 - test acc: 0.3168 - 5m 21s\n",
      "batch: 300/1563 - train loss: 11.7481 - test loss: 15.1055 - train acc: 0.4407 - test acc: 0.3433 - 5m 26s\n",
      "batch: 400/1563 - train loss: 11.8549 - test loss: 14.5610 - train acc: 0.4437 - test acc: 0.3529 - 5m 31s\n",
      "batch: 500/1563 - train loss: 11.9193 - test loss: 14.0092 - train acc: 0.4409 - test acc: 0.3767 - 5m 36s\n",
      "batch: 600/1563 - train loss: 12.0155 - test loss: 14.6370 - train acc: 0.4204 - test acc: 0.3554 - 5m 41s\n",
      "batch: 700/1563 - train loss: 12.2537 - test loss: 14.1222 - train acc: 0.4198 - test acc: 0.3748 - 5m 46s\n",
      "batch: 800/1563 - train loss: 12.3138 - test loss: 14.5463 - train acc: 0.4354 - test acc: 0.3519 - 5m 52s\n",
      "batch: 900/1563 - train loss: 12.0524 - test loss: 14.4528 - train acc: 0.4341 - test acc: 0.3552 - 5m 57s\n",
      "batch: 1000/1563 - train loss: 11.8636 - test loss: 14.9590 - train acc: 0.4375 - test acc: 0.3490 - 6m 2s\n",
      "batch: 1100/1563 - train loss: 12.0643 - test loss: 14.3821 - train acc: 0.4359 - test acc: 0.3645 - 6m 7s\n",
      "batch: 1200/1563 - train loss: 12.1050 - test loss: 15.7628 - train acc: 0.4316 - test acc: 0.3342 - 6m 12s\n",
      "batch: 1300/1563 - train loss: 11.8657 - test loss: 15.3448 - train acc: 0.4397 - test acc: 0.3520 - 6m 17s\n",
      "batch: 1400/1563 - train loss: 11.9954 - test loss: 15.3233 - train acc: 0.4347 - test acc: 0.3391 - 6m 21s\n",
      "batch: 1500/1563 - train loss: 12.1283 - test loss: 14.2837 - train acc: 0.4306 - test acc: 0.3655 - 6m 26s\n",
      "batch: 1563/1563 - train loss: 12.0116 - test loss: 13.8438 - train acc: 0.4319 - test acc: 0.3876 - 6m 31s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.8683 - test loss: 13.5674 - train acc: 0.5162 - test acc: 0.3938 - 6m 36s\n",
      "batch: 200/1563 - train loss: 10.3060 - test loss: 13.7326 - train acc: 0.5056 - test acc: 0.3919 - 6m 41s\n",
      "batch: 300/1563 - train loss: 10.5446 - test loss: 14.1624 - train acc: 0.4950 - test acc: 0.3791 - 6m 46s\n",
      "batch: 400/1563 - train loss: 10.5001 - test loss: 14.9445 - train acc: 0.4909 - test acc: 0.3533 - 6m 51s\n",
      "batch: 500/1563 - train loss: 10.7898 - test loss: 14.1396 - train acc: 0.4879 - test acc: 0.3799 - 6m 56s\n",
      "batch: 600/1563 - train loss: 10.1954 - test loss: 14.3597 - train acc: 0.5016 - test acc: 0.3736 - 7m 1s\n",
      "batch: 700/1563 - train loss: 10.8622 - test loss: 14.2698 - train acc: 0.4669 - test acc: 0.3762 - 7m 6s\n",
      "batch: 800/1563 - train loss: 10.7910 - test loss: 14.2082 - train acc: 0.4866 - test acc: 0.3755 - 7m 11s\n",
      "batch: 900/1563 - train loss: 10.9277 - test loss: 13.8952 - train acc: 0.4722 - test acc: 0.3858 - 7m 16s\n",
      "batch: 1000/1563 - train loss: 11.1467 - test loss: 13.6972 - train acc: 0.4616 - test acc: 0.3954 - 7m 21s\n",
      "batch: 1100/1563 - train loss: 10.8812 - test loss: 14.1984 - train acc: 0.4713 - test acc: 0.3797 - 7m 25s\n",
      "batch: 1200/1563 - train loss: 10.6757 - test loss: 14.4989 - train acc: 0.4804 - test acc: 0.3697 - 7m 30s\n",
      "batch: 1300/1563 - train loss: 11.1056 - test loss: 14.0213 - train acc: 0.4559 - test acc: 0.3858 - 7m 35s\n",
      "batch: 1400/1563 - train loss: 10.8543 - test loss: 13.6717 - train acc: 0.4722 - test acc: 0.3919 - 7m 41s\n",
      "batch: 1500/1563 - train loss: 10.9798 - test loss: 13.9266 - train acc: 0.4691 - test acc: 0.3883 - 7m 45s\n",
      "batch: 1563/1563 - train loss: 11.0041 - test loss: 13.6621 - train acc: 0.4560 - test acc: 0.3896 - 7m 49s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.7513 - test loss: 14.1343 - train acc: 0.5653 - test acc: 0.3936 - 7m 54s\n",
      "batch: 200/1563 - train loss: 9.0854 - test loss: 14.5378 - train acc: 0.5494 - test acc: 0.3817 - 7m 59s\n",
      "batch: 300/1563 - train loss: 9.3979 - test loss: 13.7036 - train acc: 0.5406 - test acc: 0.3972 - 8m 4s\n",
      "batch: 400/1563 - train loss: 9.5766 - test loss: 13.9170 - train acc: 0.5197 - test acc: 0.3942 - 8m 9s\n",
      "batch: 500/1563 - train loss: 9.2624 - test loss: 13.7652 - train acc: 0.5538 - test acc: 0.3990 - 8m 14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 600/1563 - train loss: 9.1837 - test loss: 15.8273 - train acc: 0.5453 - test acc: 0.3503 - 8m 19s\n",
      "batch: 700/1563 - train loss: 9.7248 - test loss: 13.5623 - train acc: 0.5260 - test acc: 0.4066 - 8m 23s\n",
      "batch: 800/1563 - train loss: 9.8079 - test loss: 13.7231 - train acc: 0.5259 - test acc: 0.4042 - 8m 28s\n",
      "batch: 900/1563 - train loss: 10.0132 - test loss: 13.6791 - train acc: 0.5163 - test acc: 0.4035 - 8m 33s\n",
      "batch: 1000/1563 - train loss: 9.9901 - test loss: 13.7310 - train acc: 0.5138 - test acc: 0.4056 - 8m 38s\n",
      "batch: 1100/1563 - train loss: 9.7010 - test loss: 13.3049 - train acc: 0.5353 - test acc: 0.4070 - 8m 43s\n",
      "batch: 1200/1563 - train loss: 9.5203 - test loss: 13.4708 - train acc: 0.5222 - test acc: 0.4057 - 8m 48s\n",
      "batch: 1300/1563 - train loss: 9.7788 - test loss: 13.8184 - train acc: 0.5231 - test acc: 0.3987 - 8m 53s\n",
      "batch: 1400/1563 - train loss: 10.1728 - test loss: 14.3975 - train acc: 0.5085 - test acc: 0.3768 - 8m 58s\n",
      "batch: 1500/1563 - train loss: 9.9297 - test loss: 13.0889 - train acc: 0.5206 - test acc: 0.4154 - 9m 3s\n",
      "batch: 1563/1563 - train loss: 9.5923 - test loss: 14.6240 - train acc: 0.5325 - test acc: 0.3763 - 9m 7s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.6462 - test loss: 14.3543 - train acc: 0.6112 - test acc: 0.3925 - 9m 12s\n",
      "batch: 200/1563 - train loss: 7.6392 - test loss: 13.7754 - train acc: 0.6078 - test acc: 0.4132 - 9m 17s\n",
      "batch: 300/1563 - train loss: 8.1643 - test loss: 14.0329 - train acc: 0.5919 - test acc: 0.4076 - 9m 22s\n",
      "batch: 400/1563 - train loss: 8.3077 - test loss: 13.7591 - train acc: 0.5781 - test acc: 0.4078 - 9m 27s\n",
      "batch: 500/1563 - train loss: 8.2848 - test loss: 14.2132 - train acc: 0.5818 - test acc: 0.4000 - 9m 32s\n",
      "batch: 600/1563 - train loss: 8.1150 - test loss: 13.8206 - train acc: 0.5859 - test acc: 0.4131 - 9m 36s\n",
      "batch: 700/1563 - train loss: 8.6591 - test loss: 14.3520 - train acc: 0.5615 - test acc: 0.3924 - 9m 41s\n",
      "batch: 800/1563 - train loss: 8.6808 - test loss: 13.7848 - train acc: 0.5641 - test acc: 0.4095 - 9m 46s\n",
      "batch: 900/1563 - train loss: 8.6380 - test loss: 13.9985 - train acc: 0.5831 - test acc: 0.4005 - 9m 51s\n",
      "batch: 1000/1563 - train loss: 8.9551 - test loss: 14.1151 - train acc: 0.5625 - test acc: 0.4009 - 9m 56s\n",
      "batch: 1100/1563 - train loss: 8.6706 - test loss: 13.5918 - train acc: 0.5637 - test acc: 0.4203 - 10m 1s\n",
      "batch: 1200/1563 - train loss: 8.8767 - test loss: 15.0924 - train acc: 0.5569 - test acc: 0.3756 - 10m 6s\n",
      "batch: 1300/1563 - train loss: 9.1328 - test loss: 14.8447 - train acc: 0.5444 - test acc: 0.3866 - 10m 10s\n",
      "batch: 1400/1563 - train loss: 8.5609 - test loss: 14.4105 - train acc: 0.5753 - test acc: 0.4009 - 10m 15s\n",
      "batch: 1500/1563 - train loss: 8.7948 - test loss: 13.4606 - train acc: 0.5662 - test acc: 0.4185 - 10m 20s\n",
      "batch: 1563/1563 - train loss: 8.9650 - test loss: 13.1799 - train acc: 0.5616 - test acc: 0.4303 - 10m 25s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.3217 - test loss: 13.6744 - train acc: 0.6779 - test acc: 0.4274 - 10m 30s\n",
      "batch: 200/1563 - train loss: 6.5344 - test loss: 14.7522 - train acc: 0.6597 - test acc: 0.4030 - 10m 35s\n",
      "batch: 300/1563 - train loss: 6.7524 - test loss: 14.4080 - train acc: 0.6425 - test acc: 0.4126 - 10m 40s\n",
      "batch: 400/1563 - train loss: 6.7319 - test loss: 14.7062 - train acc: 0.6447 - test acc: 0.4019 - 10m 44s\n",
      "batch: 500/1563 - train loss: 7.1293 - test loss: 14.3612 - train acc: 0.6284 - test acc: 0.4101 - 10m 50s\n",
      "batch: 600/1563 - train loss: 7.1595 - test loss: 16.1532 - train acc: 0.6310 - test acc: 0.3802 - 10m 55s\n",
      "batch: 700/1563 - train loss: 7.6691 - test loss: 14.4710 - train acc: 0.6072 - test acc: 0.3982 - 11m 0s\n",
      "batch: 800/1563 - train loss: 7.6255 - test loss: 14.0532 - train acc: 0.6072 - test acc: 0.4147 - 11m 5s\n",
      "batch: 900/1563 - train loss: 7.3144 - test loss: 14.3072 - train acc: 0.6153 - test acc: 0.4160 - 11m 10s\n",
      "batch: 1000/1563 - train loss: 7.9950 - test loss: 14.2366 - train acc: 0.6015 - test acc: 0.4127 - 11m 14s\n",
      "batch: 1100/1563 - train loss: 7.7135 - test loss: 14.0461 - train acc: 0.6104 - test acc: 0.4154 - 11m 19s\n",
      "batch: 1200/1563 - train loss: 7.9754 - test loss: 14.2922 - train acc: 0.5937 - test acc: 0.4069 - 11m 25s\n",
      "batch: 1300/1563 - train loss: 8.0198 - test loss: 14.0807 - train acc: 0.5884 - test acc: 0.4058 - 11m 30s\n",
      "batch: 1400/1563 - train loss: 7.8984 - test loss: 14.1901 - train acc: 0.5988 - test acc: 0.4145 - 11m 35s\n",
      "batch: 1500/1563 - train loss: 8.3691 - test loss: 13.8223 - train acc: 0.5853 - test acc: 0.4183 - 11m 40s\n",
      "batch: 1563/1563 - train loss: 8.3053 - test loss: 13.6755 - train acc: 0.5869 - test acc: 0.4200 - 11m 44s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.5879 - test loss: 13.8896 - train acc: 0.7166 - test acc: 0.4270 - 11m 49s\n",
      "batch: 200/1563 - train loss: 5.5500 - test loss: 14.3315 - train acc: 0.7038 - test acc: 0.4201 - 11m 54s\n",
      "batch: 300/1563 - train loss: 5.8302 - test loss: 14.9384 - train acc: 0.6859 - test acc: 0.4087 - 11m 59s\n",
      "batch: 400/1563 - train loss: 6.0910 - test loss: 15.0174 - train acc: 0.6847 - test acc: 0.4057 - 12m 4s\n",
      "batch: 500/1563 - train loss: 6.2067 - test loss: 14.8392 - train acc: 0.6832 - test acc: 0.4116 - 12m 9s\n",
      "batch: 600/1563 - train loss: 6.5184 - test loss: 15.2823 - train acc: 0.6616 - test acc: 0.3956 - 12m 14s\n",
      "batch: 700/1563 - train loss: 6.4716 - test loss: 14.9662 - train acc: 0.6725 - test acc: 0.4081 - 12m 19s\n",
      "batch: 800/1563 - train loss: 6.5857 - test loss: 14.0968 - train acc: 0.6547 - test acc: 0.4246 - 12m 23s\n",
      "batch: 900/1563 - train loss: 6.2981 - test loss: 16.0821 - train acc: 0.6622 - test acc: 0.3931 - 12m 29s\n",
      "batch: 1000/1563 - train loss: 6.8179 - test loss: 17.2922 - train acc: 0.6535 - test acc: 0.3617 - 12m 34s\n",
      "batch: 1100/1563 - train loss: 7.0361 - test loss: 14.4866 - train acc: 0.6409 - test acc: 0.4246 - 12m 39s\n",
      "batch: 1200/1563 - train loss: 6.6996 - test loss: 15.1735 - train acc: 0.6509 - test acc: 0.4080 - 12m 44s\n",
      "batch: 1300/1563 - train loss: 7.1651 - test loss: 14.6595 - train acc: 0.6322 - test acc: 0.4087 - 12m 49s\n",
      "batch: 1400/1563 - train loss: 7.2140 - test loss: 14.7911 - train acc: 0.6228 - test acc: 0.4012 - 12m 54s\n",
      "batch: 1500/1563 - train loss: 7.1465 - test loss: 14.5197 - train acc: 0.6335 - test acc: 0.4091 - 12m 59s\n",
      "batch: 1563/1563 - train loss: 7.1282 - test loss: 14.3355 - train acc: 0.6350 - test acc: 0.4201 - 13m 3s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.5415 - test loss: 14.3054 - train acc: 0.7637 - test acc: 0.4350 - 13m 8s\n",
      "batch: 200/1563 - train loss: 4.5070 - test loss: 14.6136 - train acc: 0.7594 - test acc: 0.4275 - 13m 12s\n",
      "batch: 300/1563 - train loss: 4.6505 - test loss: 14.8698 - train acc: 0.7562 - test acc: 0.4219 - 13m 17s\n",
      "batch: 400/1563 - train loss: 4.8172 - test loss: 15.8781 - train acc: 0.7394 - test acc: 0.4025 - 13m 24s\n",
      "batch: 500/1563 - train loss: 5.5027 - test loss: 16.1530 - train acc: 0.7187 - test acc: 0.3965 - 13m 29s\n",
      "batch: 600/1563 - train loss: 5.2159 - test loss: 15.1993 - train acc: 0.7185 - test acc: 0.4241 - 13m 34s\n",
      "batch: 700/1563 - train loss: 5.7370 - test loss: 14.6168 - train acc: 0.7047 - test acc: 0.4229 - 13m 39s\n",
      "batch: 800/1563 - train loss: 5.3960 - test loss: 15.0791 - train acc: 0.7094 - test acc: 0.4235 - 13m 43s\n",
      "batch: 900/1563 - train loss: 5.6511 - test loss: 15.5681 - train acc: 0.7047 - test acc: 0.4062 - 13m 48s\n",
      "batch: 1000/1563 - train loss: 5.6639 - test loss: 15.4745 - train acc: 0.7041 - test acc: 0.4048 - 13m 53s\n",
      "batch: 1100/1563 - train loss: 5.9574 - test loss: 14.8540 - train acc: 0.6844 - test acc: 0.4265 - 13m 58s\n",
      "batch: 1200/1563 - train loss: 6.1333 - test loss: 15.3003 - train acc: 0.6897 - test acc: 0.4108 - 14m 3s\n",
      "batch: 1300/1563 - train loss: 5.8116 - test loss: 15.5764 - train acc: 0.6897 - test acc: 0.3968 - 14m 8s\n",
      "batch: 1400/1563 - train loss: 6.1396 - test loss: 14.6438 - train acc: 0.6790 - test acc: 0.4236 - 14m 13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1500/1563 - train loss: 6.4538 - test loss: 14.7082 - train acc: 0.6644 - test acc: 0.4206 - 14m 18s\n",
      "batch: 1563/1563 - train loss: 6.4238 - test loss: 15.0024 - train acc: 0.6713 - test acc: 0.4103 - 14m 22s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.7537 - test loss: 14.8353 - train acc: 0.8021 - test acc: 0.4316 - 14m 27s\n",
      "batch: 200/1563 - train loss: 4.0086 - test loss: 15.2890 - train acc: 0.7878 - test acc: 0.4247 - 14m 32s\n",
      "batch: 300/1563 - train loss: 4.1391 - test loss: 16.0360 - train acc: 0.7737 - test acc: 0.4135 - 14m 37s\n",
      "batch: 400/1563 - train loss: 4.2072 - test loss: 15.4531 - train acc: 0.7716 - test acc: 0.4236 - 14m 42s\n",
      "batch: 500/1563 - train loss: 4.1613 - test loss: 15.6804 - train acc: 0.7812 - test acc: 0.4191 - 14m 47s\n",
      "batch: 600/1563 - train loss: 4.7025 - test loss: 16.1619 - train acc: 0.7431 - test acc: 0.4152 - 14m 52s\n",
      "batch: 700/1563 - train loss: 4.6353 - test loss: 16.5135 - train acc: 0.7490 - test acc: 0.3935 - 14m 57s\n",
      "batch: 800/1563 - train loss: 4.6500 - test loss: 15.4385 - train acc: 0.7541 - test acc: 0.4274 - 15m 2s\n",
      "batch: 900/1563 - train loss: 4.4000 - test loss: 16.2139 - train acc: 0.7575 - test acc: 0.4148 - 15m 7s\n",
      "batch: 1000/1563 - train loss: 5.0323 - test loss: 15.6462 - train acc: 0.7244 - test acc: 0.4294 - 15m 12s\n",
      "batch: 1100/1563 - train loss: 5.1081 - test loss: 15.2832 - train acc: 0.7287 - test acc: 0.4252 - 15m 17s\n",
      "batch: 1200/1563 - train loss: 5.0162 - test loss: 15.8632 - train acc: 0.7281 - test acc: 0.4107 - 15m 23s\n",
      "batch: 1300/1563 - train loss: 5.2107 - test loss: 15.7987 - train acc: 0.7191 - test acc: 0.4138 - 15m 28s\n",
      "batch: 1400/1563 - train loss: 5.2822 - test loss: 15.4945 - train acc: 0.7157 - test acc: 0.4206 - 15m 33s\n",
      "batch: 1500/1563 - train loss: 5.5968 - test loss: 15.3014 - train acc: 0.6907 - test acc: 0.4230 - 15m 39s\n",
      "batch: 1563/1563 - train loss: 5.4894 - test loss: 16.1135 - train acc: 0.7047 - test acc: 0.4150 - 15m 43s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.1808 - test loss: 16.1050 - train acc: 0.8268 - test acc: 0.4195 - 15m 48s\n",
      "batch: 200/1563 - train loss: 3.0207 - test loss: 16.0292 - train acc: 0.8299 - test acc: 0.4273 - 15m 53s\n",
      "batch: 300/1563 - train loss: 3.2149 - test loss: 16.3465 - train acc: 0.8196 - test acc: 0.4185 - 15m 58s\n",
      "batch: 400/1563 - train loss: 3.5827 - test loss: 17.4154 - train acc: 0.8003 - test acc: 0.3942 - 16m 3s\n",
      "batch: 500/1563 - train loss: 3.5786 - test loss: 16.7565 - train acc: 0.7965 - test acc: 0.4100 - 16m 8s\n",
      "batch: 600/1563 - train loss: 3.8132 - test loss: 17.0512 - train acc: 0.7893 - test acc: 0.4077 - 16m 13s\n",
      "batch: 700/1563 - train loss: 4.0376 - test loss: 16.2362 - train acc: 0.7756 - test acc: 0.4161 - 16m 19s\n",
      "batch: 800/1563 - train loss: 3.8140 - test loss: 16.7402 - train acc: 0.7856 - test acc: 0.4129 - 16m 24s\n",
      "batch: 900/1563 - train loss: 4.1607 - test loss: 16.6646 - train acc: 0.7699 - test acc: 0.4136 - 16m 29s\n",
      "batch: 1000/1563 - train loss: 4.0922 - test loss: 16.5853 - train acc: 0.7710 - test acc: 0.4079 - 16m 34s\n",
      "batch: 1100/1563 - train loss: 4.3749 - test loss: 16.1120 - train acc: 0.7643 - test acc: 0.4228 - 16m 39s\n",
      "batch: 1200/1563 - train loss: 4.2138 - test loss: 16.8788 - train acc: 0.7675 - test acc: 0.4096 - 16m 44s\n",
      "batch: 1300/1563 - train loss: 4.3299 - test loss: 17.2760 - train acc: 0.7662 - test acc: 0.4033 - 16m 49s\n",
      "batch: 1400/1563 - train loss: 4.5715 - test loss: 16.8104 - train acc: 0.7462 - test acc: 0.4131 - 16m 54s\n",
      "batch: 1500/1563 - train loss: 4.9662 - test loss: 16.3350 - train acc: 0.7329 - test acc: 0.4202 - 16m 59s\n",
      "batch: 1563/1563 - train loss: 4.9416 - test loss: 15.6801 - train acc: 0.7291 - test acc: 0.4318 - 17m 3s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8984 - test loss: 15.7057 - train acc: 0.8475 - test acc: 0.4313 - 17m 8s\n",
      "batch: 200/1563 - train loss: 2.4764 - test loss: 16.4330 - train acc: 0.8631 - test acc: 0.4226 - 17m 14s\n",
      "batch: 300/1563 - train loss: 2.4707 - test loss: 16.9791 - train acc: 0.8629 - test acc: 0.4290 - 17m 20s\n",
      "batch: 400/1563 - train loss: 2.8214 - test loss: 16.6085 - train acc: 0.8362 - test acc: 0.4326 - 17m 25s\n",
      "batch: 500/1563 - train loss: 2.7682 - test loss: 17.0089 - train acc: 0.8384 - test acc: 0.4261 - 17m 30s\n",
      "batch: 600/1563 - train loss: 3.2126 - test loss: 16.9856 - train acc: 0.8144 - test acc: 0.4195 - 17m 35s\n",
      "batch: 700/1563 - train loss: 3.2348 - test loss: 17.2001 - train acc: 0.8184 - test acc: 0.4100 - 17m 41s\n",
      "batch: 800/1563 - train loss: 3.0624 - test loss: 16.7130 - train acc: 0.8233 - test acc: 0.4253 - 17m 46s\n",
      "batch: 900/1563 - train loss: 3.3420 - test loss: 17.6637 - train acc: 0.8152 - test acc: 0.4160 - 17m 51s\n",
      "batch: 1000/1563 - train loss: 3.4037 - test loss: 16.9720 - train acc: 0.8065 - test acc: 0.4199 - 17m 56s\n",
      "batch: 1100/1563 - train loss: 3.5849 - test loss: 17.0400 - train acc: 0.7909 - test acc: 0.4144 - 18m 1s\n",
      "batch: 1200/1563 - train loss: 3.8124 - test loss: 17.1183 - train acc: 0.7881 - test acc: 0.4185 - 18m 6s\n",
      "batch: 1300/1563 - train loss: 3.8241 - test loss: 17.1147 - train acc: 0.7774 - test acc: 0.4106 - 18m 11s\n",
      "batch: 1400/1563 - train loss: 3.9218 - test loss: 17.2143 - train acc: 0.7808 - test acc: 0.4135 - 18m 16s\n",
      "batch: 1500/1563 - train loss: 3.9443 - test loss: 16.9958 - train acc: 0.7784 - test acc: 0.4235 - 18m 22s\n",
      "batch: 1563/1563 - train loss: 4.0508 - test loss: 16.7759 - train acc: 0.7750 - test acc: 0.4247 - 18m 26s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.2735 - test loss: 17.4248 - train acc: 0.8741 - test acc: 0.4100 - 18m 31s\n",
      "batch: 200/1563 - train loss: 2.1752 - test loss: 17.5819 - train acc: 0.8794 - test acc: 0.4177 - 18m 36s\n",
      "batch: 300/1563 - train loss: 2.2611 - test loss: 19.1282 - train acc: 0.8737 - test acc: 0.3941 - 18m 41s\n",
      "batch: 400/1563 - train loss: 2.1658 - test loss: 18.1726 - train acc: 0.8784 - test acc: 0.4219 - 18m 46s\n",
      "batch: 500/1563 - train loss: 2.5995 - test loss: 17.7216 - train acc: 0.8488 - test acc: 0.4244 - 18m 52s\n",
      "batch: 600/1563 - train loss: 2.6953 - test loss: 17.3491 - train acc: 0.8487 - test acc: 0.4217 - 18m 57s\n",
      "batch: 700/1563 - train loss: 2.5971 - test loss: 17.5271 - train acc: 0.8591 - test acc: 0.4203 - 19m 1s\n",
      "batch: 800/1563 - train loss: 2.7912 - test loss: 17.6086 - train acc: 0.8462 - test acc: 0.4192 - 19m 6s\n",
      "batch: 900/1563 - train loss: 2.6974 - test loss: 18.6462 - train acc: 0.8490 - test acc: 0.4090 - 19m 11s\n",
      "batch: 1000/1563 - train loss: 2.9043 - test loss: 17.4362 - train acc: 0.8394 - test acc: 0.4329 - 19m 16s\n",
      "batch: 1100/1563 - train loss: 2.9629 - test loss: 17.8650 - train acc: 0.8302 - test acc: 0.4224 - 19m 21s\n",
      "batch: 1200/1563 - train loss: 3.2362 - test loss: 18.7678 - train acc: 0.8240 - test acc: 0.4126 - 19m 26s\n",
      "batch: 1300/1563 - train loss: 3.5836 - test loss: 18.3675 - train acc: 0.8028 - test acc: 0.4041 - 19m 31s\n",
      "batch: 1400/1563 - train loss: 3.3601 - test loss: 17.5409 - train acc: 0.8071 - test acc: 0.4158 - 19m 36s\n",
      "batch: 1500/1563 - train loss: 3.3895 - test loss: 17.3662 - train acc: 0.8091 - test acc: 0.4212 - 19m 41s\n",
      "batch: 1563/1563 - train loss: 3.5315 - test loss: 17.2253 - train acc: 0.8028 - test acc: 0.4213 - 19m 46s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.8232 - test loss: 17.7475 - train acc: 0.9017 - test acc: 0.4231 - 19m 51s\n",
      "batch: 200/1563 - train loss: 1.8395 - test loss: 18.1911 - train acc: 0.9025 - test acc: 0.4207 - 19m 57s\n",
      "time is up! finishing training\n",
      "batch: 243/1563 - train loss: 1.7590 - test loss: 18.1885 - train acc: 0.9063 - test acc: 0.4273 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 26.1413 - test loss: 25.1279 - train acc: 0.0420 - test acc: 0.0666 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.2340 - test loss: 23.2539 - train acc: 0.0651 - test acc: 0.0838 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.0261 - test loss: 22.8523 - train acc: 0.0868 - test acc: 0.0908 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.3180 - test loss: 21.4572 - train acc: 0.0996 - test acc: 0.1221 - 0m 16s\n",
      "batch: 500/1563 - train loss: 21.8881 - test loss: 21.0903 - train acc: 0.1169 - test acc: 0.1349 - 0m 21s\n",
      "batch: 600/1563 - train loss: 21.3262 - test loss: 20.8301 - train acc: 0.1218 - test acc: 0.1372 - 0m 26s\n",
      "batch: 700/1563 - train loss: 20.9360 - test loss: 20.6491 - train acc: 0.1369 - test acc: 0.1441 - 0m 31s\n",
      "batch: 800/1563 - train loss: 20.6004 - test loss: 20.3420 - train acc: 0.1419 - test acc: 0.1549 - 0m 36s\n",
      "batch: 900/1563 - train loss: 20.5764 - test loss: 20.3421 - train acc: 0.1519 - test acc: 0.1573 - 0m 41s\n",
      "batch: 1000/1563 - train loss: 20.1110 - test loss: 20.0155 - train acc: 0.1575 - test acc: 0.1544 - 0m 46s\n",
      "batch: 1100/1563 - train loss: 19.7734 - test loss: 19.3209 - train acc: 0.1750 - test acc: 0.1840 - 0m 51s\n",
      "batch: 1200/1563 - train loss: 19.5301 - test loss: 19.1825 - train acc: 0.1822 - test acc: 0.1840 - 0m 56s\n",
      "batch: 1300/1563 - train loss: 19.3690 - test loss: 19.6863 - train acc: 0.1738 - test acc: 0.1794 - 1m 0s\n",
      "batch: 1400/1563 - train loss: 19.1166 - test loss: 19.3254 - train acc: 0.1820 - test acc: 0.1813 - 1m 5s\n",
      "batch: 1500/1563 - train loss: 18.9659 - test loss: 18.4865 - train acc: 0.1897 - test acc: 0.2121 - 1m 10s\n",
      "batch: 1563/1563 - train loss: 18.8798 - test loss: 18.2755 - train acc: 0.1975 - test acc: 0.2140 - 1m 14s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 18.0995 - test loss: 17.9954 - train acc: 0.2138 - test acc: 0.2262 - 1m 19s\n",
      "batch: 200/1563 - train loss: 17.9856 - test loss: 20.2905 - train acc: 0.2197 - test acc: 0.1609 - 1m 25s\n",
      "batch: 300/1563 - train loss: 17.9351 - test loss: 17.7578 - train acc: 0.2198 - test acc: 0.2334 - 1m 29s\n",
      "batch: 400/1563 - train loss: 17.8150 - test loss: 20.2188 - train acc: 0.2251 - test acc: 0.1869 - 1m 34s\n",
      "batch: 500/1563 - train loss: 17.8762 - test loss: 20.2673 - train acc: 0.2282 - test acc: 0.1780 - 1m 39s\n",
      "batch: 600/1563 - train loss: 17.7596 - test loss: 18.0667 - train acc: 0.2322 - test acc: 0.2275 - 1m 44s\n",
      "batch: 700/1563 - train loss: 17.4475 - test loss: 17.7843 - train acc: 0.2384 - test acc: 0.2351 - 1m 49s\n",
      "batch: 800/1563 - train loss: 16.9789 - test loss: 17.8630 - train acc: 0.2485 - test acc: 0.2299 - 1m 54s\n",
      "batch: 900/1563 - train loss: 17.3597 - test loss: 18.2856 - train acc: 0.2428 - test acc: 0.2300 - 1m 59s\n",
      "batch: 1000/1563 - train loss: 16.9283 - test loss: 17.1810 - train acc: 0.2687 - test acc: 0.2478 - 2m 4s\n",
      "batch: 1100/1563 - train loss: 17.0035 - test loss: 17.9171 - train acc: 0.2625 - test acc: 0.2406 - 2m 9s\n",
      "batch: 1200/1563 - train loss: 16.7249 - test loss: 18.2691 - train acc: 0.2690 - test acc: 0.2329 - 2m 13s\n",
      "batch: 1300/1563 - train loss: 16.5463 - test loss: 18.7073 - train acc: 0.2762 - test acc: 0.2184 - 2m 19s\n",
      "batch: 1400/1563 - train loss: 16.4291 - test loss: 16.8942 - train acc: 0.2797 - test acc: 0.2573 - 2m 24s\n",
      "batch: 1500/1563 - train loss: 16.3582 - test loss: 16.4773 - train acc: 0.2659 - test acc: 0.2747 - 2m 29s\n",
      "batch: 1563/1563 - train loss: 16.2859 - test loss: 17.1843 - train acc: 0.2778 - test acc: 0.2633 - 2m 33s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.3387 - test loss: 17.0404 - train acc: 0.3016 - test acc: 0.2672 - 2m 38s\n",
      "batch: 200/1563 - train loss: 15.2858 - test loss: 16.3708 - train acc: 0.3084 - test acc: 0.2804 - 2m 43s\n",
      "batch: 300/1563 - train loss: 15.2885 - test loss: 16.7805 - train acc: 0.3106 - test acc: 0.2710 - 2m 47s\n",
      "batch: 400/1563 - train loss: 15.5086 - test loss: 17.6628 - train acc: 0.3099 - test acc: 0.2490 - 2m 52s\n",
      "batch: 500/1563 - train loss: 14.9889 - test loss: 16.1193 - train acc: 0.3181 - test acc: 0.2914 - 2m 57s\n",
      "batch: 600/1563 - train loss: 15.1603 - test loss: 16.6015 - train acc: 0.3200 - test acc: 0.2684 - 3m 2s\n",
      "batch: 700/1563 - train loss: 15.3158 - test loss: 15.5242 - train acc: 0.3150 - test acc: 0.3152 - 3m 7s\n",
      "batch: 800/1563 - train loss: 15.1607 - test loss: 16.3850 - train acc: 0.3084 - test acc: 0.2842 - 3m 11s\n",
      "batch: 900/1563 - train loss: 14.9450 - test loss: 16.3124 - train acc: 0.3275 - test acc: 0.2921 - 3m 16s\n",
      "batch: 1000/1563 - train loss: 15.0489 - test loss: 16.1069 - train acc: 0.3234 - test acc: 0.2914 - 3m 21s\n",
      "batch: 1100/1563 - train loss: 15.3252 - test loss: 16.9740 - train acc: 0.3065 - test acc: 0.2735 - 3m 27s\n",
      "batch: 1200/1563 - train loss: 14.8537 - test loss: 15.6385 - train acc: 0.3325 - test acc: 0.3122 - 3m 31s\n",
      "batch: 1300/1563 - train loss: 14.6935 - test loss: 16.8095 - train acc: 0.3397 - test acc: 0.2894 - 3m 36s\n",
      "batch: 1400/1563 - train loss: 14.7163 - test loss: 15.2487 - train acc: 0.3318 - test acc: 0.3177 - 3m 41s\n",
      "batch: 1500/1563 - train loss: 14.8220 - test loss: 15.2019 - train acc: 0.3296 - test acc: 0.3210 - 3m 46s\n",
      "batch: 1563/1563 - train loss: 14.5671 - test loss: 15.4320 - train acc: 0.3362 - test acc: 0.3133 - 3m 50s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.3333 - test loss: 15.4164 - train acc: 0.3850 - test acc: 0.3197 - 3m 55s\n",
      "batch: 200/1563 - train loss: 13.6298 - test loss: 15.6333 - train acc: 0.3775 - test acc: 0.3192 - 4m 0s\n",
      "batch: 300/1563 - train loss: 13.5136 - test loss: 15.4667 - train acc: 0.3850 - test acc: 0.3175 - 4m 4s\n",
      "batch: 400/1563 - train loss: 13.5130 - test loss: 15.1504 - train acc: 0.3798 - test acc: 0.3247 - 4m 9s\n",
      "batch: 500/1563 - train loss: 13.4917 - test loss: 15.2327 - train acc: 0.3731 - test acc: 0.3227 - 4m 14s\n",
      "batch: 600/1563 - train loss: 13.4257 - test loss: 19.0535 - train acc: 0.3775 - test acc: 0.2591 - 4m 19s\n",
      "batch: 700/1563 - train loss: 13.6332 - test loss: 15.7913 - train acc: 0.3679 - test acc: 0.3142 - 4m 24s\n",
      "batch: 800/1563 - train loss: 13.5350 - test loss: 16.6607 - train acc: 0.3843 - test acc: 0.2984 - 4m 29s\n",
      "batch: 900/1563 - train loss: 13.7624 - test loss: 14.5613 - train acc: 0.3709 - test acc: 0.3422 - 4m 33s\n",
      "batch: 1000/1563 - train loss: 13.3836 - test loss: 14.3260 - train acc: 0.3825 - test acc: 0.3587 - 4m 38s\n",
      "batch: 1100/1563 - train loss: 13.1902 - test loss: 15.3992 - train acc: 0.3897 - test acc: 0.3223 - 4m 43s\n",
      "batch: 1200/1563 - train loss: 13.2684 - test loss: 14.5929 - train acc: 0.3888 - test acc: 0.3453 - 4m 48s\n",
      "batch: 1300/1563 - train loss: 13.5764 - test loss: 14.5916 - train acc: 0.3759 - test acc: 0.3374 - 4m 53s\n",
      "batch: 1400/1563 - train loss: 13.2195 - test loss: 14.4019 - train acc: 0.3887 - test acc: 0.3486 - 4m 58s\n",
      "batch: 1500/1563 - train loss: 13.5270 - test loss: 14.1721 - train acc: 0.3759 - test acc: 0.3626 - 5m 2s\n",
      "batch: 1563/1563 - train loss: 13.8034 - test loss: 14.8359 - train acc: 0.3590 - test acc: 0.3415 - 5m 6s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.6359 - test loss: 15.4921 - train acc: 0.4547 - test acc: 0.3224 - 5m 11s\n",
      "batch: 200/1563 - train loss: 11.8231 - test loss: 14.0432 - train acc: 0.4338 - test acc: 0.3677 - 5m 16s\n",
      "batch: 300/1563 - train loss: 11.6482 - test loss: 14.1193 - train acc: 0.4463 - test acc: 0.3630 - 5m 21s\n",
      "batch: 400/1563 - train loss: 11.9228 - test loss: 14.5725 - train acc: 0.4253 - test acc: 0.3591 - 5m 25s\n",
      "batch: 500/1563 - train loss: 11.8765 - test loss: 14.6700 - train acc: 0.4306 - test acc: 0.3541 - 5m 30s\n",
      "batch: 600/1563 - train loss: 12.0425 - test loss: 14.0194 - train acc: 0.4356 - test acc: 0.3700 - 5m 35s\n",
      "batch: 700/1563 - train loss: 11.7715 - test loss: 14.4194 - train acc: 0.4441 - test acc: 0.3670 - 5m 40s\n",
      "batch: 800/1563 - train loss: 12.5431 - test loss: 14.7324 - train acc: 0.4091 - test acc: 0.3534 - 5m 44s\n",
      "batch: 900/1563 - train loss: 12.0795 - test loss: 14.5104 - train acc: 0.4266 - test acc: 0.3645 - 5m 49s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 12.3056 - test loss: 14.9795 - train acc: 0.4278 - test acc: 0.3446 - 5m 54s\n",
      "batch: 1100/1563 - train loss: 12.1769 - test loss: 14.7823 - train acc: 0.4259 - test acc: 0.3521 - 5m 59s\n",
      "batch: 1200/1563 - train loss: 12.1395 - test loss: 13.6887 - train acc: 0.4278 - test acc: 0.3834 - 6m 4s\n",
      "batch: 1300/1563 - train loss: 12.0825 - test loss: 13.8089 - train acc: 0.4297 - test acc: 0.3723 - 6m 9s\n",
      "batch: 1400/1563 - train loss: 12.2734 - test loss: 13.9290 - train acc: 0.4266 - test acc: 0.3799 - 6m 14s\n",
      "batch: 1500/1563 - train loss: 12.3146 - test loss: 15.0296 - train acc: 0.4216 - test acc: 0.3410 - 6m 18s\n",
      "batch: 1563/1563 - train loss: 12.2270 - test loss: 13.6223 - train acc: 0.4203 - test acc: 0.3878 - 6m 22s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.3183 - test loss: 14.2680 - train acc: 0.4928 - test acc: 0.3747 - 6m 27s\n",
      "batch: 200/1563 - train loss: 10.1632 - test loss: 17.0484 - train acc: 0.5041 - test acc: 0.3195 - 6m 32s\n",
      "batch: 300/1563 - train loss: 10.2838 - test loss: 13.9651 - train acc: 0.4997 - test acc: 0.3900 - 6m 37s\n",
      "batch: 400/1563 - train loss: 10.2109 - test loss: 14.4725 - train acc: 0.5012 - test acc: 0.3786 - 6m 42s\n",
      "batch: 500/1563 - train loss: 10.8377 - test loss: 14.1339 - train acc: 0.4910 - test acc: 0.3756 - 6m 46s\n",
      "batch: 600/1563 - train loss: 10.8690 - test loss: 13.8820 - train acc: 0.4793 - test acc: 0.3834 - 6m 51s\n",
      "batch: 700/1563 - train loss: 11.0734 - test loss: 14.4657 - train acc: 0.4754 - test acc: 0.3660 - 6m 56s\n",
      "batch: 800/1563 - train loss: 10.8635 - test loss: 14.9718 - train acc: 0.4756 - test acc: 0.3480 - 7m 1s\n",
      "batch: 900/1563 - train loss: 10.8977 - test loss: 14.5842 - train acc: 0.4731 - test acc: 0.3597 - 7m 6s\n",
      "batch: 1000/1563 - train loss: 11.1909 - test loss: 13.4575 - train acc: 0.4766 - test acc: 0.3987 - 7m 11s\n",
      "batch: 1100/1563 - train loss: 11.0295 - test loss: 14.3702 - train acc: 0.4704 - test acc: 0.3677 - 7m 16s\n",
      "batch: 1200/1563 - train loss: 11.0102 - test loss: 13.7434 - train acc: 0.4728 - test acc: 0.3945 - 7m 20s\n",
      "batch: 1300/1563 - train loss: 11.1711 - test loss: 13.9476 - train acc: 0.4647 - test acc: 0.3896 - 7m 25s\n",
      "batch: 1400/1563 - train loss: 10.8503 - test loss: 15.4041 - train acc: 0.4887 - test acc: 0.3479 - 7m 30s\n",
      "batch: 1500/1563 - train loss: 11.3679 - test loss: 14.4613 - train acc: 0.4660 - test acc: 0.3686 - 7m 35s\n",
      "batch: 1563/1563 - train loss: 11.4880 - test loss: 14.0446 - train acc: 0.4682 - test acc: 0.3746 - 7m 39s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.7015 - test loss: 13.6974 - train acc: 0.5675 - test acc: 0.4005 - 7m 44s\n",
      "batch: 200/1563 - train loss: 9.1667 - test loss: 13.6115 - train acc: 0.5503 - test acc: 0.4075 - 7m 49s\n",
      "batch: 300/1563 - train loss: 9.0429 - test loss: 13.8537 - train acc: 0.5547 - test acc: 0.3992 - 7m 54s\n",
      "batch: 400/1563 - train loss: 9.3208 - test loss: 14.6265 - train acc: 0.5375 - test acc: 0.3731 - 7m 59s\n",
      "batch: 500/1563 - train loss: 9.7338 - test loss: 13.5995 - train acc: 0.5203 - test acc: 0.4057 - 8m 4s\n",
      "batch: 600/1563 - train loss: 9.5504 - test loss: 14.1074 - train acc: 0.5300 - test acc: 0.3871 - 8m 9s\n",
      "batch: 700/1563 - train loss: 9.6634 - test loss: 14.9211 - train acc: 0.5319 - test acc: 0.3716 - 8m 14s\n",
      "batch: 800/1563 - train loss: 9.4824 - test loss: 14.2988 - train acc: 0.5228 - test acc: 0.3848 - 8m 19s\n",
      "batch: 900/1563 - train loss: 9.5517 - test loss: 13.5165 - train acc: 0.5294 - test acc: 0.4120 - 8m 24s\n",
      "batch: 1000/1563 - train loss: 10.1468 - test loss: 15.4505 - train acc: 0.5050 - test acc: 0.3539 - 8m 28s\n",
      "batch: 1100/1563 - train loss: 9.9722 - test loss: 15.6820 - train acc: 0.5122 - test acc: 0.3388 - 8m 33s\n",
      "batch: 1200/1563 - train loss: 10.0587 - test loss: 13.6985 - train acc: 0.5113 - test acc: 0.3975 - 8m 38s\n",
      "batch: 1300/1563 - train loss: 9.9516 - test loss: 13.4687 - train acc: 0.5156 - test acc: 0.4065 - 8m 44s\n",
      "batch: 1400/1563 - train loss: 10.0241 - test loss: 13.5802 - train acc: 0.5078 - test acc: 0.4065 - 8m 49s\n",
      "batch: 1500/1563 - train loss: 9.8683 - test loss: 14.3257 - train acc: 0.5222 - test acc: 0.3860 - 8m 54s\n",
      "batch: 1563/1563 - train loss: 10.2470 - test loss: 13.9141 - train acc: 0.5066 - test acc: 0.3980 - 8m 58s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.6476 - test loss: 13.6263 - train acc: 0.6222 - test acc: 0.4140 - 9m 2s\n",
      "batch: 200/1563 - train loss: 8.0143 - test loss: 14.0212 - train acc: 0.5906 - test acc: 0.4053 - 9m 7s\n",
      "batch: 300/1563 - train loss: 7.8035 - test loss: 14.3251 - train acc: 0.6012 - test acc: 0.3910 - 9m 12s\n",
      "batch: 400/1563 - train loss: 8.3900 - test loss: 15.1278 - train acc: 0.5709 - test acc: 0.3765 - 9m 17s\n",
      "batch: 500/1563 - train loss: 8.2613 - test loss: 13.9352 - train acc: 0.5875 - test acc: 0.4103 - 9m 22s\n",
      "batch: 600/1563 - train loss: 8.6300 - test loss: 14.6856 - train acc: 0.5690 - test acc: 0.3916 - 9m 27s\n",
      "batch: 700/1563 - train loss: 8.3408 - test loss: 14.2746 - train acc: 0.5834 - test acc: 0.3963 - 9m 31s\n",
      "batch: 800/1563 - train loss: 8.8530 - test loss: 15.3843 - train acc: 0.5678 - test acc: 0.3666 - 9m 36s\n",
      "batch: 900/1563 - train loss: 8.5762 - test loss: 15.8273 - train acc: 0.5756 - test acc: 0.3620 - 9m 41s\n",
      "batch: 1000/1563 - train loss: 8.7468 - test loss: 13.4854 - train acc: 0.5631 - test acc: 0.4198 - 9m 46s\n",
      "batch: 1100/1563 - train loss: 9.0608 - test loss: 14.8909 - train acc: 0.5450 - test acc: 0.3777 - 9m 51s\n",
      "batch: 1200/1563 - train loss: 8.9487 - test loss: 14.2772 - train acc: 0.5497 - test acc: 0.3956 - 9m 56s\n",
      "batch: 1300/1563 - train loss: 9.0174 - test loss: 15.2184 - train acc: 0.5481 - test acc: 0.3676 - 10m 1s\n",
      "batch: 1400/1563 - train loss: 8.9636 - test loss: 13.8656 - train acc: 0.5550 - test acc: 0.4074 - 10m 5s\n",
      "batch: 1500/1563 - train loss: 9.3775 - test loss: 14.8433 - train acc: 0.5459 - test acc: 0.3774 - 10m 10s\n",
      "batch: 1563/1563 - train loss: 9.0209 - test loss: 13.4924 - train acc: 0.5544 - test acc: 0.4160 - 10m 14s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.3246 - test loss: 14.1651 - train acc: 0.6829 - test acc: 0.4069 - 10m 19s\n",
      "batch: 200/1563 - train loss: 6.6140 - test loss: 14.8163 - train acc: 0.6579 - test acc: 0.3998 - 10m 24s\n",
      "batch: 300/1563 - train loss: 6.8787 - test loss: 14.6861 - train acc: 0.6478 - test acc: 0.4013 - 10m 29s\n",
      "batch: 400/1563 - train loss: 6.8933 - test loss: 14.4531 - train acc: 0.6372 - test acc: 0.4073 - 10m 34s\n",
      "batch: 500/1563 - train loss: 7.2853 - test loss: 14.1976 - train acc: 0.6229 - test acc: 0.4054 - 10m 39s\n",
      "batch: 600/1563 - train loss: 7.6243 - test loss: 15.3582 - train acc: 0.6112 - test acc: 0.3821 - 10m 44s\n",
      "batch: 700/1563 - train loss: 7.4335 - test loss: 14.5709 - train acc: 0.6291 - test acc: 0.4069 - 10m 49s\n",
      "batch: 800/1563 - train loss: 7.6506 - test loss: 14.6163 - train acc: 0.6072 - test acc: 0.4021 - 10m 54s\n",
      "batch: 900/1563 - train loss: 8.0974 - test loss: 14.0197 - train acc: 0.5860 - test acc: 0.4142 - 10m 58s\n",
      "batch: 1000/1563 - train loss: 7.6165 - test loss: 14.3662 - train acc: 0.6050 - test acc: 0.4011 - 11m 3s\n",
      "batch: 1100/1563 - train loss: 8.1166 - test loss: 14.3533 - train acc: 0.5956 - test acc: 0.4031 - 11m 8s\n",
      "batch: 1200/1563 - train loss: 8.3485 - test loss: 14.3192 - train acc: 0.5734 - test acc: 0.4049 - 11m 13s\n",
      "batch: 1300/1563 - train loss: 7.9299 - test loss: 14.1130 - train acc: 0.6007 - test acc: 0.4077 - 11m 18s\n",
      "batch: 1400/1563 - train loss: 7.9903 - test loss: 14.2333 - train acc: 0.5978 - test acc: 0.4168 - 11m 23s\n",
      "batch: 1500/1563 - train loss: 7.8263 - test loss: 14.0274 - train acc: 0.5968 - test acc: 0.4092 - 11m 28s\n",
      "batch: 1563/1563 - train loss: 8.0022 - test loss: 15.0281 - train acc: 0.5954 - test acc: 0.3947 - 11m 32s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.6825 - test loss: 14.1703 - train acc: 0.7037 - test acc: 0.4247 - 11m 37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 5.5523 - test loss: 14.9269 - train acc: 0.7063 - test acc: 0.4024 - 11m 42s\n",
      "batch: 300/1563 - train loss: 6.0519 - test loss: 15.2459 - train acc: 0.6844 - test acc: 0.3979 - 11m 47s\n",
      "batch: 400/1563 - train loss: 6.0495 - test loss: 15.5086 - train acc: 0.6885 - test acc: 0.3923 - 11m 52s\n",
      "batch: 500/1563 - train loss: 6.0874 - test loss: 14.8435 - train acc: 0.6853 - test acc: 0.4016 - 11m 57s\n",
      "batch: 600/1563 - train loss: 6.6062 - test loss: 14.6215 - train acc: 0.6544 - test acc: 0.4096 - 12m 2s\n",
      "batch: 700/1563 - train loss: 6.7033 - test loss: 14.9255 - train acc: 0.6600 - test acc: 0.4035 - 12m 7s\n",
      "batch: 800/1563 - train loss: 6.8209 - test loss: 15.0935 - train acc: 0.6466 - test acc: 0.3994 - 12m 12s\n",
      "batch: 900/1563 - train loss: 6.8351 - test loss: 14.8991 - train acc: 0.6378 - test acc: 0.3992 - 12m 16s\n",
      "batch: 1000/1563 - train loss: 6.6655 - test loss: 15.4489 - train acc: 0.6556 - test acc: 0.3902 - 12m 21s\n",
      "batch: 1100/1563 - train loss: 6.8121 - test loss: 14.9474 - train acc: 0.6478 - test acc: 0.4025 - 12m 26s\n",
      "batch: 1200/1563 - train loss: 7.1525 - test loss: 14.4596 - train acc: 0.6294 - test acc: 0.4092 - 12m 31s\n",
      "batch: 1300/1563 - train loss: 6.8215 - test loss: 14.2686 - train acc: 0.6500 - test acc: 0.4224 - 12m 36s\n",
      "batch: 1400/1563 - train loss: 7.0524 - test loss: 15.0238 - train acc: 0.6369 - test acc: 0.3961 - 12m 41s\n",
      "batch: 1500/1563 - train loss: 7.2078 - test loss: 14.5136 - train acc: 0.6237 - test acc: 0.4046 - 12m 46s\n",
      "batch: 1563/1563 - train loss: 7.1598 - test loss: 14.3692 - train acc: 0.6315 - test acc: 0.4109 - 12m 50s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.6146 - test loss: 14.4282 - train acc: 0.7594 - test acc: 0.4235 - 12m 56s\n",
      "batch: 200/1563 - train loss: 4.6537 - test loss: 15.4410 - train acc: 0.7481 - test acc: 0.4026 - 13m 0s\n",
      "batch: 300/1563 - train loss: 4.8665 - test loss: 15.3696 - train acc: 0.7378 - test acc: 0.4117 - 13m 5s\n",
      "batch: 400/1563 - train loss: 4.9758 - test loss: 15.3942 - train acc: 0.7307 - test acc: 0.4041 - 13m 10s\n",
      "batch: 500/1563 - train loss: 5.1882 - test loss: 16.3546 - train acc: 0.7281 - test acc: 0.3908 - 13m 15s\n",
      "batch: 600/1563 - train loss: 5.1353 - test loss: 15.8100 - train acc: 0.7188 - test acc: 0.4012 - 13m 20s\n",
      "batch: 700/1563 - train loss: 5.4861 - test loss: 17.4036 - train acc: 0.7084 - test acc: 0.3651 - 13m 25s\n",
      "batch: 800/1563 - train loss: 5.5674 - test loss: 15.4985 - train acc: 0.7081 - test acc: 0.4059 - 13m 31s\n",
      "batch: 900/1563 - train loss: 6.1440 - test loss: 15.5935 - train acc: 0.6801 - test acc: 0.4030 - 13m 35s\n",
      "batch: 1000/1563 - train loss: 5.9979 - test loss: 15.3021 - train acc: 0.6935 - test acc: 0.4131 - 13m 40s\n",
      "batch: 1100/1563 - train loss: 6.1286 - test loss: 17.1416 - train acc: 0.6772 - test acc: 0.3728 - 13m 45s\n",
      "batch: 1200/1563 - train loss: 6.1759 - test loss: 15.6713 - train acc: 0.6835 - test acc: 0.4031 - 13m 50s\n",
      "batch: 1300/1563 - train loss: 6.2894 - test loss: 15.5613 - train acc: 0.6713 - test acc: 0.4004 - 13m 55s\n",
      "batch: 1400/1563 - train loss: 6.6383 - test loss: 15.1257 - train acc: 0.6503 - test acc: 0.4117 - 14m 0s\n",
      "batch: 1500/1563 - train loss: 6.4236 - test loss: 14.4986 - train acc: 0.6613 - test acc: 0.4178 - 14m 5s\n",
      "batch: 1563/1563 - train loss: 6.2257 - test loss: 14.8096 - train acc: 0.6685 - test acc: 0.4119 - 14m 9s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 4.0725 - test loss: 15.5241 - train acc: 0.7831 - test acc: 0.4091 - 14m 14s\n",
      "batch: 200/1563 - train loss: 3.7984 - test loss: 15.6023 - train acc: 0.7991 - test acc: 0.4057 - 14m 18s\n",
      "batch: 300/1563 - train loss: 3.8547 - test loss: 15.6238 - train acc: 0.7931 - test acc: 0.4092 - 14m 24s\n",
      "batch: 400/1563 - train loss: 3.8696 - test loss: 15.7053 - train acc: 0.7897 - test acc: 0.4093 - 14m 28s\n",
      "batch: 500/1563 - train loss: 4.3219 - test loss: 16.3722 - train acc: 0.7662 - test acc: 0.4022 - 14m 33s\n",
      "batch: 600/1563 - train loss: 4.4255 - test loss: 15.9406 - train acc: 0.7590 - test acc: 0.4093 - 14m 39s\n",
      "batch: 700/1563 - train loss: 4.6381 - test loss: 15.5830 - train acc: 0.7425 - test acc: 0.4127 - 14m 43s\n",
      "batch: 800/1563 - train loss: 4.7356 - test loss: 16.7217 - train acc: 0.7462 - test acc: 0.3864 - 14m 48s\n",
      "batch: 900/1563 - train loss: 4.8636 - test loss: 16.3840 - train acc: 0.7372 - test acc: 0.3995 - 14m 53s\n",
      "batch: 1000/1563 - train loss: 5.0622 - test loss: 17.5479 - train acc: 0.7359 - test acc: 0.3905 - 14m 58s\n",
      "batch: 1100/1563 - train loss: 5.1570 - test loss: 15.6747 - train acc: 0.7253 - test acc: 0.4120 - 15m 3s\n",
      "batch: 1200/1563 - train loss: 5.1527 - test loss: 16.1340 - train acc: 0.7247 - test acc: 0.4066 - 15m 9s\n",
      "batch: 1300/1563 - train loss: 5.3624 - test loss: 15.8352 - train acc: 0.7231 - test acc: 0.4060 - 15m 13s\n",
      "batch: 1400/1563 - train loss: 5.3358 - test loss: 15.7844 - train acc: 0.7197 - test acc: 0.4098 - 15m 18s\n",
      "batch: 1500/1563 - train loss: 5.6116 - test loss: 15.5891 - train acc: 0.6941 - test acc: 0.4163 - 15m 23s\n",
      "batch: 1563/1563 - train loss: 5.7100 - test loss: 16.0689 - train acc: 0.6928 - test acc: 0.4069 - 15m 27s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.2473 - test loss: 15.6992 - train acc: 0.8309 - test acc: 0.4223 - 15m 33s\n",
      "batch: 200/1563 - train loss: 3.2116 - test loss: 17.4342 - train acc: 0.8246 - test acc: 0.3972 - 15m 38s\n",
      "batch: 300/1563 - train loss: 3.2256 - test loss: 16.1198 - train acc: 0.8212 - test acc: 0.4271 - 15m 43s\n",
      "batch: 400/1563 - train loss: 3.3968 - test loss: 16.5370 - train acc: 0.8072 - test acc: 0.4134 - 15m 48s\n",
      "batch: 500/1563 - train loss: 3.5648 - test loss: 16.4176 - train acc: 0.7981 - test acc: 0.4145 - 15m 53s\n",
      "batch: 600/1563 - train loss: 3.5845 - test loss: 16.4583 - train acc: 0.7990 - test acc: 0.4111 - 15m 58s\n",
      "batch: 700/1563 - train loss: 3.8594 - test loss: 16.3026 - train acc: 0.7862 - test acc: 0.4206 - 16m 3s\n",
      "batch: 800/1563 - train loss: 3.9221 - test loss: 16.1906 - train acc: 0.7837 - test acc: 0.4264 - 16m 8s\n",
      "batch: 900/1563 - train loss: 4.1444 - test loss: 16.5905 - train acc: 0.7778 - test acc: 0.4134 - 16m 13s\n",
      "batch: 1000/1563 - train loss: 4.2191 - test loss: 16.5534 - train acc: 0.7706 - test acc: 0.4183 - 16m 18s\n",
      "batch: 1100/1563 - train loss: 4.3907 - test loss: 17.1334 - train acc: 0.7678 - test acc: 0.4085 - 16m 23s\n",
      "batch: 1200/1563 - train loss: 4.2278 - test loss: 16.9090 - train acc: 0.7705 - test acc: 0.4040 - 16m 27s\n",
      "batch: 1300/1563 - train loss: 4.6154 - test loss: 17.4238 - train acc: 0.7550 - test acc: 0.3954 - 16m 32s\n",
      "batch: 1400/1563 - train loss: 4.6480 - test loss: 16.4092 - train acc: 0.7400 - test acc: 0.4103 - 16m 37s\n",
      "batch: 1500/1563 - train loss: 4.7043 - test loss: 16.8470 - train acc: 0.7450 - test acc: 0.4031 - 16m 43s\n",
      "batch: 1563/1563 - train loss: 4.7218 - test loss: 17.5611 - train acc: 0.7444 - test acc: 0.4004 - 16m 47s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.8528 - test loss: 16.0537 - train acc: 0.8446 - test acc: 0.4269 - 16m 51s\n",
      "batch: 200/1563 - train loss: 2.4914 - test loss: 16.7793 - train acc: 0.8615 - test acc: 0.4154 - 16m 56s\n",
      "batch: 300/1563 - train loss: 2.7063 - test loss: 16.8047 - train acc: 0.8556 - test acc: 0.4190 - 17m 1s\n",
      "batch: 400/1563 - train loss: 2.6540 - test loss: 16.9262 - train acc: 0.8525 - test acc: 0.4224 - 17m 6s\n",
      "batch: 500/1563 - train loss: 2.7291 - test loss: 17.3176 - train acc: 0.8443 - test acc: 0.4205 - 17m 11s\n",
      "batch: 600/1563 - train loss: 3.1900 - test loss: 17.1652 - train acc: 0.8178 - test acc: 0.4182 - 17m 16s\n",
      "batch: 700/1563 - train loss: 3.2405 - test loss: 17.2771 - train acc: 0.8218 - test acc: 0.4106 - 17m 21s\n",
      "batch: 800/1563 - train loss: 3.3229 - test loss: 17.8806 - train acc: 0.8109 - test acc: 0.4101 - 17m 25s\n",
      "batch: 900/1563 - train loss: 3.3698 - test loss: 17.2964 - train acc: 0.8031 - test acc: 0.4146 - 17m 30s\n",
      "batch: 1000/1563 - train loss: 3.4234 - test loss: 17.7113 - train acc: 0.8081 - test acc: 0.4017 - 17m 35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 3.6406 - test loss: 16.6938 - train acc: 0.8018 - test acc: 0.4255 - 17m 40s\n",
      "batch: 1200/1563 - train loss: 3.6878 - test loss: 17.3696 - train acc: 0.8025 - test acc: 0.4011 - 17m 45s\n",
      "batch: 1300/1563 - train loss: 3.6951 - test loss: 17.6257 - train acc: 0.7980 - test acc: 0.4002 - 17m 50s\n",
      "batch: 1400/1563 - train loss: 3.8805 - test loss: 17.1659 - train acc: 0.7837 - test acc: 0.4104 - 17m 55s\n",
      "batch: 1500/1563 - train loss: 3.7196 - test loss: 19.0015 - train acc: 0.7919 - test acc: 0.3904 - 18m 0s\n",
      "batch: 1563/1563 - train loss: 4.0792 - test loss: 17.1438 - train acc: 0.7765 - test acc: 0.4199 - 18m 4s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.1841 - test loss: 17.8457 - train acc: 0.8788 - test acc: 0.4101 - 18m 9s\n",
      "batch: 200/1563 - train loss: 2.2305 - test loss: 16.9811 - train acc: 0.8763 - test acc: 0.4318 - 18m 14s\n",
      "batch: 300/1563 - train loss: 2.1817 - test loss: 17.5458 - train acc: 0.8725 - test acc: 0.4253 - 18m 19s\n",
      "batch: 400/1563 - train loss: 2.4131 - test loss: 18.9850 - train acc: 0.8612 - test acc: 0.4034 - 18m 24s\n",
      "batch: 500/1563 - train loss: 2.5625 - test loss: 17.3571 - train acc: 0.8469 - test acc: 0.4234 - 18m 29s\n",
      "batch: 600/1563 - train loss: 2.6106 - test loss: 17.8049 - train acc: 0.8534 - test acc: 0.4188 - 18m 34s\n",
      "batch: 700/1563 - train loss: 2.7133 - test loss: 17.8475 - train acc: 0.8556 - test acc: 0.4202 - 18m 39s\n",
      "batch: 800/1563 - train loss: 2.8921 - test loss: 17.6137 - train acc: 0.8322 - test acc: 0.4223 - 18m 44s\n",
      "batch: 900/1563 - train loss: 2.9214 - test loss: 17.7278 - train acc: 0.8318 - test acc: 0.4244 - 18m 49s\n",
      "batch: 1000/1563 - train loss: 2.9658 - test loss: 17.7625 - train acc: 0.8266 - test acc: 0.4226 - 18m 54s\n",
      "batch: 1100/1563 - train loss: 3.1866 - test loss: 18.3767 - train acc: 0.8159 - test acc: 0.3983 - 18m 59s\n",
      "batch: 1200/1563 - train loss: 2.9496 - test loss: 17.8411 - train acc: 0.8346 - test acc: 0.4169 - 19m 4s\n",
      "batch: 1300/1563 - train loss: 3.2988 - test loss: 17.5568 - train acc: 0.8097 - test acc: 0.4254 - 19m 9s\n",
      "batch: 1400/1563 - train loss: 3.2628 - test loss: 17.9839 - train acc: 0.8212 - test acc: 0.4099 - 19m 14s\n",
      "batch: 1500/1563 - train loss: 3.4158 - test loss: 17.6774 - train acc: 0.8115 - test acc: 0.4125 - 19m 19s\n",
      "batch: 1563/1563 - train loss: 3.5518 - test loss: 17.4561 - train acc: 0.8075 - test acc: 0.4148 - 19m 23s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.0056 - test loss: 17.7811 - train acc: 0.8847 - test acc: 0.4258 - 19m 28s\n",
      "batch: 200/1563 - train loss: 1.9979 - test loss: 17.7640 - train acc: 0.8941 - test acc: 0.4214 - 19m 33s\n",
      "batch: 300/1563 - train loss: 1.9702 - test loss: 17.7887 - train acc: 0.8876 - test acc: 0.4256 - 19m 38s\n",
      "batch: 400/1563 - train loss: 1.8389 - test loss: 17.9783 - train acc: 0.8926 - test acc: 0.4262 - 19m 43s\n",
      "batch: 500/1563 - train loss: 2.1531 - test loss: 19.8934 - train acc: 0.8744 - test acc: 0.3943 - 19m 48s\n",
      "batch: 600/1563 - train loss: 2.3611 - test loss: 18.6475 - train acc: 0.8700 - test acc: 0.4166 - 19m 54s\n",
      "batch: 700/1563 - train loss: 2.2311 - test loss: 18.7667 - train acc: 0.8694 - test acc: 0.4146 - 19m 59s\n",
      "time is up! finishing training\n",
      "batch: 701/1563 - train loss: 2.2279 - test loss: 18.8357 - train acc: 0.8688 - test acc: 0.4130 - 20m 2s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 26.2498 - test loss: 24.6922 - train acc: 0.0320 - test acc: 0.0475 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.0803 - test loss: 23.4666 - train acc: 0.0558 - test acc: 0.0738 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.2021 - test loss: 22.6839 - train acc: 0.0758 - test acc: 0.0836 - 0m 12s\n",
      "batch: 400/1563 - train loss: 22.5097 - test loss: 22.3857 - train acc: 0.0905 - test acc: 0.1020 - 0m 16s\n",
      "batch: 500/1563 - train loss: 22.2769 - test loss: 21.6445 - train acc: 0.0974 - test acc: 0.1065 - 0m 21s\n",
      "batch: 600/1563 - train loss: 21.6220 - test loss: 20.7097 - train acc: 0.1187 - test acc: 0.1413 - 0m 26s\n",
      "batch: 700/1563 - train loss: 21.0084 - test loss: 20.8754 - train acc: 0.1369 - test acc: 0.1409 - 0m 31s\n",
      "batch: 800/1563 - train loss: 20.9056 - test loss: 20.0174 - train acc: 0.1347 - test acc: 0.1648 - 0m 36s\n",
      "batch: 900/1563 - train loss: 20.4324 - test loss: 20.1556 - train acc: 0.1454 - test acc: 0.1581 - 0m 42s\n",
      "batch: 1000/1563 - train loss: 20.0404 - test loss: 20.5186 - train acc: 0.1544 - test acc: 0.1622 - 0m 47s\n",
      "batch: 1100/1563 - train loss: 20.4136 - test loss: 19.9624 - train acc: 0.1597 - test acc: 0.1625 - 0m 51s\n",
      "batch: 1200/1563 - train loss: 19.5701 - test loss: 20.6412 - train acc: 0.1672 - test acc: 0.1532 - 0m 57s\n",
      "batch: 1300/1563 - train loss: 19.6501 - test loss: 19.4996 - train acc: 0.1729 - test acc: 0.1783 - 1m 1s\n",
      "batch: 1400/1563 - train loss: 19.3668 - test loss: 18.5999 - train acc: 0.1807 - test acc: 0.2023 - 1m 6s\n",
      "batch: 1500/1563 - train loss: 19.1475 - test loss: 18.8382 - train acc: 0.1964 - test acc: 0.1993 - 1m 11s\n",
      "batch: 1563/1563 - train loss: 18.8376 - test loss: 18.5167 - train acc: 0.2069 - test acc: 0.2066 - 1m 15s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 18.4685 - test loss: 19.3923 - train acc: 0.2101 - test acc: 0.1946 - 1m 20s\n",
      "batch: 200/1563 - train loss: 17.8764 - test loss: 19.2903 - train acc: 0.2172 - test acc: 0.1944 - 1m 25s\n",
      "batch: 300/1563 - train loss: 18.0467 - test loss: 18.6311 - train acc: 0.2207 - test acc: 0.2099 - 1m 30s\n",
      "batch: 400/1563 - train loss: 17.9009 - test loss: 17.8458 - train acc: 0.2195 - test acc: 0.2322 - 1m 35s\n",
      "batch: 500/1563 - train loss: 17.6689 - test loss: 17.6169 - train acc: 0.2388 - test acc: 0.2391 - 1m 40s\n",
      "batch: 600/1563 - train loss: 17.7148 - test loss: 17.9925 - train acc: 0.2260 - test acc: 0.2264 - 1m 45s\n",
      "batch: 700/1563 - train loss: 17.5404 - test loss: 17.5269 - train acc: 0.2416 - test acc: 0.2356 - 1m 50s\n",
      "batch: 800/1563 - train loss: 17.2980 - test loss: 17.4340 - train acc: 0.2444 - test acc: 0.2499 - 1m 55s\n",
      "batch: 900/1563 - train loss: 17.4502 - test loss: 17.4102 - train acc: 0.2478 - test acc: 0.2461 - 1m 59s\n",
      "batch: 1000/1563 - train loss: 17.2937 - test loss: 17.6074 - train acc: 0.2509 - test acc: 0.2410 - 2m 4s\n",
      "batch: 1100/1563 - train loss: 17.1717 - test loss: 17.2387 - train acc: 0.2419 - test acc: 0.2455 - 2m 9s\n",
      "batch: 1200/1563 - train loss: 16.9002 - test loss: 16.9902 - train acc: 0.2585 - test acc: 0.2567 - 2m 14s\n",
      "batch: 1300/1563 - train loss: 16.7417 - test loss: 17.6557 - train acc: 0.2634 - test acc: 0.2467 - 2m 19s\n",
      "batch: 1400/1563 - train loss: 16.7431 - test loss: 17.5048 - train acc: 0.2691 - test acc: 0.2376 - 2m 24s\n",
      "batch: 1500/1563 - train loss: 16.6382 - test loss: 16.9286 - train acc: 0.2591 - test acc: 0.2718 - 2m 29s\n",
      "batch: 1563/1563 - train loss: 16.4452 - test loss: 16.9557 - train acc: 0.2703 - test acc: 0.2702 - 2m 33s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.6594 - test loss: 17.4843 - train acc: 0.3037 - test acc: 0.2566 - 2m 38s\n",
      "batch: 200/1563 - train loss: 15.4173 - test loss: 16.2652 - train acc: 0.3037 - test acc: 0.2857 - 2m 43s\n",
      "batch: 300/1563 - train loss: 15.4682 - test loss: 16.6115 - train acc: 0.2984 - test acc: 0.2717 - 2m 48s\n",
      "batch: 400/1563 - train loss: 15.2955 - test loss: 16.6795 - train acc: 0.3137 - test acc: 0.2780 - 2m 53s\n",
      "batch: 500/1563 - train loss: 15.7222 - test loss: 16.0994 - train acc: 0.2975 - test acc: 0.2846 - 2m 58s\n",
      "batch: 600/1563 - train loss: 15.1388 - test loss: 15.9329 - train acc: 0.3121 - test acc: 0.2986 - 3m 3s\n",
      "batch: 700/1563 - train loss: 15.0861 - test loss: 15.8700 - train acc: 0.3190 - test acc: 0.2966 - 3m 8s\n",
      "batch: 800/1563 - train loss: 15.2182 - test loss: 16.6268 - train acc: 0.3165 - test acc: 0.2715 - 3m 12s\n",
      "batch: 900/1563 - train loss: 14.9100 - test loss: 16.2172 - train acc: 0.3299 - test acc: 0.2895 - 3m 17s\n",
      "batch: 1000/1563 - train loss: 15.3028 - test loss: 15.5842 - train acc: 0.3121 - test acc: 0.3088 - 3m 23s\n",
      "batch: 1100/1563 - train loss: 14.9102 - test loss: 16.7836 - train acc: 0.3309 - test acc: 0.2821 - 3m 27s\n",
      "batch: 1200/1563 - train loss: 15.1447 - test loss: 16.0105 - train acc: 0.3131 - test acc: 0.2992 - 3m 32s\n",
      "batch: 1300/1563 - train loss: 15.1294 - test loss: 15.9793 - train acc: 0.3097 - test acc: 0.2992 - 3m 37s\n",
      "batch: 1400/1563 - train loss: 15.1489 - test loss: 15.2096 - train acc: 0.3284 - test acc: 0.3191 - 3m 42s\n",
      "batch: 1500/1563 - train loss: 14.6863 - test loss: 16.0075 - train acc: 0.3309 - test acc: 0.3027 - 3m 47s\n",
      "batch: 1563/1563 - train loss: 14.7752 - test loss: 21.4186 - train acc: 0.3275 - test acc: 0.2116 - 3m 51s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.3458 - test loss: 15.6843 - train acc: 0.3860 - test acc: 0.3117 - 3m 56s\n",
      "batch: 200/1563 - train loss: 13.7849 - test loss: 16.5306 - train acc: 0.3568 - test acc: 0.2807 - 4m 1s\n",
      "batch: 300/1563 - train loss: 13.5220 - test loss: 14.8620 - train acc: 0.3812 - test acc: 0.3415 - 4m 6s\n",
      "batch: 400/1563 - train loss: 13.6381 - test loss: 15.8120 - train acc: 0.3806 - test acc: 0.3113 - 4m 11s\n",
      "batch: 500/1563 - train loss: 13.5379 - test loss: 15.9753 - train acc: 0.3844 - test acc: 0.3056 - 4m 16s\n",
      "batch: 600/1563 - train loss: 13.6053 - test loss: 15.3669 - train acc: 0.3716 - test acc: 0.3253 - 4m 21s\n",
      "batch: 700/1563 - train loss: 13.6130 - test loss: 14.7249 - train acc: 0.3659 - test acc: 0.3443 - 4m 26s\n",
      "batch: 800/1563 - train loss: 13.5734 - test loss: 14.7143 - train acc: 0.3734 - test acc: 0.3476 - 4m 31s\n",
      "batch: 900/1563 - train loss: 13.4094 - test loss: 14.6677 - train acc: 0.3851 - test acc: 0.3368 - 4m 36s\n",
      "batch: 1000/1563 - train loss: 13.5889 - test loss: 15.0540 - train acc: 0.3797 - test acc: 0.3333 - 4m 40s\n",
      "batch: 1100/1563 - train loss: 13.4258 - test loss: 14.9573 - train acc: 0.3754 - test acc: 0.3355 - 4m 45s\n",
      "batch: 1200/1563 - train loss: 13.2304 - test loss: 14.9515 - train acc: 0.3962 - test acc: 0.3412 - 4m 50s\n",
      "batch: 1300/1563 - train loss: 13.4766 - test loss: 14.6061 - train acc: 0.3878 - test acc: 0.3461 - 4m 56s\n",
      "batch: 1400/1563 - train loss: 13.3164 - test loss: 14.4059 - train acc: 0.3794 - test acc: 0.3482 - 5m 1s\n",
      "batch: 1500/1563 - train loss: 13.3761 - test loss: 14.1809 - train acc: 0.3963 - test acc: 0.3557 - 5m 6s\n",
      "batch: 1563/1563 - train loss: 13.4456 - test loss: 15.8681 - train acc: 0.3884 - test acc: 0.3183 - 5m 10s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.7086 - test loss: 15.0425 - train acc: 0.4391 - test acc: 0.3433 - 5m 15s\n",
      "batch: 200/1563 - train loss: 11.8643 - test loss: 14.1541 - train acc: 0.4472 - test acc: 0.3597 - 5m 20s\n",
      "batch: 300/1563 - train loss: 11.7673 - test loss: 14.8526 - train acc: 0.4484 - test acc: 0.3523 - 5m 24s\n",
      "batch: 400/1563 - train loss: 12.1592 - test loss: 14.3376 - train acc: 0.4323 - test acc: 0.3552 - 5m 30s\n",
      "batch: 500/1563 - train loss: 12.2885 - test loss: 14.1018 - train acc: 0.4253 - test acc: 0.3731 - 5m 35s\n",
      "batch: 600/1563 - train loss: 12.0199 - test loss: 13.9812 - train acc: 0.4388 - test acc: 0.3742 - 5m 40s\n",
      "batch: 700/1563 - train loss: 12.3425 - test loss: 15.6060 - train acc: 0.4193 - test acc: 0.3185 - 5m 45s\n",
      "batch: 800/1563 - train loss: 12.1053 - test loss: 14.1497 - train acc: 0.4250 - test acc: 0.3697 - 5m 49s\n",
      "batch: 900/1563 - train loss: 12.3693 - test loss: 14.3375 - train acc: 0.4297 - test acc: 0.3650 - 5m 54s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 11.9181 - test loss: 13.9482 - train acc: 0.4388 - test acc: 0.3723 - 5m 59s\n",
      "batch: 1100/1563 - train loss: 12.0480 - test loss: 14.4789 - train acc: 0.4400 - test acc: 0.3644 - 6m 5s\n",
      "batch: 1200/1563 - train loss: 12.0388 - test loss: 13.7270 - train acc: 0.4290 - test acc: 0.3796 - 6m 9s\n",
      "batch: 1300/1563 - train loss: 12.1664 - test loss: 14.4779 - train acc: 0.4325 - test acc: 0.3600 - 6m 14s\n",
      "batch: 1400/1563 - train loss: 12.2764 - test loss: 13.9978 - train acc: 0.4222 - test acc: 0.3742 - 6m 19s\n",
      "batch: 1500/1563 - train loss: 12.3340 - test loss: 14.5422 - train acc: 0.4147 - test acc: 0.3592 - 6m 24s\n",
      "batch: 1563/1563 - train loss: 12.1987 - test loss: 14.3305 - train acc: 0.4288 - test acc: 0.3655 - 6m 28s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.3594 - test loss: 14.5289 - train acc: 0.4997 - test acc: 0.3687 - 6m 33s\n",
      "batch: 200/1563 - train loss: 10.3710 - test loss: 14.5668 - train acc: 0.4959 - test acc: 0.3714 - 6m 38s\n",
      "batch: 300/1563 - train loss: 10.4053 - test loss: 13.9644 - train acc: 0.4997 - test acc: 0.3903 - 6m 43s\n",
      "batch: 400/1563 - train loss: 10.6442 - test loss: 14.9831 - train acc: 0.4856 - test acc: 0.3520 - 6m 48s\n",
      "batch: 500/1563 - train loss: 10.8608 - test loss: 14.4138 - train acc: 0.4738 - test acc: 0.3673 - 6m 53s\n",
      "batch: 600/1563 - train loss: 11.0042 - test loss: 14.8734 - train acc: 0.4697 - test acc: 0.3552 - 6m 58s\n",
      "batch: 700/1563 - train loss: 10.9627 - test loss: 14.0096 - train acc: 0.4743 - test acc: 0.3799 - 7m 4s\n",
      "batch: 800/1563 - train loss: 11.1135 - test loss: 14.8324 - train acc: 0.4629 - test acc: 0.3563 - 7m 9s\n",
      "batch: 900/1563 - train loss: 10.8356 - test loss: 14.3838 - train acc: 0.4781 - test acc: 0.3726 - 7m 14s\n",
      "batch: 1000/1563 - train loss: 10.9873 - test loss: 13.6475 - train acc: 0.4628 - test acc: 0.3879 - 7m 19s\n",
      "batch: 1100/1563 - train loss: 10.8502 - test loss: 13.8605 - train acc: 0.4860 - test acc: 0.3889 - 7m 24s\n",
      "batch: 1200/1563 - train loss: 11.0072 - test loss: 14.3959 - train acc: 0.4784 - test acc: 0.3661 - 7m 29s\n",
      "batch: 1300/1563 - train loss: 10.9165 - test loss: 14.4859 - train acc: 0.4656 - test acc: 0.3697 - 7m 34s\n",
      "batch: 1400/1563 - train loss: 11.1716 - test loss: 13.5861 - train acc: 0.4850 - test acc: 0.3911 - 7m 39s\n",
      "batch: 1500/1563 - train loss: 10.7345 - test loss: 13.3789 - train acc: 0.4694 - test acc: 0.4052 - 7m 44s\n",
      "batch: 1563/1563 - train loss: 10.7918 - test loss: 14.6930 - train acc: 0.4788 - test acc: 0.3606 - 7m 48s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 9.0776 - test loss: 14.3599 - train acc: 0.5472 - test acc: 0.3827 - 7m 52s\n",
      "batch: 200/1563 - train loss: 8.7747 - test loss: 15.1061 - train acc: 0.5616 - test acc: 0.3618 - 7m 58s\n",
      "batch: 300/1563 - train loss: 9.2086 - test loss: 16.7813 - train acc: 0.5418 - test acc: 0.3359 - 8m 2s\n",
      "batch: 400/1563 - train loss: 9.4458 - test loss: 14.2637 - train acc: 0.5253 - test acc: 0.3809 - 8m 8s\n",
      "batch: 500/1563 - train loss: 9.2079 - test loss: 13.9704 - train acc: 0.5469 - test acc: 0.3957 - 8m 12s\n",
      "batch: 600/1563 - train loss: 9.5825 - test loss: 18.2034 - train acc: 0.5225 - test acc: 0.3063 - 8m 17s\n",
      "batch: 700/1563 - train loss: 9.3719 - test loss: 14.4994 - train acc: 0.5340 - test acc: 0.3824 - 8m 22s\n",
      "batch: 800/1563 - train loss: 9.9556 - test loss: 15.5754 - train acc: 0.5103 - test acc: 0.3571 - 8m 27s\n",
      "batch: 900/1563 - train loss: 9.7343 - test loss: 14.7672 - train acc: 0.5287 - test acc: 0.3710 - 8m 32s\n",
      "batch: 1000/1563 - train loss: 10.0831 - test loss: 13.4974 - train acc: 0.5162 - test acc: 0.4018 - 8m 38s\n",
      "batch: 1100/1563 - train loss: 9.7750 - test loss: 15.8514 - train acc: 0.5181 - test acc: 0.3412 - 8m 43s\n",
      "batch: 1200/1563 - train loss: 10.0297 - test loss: 13.9926 - train acc: 0.5193 - test acc: 0.3964 - 8m 48s\n",
      "batch: 1300/1563 - train loss: 10.0776 - test loss: 13.5413 - train acc: 0.5057 - test acc: 0.4002 - 8m 53s\n",
      "batch: 1400/1563 - train loss: 10.0115 - test loss: 13.9326 - train acc: 0.5116 - test acc: 0.3970 - 8m 58s\n",
      "batch: 1500/1563 - train loss: 10.1302 - test loss: 13.9884 - train acc: 0.4975 - test acc: 0.3879 - 9m 2s\n",
      "batch: 1563/1563 - train loss: 10.3109 - test loss: 14.3149 - train acc: 0.4844 - test acc: 0.3734 - 9m 7s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.3574 - test loss: 13.9333 - train acc: 0.6322 - test acc: 0.4072 - 9m 12s\n",
      "batch: 200/1563 - train loss: 7.7270 - test loss: 15.1077 - train acc: 0.6100 - test acc: 0.3761 - 9m 17s\n",
      "batch: 300/1563 - train loss: 8.1122 - test loss: 14.2547 - train acc: 0.5883 - test acc: 0.3978 - 9m 22s\n",
      "batch: 400/1563 - train loss: 8.2376 - test loss: 13.9764 - train acc: 0.5794 - test acc: 0.3979 - 9m 27s\n",
      "batch: 500/1563 - train loss: 8.2081 - test loss: 14.2541 - train acc: 0.5975 - test acc: 0.3943 - 9m 32s\n",
      "batch: 600/1563 - train loss: 8.2990 - test loss: 14.6341 - train acc: 0.5768 - test acc: 0.3817 - 9m 37s\n",
      "batch: 700/1563 - train loss: 8.6139 - test loss: 14.4656 - train acc: 0.5640 - test acc: 0.3859 - 9m 42s\n",
      "batch: 800/1563 - train loss: 8.7371 - test loss: 14.4399 - train acc: 0.5656 - test acc: 0.3867 - 9m 47s\n",
      "batch: 900/1563 - train loss: 8.9073 - test loss: 14.8184 - train acc: 0.5596 - test acc: 0.3782 - 9m 51s\n",
      "batch: 1000/1563 - train loss: 8.6520 - test loss: 14.3954 - train acc: 0.5678 - test acc: 0.3879 - 9m 56s\n",
      "batch: 1100/1563 - train loss: 8.6609 - test loss: 14.1039 - train acc: 0.5665 - test acc: 0.4022 - 10m 1s\n",
      "batch: 1200/1563 - train loss: 9.0139 - test loss: 13.9296 - train acc: 0.5513 - test acc: 0.4092 - 10m 6s\n",
      "batch: 1300/1563 - train loss: 8.9191 - test loss: 13.5632 - train acc: 0.5556 - test acc: 0.4169 - 10m 11s\n",
      "batch: 1400/1563 - train loss: 8.9306 - test loss: 14.0173 - train acc: 0.5534 - test acc: 0.3965 - 10m 16s\n",
      "batch: 1500/1563 - train loss: 8.8536 - test loss: 13.7168 - train acc: 0.5562 - test acc: 0.4088 - 10m 21s\n",
      "batch: 1563/1563 - train loss: 9.0353 - test loss: 13.8472 - train acc: 0.5528 - test acc: 0.4076 - 10m 25s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.5266 - test loss: 15.0044 - train acc: 0.6587 - test acc: 0.3961 - 10m 30s\n",
      "batch: 200/1563 - train loss: 6.5582 - test loss: 14.9744 - train acc: 0.6634 - test acc: 0.3890 - 10m 35s\n",
      "batch: 300/1563 - train loss: 6.7120 - test loss: 15.1518 - train acc: 0.6516 - test acc: 0.3929 - 10m 40s\n",
      "batch: 400/1563 - train loss: 6.7270 - test loss: 15.0959 - train acc: 0.6544 - test acc: 0.3926 - 10m 45s\n",
      "batch: 500/1563 - train loss: 7.1989 - test loss: 15.2572 - train acc: 0.6272 - test acc: 0.3856 - 10m 50s\n",
      "batch: 600/1563 - train loss: 7.3586 - test loss: 14.3398 - train acc: 0.6250 - test acc: 0.4075 - 10m 55s\n",
      "batch: 700/1563 - train loss: 7.2648 - test loss: 14.1349 - train acc: 0.6313 - test acc: 0.4138 - 10m 59s\n",
      "batch: 800/1563 - train loss: 7.3951 - test loss: 15.3065 - train acc: 0.6172 - test acc: 0.3827 - 11m 4s\n",
      "batch: 900/1563 - train loss: 7.7290 - test loss: 14.8309 - train acc: 0.6103 - test acc: 0.3930 - 11m 9s\n",
      "batch: 1000/1563 - train loss: 7.5327 - test loss: 14.4029 - train acc: 0.6125 - test acc: 0.4067 - 11m 14s\n",
      "batch: 1100/1563 - train loss: 8.0387 - test loss: 14.0703 - train acc: 0.5953 - test acc: 0.4083 - 11m 20s\n",
      "batch: 1200/1563 - train loss: 7.9887 - test loss: 14.5552 - train acc: 0.5944 - test acc: 0.3967 - 11m 25s\n",
      "batch: 1300/1563 - train loss: 7.9032 - test loss: 20.7261 - train acc: 0.5993 - test acc: 0.3069 - 11m 29s\n",
      "batch: 1400/1563 - train loss: 7.9627 - test loss: 14.6190 - train acc: 0.5912 - test acc: 0.3978 - 11m 34s\n",
      "batch: 1500/1563 - train loss: 7.7024 - test loss: 14.2444 - train acc: 0.6166 - test acc: 0.4106 - 11m 39s\n",
      "batch: 1563/1563 - train loss: 8.0832 - test loss: 14.2998 - train acc: 0.5912 - test acc: 0.4047 - 11m 43s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.2946 - test loss: 14.6942 - train acc: 0.7213 - test acc: 0.4049 - 11m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 5.7168 - test loss: 16.0168 - train acc: 0.6997 - test acc: 0.3868 - 11m 53s\n",
      "batch: 300/1563 - train loss: 5.7294 - test loss: 14.9295 - train acc: 0.6957 - test acc: 0.4034 - 11m 58s\n",
      "batch: 400/1563 - train loss: 5.9495 - test loss: 14.7884 - train acc: 0.6931 - test acc: 0.4016 - 12m 3s\n",
      "batch: 500/1563 - train loss: 5.9664 - test loss: 16.2718 - train acc: 0.6860 - test acc: 0.3802 - 12m 8s\n",
      "batch: 600/1563 - train loss: 6.1125 - test loss: 15.8986 - train acc: 0.6841 - test acc: 0.3899 - 12m 13s\n",
      "batch: 700/1563 - train loss: 6.4055 - test loss: 16.0047 - train acc: 0.6688 - test acc: 0.3865 - 12m 18s\n",
      "batch: 800/1563 - train loss: 6.6417 - test loss: 15.0216 - train acc: 0.6544 - test acc: 0.4041 - 12m 23s\n",
      "batch: 900/1563 - train loss: 6.3457 - test loss: 15.6117 - train acc: 0.6610 - test acc: 0.3868 - 12m 28s\n",
      "batch: 1000/1563 - train loss: 6.9756 - test loss: 14.6366 - train acc: 0.6347 - test acc: 0.4132 - 12m 33s\n",
      "batch: 1100/1563 - train loss: 6.7910 - test loss: 14.7560 - train acc: 0.6459 - test acc: 0.4073 - 12m 38s\n",
      "batch: 1200/1563 - train loss: 6.7440 - test loss: 14.6668 - train acc: 0.6410 - test acc: 0.4147 - 12m 43s\n",
      "batch: 1300/1563 - train loss: 6.7054 - test loss: 14.5544 - train acc: 0.6526 - test acc: 0.4055 - 12m 48s\n",
      "batch: 1400/1563 - train loss: 6.5949 - test loss: 14.8471 - train acc: 0.6587 - test acc: 0.4025 - 12m 53s\n",
      "batch: 1500/1563 - train loss: 7.1583 - test loss: 15.5731 - train acc: 0.6325 - test acc: 0.3969 - 12m 58s\n",
      "batch: 1563/1563 - train loss: 7.0355 - test loss: 14.7588 - train acc: 0.6378 - test acc: 0.4093 - 13m 2s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.8299 - test loss: 15.7902 - train acc: 0.7459 - test acc: 0.4071 - 13m 7s\n",
      "batch: 200/1563 - train loss: 4.6764 - test loss: 15.8439 - train acc: 0.7544 - test acc: 0.4053 - 13m 12s\n",
      "batch: 300/1563 - train loss: 4.7298 - test loss: 15.3473 - train acc: 0.7453 - test acc: 0.4059 - 13m 17s\n",
      "batch: 400/1563 - train loss: 4.7876 - test loss: 15.9373 - train acc: 0.7425 - test acc: 0.4001 - 13m 22s\n",
      "batch: 500/1563 - train loss: 5.0458 - test loss: 15.8699 - train acc: 0.7356 - test acc: 0.3996 - 13m 28s\n",
      "batch: 600/1563 - train loss: 5.0621 - test loss: 15.5572 - train acc: 0.7338 - test acc: 0.4019 - 13m 32s\n",
      "batch: 700/1563 - train loss: 5.2385 - test loss: 15.9546 - train acc: 0.7222 - test acc: 0.4011 - 13m 38s\n",
      "batch: 800/1563 - train loss: 5.5541 - test loss: 15.4455 - train acc: 0.6966 - test acc: 0.4077 - 13m 42s\n",
      "batch: 900/1563 - train loss: 5.5208 - test loss: 15.7801 - train acc: 0.6988 - test acc: 0.4014 - 13m 47s\n",
      "batch: 1000/1563 - train loss: 5.5990 - test loss: 16.2020 - train acc: 0.7015 - test acc: 0.3912 - 13m 52s\n",
      "batch: 1100/1563 - train loss: 5.8182 - test loss: 15.9322 - train acc: 0.6885 - test acc: 0.3959 - 13m 58s\n",
      "batch: 1200/1563 - train loss: 5.8354 - test loss: 15.3926 - train acc: 0.6919 - test acc: 0.4029 - 14m 2s\n",
      "batch: 1300/1563 - train loss: 5.8998 - test loss: 15.6637 - train acc: 0.6869 - test acc: 0.4065 - 14m 8s\n",
      "batch: 1400/1563 - train loss: 6.0830 - test loss: 15.3657 - train acc: 0.6788 - test acc: 0.4056 - 14m 13s\n",
      "batch: 1500/1563 - train loss: 6.1342 - test loss: 15.7454 - train acc: 0.6778 - test acc: 0.3989 - 14m 17s\n",
      "batch: 1563/1563 - train loss: 6.1016 - test loss: 16.0336 - train acc: 0.6788 - test acc: 0.3847 - 14m 22s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.7313 - test loss: 15.2996 - train acc: 0.8074 - test acc: 0.4195 - 14m 26s\n",
      "batch: 200/1563 - train loss: 3.4965 - test loss: 15.6673 - train acc: 0.8128 - test acc: 0.4218 - 14m 32s\n",
      "batch: 300/1563 - train loss: 3.7953 - test loss: 16.5151 - train acc: 0.7950 - test acc: 0.4067 - 14m 37s\n",
      "batch: 400/1563 - train loss: 4.0367 - test loss: 16.3744 - train acc: 0.7840 - test acc: 0.4096 - 14m 42s\n",
      "batch: 500/1563 - train loss: 3.9842 - test loss: 16.3024 - train acc: 0.7865 - test acc: 0.4113 - 14m 46s\n",
      "batch: 600/1563 - train loss: 4.2066 - test loss: 16.4042 - train acc: 0.7650 - test acc: 0.4118 - 14m 51s\n",
      "batch: 700/1563 - train loss: 4.4741 - test loss: 17.1092 - train acc: 0.7534 - test acc: 0.3958 - 14m 56s\n",
      "batch: 800/1563 - train loss: 4.6878 - test loss: 16.4183 - train acc: 0.7494 - test acc: 0.4060 - 15m 1s\n",
      "batch: 900/1563 - train loss: 4.6568 - test loss: 16.5275 - train acc: 0.7494 - test acc: 0.4057 - 15m 6s\n",
      "batch: 1000/1563 - train loss: 4.9634 - test loss: 15.8635 - train acc: 0.7316 - test acc: 0.4098 - 15m 11s\n",
      "batch: 1100/1563 - train loss: 5.0106 - test loss: 15.9829 - train acc: 0.7303 - test acc: 0.4080 - 15m 16s\n",
      "batch: 1200/1563 - train loss: 4.7979 - test loss: 16.2253 - train acc: 0.7472 - test acc: 0.4043 - 15m 21s\n",
      "batch: 1300/1563 - train loss: 5.0319 - test loss: 15.7175 - train acc: 0.7337 - test acc: 0.4173 - 15m 26s\n",
      "batch: 1400/1563 - train loss: 5.2296 - test loss: 16.4545 - train acc: 0.7200 - test acc: 0.4065 - 15m 31s\n",
      "batch: 1500/1563 - train loss: 5.2507 - test loss: 15.9624 - train acc: 0.7153 - test acc: 0.4055 - 15m 36s\n",
      "batch: 1563/1563 - train loss: 5.2369 - test loss: 16.1388 - train acc: 0.7185 - test acc: 0.4066 - 15m 40s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.0482 - test loss: 16.0413 - train acc: 0.8334 - test acc: 0.4196 - 15m 45s\n",
      "batch: 200/1563 - train loss: 3.1639 - test loss: 16.2329 - train acc: 0.8215 - test acc: 0.4177 - 15m 51s\n",
      "batch: 300/1563 - train loss: 3.0176 - test loss: 17.2237 - train acc: 0.8312 - test acc: 0.4014 - 15m 55s\n",
      "batch: 400/1563 - train loss: 3.4491 - test loss: 16.8819 - train acc: 0.8114 - test acc: 0.4163 - 16m 0s\n",
      "batch: 500/1563 - train loss: 3.2809 - test loss: 16.7270 - train acc: 0.8203 - test acc: 0.4192 - 16m 5s\n",
      "batch: 600/1563 - train loss: 3.4850 - test loss: 16.8579 - train acc: 0.8040 - test acc: 0.4124 - 16m 10s\n",
      "batch: 700/1563 - train loss: 3.5626 - test loss: 17.1178 - train acc: 0.8071 - test acc: 0.4117 - 16m 15s\n",
      "batch: 800/1563 - train loss: 3.7283 - test loss: 16.8741 - train acc: 0.7943 - test acc: 0.4089 - 16m 20s\n",
      "batch: 900/1563 - train loss: 3.9123 - test loss: 17.9490 - train acc: 0.7840 - test acc: 0.3837 - 16m 25s\n",
      "batch: 1000/1563 - train loss: 4.1459 - test loss: 16.4548 - train acc: 0.7775 - test acc: 0.4118 - 16m 30s\n",
      "batch: 1100/1563 - train loss: 4.0647 - test loss: 16.8657 - train acc: 0.7697 - test acc: 0.4013 - 16m 35s\n",
      "batch: 1200/1563 - train loss: 4.1670 - test loss: 17.2665 - train acc: 0.7778 - test acc: 0.4004 - 16m 40s\n",
      "batch: 1300/1563 - train loss: 4.3567 - test loss: 17.3861 - train acc: 0.7559 - test acc: 0.3912 - 16m 45s\n",
      "batch: 1400/1563 - train loss: 4.4242 - test loss: 16.7221 - train acc: 0.7637 - test acc: 0.4097 - 16m 50s\n",
      "batch: 1500/1563 - train loss: 4.3845 - test loss: 17.0134 - train acc: 0.7600 - test acc: 0.3995 - 16m 55s\n",
      "batch: 1563/1563 - train loss: 4.3906 - test loss: 16.5936 - train acc: 0.7587 - test acc: 0.4040 - 16m 59s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.5768 - test loss: 16.7326 - train acc: 0.8612 - test acc: 0.4179 - 17m 4s\n",
      "batch: 200/1563 - train loss: 2.3280 - test loss: 17.6638 - train acc: 0.8669 - test acc: 0.4124 - 17m 9s\n",
      "batch: 300/1563 - train loss: 2.6613 - test loss: 17.2031 - train acc: 0.8543 - test acc: 0.4200 - 17m 14s\n",
      "batch: 400/1563 - train loss: 2.5439 - test loss: 17.1674 - train acc: 0.8606 - test acc: 0.4151 - 17m 19s\n",
      "batch: 500/1563 - train loss: 2.8395 - test loss: 17.7033 - train acc: 0.8434 - test acc: 0.4112 - 17m 23s\n",
      "batch: 600/1563 - train loss: 2.6377 - test loss: 17.8094 - train acc: 0.8534 - test acc: 0.4155 - 17m 28s\n",
      "batch: 700/1563 - train loss: 3.1330 - test loss: 17.5826 - train acc: 0.8243 - test acc: 0.4139 - 17m 33s\n",
      "batch: 800/1563 - train loss: 2.9763 - test loss: 18.0433 - train acc: 0.8353 - test acc: 0.4007 - 17m 38s\n",
      "batch: 900/1563 - train loss: 3.1496 - test loss: 17.7908 - train acc: 0.8256 - test acc: 0.4138 - 17m 43s\n",
      "batch: 1000/1563 - train loss: 3.3331 - test loss: 17.5052 - train acc: 0.8159 - test acc: 0.4167 - 17m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 3.2830 - test loss: 18.4872 - train acc: 0.8150 - test acc: 0.3938 - 17m 53s\n",
      "batch: 1200/1563 - train loss: 3.7410 - test loss: 18.9282 - train acc: 0.7962 - test acc: 0.3921 - 17m 58s\n",
      "batch: 1300/1563 - train loss: 3.9129 - test loss: 17.2794 - train acc: 0.7747 - test acc: 0.4145 - 18m 3s\n",
      "batch: 1400/1563 - train loss: 3.8517 - test loss: 17.0673 - train acc: 0.7859 - test acc: 0.4166 - 18m 8s\n",
      "batch: 1500/1563 - train loss: 3.9117 - test loss: 17.8774 - train acc: 0.7881 - test acc: 0.3989 - 18m 13s\n",
      "batch: 1563/1563 - train loss: 4.0548 - test loss: 17.6463 - train acc: 0.7721 - test acc: 0.4065 - 18m 17s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.0742 - test loss: 17.2181 - train acc: 0.8875 - test acc: 0.4173 - 18m 22s\n",
      "batch: 200/1563 - train loss: 2.0737 - test loss: 17.3299 - train acc: 0.8810 - test acc: 0.4203 - 18m 27s\n",
      "batch: 300/1563 - train loss: 2.3292 - test loss: 17.3196 - train acc: 0.8725 - test acc: 0.4137 - 18m 32s\n",
      "batch: 400/1563 - train loss: 2.0113 - test loss: 18.2948 - train acc: 0.8838 - test acc: 0.4145 - 18m 37s\n",
      "batch: 500/1563 - train loss: 2.1915 - test loss: 18.0234 - train acc: 0.8778 - test acc: 0.4173 - 18m 42s\n",
      "batch: 600/1563 - train loss: 2.2952 - test loss: 18.1598 - train acc: 0.8769 - test acc: 0.4123 - 18m 47s\n",
      "batch: 700/1563 - train loss: 2.5689 - test loss: 17.8606 - train acc: 0.8562 - test acc: 0.4179 - 18m 52s\n",
      "batch: 800/1563 - train loss: 2.3285 - test loss: 18.1183 - train acc: 0.8693 - test acc: 0.4236 - 18m 57s\n",
      "batch: 900/1563 - train loss: 2.3888 - test loss: 19.3229 - train acc: 0.8775 - test acc: 0.3989 - 19m 2s\n",
      "batch: 1000/1563 - train loss: 2.6929 - test loss: 18.4597 - train acc: 0.8541 - test acc: 0.4143 - 19m 7s\n",
      "batch: 1100/1563 - train loss: 2.7653 - test loss: 19.1537 - train acc: 0.8478 - test acc: 0.4067 - 19m 12s\n",
      "batch: 1200/1563 - train loss: 2.8701 - test loss: 18.2118 - train acc: 0.8400 - test acc: 0.4131 - 19m 17s\n",
      "batch: 1300/1563 - train loss: 3.1631 - test loss: 18.1809 - train acc: 0.8259 - test acc: 0.4107 - 19m 22s\n",
      "batch: 1400/1563 - train loss: 3.1159 - test loss: 18.5497 - train acc: 0.8240 - test acc: 0.3988 - 19m 27s\n",
      "batch: 1500/1563 - train loss: 3.1268 - test loss: 17.8150 - train acc: 0.8208 - test acc: 0.4115 - 19m 32s\n",
      "batch: 1563/1563 - train loss: 3.0370 - test loss: 18.4147 - train acc: 0.8287 - test acc: 0.4116 - 19m 36s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.7756 - test loss: 18.0502 - train acc: 0.8976 - test acc: 0.4168 - 19m 41s\n",
      "batch: 200/1563 - train loss: 1.7953 - test loss: 18.9875 - train acc: 0.9023 - test acc: 0.4154 - 19m 45s\n",
      "batch: 300/1563 - train loss: 1.7407 - test loss: 18.0554 - train acc: 0.9032 - test acc: 0.4290 - 19m 51s\n",
      "batch: 400/1563 - train loss: 1.8097 - test loss: 18.3685 - train acc: 0.9007 - test acc: 0.4214 - 19m 56s\n",
      "batch: 500/1563 - train loss: 2.0444 - test loss: 18.7160 - train acc: 0.8841 - test acc: 0.4151 - 20m 0s\n",
      "time is up! finishing training\n",
      "batch: 501/1563 - train loss: 2.0610 - test loss: 18.6502 - train acc: 0.8825 - test acc: 0.4176 - 20m 4s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 26.1700 - test loss: 25.6129 - train acc: 0.0367 - test acc: 0.0499 - 0m 2s\n",
      "batch: 200/1563 - train loss: 24.1642 - test loss: 23.2894 - train acc: 0.0698 - test acc: 0.0765 - 0m 7s\n",
      "batch: 300/1563 - train loss: 23.0673 - test loss: 22.0755 - train acc: 0.0817 - test acc: 0.0981 - 0m 12s\n",
      "batch: 400/1563 - train loss: 22.5095 - test loss: 21.9980 - train acc: 0.0971 - test acc: 0.1089 - 0m 16s\n",
      "batch: 500/1563 - train loss: 21.9867 - test loss: 22.0342 - train acc: 0.1112 - test acc: 0.1065 - 0m 21s\n",
      "batch: 600/1563 - train loss: 21.4721 - test loss: 20.8377 - train acc: 0.1241 - test acc: 0.1401 - 0m 26s\n",
      "batch: 700/1563 - train loss: 20.6984 - test loss: 20.9734 - train acc: 0.1319 - test acc: 0.1357 - 0m 31s\n",
      "batch: 800/1563 - train loss: 20.6961 - test loss: 20.3889 - train acc: 0.1450 - test acc: 0.1480 - 0m 36s\n",
      "batch: 900/1563 - train loss: 20.1303 - test loss: 21.0419 - train acc: 0.1573 - test acc: 0.1338 - 0m 41s\n",
      "batch: 1000/1563 - train loss: 20.0788 - test loss: 19.7752 - train acc: 0.1766 - test acc: 0.1644 - 0m 45s\n",
      "batch: 1100/1563 - train loss: 19.6678 - test loss: 21.4187 - train acc: 0.1601 - test acc: 0.1350 - 0m 50s\n",
      "batch: 1200/1563 - train loss: 19.7851 - test loss: 20.1413 - train acc: 0.1685 - test acc: 0.1704 - 0m 55s\n",
      "batch: 1300/1563 - train loss: 19.3901 - test loss: 18.9606 - train acc: 0.1769 - test acc: 0.2000 - 0m 59s\n",
      "batch: 1400/1563 - train loss: 19.0440 - test loss: 18.4304 - train acc: 0.1875 - test acc: 0.2131 - 1m 4s\n",
      "batch: 1500/1563 - train loss: 18.5375 - test loss: 18.1780 - train acc: 0.2059 - test acc: 0.2147 - 1m 9s\n",
      "batch: 1563/1563 - train loss: 18.4108 - test loss: 18.1297 - train acc: 0.2013 - test acc: 0.2180 - 1m 14s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 18.0329 - test loss: 18.7566 - train acc: 0.2238 - test acc: 0.2026 - 1m 19s\n",
      "batch: 200/1563 - train loss: 17.9799 - test loss: 19.5548 - train acc: 0.2204 - test acc: 0.1864 - 1m 24s\n",
      "batch: 300/1563 - train loss: 17.5975 - test loss: 18.1805 - train acc: 0.2251 - test acc: 0.2162 - 1m 28s\n",
      "batch: 400/1563 - train loss: 17.7797 - test loss: 17.8139 - train acc: 0.2294 - test acc: 0.2338 - 1m 33s\n",
      "batch: 500/1563 - train loss: 17.4170 - test loss: 20.8253 - train acc: 0.2372 - test acc: 0.1587 - 1m 38s\n",
      "batch: 600/1563 - train loss: 17.5873 - test loss: 17.1964 - train acc: 0.2273 - test acc: 0.2488 - 1m 43s\n",
      "batch: 700/1563 - train loss: 17.4416 - test loss: 17.9665 - train acc: 0.2350 - test acc: 0.2343 - 1m 47s\n",
      "batch: 800/1563 - train loss: 17.1918 - test loss: 17.7210 - train acc: 0.2497 - test acc: 0.2358 - 1m 52s\n",
      "batch: 900/1563 - train loss: 16.8057 - test loss: 17.9819 - train acc: 0.2668 - test acc: 0.2259 - 1m 57s\n",
      "batch: 1000/1563 - train loss: 16.9928 - test loss: 18.6320 - train acc: 0.2556 - test acc: 0.2099 - 2m 1s\n",
      "batch: 1100/1563 - train loss: 16.8312 - test loss: 17.4207 - train acc: 0.2622 - test acc: 0.2452 - 2m 6s\n",
      "batch: 1200/1563 - train loss: 16.8896 - test loss: 16.3961 - train acc: 0.2503 - test acc: 0.2833 - 2m 12s\n",
      "batch: 1300/1563 - train loss: 16.7816 - test loss: 18.0931 - train acc: 0.2675 - test acc: 0.2343 - 2m 16s\n",
      "batch: 1400/1563 - train loss: 16.3365 - test loss: 16.2127 - train acc: 0.2787 - test acc: 0.2877 - 2m 21s\n",
      "batch: 1500/1563 - train loss: 16.2818 - test loss: 16.3836 - train acc: 0.2778 - test acc: 0.2737 - 2m 26s\n",
      "batch: 1563/1563 - train loss: 16.2514 - test loss: 16.6436 - train acc: 0.2762 - test acc: 0.2727 - 2m 30s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.1985 - test loss: 15.6419 - train acc: 0.3200 - test acc: 0.3087 - 2m 35s\n",
      "batch: 200/1563 - train loss: 14.8715 - test loss: 17.4325 - train acc: 0.3237 - test acc: 0.2598 - 2m 40s\n",
      "batch: 300/1563 - train loss: 15.4641 - test loss: 16.6772 - train acc: 0.3012 - test acc: 0.2786 - 2m 45s\n",
      "batch: 400/1563 - train loss: 15.2660 - test loss: 16.9308 - train acc: 0.3090 - test acc: 0.2717 - 2m 50s\n",
      "batch: 500/1563 - train loss: 15.2840 - test loss: 15.9007 - train acc: 0.3165 - test acc: 0.2986 - 2m 55s\n",
      "batch: 600/1563 - train loss: 14.8432 - test loss: 16.6540 - train acc: 0.3263 - test acc: 0.2728 - 2m 59s\n",
      "batch: 700/1563 - train loss: 14.9550 - test loss: 15.5190 - train acc: 0.3335 - test acc: 0.3047 - 3m 4s\n",
      "batch: 800/1563 - train loss: 14.7987 - test loss: 17.7297 - train acc: 0.3266 - test acc: 0.2461 - 3m 9s\n",
      "batch: 900/1563 - train loss: 15.0382 - test loss: 15.4160 - train acc: 0.3203 - test acc: 0.3137 - 3m 14s\n",
      "batch: 1000/1563 - train loss: 14.9798 - test loss: 15.1971 - train acc: 0.3206 - test acc: 0.3258 - 3m 19s\n",
      "batch: 1100/1563 - train loss: 14.8353 - test loss: 16.9473 - train acc: 0.3343 - test acc: 0.2703 - 3m 24s\n",
      "batch: 1200/1563 - train loss: 14.9231 - test loss: 16.3381 - train acc: 0.3300 - test acc: 0.2881 - 3m 29s\n",
      "batch: 1300/1563 - train loss: 14.4458 - test loss: 15.2903 - train acc: 0.3378 - test acc: 0.3176 - 3m 34s\n",
      "batch: 1400/1563 - train loss: 14.8391 - test loss: 20.7479 - train acc: 0.3209 - test acc: 0.2031 - 3m 39s\n",
      "batch: 1500/1563 - train loss: 14.4878 - test loss: 15.1890 - train acc: 0.3469 - test acc: 0.3233 - 3m 44s\n",
      "batch: 1563/1563 - train loss: 14.4666 - test loss: 17.7536 - train acc: 0.3412 - test acc: 0.2630 - 3m 48s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.2690 - test loss: 15.3226 - train acc: 0.3774 - test acc: 0.3222 - 3m 53s\n",
      "batch: 200/1563 - train loss: 13.0788 - test loss: 16.3421 - train acc: 0.3903 - test acc: 0.2871 - 3m 58s\n",
      "batch: 300/1563 - train loss: 13.3732 - test loss: 16.1299 - train acc: 0.3843 - test acc: 0.2964 - 4m 3s\n",
      "batch: 400/1563 - train loss: 13.4389 - test loss: 14.7148 - train acc: 0.3690 - test acc: 0.3396 - 4m 8s\n",
      "batch: 500/1563 - train loss: 13.2878 - test loss: 14.8772 - train acc: 0.3850 - test acc: 0.3357 - 4m 12s\n",
      "batch: 600/1563 - train loss: 13.6118 - test loss: 14.6311 - train acc: 0.3697 - test acc: 0.3443 - 4m 18s\n",
      "batch: 700/1563 - train loss: 13.1789 - test loss: 15.0348 - train acc: 0.3937 - test acc: 0.3349 - 4m 22s\n",
      "batch: 800/1563 - train loss: 13.4628 - test loss: 16.5440 - train acc: 0.3684 - test acc: 0.2924 - 4m 27s\n",
      "batch: 900/1563 - train loss: 13.2464 - test loss: 15.6676 - train acc: 0.3916 - test acc: 0.3145 - 4m 32s\n",
      "batch: 1000/1563 - train loss: 13.3352 - test loss: 14.8503 - train acc: 0.3875 - test acc: 0.3436 - 4m 37s\n",
      "batch: 1100/1563 - train loss: 13.2881 - test loss: 15.2254 - train acc: 0.3928 - test acc: 0.3383 - 4m 42s\n",
      "batch: 1200/1563 - train loss: 13.4209 - test loss: 14.9970 - train acc: 0.3898 - test acc: 0.3309 - 4m 46s\n",
      "batch: 1300/1563 - train loss: 13.5834 - test loss: 14.5660 - train acc: 0.3712 - test acc: 0.3431 - 4m 52s\n",
      "batch: 1400/1563 - train loss: 13.0229 - test loss: 15.2734 - train acc: 0.3960 - test acc: 0.3290 - 4m 56s\n",
      "batch: 1500/1563 - train loss: 13.1583 - test loss: 14.4806 - train acc: 0.4013 - test acc: 0.3470 - 5m 1s\n",
      "batch: 1563/1563 - train loss: 13.1246 - test loss: 15.0457 - train acc: 0.3982 - test acc: 0.3336 - 5m 5s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.6006 - test loss: 14.4110 - train acc: 0.4432 - test acc: 0.3583 - 5m 10s\n",
      "batch: 200/1563 - train loss: 11.3948 - test loss: 14.3220 - train acc: 0.4597 - test acc: 0.3594 - 5m 15s\n",
      "batch: 300/1563 - train loss: 11.8560 - test loss: 14.7227 - train acc: 0.4375 - test acc: 0.3518 - 5m 19s\n",
      "batch: 400/1563 - train loss: 11.9261 - test loss: 16.8643 - train acc: 0.4435 - test acc: 0.3017 - 5m 25s\n",
      "batch: 500/1563 - train loss: 12.1106 - test loss: 15.5817 - train acc: 0.4363 - test acc: 0.3229 - 5m 29s\n",
      "batch: 600/1563 - train loss: 12.0578 - test loss: 16.1223 - train acc: 0.4291 - test acc: 0.3157 - 5m 34s\n",
      "batch: 700/1563 - train loss: 11.8225 - test loss: 14.6311 - train acc: 0.4498 - test acc: 0.3572 - 5m 39s\n",
      "batch: 800/1563 - train loss: 12.1754 - test loss: 14.8656 - train acc: 0.4341 - test acc: 0.3551 - 5m 44s\n",
      "batch: 900/1563 - train loss: 12.4967 - test loss: 13.9502 - train acc: 0.4163 - test acc: 0.3786 - 5m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 11.8135 - test loss: 14.0927 - train acc: 0.4428 - test acc: 0.3769 - 5m 53s\n",
      "batch: 1100/1563 - train loss: 11.8543 - test loss: 14.5836 - train acc: 0.4353 - test acc: 0.3534 - 5m 58s\n",
      "batch: 1200/1563 - train loss: 12.0000 - test loss: 13.9614 - train acc: 0.4338 - test acc: 0.3773 - 6m 3s\n",
      "batch: 1300/1563 - train loss: 11.8078 - test loss: 15.2897 - train acc: 0.4366 - test acc: 0.3375 - 6m 7s\n",
      "batch: 1400/1563 - train loss: 12.0655 - test loss: 13.8917 - train acc: 0.4325 - test acc: 0.3788 - 6m 12s\n",
      "batch: 1500/1563 - train loss: 12.1163 - test loss: 14.1341 - train acc: 0.4362 - test acc: 0.3722 - 6m 17s\n",
      "batch: 1563/1563 - train loss: 12.0964 - test loss: 13.5357 - train acc: 0.4403 - test acc: 0.3901 - 6m 21s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.3166 - test loss: 14.8735 - train acc: 0.4910 - test acc: 0.3506 - 6m 27s\n",
      "batch: 200/1563 - train loss: 10.3216 - test loss: 13.7328 - train acc: 0.5046 - test acc: 0.3904 - 6m 31s\n",
      "batch: 300/1563 - train loss: 10.3694 - test loss: 16.9713 - train acc: 0.4932 - test acc: 0.3180 - 6m 36s\n",
      "batch: 400/1563 - train loss: 10.7145 - test loss: 14.2928 - train acc: 0.4882 - test acc: 0.3742 - 6m 41s\n",
      "batch: 500/1563 - train loss: 10.5422 - test loss: 15.6056 - train acc: 0.4913 - test acc: 0.3395 - 6m 46s\n",
      "batch: 600/1563 - train loss: 10.7412 - test loss: 14.2128 - train acc: 0.4794 - test acc: 0.3794 - 6m 51s\n",
      "batch: 700/1563 - train loss: 10.9509 - test loss: 14.1580 - train acc: 0.4687 - test acc: 0.3757 - 6m 56s\n",
      "batch: 800/1563 - train loss: 10.8234 - test loss: 13.9637 - train acc: 0.4791 - test acc: 0.3809 - 7m 0s\n",
      "batch: 900/1563 - train loss: 10.7856 - test loss: 14.3764 - train acc: 0.4831 - test acc: 0.3677 - 7m 5s\n",
      "batch: 1000/1563 - train loss: 11.0644 - test loss: 14.5736 - train acc: 0.4678 - test acc: 0.3644 - 7m 10s\n",
      "batch: 1100/1563 - train loss: 11.1533 - test loss: 13.3327 - train acc: 0.4616 - test acc: 0.4031 - 7m 14s\n",
      "batch: 1200/1563 - train loss: 10.7295 - test loss: 13.3683 - train acc: 0.4828 - test acc: 0.4005 - 7m 19s\n",
      "batch: 1300/1563 - train loss: 10.9541 - test loss: 14.5389 - train acc: 0.4697 - test acc: 0.3667 - 7m 24s\n",
      "batch: 1400/1563 - train loss: 10.8765 - test loss: 13.4476 - train acc: 0.4784 - test acc: 0.3968 - 7m 29s\n",
      "batch: 1500/1563 - train loss: 10.6524 - test loss: 14.7406 - train acc: 0.4916 - test acc: 0.3703 - 7m 34s\n",
      "batch: 1563/1563 - train loss: 10.6216 - test loss: 14.5056 - train acc: 0.4888 - test acc: 0.3613 - 7m 38s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.7068 - test loss: 13.7454 - train acc: 0.5731 - test acc: 0.3954 - 7m 43s\n",
      "batch: 200/1563 - train loss: 8.9046 - test loss: 14.2574 - train acc: 0.5628 - test acc: 0.3850 - 7m 48s\n",
      "batch: 300/1563 - train loss: 9.3372 - test loss: 16.5564 - train acc: 0.5453 - test acc: 0.3391 - 7m 52s\n",
      "batch: 400/1563 - train loss: 9.6380 - test loss: 13.8165 - train acc: 0.5228 - test acc: 0.3958 - 7m 57s\n",
      "batch: 500/1563 - train loss: 9.6485 - test loss: 14.2640 - train acc: 0.5238 - test acc: 0.3855 - 8m 2s\n",
      "batch: 600/1563 - train loss: 9.4815 - test loss: 13.9518 - train acc: 0.5362 - test acc: 0.4037 - 8m 7s\n",
      "batch: 700/1563 - train loss: 9.5452 - test loss: 14.6047 - train acc: 0.5334 - test acc: 0.3743 - 8m 12s\n",
      "batch: 800/1563 - train loss: 9.8448 - test loss: 14.0148 - train acc: 0.5194 - test acc: 0.3883 - 8m 16s\n",
      "batch: 900/1563 - train loss: 9.3005 - test loss: 14.2112 - train acc: 0.5456 - test acc: 0.3854 - 8m 21s\n",
      "batch: 1000/1563 - train loss: 9.6933 - test loss: 14.6976 - train acc: 0.5209 - test acc: 0.3719 - 8m 26s\n",
      "batch: 1100/1563 - train loss: 9.8423 - test loss: 15.7073 - train acc: 0.5237 - test acc: 0.3570 - 8m 31s\n",
      "batch: 1200/1563 - train loss: 9.8605 - test loss: 14.2472 - train acc: 0.5156 - test acc: 0.3815 - 8m 36s\n",
      "batch: 1300/1563 - train loss: 9.9474 - test loss: 13.9786 - train acc: 0.5144 - test acc: 0.3959 - 8m 40s\n",
      "batch: 1400/1563 - train loss: 9.8264 - test loss: 13.6690 - train acc: 0.5244 - test acc: 0.4001 - 8m 45s\n",
      "batch: 1500/1563 - train loss: 10.1465 - test loss: 13.8540 - train acc: 0.5147 - test acc: 0.3909 - 8m 50s\n",
      "batch: 1563/1563 - train loss: 9.8143 - test loss: 14.0259 - train acc: 0.5250 - test acc: 0.3908 - 8m 54s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.5889 - test loss: 13.6950 - train acc: 0.6112 - test acc: 0.4154 - 8m 59s\n",
      "batch: 200/1563 - train loss: 7.8261 - test loss: 13.8319 - train acc: 0.6012 - test acc: 0.4127 - 9m 4s\n",
      "batch: 300/1563 - train loss: 7.8429 - test loss: 14.7569 - train acc: 0.6003 - test acc: 0.3899 - 9m 9s\n",
      "batch: 400/1563 - train loss: 7.9784 - test loss: 16.1338 - train acc: 0.5891 - test acc: 0.3650 - 9m 14s\n",
      "batch: 500/1563 - train loss: 8.2982 - test loss: 15.6132 - train acc: 0.5766 - test acc: 0.3630 - 9m 18s\n",
      "batch: 600/1563 - train loss: 8.5688 - test loss: 14.8739 - train acc: 0.5762 - test acc: 0.3855 - 9m 23s\n",
      "batch: 700/1563 - train loss: 8.5896 - test loss: 13.8810 - train acc: 0.5619 - test acc: 0.4050 - 9m 27s\n",
      "batch: 800/1563 - train loss: 8.3766 - test loss: 14.7120 - train acc: 0.5840 - test acc: 0.3891 - 9m 32s\n",
      "batch: 900/1563 - train loss: 8.7691 - test loss: 14.7068 - train acc: 0.5619 - test acc: 0.3982 - 9m 37s\n",
      "batch: 1000/1563 - train loss: 8.6486 - test loss: 14.5656 - train acc: 0.5759 - test acc: 0.3870 - 9m 42s\n",
      "batch: 1100/1563 - train loss: 8.8695 - test loss: 13.8105 - train acc: 0.5637 - test acc: 0.4116 - 9m 47s\n",
      "batch: 1200/1563 - train loss: 8.6925 - test loss: 13.5824 - train acc: 0.5612 - test acc: 0.4141 - 9m 52s\n",
      "batch: 1300/1563 - train loss: 8.7318 - test loss: 14.0200 - train acc: 0.5600 - test acc: 0.3965 - 9m 57s\n",
      "batch: 1400/1563 - train loss: 9.2299 - test loss: 13.8357 - train acc: 0.5518 - test acc: 0.4018 - 10m 1s\n",
      "batch: 1500/1563 - train loss: 9.0239 - test loss: 16.5648 - train acc: 0.5500 - test acc: 0.3554 - 10m 6s\n",
      "batch: 1563/1563 - train loss: 8.9492 - test loss: 13.9781 - train acc: 0.5513 - test acc: 0.4009 - 10m 11s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.6188 - test loss: 13.9967 - train acc: 0.6566 - test acc: 0.4220 - 10m 15s\n",
      "batch: 200/1563 - train loss: 6.6066 - test loss: 14.1666 - train acc: 0.6522 - test acc: 0.4018 - 10m 20s\n",
      "batch: 300/1563 - train loss: 6.8149 - test loss: 14.6954 - train acc: 0.6512 - test acc: 0.3920 - 10m 25s\n",
      "batch: 400/1563 - train loss: 7.0507 - test loss: 14.6476 - train acc: 0.6396 - test acc: 0.4008 - 10m 30s\n",
      "batch: 500/1563 - train loss: 7.1946 - test loss: 14.9045 - train acc: 0.6246 - test acc: 0.3930 - 10m 34s\n",
      "batch: 600/1563 - train loss: 7.3974 - test loss: 15.0013 - train acc: 0.6288 - test acc: 0.3945 - 10m 39s\n",
      "batch: 700/1563 - train loss: 7.3611 - test loss: 14.4791 - train acc: 0.6316 - test acc: 0.3972 - 10m 44s\n",
      "batch: 800/1563 - train loss: 7.6824 - test loss: 14.5944 - train acc: 0.6060 - test acc: 0.3946 - 10m 49s\n",
      "batch: 900/1563 - train loss: 7.6663 - test loss: 14.5729 - train acc: 0.6035 - test acc: 0.3953 - 10m 54s\n",
      "batch: 1000/1563 - train loss: 7.6021 - test loss: 14.4090 - train acc: 0.6084 - test acc: 0.4016 - 10m 59s\n",
      "batch: 1100/1563 - train loss: 8.0072 - test loss: 14.3079 - train acc: 0.5975 - test acc: 0.4120 - 11m 4s\n",
      "batch: 1200/1563 - train loss: 7.9543 - test loss: 14.2040 - train acc: 0.5988 - test acc: 0.4024 - 11m 8s\n",
      "batch: 1300/1563 - train loss: 7.9008 - test loss: 14.3906 - train acc: 0.5996 - test acc: 0.4057 - 11m 14s\n",
      "batch: 1400/1563 - train loss: 8.0448 - test loss: 14.2009 - train acc: 0.5893 - test acc: 0.4147 - 11m 18s\n",
      "batch: 1500/1563 - train loss: 7.8293 - test loss: 14.0811 - train acc: 0.5985 - test acc: 0.4145 - 11m 23s\n",
      "batch: 1563/1563 - train loss: 7.7902 - test loss: 13.8285 - train acc: 0.5940 - test acc: 0.4233 - 11m 27s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.3928 - test loss: 14.3381 - train acc: 0.7166 - test acc: 0.4188 - 11m 32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 5.4876 - test loss: 14.9744 - train acc: 0.7110 - test acc: 0.4119 - 11m 37s\n",
      "batch: 300/1563 - train loss: 6.1167 - test loss: 15.0811 - train acc: 0.6788 - test acc: 0.4052 - 11m 41s\n",
      "batch: 400/1563 - train loss: 5.9396 - test loss: 15.0649 - train acc: 0.6906 - test acc: 0.4069 - 11m 47s\n",
      "batch: 500/1563 - train loss: 6.2394 - test loss: 15.1537 - train acc: 0.6638 - test acc: 0.4013 - 11m 52s\n",
      "batch: 600/1563 - train loss: 6.3018 - test loss: 14.7866 - train acc: 0.6697 - test acc: 0.4171 - 11m 56s\n",
      "batch: 700/1563 - train loss: 6.4270 - test loss: 14.5882 - train acc: 0.6688 - test acc: 0.4177 - 12m 1s\n",
      "batch: 800/1563 - train loss: 6.3708 - test loss: 14.7501 - train acc: 0.6657 - test acc: 0.4217 - 12m 6s\n",
      "batch: 900/1563 - train loss: 6.5826 - test loss: 15.0464 - train acc: 0.6581 - test acc: 0.4062 - 12m 11s\n",
      "batch: 1000/1563 - train loss: 6.6731 - test loss: 16.3807 - train acc: 0.6534 - test acc: 0.3868 - 12m 16s\n",
      "batch: 1100/1563 - train loss: 6.6317 - test loss: 14.9522 - train acc: 0.6497 - test acc: 0.4115 - 12m 21s\n",
      "batch: 1200/1563 - train loss: 7.1132 - test loss: 14.6190 - train acc: 0.6315 - test acc: 0.4110 - 12m 26s\n",
      "batch: 1300/1563 - train loss: 6.8533 - test loss: 14.7871 - train acc: 0.6510 - test acc: 0.4143 - 12m 31s\n",
      "batch: 1400/1563 - train loss: 6.7004 - test loss: 14.4859 - train acc: 0.6484 - test acc: 0.4148 - 12m 36s\n",
      "batch: 1500/1563 - train loss: 7.3048 - test loss: 15.0774 - train acc: 0.6197 - test acc: 0.4033 - 12m 40s\n",
      "batch: 1563/1563 - train loss: 7.1740 - test loss: 14.6190 - train acc: 0.6313 - test acc: 0.4191 - 12m 45s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.4514 - test loss: 14.6580 - train acc: 0.7660 - test acc: 0.4273 - 12m 50s\n",
      "batch: 200/1563 - train loss: 4.4661 - test loss: 14.7175 - train acc: 0.7588 - test acc: 0.4262 - 12m 55s\n",
      "batch: 300/1563 - train loss: 4.6700 - test loss: 15.2657 - train acc: 0.7475 - test acc: 0.4108 - 13m 0s\n",
      "batch: 400/1563 - train loss: 4.9581 - test loss: 15.6067 - train acc: 0.7331 - test acc: 0.4123 - 13m 5s\n",
      "batch: 500/1563 - train loss: 5.5085 - test loss: 15.6596 - train acc: 0.6988 - test acc: 0.4104 - 13m 9s\n",
      "batch: 600/1563 - train loss: 5.4014 - test loss: 15.2933 - train acc: 0.7000 - test acc: 0.4174 - 13m 14s\n",
      "batch: 700/1563 - train loss: 5.4722 - test loss: 15.7353 - train acc: 0.7063 - test acc: 0.4069 - 13m 19s\n",
      "batch: 800/1563 - train loss: 5.3158 - test loss: 15.6401 - train acc: 0.7085 - test acc: 0.4096 - 13m 24s\n",
      "batch: 900/1563 - train loss: 5.8429 - test loss: 15.7127 - train acc: 0.6929 - test acc: 0.4042 - 13m 29s\n",
      "batch: 1000/1563 - train loss: 6.1621 - test loss: 15.7377 - train acc: 0.6754 - test acc: 0.3971 - 13m 34s\n",
      "batch: 1100/1563 - train loss: 5.9138 - test loss: 15.0939 - train acc: 0.6884 - test acc: 0.4178 - 13m 39s\n",
      "batch: 1200/1563 - train loss: 5.8480 - test loss: 15.0598 - train acc: 0.6972 - test acc: 0.4102 - 13m 44s\n",
      "batch: 1300/1563 - train loss: 6.0392 - test loss: 16.5054 - train acc: 0.6838 - test acc: 0.3899 - 13m 48s\n",
      "batch: 1400/1563 - train loss: 6.1950 - test loss: 15.3118 - train acc: 0.6722 - test acc: 0.4165 - 13m 53s\n",
      "batch: 1500/1563 - train loss: 6.1680 - test loss: 15.5310 - train acc: 0.6716 - test acc: 0.4032 - 13m 58s\n",
      "batch: 1563/1563 - train loss: 6.0911 - test loss: 15.8602 - train acc: 0.6710 - test acc: 0.4027 - 14m 2s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.7469 - test loss: 15.3493 - train acc: 0.7940 - test acc: 0.4197 - 14m 7s\n",
      "batch: 200/1563 - train loss: 3.6858 - test loss: 15.6206 - train acc: 0.8005 - test acc: 0.4171 - 14m 12s\n",
      "batch: 300/1563 - train loss: 3.9965 - test loss: 18.1097 - train acc: 0.7794 - test acc: 0.3794 - 14m 17s\n",
      "batch: 400/1563 - train loss: 4.3687 - test loss: 15.9595 - train acc: 0.7641 - test acc: 0.4181 - 14m 22s\n",
      "batch: 500/1563 - train loss: 4.4146 - test loss: 16.8148 - train acc: 0.7584 - test acc: 0.3963 - 14m 27s\n",
      "batch: 600/1563 - train loss: 4.4337 - test loss: 16.0740 - train acc: 0.7584 - test acc: 0.4196 - 14m 32s\n",
      "batch: 700/1563 - train loss: 4.5809 - test loss: 16.6599 - train acc: 0.7472 - test acc: 0.4055 - 14m 37s\n",
      "batch: 800/1563 - train loss: 4.7078 - test loss: 16.1546 - train acc: 0.7400 - test acc: 0.4153 - 14m 42s\n",
      "batch: 900/1563 - train loss: 4.7645 - test loss: 15.9687 - train acc: 0.7463 - test acc: 0.4176 - 14m 46s\n",
      "batch: 1000/1563 - train loss: 4.9141 - test loss: 16.7370 - train acc: 0.7303 - test acc: 0.4129 - 14m 51s\n",
      "batch: 1100/1563 - train loss: 5.1970 - test loss: 16.3268 - train acc: 0.7210 - test acc: 0.4052 - 14m 57s\n",
      "batch: 1200/1563 - train loss: 5.2093 - test loss: 15.5834 - train acc: 0.7238 - test acc: 0.4161 - 15m 1s\n",
      "batch: 1300/1563 - train loss: 5.0821 - test loss: 19.0581 - train acc: 0.7238 - test acc: 0.3646 - 15m 7s\n",
      "batch: 1400/1563 - train loss: 5.4621 - test loss: 16.0094 - train acc: 0.6981 - test acc: 0.4152 - 15m 11s\n",
      "batch: 1500/1563 - train loss: 5.2159 - test loss: 15.5949 - train acc: 0.7210 - test acc: 0.4158 - 15m 17s\n",
      "batch: 1563/1563 - train loss: 5.4446 - test loss: 16.2911 - train acc: 0.7119 - test acc: 0.4070 - 15m 21s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.4155 - test loss: 15.9576 - train acc: 0.8190 - test acc: 0.4191 - 15m 26s\n",
      "batch: 200/1563 - train loss: 2.9212 - test loss: 16.1046 - train acc: 0.8447 - test acc: 0.4188 - 15m 31s\n",
      "batch: 300/1563 - train loss: 3.3001 - test loss: 16.5851 - train acc: 0.8196 - test acc: 0.4215 - 15m 36s\n",
      "batch: 400/1563 - train loss: 3.1584 - test loss: 16.5345 - train acc: 0.8265 - test acc: 0.4232 - 15m 41s\n",
      "batch: 500/1563 - train loss: 3.4260 - test loss: 16.4263 - train acc: 0.8055 - test acc: 0.4195 - 15m 46s\n",
      "batch: 600/1563 - train loss: 3.6207 - test loss: 16.5434 - train acc: 0.8041 - test acc: 0.4119 - 15m 51s\n",
      "batch: 700/1563 - train loss: 3.9853 - test loss: 17.1526 - train acc: 0.7796 - test acc: 0.4034 - 15m 56s\n",
      "batch: 800/1563 - train loss: 4.1513 - test loss: 16.7598 - train acc: 0.7747 - test acc: 0.4131 - 16m 1s\n",
      "batch: 900/1563 - train loss: 3.9177 - test loss: 16.1445 - train acc: 0.7931 - test acc: 0.4177 - 16m 6s\n",
      "batch: 1000/1563 - train loss: 4.0627 - test loss: 16.5244 - train acc: 0.7749 - test acc: 0.4164 - 16m 10s\n",
      "batch: 1100/1563 - train loss: 4.4818 - test loss: 16.4399 - train acc: 0.7575 - test acc: 0.4134 - 16m 15s\n",
      "batch: 1200/1563 - train loss: 4.3422 - test loss: 16.4477 - train acc: 0.7622 - test acc: 0.4139 - 16m 20s\n",
      "batch: 1300/1563 - train loss: 4.3177 - test loss: 16.8834 - train acc: 0.7725 - test acc: 0.4101 - 16m 25s\n",
      "batch: 1400/1563 - train loss: 4.5562 - test loss: 16.1394 - train acc: 0.7534 - test acc: 0.4188 - 16m 30s\n",
      "batch: 1500/1563 - train loss: 4.4345 - test loss: 16.7925 - train acc: 0.7559 - test acc: 0.4093 - 16m 35s\n",
      "batch: 1563/1563 - train loss: 4.7395 - test loss: 16.2556 - train acc: 0.7447 - test acc: 0.4191 - 16m 39s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.6586 - test loss: 17.4864 - train acc: 0.8550 - test acc: 0.4113 - 16m 44s\n",
      "batch: 200/1563 - train loss: 2.4624 - test loss: 16.8187 - train acc: 0.8672 - test acc: 0.4270 - 16m 48s\n",
      "batch: 300/1563 - train loss: 2.4701 - test loss: 16.7058 - train acc: 0.8625 - test acc: 0.4310 - 16m 53s\n",
      "batch: 400/1563 - train loss: 2.6682 - test loss: 17.8175 - train acc: 0.8559 - test acc: 0.4104 - 16m 58s\n",
      "batch: 500/1563 - train loss: 2.7769 - test loss: 19.9485 - train acc: 0.8437 - test acc: 0.3807 - 17m 4s\n",
      "batch: 600/1563 - train loss: 2.9863 - test loss: 17.4609 - train acc: 0.8353 - test acc: 0.4171 - 17m 8s\n",
      "batch: 700/1563 - train loss: 3.1960 - test loss: 17.7256 - train acc: 0.8209 - test acc: 0.4092 - 17m 13s\n",
      "batch: 800/1563 - train loss: 3.1687 - test loss: 17.7848 - train acc: 0.8274 - test acc: 0.4074 - 17m 18s\n",
      "batch: 900/1563 - train loss: 3.2960 - test loss: 17.5237 - train acc: 0.8137 - test acc: 0.4128 - 17m 23s\n",
      "batch: 1000/1563 - train loss: 3.6208 - test loss: 17.2150 - train acc: 0.7918 - test acc: 0.4124 - 17m 28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 3.7529 - test loss: 18.1386 - train acc: 0.7946 - test acc: 0.3995 - 17m 33s\n",
      "batch: 1200/1563 - train loss: 3.9111 - test loss: 17.1014 - train acc: 0.7884 - test acc: 0.4214 - 17m 37s\n",
      "batch: 1300/1563 - train loss: 3.5279 - test loss: 16.8568 - train acc: 0.7980 - test acc: 0.4276 - 17m 42s\n",
      "batch: 1400/1563 - train loss: 3.6758 - test loss: 17.8342 - train acc: 0.7959 - test acc: 0.4048 - 17m 47s\n",
      "batch: 1500/1563 - train loss: 3.8963 - test loss: 17.7599 - train acc: 0.7853 - test acc: 0.4059 - 17m 52s\n",
      "batch: 1563/1563 - train loss: 3.7681 - test loss: 17.2786 - train acc: 0.7943 - test acc: 0.4069 - 17m 56s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.2352 - test loss: 16.9954 - train acc: 0.8829 - test acc: 0.4302 - 18m 1s\n",
      "batch: 200/1563 - train loss: 2.2405 - test loss: 17.6442 - train acc: 0.8707 - test acc: 0.4288 - 18m 6s\n",
      "batch: 300/1563 - train loss: 2.0748 - test loss: 18.0604 - train acc: 0.8872 - test acc: 0.4259 - 18m 11s\n",
      "batch: 400/1563 - train loss: 2.3480 - test loss: 17.6046 - train acc: 0.8640 - test acc: 0.4278 - 18m 16s\n",
      "batch: 500/1563 - train loss: 2.2959 - test loss: 18.7478 - train acc: 0.8719 - test acc: 0.4133 - 18m 21s\n",
      "batch: 600/1563 - train loss: 2.4560 - test loss: 18.0262 - train acc: 0.8596 - test acc: 0.4275 - 18m 26s\n",
      "batch: 700/1563 - train loss: 2.7968 - test loss: 18.6129 - train acc: 0.8449 - test acc: 0.4054 - 18m 30s\n",
      "batch: 800/1563 - train loss: 2.6574 - test loss: 18.0375 - train acc: 0.8494 - test acc: 0.4184 - 18m 35s\n",
      "batch: 900/1563 - train loss: 2.8378 - test loss: 17.9829 - train acc: 0.8368 - test acc: 0.4187 - 18m 40s\n",
      "batch: 1000/1563 - train loss: 2.9512 - test loss: 17.6686 - train acc: 0.8309 - test acc: 0.4266 - 18m 45s\n",
      "batch: 1100/1563 - train loss: 3.0717 - test loss: 17.8758 - train acc: 0.8319 - test acc: 0.4238 - 18m 50s\n",
      "batch: 1200/1563 - train loss: 3.0429 - test loss: 17.6713 - train acc: 0.8228 - test acc: 0.4174 - 18m 56s\n",
      "batch: 1300/1563 - train loss: 3.1630 - test loss: 18.4141 - train acc: 0.8193 - test acc: 0.4084 - 19m 0s\n",
      "batch: 1400/1563 - train loss: 3.1701 - test loss: 18.1758 - train acc: 0.8228 - test acc: 0.4135 - 19m 6s\n",
      "batch: 1500/1563 - train loss: 3.3636 - test loss: 18.7147 - train acc: 0.8067 - test acc: 0.4035 - 19m 11s\n",
      "batch: 1563/1563 - train loss: 3.5066 - test loss: 17.9742 - train acc: 0.8012 - test acc: 0.4076 - 19m 16s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.8112 - test loss: 17.7556 - train acc: 0.9039 - test acc: 0.4262 - 19m 21s\n",
      "batch: 200/1563 - train loss: 1.8278 - test loss: 18.1607 - train acc: 0.8960 - test acc: 0.4225 - 19m 26s\n",
      "batch: 300/1563 - train loss: 2.0065 - test loss: 18.1435 - train acc: 0.8865 - test acc: 0.4177 - 19m 31s\n",
      "batch: 400/1563 - train loss: 1.8110 - test loss: 19.3904 - train acc: 0.8960 - test acc: 0.4074 - 19m 36s\n",
      "batch: 500/1563 - train loss: 2.1018 - test loss: 18.5463 - train acc: 0.8791 - test acc: 0.4182 - 19m 41s\n",
      "batch: 600/1563 - train loss: 2.0511 - test loss: 18.4168 - train acc: 0.8866 - test acc: 0.4218 - 19m 46s\n",
      "batch: 700/1563 - train loss: 2.3321 - test loss: 19.5158 - train acc: 0.8622 - test acc: 0.4047 - 19m 52s\n",
      "batch: 800/1563 - train loss: 2.1669 - test loss: 18.9113 - train acc: 0.8769 - test acc: 0.4129 - 19m 57s\n",
      "time is up! finishing training\n",
      "batch: 835/1563 - train loss: 2.2527 - test loss: 18.7477 - train acc: 0.8678 - test acc: 0.4129 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 26.2123 - test loss: 25.2478 - train acc: 0.0358 - test acc: 0.0560 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.2439 - test loss: 23.1519 - train acc: 0.0573 - test acc: 0.0785 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.0845 - test loss: 22.4922 - train acc: 0.0855 - test acc: 0.0924 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.1205 - test loss: 22.3754 - train acc: 0.1087 - test acc: 0.1160 - 0m 16s\n",
      "batch: 500/1563 - train loss: 21.6682 - test loss: 21.0654 - train acc: 0.1159 - test acc: 0.1367 - 0m 21s\n",
      "batch: 600/1563 - train loss: 21.3757 - test loss: 20.8179 - train acc: 0.1237 - test acc: 0.1375 - 0m 25s\n",
      "batch: 700/1563 - train loss: 21.2517 - test loss: 20.2409 - train acc: 0.1296 - test acc: 0.1543 - 0m 31s\n",
      "batch: 800/1563 - train loss: 20.5704 - test loss: 20.3885 - train acc: 0.1428 - test acc: 0.1531 - 0m 35s\n",
      "batch: 900/1563 - train loss: 20.2043 - test loss: 20.9815 - train acc: 0.1585 - test acc: 0.1256 - 0m 40s\n",
      "batch: 1000/1563 - train loss: 20.0708 - test loss: 19.5774 - train acc: 0.1685 - test acc: 0.1795 - 0m 45s\n",
      "batch: 1100/1563 - train loss: 19.8145 - test loss: 19.9835 - train acc: 0.1735 - test acc: 0.1702 - 0m 50s\n",
      "batch: 1200/1563 - train loss: 19.6443 - test loss: 19.9916 - train acc: 0.1757 - test acc: 0.1636 - 0m 55s\n",
      "batch: 1300/1563 - train loss: 19.6991 - test loss: 19.3020 - train acc: 0.1694 - test acc: 0.1926 - 1m 0s\n",
      "batch: 1400/1563 - train loss: 19.3703 - test loss: 19.1022 - train acc: 0.1679 - test acc: 0.1888 - 1m 5s\n",
      "batch: 1500/1563 - train loss: 19.1065 - test loss: 18.6424 - train acc: 0.1888 - test acc: 0.2073 - 1m 10s\n",
      "batch: 1563/1563 - train loss: 18.6700 - test loss: 18.2887 - train acc: 0.2053 - test acc: 0.2208 - 1m 14s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.8368 - test loss: 18.2445 - train acc: 0.2207 - test acc: 0.2222 - 1m 19s\n",
      "batch: 200/1563 - train loss: 17.7642 - test loss: 18.3142 - train acc: 0.2247 - test acc: 0.2244 - 1m 23s\n",
      "batch: 300/1563 - train loss: 17.6876 - test loss: 17.9899 - train acc: 0.2284 - test acc: 0.2227 - 1m 28s\n",
      "batch: 400/1563 - train loss: 17.3859 - test loss: 18.0802 - train acc: 0.2385 - test acc: 0.2281 - 1m 33s\n",
      "batch: 500/1563 - train loss: 17.5268 - test loss: 18.2926 - train acc: 0.2391 - test acc: 0.2217 - 1m 38s\n",
      "batch: 600/1563 - train loss: 17.5386 - test loss: 17.5770 - train acc: 0.2450 - test acc: 0.2439 - 1m 43s\n",
      "batch: 700/1563 - train loss: 17.3481 - test loss: 17.9745 - train acc: 0.2506 - test acc: 0.2327 - 1m 48s\n",
      "batch: 800/1563 - train loss: 17.1633 - test loss: 17.5470 - train acc: 0.2491 - test acc: 0.2355 - 1m 53s\n",
      "batch: 900/1563 - train loss: 16.8072 - test loss: 17.8709 - train acc: 0.2538 - test acc: 0.2300 - 1m 58s\n",
      "batch: 1000/1563 - train loss: 17.1554 - test loss: 16.8031 - train acc: 0.2450 - test acc: 0.2711 - 2m 3s\n",
      "batch: 1100/1563 - train loss: 16.9252 - test loss: 16.7551 - train acc: 0.2650 - test acc: 0.2680 - 2m 8s\n",
      "batch: 1200/1563 - train loss: 17.0074 - test loss: 17.0028 - train acc: 0.2682 - test acc: 0.2697 - 2m 13s\n",
      "batch: 1300/1563 - train loss: 16.6233 - test loss: 16.6390 - train acc: 0.2656 - test acc: 0.2702 - 2m 17s\n",
      "batch: 1400/1563 - train loss: 16.6618 - test loss: 16.7968 - train acc: 0.2668 - test acc: 0.2646 - 2m 22s\n",
      "batch: 1500/1563 - train loss: 16.6111 - test loss: 17.3206 - train acc: 0.2757 - test acc: 0.2547 - 2m 27s\n",
      "batch: 1563/1563 - train loss: 16.3884 - test loss: 16.3216 - train acc: 0.2809 - test acc: 0.2878 - 2m 31s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.1316 - test loss: 16.7936 - train acc: 0.3131 - test acc: 0.2791 - 2m 36s\n",
      "batch: 200/1563 - train loss: 15.3430 - test loss: 16.2462 - train acc: 0.3043 - test acc: 0.2880 - 2m 41s\n",
      "batch: 300/1563 - train loss: 15.1928 - test loss: 16.0798 - train acc: 0.3297 - test acc: 0.2965 - 2m 46s\n",
      "batch: 400/1563 - train loss: 15.1748 - test loss: 16.9369 - train acc: 0.3168 - test acc: 0.2665 - 2m 51s\n",
      "batch: 500/1563 - train loss: 15.3233 - test loss: 16.0650 - train acc: 0.3169 - test acc: 0.2929 - 2m 56s\n",
      "batch: 600/1563 - train loss: 15.0103 - test loss: 15.6801 - train acc: 0.3335 - test acc: 0.3078 - 3m 0s\n",
      "batch: 700/1563 - train loss: 14.9264 - test loss: 16.4588 - train acc: 0.3315 - test acc: 0.2816 - 3m 5s\n",
      "batch: 800/1563 - train loss: 15.1850 - test loss: 16.8497 - train acc: 0.3166 - test acc: 0.2782 - 3m 10s\n",
      "batch: 900/1563 - train loss: 15.1056 - test loss: 15.1252 - train acc: 0.3231 - test acc: 0.3289 - 3m 15s\n",
      "batch: 1000/1563 - train loss: 14.9468 - test loss: 16.6297 - train acc: 0.3269 - test acc: 0.2757 - 3m 20s\n",
      "batch: 1100/1563 - train loss: 14.8149 - test loss: 16.1756 - train acc: 0.3319 - test acc: 0.2940 - 3m 25s\n",
      "batch: 1200/1563 - train loss: 14.7727 - test loss: 15.5085 - train acc: 0.3371 - test acc: 0.3128 - 3m 30s\n",
      "batch: 1300/1563 - train loss: 14.6519 - test loss: 14.8959 - train acc: 0.3484 - test acc: 0.3369 - 3m 35s\n",
      "batch: 1400/1563 - train loss: 14.6867 - test loss: 15.3284 - train acc: 0.3349 - test acc: 0.3261 - 3m 40s\n",
      "batch: 1500/1563 - train loss: 14.5750 - test loss: 15.7190 - train acc: 0.3483 - test acc: 0.3123 - 3m 45s\n",
      "batch: 1563/1563 - train loss: 14.6187 - test loss: 14.9757 - train acc: 0.3463 - test acc: 0.3326 - 3m 49s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.2879 - test loss: 15.8445 - train acc: 0.3844 - test acc: 0.3022 - 3m 54s\n",
      "batch: 200/1563 - train loss: 13.0667 - test loss: 16.3525 - train acc: 0.3831 - test acc: 0.2991 - 3m 58s\n",
      "batch: 300/1563 - train loss: 13.3523 - test loss: 16.4723 - train acc: 0.3734 - test acc: 0.3062 - 4m 3s\n",
      "batch: 400/1563 - train loss: 13.5132 - test loss: 14.8063 - train acc: 0.3850 - test acc: 0.3408 - 4m 8s\n",
      "batch: 500/1563 - train loss: 13.6373 - test loss: 14.6125 - train acc: 0.3731 - test acc: 0.3518 - 4m 13s\n",
      "batch: 600/1563 - train loss: 13.4079 - test loss: 14.4704 - train acc: 0.3785 - test acc: 0.3522 - 4m 18s\n",
      "batch: 700/1563 - train loss: 13.4744 - test loss: 14.9176 - train acc: 0.3806 - test acc: 0.3479 - 4m 23s\n",
      "batch: 800/1563 - train loss: 13.3837 - test loss: 14.7550 - train acc: 0.3787 - test acc: 0.3425 - 4m 28s\n",
      "batch: 900/1563 - train loss: 13.4131 - test loss: 14.9381 - train acc: 0.3797 - test acc: 0.3387 - 4m 32s\n",
      "batch: 1000/1563 - train loss: 13.2329 - test loss: 15.0826 - train acc: 0.3916 - test acc: 0.3383 - 4m 37s\n",
      "batch: 1100/1563 - train loss: 13.2407 - test loss: 14.5971 - train acc: 0.3815 - test acc: 0.3515 - 4m 42s\n",
      "batch: 1200/1563 - train loss: 13.5615 - test loss: 14.8004 - train acc: 0.3822 - test acc: 0.3399 - 4m 47s\n",
      "batch: 1300/1563 - train loss: 12.9724 - test loss: 14.5738 - train acc: 0.4044 - test acc: 0.3471 - 4m 52s\n",
      "batch: 1400/1563 - train loss: 13.2928 - test loss: 14.4498 - train acc: 0.3881 - test acc: 0.3541 - 4m 57s\n",
      "batch: 1500/1563 - train loss: 13.0203 - test loss: 14.7899 - train acc: 0.3994 - test acc: 0.3443 - 5m 2s\n",
      "batch: 1563/1563 - train loss: 12.8784 - test loss: 14.3144 - train acc: 0.4034 - test acc: 0.3587 - 5m 6s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.3075 - test loss: 15.6218 - train acc: 0.4679 - test acc: 0.3279 - 5m 11s\n",
      "batch: 200/1563 - train loss: 11.4679 - test loss: 14.3958 - train acc: 0.4591 - test acc: 0.3629 - 5m 16s\n",
      "batch: 300/1563 - train loss: 11.7864 - test loss: 15.7207 - train acc: 0.4432 - test acc: 0.3263 - 5m 21s\n",
      "batch: 400/1563 - train loss: 11.5040 - test loss: 14.1243 - train acc: 0.4522 - test acc: 0.3734 - 5m 25s\n",
      "batch: 500/1563 - train loss: 11.7756 - test loss: 14.4118 - train acc: 0.4388 - test acc: 0.3641 - 5m 30s\n",
      "batch: 600/1563 - train loss: 11.8431 - test loss: 13.9751 - train acc: 0.4429 - test acc: 0.3800 - 5m 35s\n",
      "batch: 700/1563 - train loss: 11.8681 - test loss: 13.8656 - train acc: 0.4388 - test acc: 0.3756 - 5m 40s\n",
      "batch: 800/1563 - train loss: 12.1051 - test loss: 14.1098 - train acc: 0.4344 - test acc: 0.3770 - 5m 45s\n",
      "batch: 900/1563 - train loss: 11.8840 - test loss: 15.2956 - train acc: 0.4481 - test acc: 0.3485 - 5m 50s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 12.2049 - test loss: 13.9411 - train acc: 0.4265 - test acc: 0.3726 - 5m 55s\n",
      "batch: 1100/1563 - train loss: 12.1647 - test loss: 14.1185 - train acc: 0.4409 - test acc: 0.3689 - 6m 0s\n",
      "batch: 1200/1563 - train loss: 11.9852 - test loss: 13.6945 - train acc: 0.4325 - test acc: 0.3882 - 6m 4s\n",
      "batch: 1300/1563 - train loss: 11.8445 - test loss: 14.2055 - train acc: 0.4347 - test acc: 0.3709 - 6m 9s\n",
      "batch: 1400/1563 - train loss: 11.7559 - test loss: 13.8546 - train acc: 0.4534 - test acc: 0.3823 - 6m 14s\n",
      "batch: 1500/1563 - train loss: 12.0866 - test loss: 13.5570 - train acc: 0.4331 - test acc: 0.3876 - 6m 19s\n",
      "batch: 1563/1563 - train loss: 11.6807 - test loss: 13.9700 - train acc: 0.4453 - test acc: 0.3773 - 6m 23s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0006 - test loss: 14.5729 - train acc: 0.5178 - test acc: 0.3722 - 6m 28s\n",
      "batch: 200/1563 - train loss: 10.0732 - test loss: 15.6614 - train acc: 0.5209 - test acc: 0.3392 - 6m 33s\n",
      "batch: 300/1563 - train loss: 10.5177 - test loss: 14.3145 - train acc: 0.4919 - test acc: 0.3757 - 6m 38s\n",
      "batch: 400/1563 - train loss: 10.3842 - test loss: 14.0272 - train acc: 0.5116 - test acc: 0.3880 - 6m 42s\n",
      "batch: 500/1563 - train loss: 10.4272 - test loss: 14.3027 - train acc: 0.4875 - test acc: 0.3747 - 6m 47s\n",
      "batch: 600/1563 - train loss: 10.5726 - test loss: 14.1779 - train acc: 0.4891 - test acc: 0.3847 - 6m 53s\n",
      "batch: 700/1563 - train loss: 10.2466 - test loss: 14.0872 - train acc: 0.5122 - test acc: 0.3826 - 6m 58s\n",
      "batch: 800/1563 - train loss: 10.8815 - test loss: 14.2598 - train acc: 0.4819 - test acc: 0.3806 - 7m 2s\n",
      "batch: 900/1563 - train loss: 10.7439 - test loss: 14.3242 - train acc: 0.4822 - test acc: 0.3744 - 7m 7s\n",
      "batch: 1000/1563 - train loss: 10.6886 - test loss: 14.2649 - train acc: 0.4819 - test acc: 0.3762 - 7m 12s\n",
      "batch: 1100/1563 - train loss: 10.8686 - test loss: 13.9436 - train acc: 0.4810 - test acc: 0.3908 - 7m 17s\n",
      "batch: 1200/1563 - train loss: 10.9645 - test loss: 13.3060 - train acc: 0.4659 - test acc: 0.4113 - 7m 22s\n",
      "batch: 1300/1563 - train loss: 10.6571 - test loss: 13.6920 - train acc: 0.4822 - test acc: 0.3985 - 7m 27s\n",
      "batch: 1400/1563 - train loss: 11.0533 - test loss: 14.1081 - train acc: 0.4825 - test acc: 0.3839 - 7m 32s\n",
      "batch: 1500/1563 - train loss: 10.8010 - test loss: 13.8947 - train acc: 0.4781 - test acc: 0.3953 - 7m 36s\n",
      "batch: 1563/1563 - train loss: 11.1872 - test loss: 14.9063 - train acc: 0.4613 - test acc: 0.3597 - 7m 41s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.3179 - test loss: 19.0996 - train acc: 0.5912 - test acc: 0.2980 - 7m 45s\n",
      "batch: 200/1563 - train loss: 8.9469 - test loss: 14.5554 - train acc: 0.5522 - test acc: 0.3766 - 7m 50s\n",
      "batch: 300/1563 - train loss: 9.0631 - test loss: 14.6423 - train acc: 0.5487 - test acc: 0.3734 - 7m 55s\n",
      "batch: 400/1563 - train loss: 9.2043 - test loss: 14.1533 - train acc: 0.5384 - test acc: 0.4003 - 8m 1s\n",
      "batch: 500/1563 - train loss: 8.9231 - test loss: 14.1232 - train acc: 0.5581 - test acc: 0.3966 - 8m 5s\n",
      "batch: 600/1563 - train loss: 9.2076 - test loss: 14.1852 - train acc: 0.5422 - test acc: 0.3904 - 8m 10s\n",
      "batch: 700/1563 - train loss: 9.3623 - test loss: 14.2523 - train acc: 0.5372 - test acc: 0.3871 - 8m 15s\n",
      "batch: 800/1563 - train loss: 9.2756 - test loss: 14.6267 - train acc: 0.5369 - test acc: 0.3819 - 8m 20s\n",
      "batch: 900/1563 - train loss: 9.6103 - test loss: 13.7747 - train acc: 0.5281 - test acc: 0.3990 - 8m 25s\n",
      "batch: 1000/1563 - train loss: 9.3097 - test loss: 14.0204 - train acc: 0.5403 - test acc: 0.3904 - 8m 30s\n",
      "batch: 1100/1563 - train loss: 9.7982 - test loss: 13.1783 - train acc: 0.5225 - test acc: 0.4157 - 8m 35s\n",
      "batch: 1200/1563 - train loss: 9.8430 - test loss: 14.4674 - train acc: 0.5159 - test acc: 0.3810 - 8m 40s\n",
      "batch: 1300/1563 - train loss: 9.5564 - test loss: 14.0782 - train acc: 0.5325 - test acc: 0.3873 - 8m 44s\n",
      "batch: 1400/1563 - train loss: 9.7570 - test loss: 13.5718 - train acc: 0.5256 - test acc: 0.4061 - 8m 49s\n",
      "batch: 1500/1563 - train loss: 9.8174 - test loss: 13.4938 - train acc: 0.5243 - test acc: 0.4118 - 8m 54s\n",
      "batch: 1563/1563 - train loss: 9.7342 - test loss: 13.5915 - train acc: 0.5271 - test acc: 0.4015 - 8m 58s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.3187 - test loss: 13.6531 - train acc: 0.6366 - test acc: 0.4189 - 9m 3s\n",
      "batch: 200/1563 - train loss: 7.5132 - test loss: 13.8881 - train acc: 0.6168 - test acc: 0.4071 - 9m 8s\n",
      "batch: 300/1563 - train loss: 8.0033 - test loss: 14.7186 - train acc: 0.5903 - test acc: 0.3902 - 9m 13s\n",
      "batch: 400/1563 - train loss: 7.5732 - test loss: 14.4324 - train acc: 0.6159 - test acc: 0.3981 - 9m 18s\n",
      "batch: 500/1563 - train loss: 7.9224 - test loss: 15.1547 - train acc: 0.5996 - test acc: 0.3894 - 9m 22s\n",
      "batch: 600/1563 - train loss: 8.3101 - test loss: 14.4832 - train acc: 0.5868 - test acc: 0.3957 - 9m 27s\n",
      "batch: 700/1563 - train loss: 8.1677 - test loss: 13.8782 - train acc: 0.5910 - test acc: 0.4161 - 9m 32s\n",
      "batch: 800/1563 - train loss: 8.5375 - test loss: 14.1957 - train acc: 0.5700 - test acc: 0.3994 - 9m 37s\n",
      "batch: 900/1563 - train loss: 8.3044 - test loss: 14.2647 - train acc: 0.5865 - test acc: 0.4033 - 9m 42s\n",
      "batch: 1000/1563 - train loss: 8.3128 - test loss: 14.1732 - train acc: 0.5778 - test acc: 0.4048 - 9m 47s\n",
      "batch: 1100/1563 - train loss: 8.5950 - test loss: 14.4781 - train acc: 0.5687 - test acc: 0.3943 - 9m 52s\n",
      "batch: 1200/1563 - train loss: 8.6331 - test loss: 13.9630 - train acc: 0.5659 - test acc: 0.4101 - 9m 57s\n",
      "batch: 1300/1563 - train loss: 8.6068 - test loss: 13.9059 - train acc: 0.5696 - test acc: 0.3986 - 10m 2s\n",
      "batch: 1400/1563 - train loss: 8.7096 - test loss: 13.8949 - train acc: 0.5696 - test acc: 0.4078 - 10m 7s\n",
      "batch: 1500/1563 - train loss: 8.7797 - test loss: 13.5923 - train acc: 0.5625 - test acc: 0.4156 - 10m 12s\n",
      "batch: 1563/1563 - train loss: 8.7479 - test loss: 13.5332 - train acc: 0.5660 - test acc: 0.4245 - 10m 16s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.1007 - test loss: 14.1999 - train acc: 0.6756 - test acc: 0.4125 - 10m 21s\n",
      "batch: 200/1563 - train loss: 5.9840 - test loss: 14.3569 - train acc: 0.6963 - test acc: 0.4125 - 10m 26s\n",
      "batch: 300/1563 - train loss: 6.5003 - test loss: 15.2834 - train acc: 0.6631 - test acc: 0.4008 - 10m 31s\n",
      "batch: 400/1563 - train loss: 6.4467 - test loss: 14.6261 - train acc: 0.6706 - test acc: 0.4145 - 10m 37s\n",
      "batch: 500/1563 - train loss: 7.1506 - test loss: 14.4738 - train acc: 0.6407 - test acc: 0.4133 - 10m 42s\n",
      "batch: 600/1563 - train loss: 7.1274 - test loss: 14.3231 - train acc: 0.6269 - test acc: 0.4206 - 10m 46s\n",
      "batch: 700/1563 - train loss: 7.2238 - test loss: 14.8804 - train acc: 0.6237 - test acc: 0.4050 - 10m 51s\n",
      "batch: 800/1563 - train loss: 7.1975 - test loss: 14.1504 - train acc: 0.6304 - test acc: 0.4265 - 10m 56s\n",
      "batch: 900/1563 - train loss: 7.2621 - test loss: 14.4831 - train acc: 0.6247 - test acc: 0.4026 - 11m 1s\n",
      "batch: 1000/1563 - train loss: 7.5412 - test loss: 14.3890 - train acc: 0.6178 - test acc: 0.4114 - 11m 6s\n",
      "batch: 1100/1563 - train loss: 7.5389 - test loss: 13.9893 - train acc: 0.6225 - test acc: 0.4176 - 11m 11s\n",
      "batch: 1200/1563 - train loss: 7.9091 - test loss: 13.7739 - train acc: 0.5947 - test acc: 0.4228 - 11m 16s\n",
      "batch: 1300/1563 - train loss: 7.4886 - test loss: 14.7742 - train acc: 0.6144 - test acc: 0.4061 - 11m 21s\n",
      "batch: 1400/1563 - train loss: 8.0107 - test loss: 14.6347 - train acc: 0.5947 - test acc: 0.4031 - 11m 26s\n",
      "batch: 1500/1563 - train loss: 7.7110 - test loss: 14.3843 - train acc: 0.5991 - test acc: 0.4096 - 11m 32s\n",
      "batch: 1563/1563 - train loss: 7.6595 - test loss: 15.2672 - train acc: 0.6057 - test acc: 0.3833 - 11m 36s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.0127 - test loss: 14.3294 - train acc: 0.7372 - test acc: 0.4239 - 11m 42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 5.2300 - test loss: 14.5976 - train acc: 0.7213 - test acc: 0.4203 - 11m 47s\n",
      "batch: 300/1563 - train loss: 5.1692 - test loss: 14.7289 - train acc: 0.7288 - test acc: 0.4285 - 11m 52s\n",
      "batch: 400/1563 - train loss: 5.5209 - test loss: 15.3914 - train acc: 0.7131 - test acc: 0.4035 - 11m 57s\n",
      "batch: 500/1563 - train loss: 5.6972 - test loss: 15.5133 - train acc: 0.7013 - test acc: 0.4069 - 12m 2s\n",
      "batch: 600/1563 - train loss: 6.0597 - test loss: 16.3880 - train acc: 0.6734 - test acc: 0.3859 - 12m 7s\n",
      "batch: 700/1563 - train loss: 6.4369 - test loss: 15.2668 - train acc: 0.6635 - test acc: 0.4039 - 12m 13s\n",
      "batch: 800/1563 - train loss: 6.1364 - test loss: 14.8397 - train acc: 0.6697 - test acc: 0.4164 - 12m 18s\n",
      "batch: 900/1563 - train loss: 6.2447 - test loss: 15.4059 - train acc: 0.6722 - test acc: 0.4043 - 12m 23s\n",
      "batch: 1000/1563 - train loss: 6.2860 - test loss: 14.7719 - train acc: 0.6734 - test acc: 0.4163 - 12m 28s\n",
      "batch: 1100/1563 - train loss: 6.4438 - test loss: 14.6780 - train acc: 0.6628 - test acc: 0.4241 - 12m 33s\n",
      "batch: 1200/1563 - train loss: 6.5317 - test loss: 15.1483 - train acc: 0.6556 - test acc: 0.4134 - 12m 38s\n",
      "batch: 1300/1563 - train loss: 6.5932 - test loss: 15.3779 - train acc: 0.6641 - test acc: 0.3994 - 12m 43s\n",
      "batch: 1400/1563 - train loss: 6.7985 - test loss: 14.3986 - train acc: 0.6469 - test acc: 0.4250 - 12m 48s\n",
      "batch: 1500/1563 - train loss: 7.0748 - test loss: 14.7895 - train acc: 0.6375 - test acc: 0.4086 - 12m 53s\n",
      "batch: 1563/1563 - train loss: 6.8524 - test loss: 15.0555 - train acc: 0.6416 - test acc: 0.4019 - 12m 57s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.2851 - test loss: 14.6535 - train acc: 0.7828 - test acc: 0.4320 - 13m 2s\n",
      "batch: 200/1563 - train loss: 4.0569 - test loss: 15.5520 - train acc: 0.7828 - test acc: 0.4139 - 13m 7s\n",
      "batch: 300/1563 - train loss: 4.4873 - test loss: 15.4147 - train acc: 0.7580 - test acc: 0.4156 - 13m 12s\n",
      "batch: 400/1563 - train loss: 4.6883 - test loss: 19.3709 - train acc: 0.7494 - test acc: 0.3527 - 13m 18s\n",
      "batch: 500/1563 - train loss: 4.8994 - test loss: 15.8557 - train acc: 0.7322 - test acc: 0.4017 - 13m 23s\n",
      "batch: 600/1563 - train loss: 5.0493 - test loss: 16.1727 - train acc: 0.7237 - test acc: 0.4002 - 13m 28s\n",
      "batch: 700/1563 - train loss: 5.0235 - test loss: 15.4536 - train acc: 0.7291 - test acc: 0.4143 - 13m 32s\n",
      "batch: 800/1563 - train loss: 5.2769 - test loss: 15.8102 - train acc: 0.7213 - test acc: 0.4069 - 13m 37s\n",
      "batch: 900/1563 - train loss: 5.2892 - test loss: 15.0490 - train acc: 0.7151 - test acc: 0.4229 - 13m 42s\n",
      "batch: 1000/1563 - train loss: 5.5489 - test loss: 16.1776 - train acc: 0.6976 - test acc: 0.4033 - 13m 47s\n",
      "batch: 1100/1563 - train loss: 5.6610 - test loss: 16.6680 - train acc: 0.6963 - test acc: 0.3891 - 13m 52s\n",
      "batch: 1200/1563 - train loss: 6.0192 - test loss: 15.7628 - train acc: 0.6810 - test acc: 0.4071 - 13m 57s\n",
      "batch: 1300/1563 - train loss: 5.7446 - test loss: 15.2912 - train acc: 0.6969 - test acc: 0.4080 - 14m 2s\n",
      "batch: 1400/1563 - train loss: 5.6621 - test loss: 15.4882 - train acc: 0.6926 - test acc: 0.4087 - 14m 7s\n",
      "batch: 1500/1563 - train loss: 5.8533 - test loss: 15.1488 - train acc: 0.6869 - test acc: 0.4176 - 14m 12s\n",
      "batch: 1563/1563 - train loss: 5.9404 - test loss: 14.9751 - train acc: 0.6831 - test acc: 0.4145 - 14m 16s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.4792 - test loss: 15.4881 - train acc: 0.8146 - test acc: 0.4210 - 14m 21s\n",
      "batch: 200/1563 - train loss: 3.3440 - test loss: 15.5227 - train acc: 0.8218 - test acc: 0.4302 - 14m 26s\n",
      "batch: 300/1563 - train loss: 3.6978 - test loss: 16.1148 - train acc: 0.7956 - test acc: 0.4132 - 14m 31s\n",
      "batch: 400/1563 - train loss: 3.6038 - test loss: 16.3380 - train acc: 0.8062 - test acc: 0.4094 - 14m 35s\n",
      "batch: 500/1563 - train loss: 3.8924 - test loss: 16.2711 - train acc: 0.7922 - test acc: 0.4188 - 14m 41s\n",
      "batch: 600/1563 - train loss: 4.0515 - test loss: 16.4046 - train acc: 0.7865 - test acc: 0.4074 - 14m 46s\n",
      "batch: 700/1563 - train loss: 4.0741 - test loss: 16.0546 - train acc: 0.7746 - test acc: 0.4166 - 14m 51s\n",
      "batch: 800/1563 - train loss: 4.5276 - test loss: 16.6725 - train acc: 0.7562 - test acc: 0.4028 - 14m 55s\n",
      "batch: 900/1563 - train loss: 4.5870 - test loss: 16.4402 - train acc: 0.7513 - test acc: 0.4124 - 15m 0s\n",
      "batch: 1000/1563 - train loss: 4.5219 - test loss: 16.5129 - train acc: 0.7500 - test acc: 0.4036 - 15m 5s\n",
      "batch: 1100/1563 - train loss: 4.7075 - test loss: 16.8920 - train acc: 0.7409 - test acc: 0.4015 - 15m 10s\n",
      "batch: 1200/1563 - train loss: 4.8218 - test loss: 16.2899 - train acc: 0.7375 - test acc: 0.3995 - 15m 15s\n",
      "batch: 1300/1563 - train loss: 4.8035 - test loss: 16.3248 - train acc: 0.7353 - test acc: 0.4148 - 15m 20s\n",
      "batch: 1400/1563 - train loss: 5.1949 - test loss: 16.2994 - train acc: 0.7154 - test acc: 0.4143 - 15m 25s\n",
      "batch: 1500/1563 - train loss: 5.1702 - test loss: 17.1398 - train acc: 0.7147 - test acc: 0.3969 - 15m 29s\n",
      "batch: 1563/1563 - train loss: 5.1857 - test loss: 16.3709 - train acc: 0.7209 - test acc: 0.4139 - 15m 34s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 2.7910 - test loss: 16.3965 - train acc: 0.8434 - test acc: 0.4234 - 15m 39s\n",
      "batch: 200/1563 - train loss: 3.0036 - test loss: 16.2967 - train acc: 0.8384 - test acc: 0.4223 - 15m 43s\n",
      "batch: 300/1563 - train loss: 2.8489 - test loss: 16.9447 - train acc: 0.8355 - test acc: 0.4120 - 15m 48s\n",
      "batch: 400/1563 - train loss: 3.1704 - test loss: 16.6221 - train acc: 0.8231 - test acc: 0.4214 - 15m 53s\n",
      "batch: 500/1563 - train loss: 3.0491 - test loss: 16.7700 - train acc: 0.8330 - test acc: 0.4217 - 15m 59s\n",
      "batch: 600/1563 - train loss: 3.1188 - test loss: 16.5236 - train acc: 0.8174 - test acc: 0.4250 - 16m 3s\n",
      "batch: 700/1563 - train loss: 3.4547 - test loss: 17.2410 - train acc: 0.8105 - test acc: 0.4196 - 16m 8s\n",
      "batch: 800/1563 - train loss: 3.6855 - test loss: 16.8616 - train acc: 0.7918 - test acc: 0.4239 - 16m 13s\n",
      "batch: 900/1563 - train loss: 3.8809 - test loss: 17.5079 - train acc: 0.7781 - test acc: 0.4015 - 16m 18s\n",
      "batch: 1000/1563 - train loss: 3.9013 - test loss: 17.2104 - train acc: 0.7812 - test acc: 0.4154 - 16m 23s\n",
      "batch: 1100/1563 - train loss: 3.8054 - test loss: 16.8176 - train acc: 0.7868 - test acc: 0.4225 - 16m 29s\n",
      "batch: 1200/1563 - train loss: 4.0819 - test loss: 16.8741 - train acc: 0.7759 - test acc: 0.4156 - 16m 33s\n",
      "batch: 1300/1563 - train loss: 4.0463 - test loss: 16.8425 - train acc: 0.7768 - test acc: 0.4202 - 16m 38s\n",
      "batch: 1400/1563 - train loss: 4.3422 - test loss: 16.6343 - train acc: 0.7641 - test acc: 0.4173 - 16m 43s\n",
      "batch: 1500/1563 - train loss: 4.3203 - test loss: 16.7450 - train acc: 0.7562 - test acc: 0.4180 - 16m 47s\n",
      "batch: 1563/1563 - train loss: 4.5718 - test loss: 16.5460 - train acc: 0.7512 - test acc: 0.4160 - 16m 52s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.4304 - test loss: 16.7477 - train acc: 0.8681 - test acc: 0.4144 - 16m 56s\n",
      "batch: 200/1563 - train loss: 2.2583 - test loss: 16.7203 - train acc: 0.8778 - test acc: 0.4227 - 17m 2s\n",
      "batch: 300/1563 - train loss: 2.2370 - test loss: 17.0365 - train acc: 0.8731 - test acc: 0.4267 - 17m 7s\n",
      "batch: 400/1563 - train loss: 2.4172 - test loss: 17.2092 - train acc: 0.8594 - test acc: 0.4187 - 17m 12s\n",
      "batch: 500/1563 - train loss: 2.5397 - test loss: 17.6237 - train acc: 0.8546 - test acc: 0.4196 - 17m 16s\n",
      "batch: 600/1563 - train loss: 2.6502 - test loss: 17.4642 - train acc: 0.8509 - test acc: 0.4223 - 17m 21s\n",
      "batch: 700/1563 - train loss: 2.9010 - test loss: 17.8891 - train acc: 0.8387 - test acc: 0.4059 - 17m 26s\n",
      "batch: 800/1563 - train loss: 3.0423 - test loss: 17.6725 - train acc: 0.8327 - test acc: 0.4164 - 17m 31s\n",
      "batch: 900/1563 - train loss: 3.1967 - test loss: 17.8822 - train acc: 0.8187 - test acc: 0.4026 - 17m 36s\n",
      "batch: 1000/1563 - train loss: 3.2297 - test loss: 17.4363 - train acc: 0.8221 - test acc: 0.4245 - 17m 41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 3.3858 - test loss: 17.7890 - train acc: 0.8159 - test acc: 0.4114 - 17m 45s\n",
      "batch: 1200/1563 - train loss: 3.4749 - test loss: 17.1420 - train acc: 0.8084 - test acc: 0.4238 - 17m 50s\n",
      "batch: 1300/1563 - train loss: 3.2584 - test loss: 17.3931 - train acc: 0.8156 - test acc: 0.4250 - 17m 55s\n",
      "batch: 1400/1563 - train loss: 3.3881 - test loss: 17.1749 - train acc: 0.8062 - test acc: 0.4205 - 18m 0s\n",
      "batch: 1500/1563 - train loss: 3.8171 - test loss: 17.3994 - train acc: 0.7837 - test acc: 0.4199 - 18m 5s\n",
      "batch: 1563/1563 - train loss: 3.7072 - test loss: 17.5677 - train acc: 0.7918 - test acc: 0.4100 - 18m 10s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.0599 - test loss: 17.1618 - train acc: 0.8904 - test acc: 0.4311 - 18m 14s\n",
      "batch: 200/1563 - train loss: 2.0623 - test loss: 17.9207 - train acc: 0.8813 - test acc: 0.4201 - 18m 19s\n",
      "batch: 300/1563 - train loss: 1.9296 - test loss: 17.9852 - train acc: 0.8919 - test acc: 0.4194 - 18m 24s\n",
      "batch: 400/1563 - train loss: 2.2571 - test loss: 18.2074 - train acc: 0.8753 - test acc: 0.4206 - 18m 29s\n",
      "batch: 500/1563 - train loss: 1.9895 - test loss: 17.7889 - train acc: 0.8901 - test acc: 0.4311 - 18m 34s\n",
      "batch: 600/1563 - train loss: 2.2558 - test loss: 17.8522 - train acc: 0.8697 - test acc: 0.4259 - 18m 39s\n",
      "batch: 700/1563 - train loss: 2.2809 - test loss: 18.2228 - train acc: 0.8722 - test acc: 0.4214 - 18m 44s\n",
      "batch: 800/1563 - train loss: 2.4040 - test loss: 18.5854 - train acc: 0.8556 - test acc: 0.4126 - 18m 49s\n",
      "batch: 900/1563 - train loss: 2.5507 - test loss: 18.5277 - train acc: 0.8556 - test acc: 0.4205 - 18m 54s\n",
      "batch: 1000/1563 - train loss: 2.6214 - test loss: 18.6007 - train acc: 0.8550 - test acc: 0.4130 - 18m 59s\n",
      "batch: 1100/1563 - train loss: 2.7652 - test loss: 18.4120 - train acc: 0.8440 - test acc: 0.4205 - 19m 4s\n",
      "batch: 1200/1563 - train loss: 2.8688 - test loss: 18.4725 - train acc: 0.8368 - test acc: 0.4158 - 19m 9s\n",
      "batch: 1300/1563 - train loss: 2.7955 - test loss: 18.6745 - train acc: 0.8384 - test acc: 0.4048 - 19m 14s\n",
      "batch: 1400/1563 - train loss: 3.1272 - test loss: 18.8165 - train acc: 0.8293 - test acc: 0.4062 - 19m 19s\n",
      "batch: 1500/1563 - train loss: 3.2926 - test loss: 18.0867 - train acc: 0.8162 - test acc: 0.4199 - 19m 24s\n",
      "batch: 1563/1563 - train loss: 3.4271 - test loss: 18.3325 - train acc: 0.8124 - test acc: 0.4085 - 19m 28s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.8186 - test loss: 17.6385 - train acc: 0.8985 - test acc: 0.4307 - 19m 32s\n",
      "batch: 200/1563 - train loss: 1.6585 - test loss: 17.8966 - train acc: 0.9063 - test acc: 0.4302 - 19m 38s\n",
      "batch: 300/1563 - train loss: 1.7925 - test loss: 18.0069 - train acc: 0.8948 - test acc: 0.4348 - 19m 42s\n",
      "batch: 400/1563 - train loss: 1.7807 - test loss: 18.3154 - train acc: 0.8970 - test acc: 0.4321 - 19m 48s\n",
      "batch: 500/1563 - train loss: 1.9111 - test loss: 19.1431 - train acc: 0.8910 - test acc: 0.4100 - 19m 53s\n",
      "batch: 600/1563 - train loss: 1.9883 - test loss: 18.5106 - train acc: 0.8894 - test acc: 0.4256 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 601/1563 - train loss: 1.9973 - test loss: 18.4402 - train acc: 0.8875 - test acc: 0.4250 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 26.2066 - test loss: 24.9718 - train acc: 0.0342 - test acc: 0.0498 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.1546 - test loss: 23.5053 - train acc: 0.0633 - test acc: 0.0745 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.2617 - test loss: 23.4603 - train acc: 0.0730 - test acc: 0.0838 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.5127 - test loss: 22.3899 - train acc: 0.0940 - test acc: 0.0966 - 0m 16s\n",
      "batch: 500/1563 - train loss: 22.0922 - test loss: 21.3847 - train acc: 0.0937 - test acc: 0.1181 - 0m 21s\n",
      "batch: 600/1563 - train loss: 21.2339 - test loss: 21.1874 - train acc: 0.1210 - test acc: 0.1320 - 0m 26s\n",
      "batch: 700/1563 - train loss: 20.9666 - test loss: 20.8021 - train acc: 0.1181 - test acc: 0.1358 - 0m 31s\n",
      "batch: 800/1563 - train loss: 20.6372 - test loss: 20.2774 - train acc: 0.1462 - test acc: 0.1522 - 0m 36s\n",
      "batch: 900/1563 - train loss: 20.2452 - test loss: 20.3122 - train acc: 0.1535 - test acc: 0.1615 - 0m 41s\n",
      "batch: 1000/1563 - train loss: 20.1778 - test loss: 20.1558 - train acc: 0.1428 - test acc: 0.1574 - 0m 46s\n",
      "batch: 1100/1563 - train loss: 20.1383 - test loss: 21.6640 - train acc: 0.1531 - test acc: 0.1359 - 0m 51s\n",
      "batch: 1200/1563 - train loss: 19.4780 - test loss: 19.0920 - train acc: 0.1685 - test acc: 0.1823 - 0m 56s\n",
      "batch: 1300/1563 - train loss: 19.5448 - test loss: 19.7642 - train acc: 0.1713 - test acc: 0.1762 - 1m 2s\n",
      "batch: 1400/1563 - train loss: 19.0647 - test loss: 19.8673 - train acc: 0.1894 - test acc: 0.1646 - 1m 7s\n",
      "batch: 1500/1563 - train loss: 19.1142 - test loss: 18.5188 - train acc: 0.1950 - test acc: 0.2034 - 1m 12s\n",
      "batch: 1563/1563 - train loss: 18.7109 - test loss: 20.4573 - train acc: 0.1985 - test acc: 0.1606 - 1m 17s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 18.2144 - test loss: 18.6939 - train acc: 0.2156 - test acc: 0.2058 - 1m 22s\n",
      "batch: 200/1563 - train loss: 17.9595 - test loss: 18.0086 - train acc: 0.2148 - test acc: 0.2194 - 1m 27s\n",
      "batch: 300/1563 - train loss: 17.8243 - test loss: 20.5789 - train acc: 0.2363 - test acc: 0.1642 - 1m 32s\n",
      "batch: 400/1563 - train loss: 17.4424 - test loss: 18.9728 - train acc: 0.2353 - test acc: 0.1952 - 1m 37s\n",
      "batch: 500/1563 - train loss: 17.4446 - test loss: 18.1930 - train acc: 0.2381 - test acc: 0.2243 - 1m 42s\n",
      "batch: 600/1563 - train loss: 17.5136 - test loss: 17.7813 - train acc: 0.2307 - test acc: 0.2384 - 1m 47s\n",
      "batch: 700/1563 - train loss: 17.3957 - test loss: 17.9963 - train acc: 0.2365 - test acc: 0.2277 - 1m 51s\n",
      "batch: 800/1563 - train loss: 17.1147 - test loss: 17.5709 - train acc: 0.2553 - test acc: 0.2358 - 1m 56s\n",
      "batch: 900/1563 - train loss: 17.2333 - test loss: 16.7608 - train acc: 0.2490 - test acc: 0.2636 - 2m 1s\n",
      "batch: 1000/1563 - train loss: 17.0550 - test loss: 18.7992 - train acc: 0.2569 - test acc: 0.2148 - 2m 6s\n",
      "batch: 1100/1563 - train loss: 16.9760 - test loss: 16.9788 - train acc: 0.2635 - test acc: 0.2574 - 2m 11s\n",
      "batch: 1200/1563 - train loss: 16.8993 - test loss: 17.3877 - train acc: 0.2532 - test acc: 0.2472 - 2m 16s\n",
      "batch: 1300/1563 - train loss: 16.7798 - test loss: 16.3799 - train acc: 0.2644 - test acc: 0.2844 - 2m 21s\n",
      "batch: 1400/1563 - train loss: 16.2495 - test loss: 17.1634 - train acc: 0.2772 - test acc: 0.2539 - 2m 26s\n",
      "batch: 1500/1563 - train loss: 16.2601 - test loss: 16.7385 - train acc: 0.2797 - test acc: 0.2699 - 2m 31s\n",
      "batch: 1563/1563 - train loss: 16.2140 - test loss: 16.1879 - train acc: 0.2859 - test acc: 0.2809 - 2m 35s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.3159 - test loss: 17.3651 - train acc: 0.3080 - test acc: 0.2563 - 2m 41s\n",
      "batch: 200/1563 - train loss: 15.3131 - test loss: 16.0703 - train acc: 0.3018 - test acc: 0.2954 - 2m 45s\n",
      "batch: 300/1563 - train loss: 15.2087 - test loss: 16.2814 - train acc: 0.3097 - test acc: 0.2848 - 2m 50s\n",
      "batch: 400/1563 - train loss: 15.2119 - test loss: 18.7055 - train acc: 0.3156 - test acc: 0.2226 - 2m 55s\n",
      "batch: 500/1563 - train loss: 15.3390 - test loss: 15.9767 - train acc: 0.3137 - test acc: 0.2950 - 3m 0s\n",
      "batch: 600/1563 - train loss: 15.3116 - test loss: 15.9674 - train acc: 0.3075 - test acc: 0.2966 - 3m 6s\n",
      "batch: 700/1563 - train loss: 14.6198 - test loss: 18.7356 - train acc: 0.3390 - test acc: 0.2276 - 3m 11s\n",
      "batch: 800/1563 - train loss: 14.9105 - test loss: 16.9092 - train acc: 0.3287 - test acc: 0.2686 - 3m 16s\n",
      "batch: 900/1563 - train loss: 15.2028 - test loss: 16.4241 - train acc: 0.3222 - test acc: 0.2747 - 3m 21s\n",
      "batch: 1000/1563 - train loss: 14.5995 - test loss: 18.8478 - train acc: 0.3303 - test acc: 0.2351 - 3m 26s\n",
      "batch: 1100/1563 - train loss: 14.9787 - test loss: 15.2841 - train acc: 0.3262 - test acc: 0.3137 - 3m 31s\n",
      "batch: 1200/1563 - train loss: 14.8186 - test loss: 17.6015 - train acc: 0.3244 - test acc: 0.2574 - 3m 37s\n",
      "batch: 1300/1563 - train loss: 14.5053 - test loss: 15.0545 - train acc: 0.3431 - test acc: 0.3273 - 3m 42s\n",
      "batch: 1400/1563 - train loss: 14.6528 - test loss: 15.7133 - train acc: 0.3397 - test acc: 0.3061 - 3m 47s\n",
      "batch: 1500/1563 - train loss: 14.5564 - test loss: 14.7788 - train acc: 0.3387 - test acc: 0.3339 - 3m 52s\n",
      "batch: 1563/1563 - train loss: 14.6595 - test loss: 14.9895 - train acc: 0.3318 - test acc: 0.3351 - 3m 56s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.1334 - test loss: 15.6870 - train acc: 0.3897 - test acc: 0.3137 - 4m 1s\n",
      "batch: 200/1563 - train loss: 13.2573 - test loss: 15.4534 - train acc: 0.3709 - test acc: 0.3157 - 4m 6s\n",
      "batch: 300/1563 - train loss: 13.1060 - test loss: 15.3652 - train acc: 0.3935 - test acc: 0.3252 - 4m 11s\n",
      "batch: 400/1563 - train loss: 13.3692 - test loss: 15.4702 - train acc: 0.3806 - test acc: 0.3211 - 4m 16s\n",
      "batch: 500/1563 - train loss: 13.5219 - test loss: 14.4050 - train acc: 0.3694 - test acc: 0.3538 - 4m 21s\n",
      "batch: 600/1563 - train loss: 13.5284 - test loss: 15.4888 - train acc: 0.3784 - test acc: 0.3220 - 4m 26s\n",
      "batch: 700/1563 - train loss: 13.6205 - test loss: 15.0345 - train acc: 0.3728 - test acc: 0.3298 - 4m 31s\n",
      "batch: 800/1563 - train loss: 13.3458 - test loss: 15.1251 - train acc: 0.3750 - test acc: 0.3255 - 4m 36s\n",
      "batch: 900/1563 - train loss: 13.1565 - test loss: 14.8984 - train acc: 0.3851 - test acc: 0.3383 - 4m 41s\n",
      "batch: 1000/1563 - train loss: 13.2841 - test loss: 14.0944 - train acc: 0.3863 - test acc: 0.3549 - 4m 46s\n",
      "batch: 1100/1563 - train loss: 13.0289 - test loss: 15.5916 - train acc: 0.4016 - test acc: 0.3272 - 4m 51s\n",
      "batch: 1200/1563 - train loss: 13.4139 - test loss: 14.1338 - train acc: 0.3778 - test acc: 0.3624 - 4m 55s\n",
      "batch: 1300/1563 - train loss: 13.4071 - test loss: 14.1724 - train acc: 0.3772 - test acc: 0.3595 - 5m 0s\n",
      "batch: 1400/1563 - train loss: 12.9805 - test loss: 13.7921 - train acc: 0.3900 - test acc: 0.3736 - 5m 5s\n",
      "batch: 1500/1563 - train loss: 13.1330 - test loss: 14.3767 - train acc: 0.3896 - test acc: 0.3551 - 5m 10s\n",
      "batch: 1563/1563 - train loss: 13.0750 - test loss: 17.9890 - train acc: 0.3978 - test acc: 0.2686 - 5m 15s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.6498 - test loss: 16.6090 - train acc: 0.4466 - test acc: 0.3158 - 5m 19s\n",
      "batch: 200/1563 - train loss: 11.7264 - test loss: 14.3219 - train acc: 0.4397 - test acc: 0.3645 - 5m 24s\n",
      "batch: 300/1563 - train loss: 11.8254 - test loss: 14.7383 - train acc: 0.4325 - test acc: 0.3467 - 5m 29s\n",
      "batch: 400/1563 - train loss: 11.6677 - test loss: 14.8891 - train acc: 0.4591 - test acc: 0.3510 - 5m 34s\n",
      "batch: 500/1563 - train loss: 11.9525 - test loss: 14.4223 - train acc: 0.4453 - test acc: 0.3617 - 5m 39s\n",
      "batch: 600/1563 - train loss: 12.1143 - test loss: 14.0115 - train acc: 0.4275 - test acc: 0.3698 - 5m 45s\n",
      "batch: 700/1563 - train loss: 11.8537 - test loss: 14.5580 - train acc: 0.4425 - test acc: 0.3505 - 5m 50s\n",
      "batch: 800/1563 - train loss: 11.9163 - test loss: 14.4996 - train acc: 0.4379 - test acc: 0.3548 - 5m 55s\n",
      "batch: 900/1563 - train loss: 11.7380 - test loss: 13.8122 - train acc: 0.4459 - test acc: 0.3838 - 5m 59s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 11.7972 - test loss: 14.0093 - train acc: 0.4403 - test acc: 0.3738 - 6m 4s\n",
      "batch: 1100/1563 - train loss: 11.7903 - test loss: 15.9466 - train acc: 0.4366 - test acc: 0.3233 - 6m 9s\n",
      "batch: 1200/1563 - train loss: 12.3833 - test loss: 15.0528 - train acc: 0.4206 - test acc: 0.3493 - 6m 14s\n",
      "batch: 1300/1563 - train loss: 11.6429 - test loss: 14.3832 - train acc: 0.4525 - test acc: 0.3600 - 6m 19s\n",
      "batch: 1400/1563 - train loss: 12.2165 - test loss: 13.8131 - train acc: 0.4209 - test acc: 0.3738 - 6m 24s\n",
      "batch: 1500/1563 - train loss: 12.0726 - test loss: 14.7969 - train acc: 0.4285 - test acc: 0.3668 - 6m 29s\n",
      "batch: 1563/1563 - train loss: 12.3061 - test loss: 13.8467 - train acc: 0.4222 - test acc: 0.3752 - 6m 33s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0417 - test loss: 14.1691 - train acc: 0.5156 - test acc: 0.3721 - 6m 37s\n",
      "batch: 200/1563 - train loss: 10.2238 - test loss: 14.2802 - train acc: 0.4984 - test acc: 0.3716 - 6m 42s\n",
      "batch: 300/1563 - train loss: 10.6156 - test loss: 14.9961 - train acc: 0.4844 - test acc: 0.3596 - 6m 47s\n",
      "batch: 400/1563 - train loss: 10.3969 - test loss: 15.1247 - train acc: 0.4915 - test acc: 0.3471 - 6m 52s\n",
      "batch: 500/1563 - train loss: 10.2932 - test loss: 14.0634 - train acc: 0.4970 - test acc: 0.3829 - 6m 57s\n",
      "batch: 600/1563 - train loss: 10.8142 - test loss: 14.1036 - train acc: 0.4759 - test acc: 0.3754 - 7m 1s\n",
      "batch: 700/1563 - train loss: 10.8019 - test loss: 13.7354 - train acc: 0.4822 - test acc: 0.3889 - 7m 6s\n",
      "batch: 800/1563 - train loss: 10.7645 - test loss: 13.8533 - train acc: 0.4916 - test acc: 0.3860 - 7m 11s\n",
      "batch: 900/1563 - train loss: 11.1039 - test loss: 13.5855 - train acc: 0.4778 - test acc: 0.3923 - 7m 16s\n",
      "batch: 1000/1563 - train loss: 10.9427 - test loss: 14.1619 - train acc: 0.4850 - test acc: 0.3854 - 7m 20s\n",
      "batch: 1100/1563 - train loss: 11.1662 - test loss: 13.8965 - train acc: 0.4697 - test acc: 0.3864 - 7m 25s\n",
      "batch: 1200/1563 - train loss: 10.5772 - test loss: 14.1773 - train acc: 0.4907 - test acc: 0.3738 - 7m 30s\n",
      "batch: 1300/1563 - train loss: 10.6199 - test loss: 14.5163 - train acc: 0.4866 - test acc: 0.3635 - 7m 35s\n",
      "batch: 1400/1563 - train loss: 10.8948 - test loss: 13.6516 - train acc: 0.4731 - test acc: 0.3959 - 7m 39s\n",
      "batch: 1500/1563 - train loss: 11.0868 - test loss: 13.2072 - train acc: 0.4757 - test acc: 0.4077 - 7m 44s\n",
      "batch: 1563/1563 - train loss: 10.4367 - test loss: 13.7674 - train acc: 0.4960 - test acc: 0.3905 - 7m 48s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.4999 - test loss: 14.5053 - train acc: 0.5768 - test acc: 0.3864 - 7m 53s\n",
      "batch: 200/1563 - train loss: 9.0338 - test loss: 13.7510 - train acc: 0.5572 - test acc: 0.3982 - 7m 58s\n",
      "batch: 300/1563 - train loss: 9.0477 - test loss: 13.5706 - train acc: 0.5502 - test acc: 0.4056 - 8m 3s\n",
      "batch: 400/1563 - train loss: 9.3002 - test loss: 14.8016 - train acc: 0.5496 - test acc: 0.3745 - 8m 8s\n",
      "batch: 500/1563 - train loss: 9.6217 - test loss: 13.6571 - train acc: 0.5187 - test acc: 0.4027 - 8m 12s\n",
      "batch: 600/1563 - train loss: 9.7009 - test loss: 13.4444 - train acc: 0.5188 - test acc: 0.4092 - 8m 17s\n",
      "batch: 700/1563 - train loss: 9.5909 - test loss: 13.7334 - train acc: 0.5231 - test acc: 0.4027 - 8m 22s\n",
      "batch: 800/1563 - train loss: 9.2153 - test loss: 13.5883 - train acc: 0.5450 - test acc: 0.4102 - 8m 27s\n",
      "batch: 900/1563 - train loss: 9.2774 - test loss: 14.3135 - train acc: 0.5471 - test acc: 0.3924 - 8m 32s\n",
      "batch: 1000/1563 - train loss: 9.7229 - test loss: 14.2665 - train acc: 0.5269 - test acc: 0.3896 - 8m 36s\n",
      "batch: 1100/1563 - train loss: 9.9123 - test loss: 13.4773 - train acc: 0.5140 - test acc: 0.4019 - 8m 41s\n",
      "batch: 1200/1563 - train loss: 9.7855 - test loss: 14.6059 - train acc: 0.5272 - test acc: 0.3821 - 8m 46s\n",
      "batch: 1300/1563 - train loss: 9.7470 - test loss: 14.2450 - train acc: 0.5288 - test acc: 0.3855 - 8m 51s\n",
      "batch: 1400/1563 - train loss: 9.9535 - test loss: 15.0703 - train acc: 0.5141 - test acc: 0.3574 - 8m 56s\n",
      "batch: 1500/1563 - train loss: 9.8439 - test loss: 13.4408 - train acc: 0.5262 - test acc: 0.4128 - 9m 1s\n",
      "batch: 1563/1563 - train loss: 9.6279 - test loss: 14.0966 - train acc: 0.5272 - test acc: 0.3872 - 9m 5s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.3769 - test loss: 14.3269 - train acc: 0.6306 - test acc: 0.3940 - 9m 9s\n",
      "batch: 200/1563 - train loss: 7.6163 - test loss: 15.1435 - train acc: 0.5994 - test acc: 0.3740 - 9m 14s\n",
      "batch: 300/1563 - train loss: 7.9904 - test loss: 14.5871 - train acc: 0.5878 - test acc: 0.3860 - 9m 19s\n",
      "batch: 400/1563 - train loss: 8.1017 - test loss: 14.4990 - train acc: 0.5972 - test acc: 0.3919 - 9m 24s\n",
      "batch: 500/1563 - train loss: 8.1000 - test loss: 16.5804 - train acc: 0.5831 - test acc: 0.3580 - 9m 29s\n",
      "batch: 600/1563 - train loss: 8.2203 - test loss: 14.2144 - train acc: 0.5784 - test acc: 0.3907 - 9m 34s\n",
      "batch: 700/1563 - train loss: 8.1832 - test loss: 14.2475 - train acc: 0.5865 - test acc: 0.4008 - 9m 38s\n",
      "batch: 800/1563 - train loss: 8.4912 - test loss: 13.5961 - train acc: 0.5708 - test acc: 0.4135 - 9m 43s\n",
      "batch: 900/1563 - train loss: 8.6985 - test loss: 15.4622 - train acc: 0.5650 - test acc: 0.3637 - 9m 48s\n",
      "batch: 1000/1563 - train loss: 8.7759 - test loss: 14.1499 - train acc: 0.5568 - test acc: 0.3988 - 9m 53s\n",
      "batch: 1100/1563 - train loss: 8.4933 - test loss: 15.8035 - train acc: 0.5822 - test acc: 0.3655 - 9m 58s\n",
      "batch: 1200/1563 - train loss: 8.9006 - test loss: 14.0807 - train acc: 0.5603 - test acc: 0.3937 - 10m 3s\n",
      "batch: 1300/1563 - train loss: 8.9264 - test loss: 13.3206 - train acc: 0.5562 - test acc: 0.4201 - 10m 7s\n",
      "batch: 1400/1563 - train loss: 8.9423 - test loss: 14.1966 - train acc: 0.5537 - test acc: 0.3971 - 10m 12s\n",
      "batch: 1500/1563 - train loss: 8.8908 - test loss: 13.6903 - train acc: 0.5553 - test acc: 0.4178 - 10m 17s\n",
      "batch: 1563/1563 - train loss: 8.9446 - test loss: 14.6655 - train acc: 0.5510 - test acc: 0.3821 - 10m 21s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.3773 - test loss: 14.9258 - train acc: 0.6732 - test acc: 0.3982 - 10m 26s\n",
      "batch: 200/1563 - train loss: 6.5967 - test loss: 14.3018 - train acc: 0.6534 - test acc: 0.4145 - 10m 31s\n",
      "batch: 300/1563 - train loss: 6.7354 - test loss: 14.2133 - train acc: 0.6491 - test acc: 0.4137 - 10m 36s\n",
      "batch: 400/1563 - train loss: 6.7909 - test loss: 14.2117 - train acc: 0.6441 - test acc: 0.4135 - 10m 40s\n",
      "batch: 500/1563 - train loss: 7.1893 - test loss: 14.4670 - train acc: 0.6301 - test acc: 0.4036 - 10m 45s\n",
      "batch: 600/1563 - train loss: 7.2568 - test loss: 14.9740 - train acc: 0.6200 - test acc: 0.3907 - 10m 50s\n",
      "batch: 700/1563 - train loss: 7.2151 - test loss: 14.3689 - train acc: 0.6447 - test acc: 0.4051 - 10m 55s\n",
      "batch: 800/1563 - train loss: 7.2714 - test loss: 13.9100 - train acc: 0.6247 - test acc: 0.4146 - 10m 59s\n",
      "batch: 900/1563 - train loss: 7.2934 - test loss: 13.8381 - train acc: 0.6162 - test acc: 0.4221 - 11m 5s\n",
      "batch: 1000/1563 - train loss: 7.4093 - test loss: 15.7725 - train acc: 0.6209 - test acc: 0.3768 - 11m 9s\n",
      "batch: 1100/1563 - train loss: 7.6918 - test loss: 14.0616 - train acc: 0.6116 - test acc: 0.4158 - 11m 14s\n",
      "batch: 1200/1563 - train loss: 7.6052 - test loss: 14.8415 - train acc: 0.6147 - test acc: 0.3897 - 11m 19s\n",
      "batch: 1300/1563 - train loss: 7.8103 - test loss: 15.0282 - train acc: 0.5984 - test acc: 0.3898 - 11m 24s\n",
      "batch: 1400/1563 - train loss: 7.6902 - test loss: 14.4563 - train acc: 0.6169 - test acc: 0.4038 - 11m 29s\n",
      "batch: 1500/1563 - train loss: 8.0216 - test loss: 14.3653 - train acc: 0.5972 - test acc: 0.4044 - 11m 33s\n",
      "batch: 1563/1563 - train loss: 8.1850 - test loss: 15.4578 - train acc: 0.5825 - test acc: 0.3911 - 11m 38s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.5839 - test loss: 15.4509 - train acc: 0.7004 - test acc: 0.4022 - 11m 42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 5.3098 - test loss: 14.2781 - train acc: 0.7153 - test acc: 0.4262 - 11m 47s\n",
      "batch: 300/1563 - train loss: 5.7626 - test loss: 14.8462 - train acc: 0.6913 - test acc: 0.4067 - 11m 52s\n",
      "batch: 400/1563 - train loss: 5.4964 - test loss: 15.0811 - train acc: 0.7110 - test acc: 0.4178 - 11m 57s\n",
      "batch: 500/1563 - train loss: 5.8879 - test loss: 15.6911 - train acc: 0.6913 - test acc: 0.4005 - 12m 2s\n",
      "batch: 600/1563 - train loss: 6.1316 - test loss: 15.0065 - train acc: 0.6757 - test acc: 0.4046 - 12m 7s\n",
      "batch: 700/1563 - train loss: 6.4510 - test loss: 17.1749 - train acc: 0.6684 - test acc: 0.3600 - 12m 12s\n",
      "batch: 800/1563 - train loss: 6.6083 - test loss: 14.4679 - train acc: 0.6563 - test acc: 0.4115 - 12m 16s\n",
      "batch: 900/1563 - train loss: 6.4974 - test loss: 14.9698 - train acc: 0.6562 - test acc: 0.4011 - 12m 21s\n",
      "batch: 1000/1563 - train loss: 6.6449 - test loss: 14.7787 - train acc: 0.6528 - test acc: 0.4074 - 12m 26s\n",
      "batch: 1100/1563 - train loss: 6.8107 - test loss: 14.6171 - train acc: 0.6531 - test acc: 0.4114 - 12m 31s\n",
      "batch: 1200/1563 - train loss: 6.7961 - test loss: 13.9328 - train acc: 0.6522 - test acc: 0.4263 - 12m 35s\n",
      "batch: 1300/1563 - train loss: 6.7465 - test loss: 14.8028 - train acc: 0.6516 - test acc: 0.4169 - 12m 41s\n",
      "batch: 1400/1563 - train loss: 7.0556 - test loss: 14.6836 - train acc: 0.6362 - test acc: 0.4116 - 12m 45s\n",
      "batch: 1500/1563 - train loss: 6.8777 - test loss: 14.3541 - train acc: 0.6413 - test acc: 0.4112 - 12m 50s\n",
      "batch: 1563/1563 - train loss: 7.1118 - test loss: 14.1157 - train acc: 0.6360 - test acc: 0.4305 - 12m 54s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.5554 - test loss: 14.9510 - train acc: 0.7640 - test acc: 0.4170 - 12m 59s\n",
      "batch: 200/1563 - train loss: 4.2817 - test loss: 14.9911 - train acc: 0.7615 - test acc: 0.4164 - 13m 4s\n",
      "batch: 300/1563 - train loss: 4.3919 - test loss: 15.6140 - train acc: 0.7628 - test acc: 0.4101 - 13m 8s\n",
      "batch: 400/1563 - train loss: 4.6570 - test loss: 14.7867 - train acc: 0.7434 - test acc: 0.4273 - 13m 13s\n",
      "batch: 500/1563 - train loss: 4.8522 - test loss: 15.8987 - train acc: 0.7434 - test acc: 0.3979 - 13m 18s\n",
      "batch: 600/1563 - train loss: 5.3675 - test loss: 17.7413 - train acc: 0.7097 - test acc: 0.3682 - 13m 22s\n",
      "batch: 700/1563 - train loss: 5.3637 - test loss: 15.3561 - train acc: 0.7219 - test acc: 0.4102 - 13m 27s\n",
      "batch: 800/1563 - train loss: 5.3738 - test loss: 18.5551 - train acc: 0.7153 - test acc: 0.3540 - 13m 32s\n",
      "batch: 900/1563 - train loss: 5.9453 - test loss: 15.5932 - train acc: 0.6756 - test acc: 0.4121 - 13m 37s\n",
      "batch: 1000/1563 - train loss: 5.7627 - test loss: 14.8667 - train acc: 0.6913 - test acc: 0.4228 - 13m 42s\n",
      "batch: 1100/1563 - train loss: 5.6222 - test loss: 14.6936 - train acc: 0.7016 - test acc: 0.4232 - 13m 46s\n",
      "batch: 1200/1563 - train loss: 5.8893 - test loss: 14.8948 - train acc: 0.6912 - test acc: 0.4236 - 13m 51s\n",
      "batch: 1300/1563 - train loss: 5.8219 - test loss: 15.1836 - train acc: 0.6897 - test acc: 0.4179 - 13m 56s\n",
      "batch: 1400/1563 - train loss: 5.8747 - test loss: 15.5561 - train acc: 0.6838 - test acc: 0.4059 - 14m 1s\n",
      "batch: 1500/1563 - train loss: 6.2346 - test loss: 14.4909 - train acc: 0.6694 - test acc: 0.4261 - 14m 5s\n",
      "batch: 1563/1563 - train loss: 5.9747 - test loss: 15.7008 - train acc: 0.6775 - test acc: 0.4108 - 14m 9s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.6158 - test loss: 15.6987 - train acc: 0.8086 - test acc: 0.4186 - 14m 14s\n",
      "batch: 200/1563 - train loss: 3.6402 - test loss: 15.8393 - train acc: 0.7972 - test acc: 0.4190 - 14m 19s\n",
      "batch: 300/1563 - train loss: 3.7898 - test loss: 15.8963 - train acc: 0.7940 - test acc: 0.4284 - 14m 24s\n",
      "batch: 400/1563 - train loss: 3.9921 - test loss: 16.0482 - train acc: 0.7809 - test acc: 0.4214 - 14m 28s\n",
      "batch: 500/1563 - train loss: 4.2226 - test loss: 15.7973 - train acc: 0.7690 - test acc: 0.4316 - 14m 33s\n",
      "batch: 600/1563 - train loss: 4.2778 - test loss: 15.7956 - train acc: 0.7656 - test acc: 0.4180 - 14m 38s\n",
      "batch: 700/1563 - train loss: 4.5307 - test loss: 16.2688 - train acc: 0.7509 - test acc: 0.4138 - 14m 43s\n",
      "batch: 800/1563 - train loss: 4.5478 - test loss: 15.8880 - train acc: 0.7528 - test acc: 0.4196 - 14m 48s\n",
      "batch: 900/1563 - train loss: 4.6774 - test loss: 15.6629 - train acc: 0.7493 - test acc: 0.4233 - 14m 52s\n",
      "batch: 1000/1563 - train loss: 4.6473 - test loss: 16.0320 - train acc: 0.7403 - test acc: 0.4119 - 14m 57s\n",
      "batch: 1100/1563 - train loss: 4.9255 - test loss: 15.9458 - train acc: 0.7303 - test acc: 0.4155 - 15m 2s\n",
      "batch: 1200/1563 - train loss: 5.2496 - test loss: 15.5139 - train acc: 0.7195 - test acc: 0.4257 - 15m 7s\n",
      "batch: 1300/1563 - train loss: 5.0632 - test loss: 15.0864 - train acc: 0.7294 - test acc: 0.4293 - 15m 11s\n",
      "batch: 1400/1563 - train loss: 5.0295 - test loss: 16.0152 - train acc: 0.7353 - test acc: 0.3979 - 15m 16s\n",
      "batch: 1500/1563 - train loss: 5.4013 - test loss: 16.3197 - train acc: 0.7119 - test acc: 0.3980 - 15m 21s\n",
      "batch: 1563/1563 - train loss: 5.4041 - test loss: 16.6299 - train acc: 0.7110 - test acc: 0.3948 - 15m 25s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 2.9788 - test loss: 15.6646 - train acc: 0.8375 - test acc: 0.4340 - 15m 30s\n",
      "batch: 200/1563 - train loss: 2.8742 - test loss: 15.9125 - train acc: 0.8419 - test acc: 0.4359 - 15m 35s\n",
      "batch: 300/1563 - train loss: 3.0842 - test loss: 15.9332 - train acc: 0.8343 - test acc: 0.4296 - 15m 40s\n",
      "batch: 400/1563 - train loss: 3.1704 - test loss: 16.2898 - train acc: 0.8240 - test acc: 0.4231 - 15m 44s\n",
      "batch: 500/1563 - train loss: 3.3906 - test loss: 16.4602 - train acc: 0.8087 - test acc: 0.4152 - 15m 50s\n",
      "batch: 600/1563 - train loss: 3.3953 - test loss: 17.2539 - train acc: 0.8144 - test acc: 0.4091 - 15m 55s\n",
      "batch: 700/1563 - train loss: 3.6070 - test loss: 16.1409 - train acc: 0.8081 - test acc: 0.4213 - 15m 59s\n",
      "batch: 800/1563 - train loss: 3.5342 - test loss: 16.9297 - train acc: 0.8084 - test acc: 0.4143 - 16m 4s\n",
      "batch: 900/1563 - train loss: 3.8322 - test loss: 17.5351 - train acc: 0.7896 - test acc: 0.3956 - 16m 9s\n",
      "batch: 1000/1563 - train loss: 4.1182 - test loss: 16.5003 - train acc: 0.7799 - test acc: 0.4230 - 16m 14s\n",
      "batch: 1100/1563 - train loss: 3.9980 - test loss: 17.8555 - train acc: 0.7793 - test acc: 0.4014 - 16m 18s\n",
      "batch: 1200/1563 - train loss: 4.0829 - test loss: 16.5244 - train acc: 0.7778 - test acc: 0.4209 - 16m 24s\n",
      "batch: 1300/1563 - train loss: 4.2598 - test loss: 16.4452 - train acc: 0.7644 - test acc: 0.4205 - 16m 28s\n",
      "batch: 1400/1563 - train loss: 4.4990 - test loss: 16.5652 - train acc: 0.7565 - test acc: 0.4042 - 16m 33s\n",
      "batch: 1500/1563 - train loss: 4.5515 - test loss: 16.2115 - train acc: 0.7447 - test acc: 0.4213 - 16m 38s\n",
      "batch: 1563/1563 - train loss: 4.7060 - test loss: 16.7679 - train acc: 0.7422 - test acc: 0.4111 - 16m 42s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.5390 - test loss: 16.4666 - train acc: 0.8559 - test acc: 0.4243 - 16m 47s\n",
      "batch: 200/1563 - train loss: 2.4761 - test loss: 17.3915 - train acc: 0.8703 - test acc: 0.4141 - 16m 52s\n",
      "batch: 300/1563 - train loss: 2.4452 - test loss: 17.0682 - train acc: 0.8628 - test acc: 0.4136 - 16m 57s\n",
      "batch: 400/1563 - train loss: 2.4958 - test loss: 17.0263 - train acc: 0.8594 - test acc: 0.4193 - 17m 2s\n",
      "batch: 500/1563 - train loss: 2.6898 - test loss: 18.6002 - train acc: 0.8497 - test acc: 0.3958 - 17m 7s\n",
      "batch: 600/1563 - train loss: 2.7068 - test loss: 17.4292 - train acc: 0.8477 - test acc: 0.4221 - 17m 12s\n",
      "batch: 700/1563 - train loss: 2.9376 - test loss: 17.3611 - train acc: 0.8409 - test acc: 0.4222 - 17m 16s\n",
      "batch: 800/1563 - train loss: 3.3793 - test loss: 18.6159 - train acc: 0.8056 - test acc: 0.4048 - 17m 21s\n",
      "batch: 900/1563 - train loss: 3.4504 - test loss: 18.0204 - train acc: 0.8103 - test acc: 0.4037 - 17m 26s\n",
      "batch: 1000/1563 - train loss: 3.5649 - test loss: 18.4239 - train acc: 0.8065 - test acc: 0.3972 - 17m 31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 3.4127 - test loss: 17.1727 - train acc: 0.8075 - test acc: 0.4178 - 17m 36s\n",
      "batch: 1200/1563 - train loss: 3.7064 - test loss: 17.7518 - train acc: 0.7900 - test acc: 0.4075 - 17m 40s\n",
      "batch: 1300/1563 - train loss: 3.5611 - test loss: 17.0436 - train acc: 0.7934 - test acc: 0.4103 - 17m 45s\n",
      "batch: 1400/1563 - train loss: 3.8769 - test loss: 16.9718 - train acc: 0.7875 - test acc: 0.4149 - 17m 50s\n",
      "batch: 1500/1563 - train loss: 3.6845 - test loss: 17.6887 - train acc: 0.7931 - test acc: 0.4074 - 17m 55s\n",
      "batch: 1563/1563 - train loss: 3.7993 - test loss: 18.3284 - train acc: 0.7865 - test acc: 0.4078 - 17m 59s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.2008 - test loss: 17.1983 - train acc: 0.8806 - test acc: 0.4252 - 18m 4s\n",
      "batch: 200/1563 - train loss: 2.0428 - test loss: 16.8895 - train acc: 0.8787 - test acc: 0.4363 - 18m 9s\n",
      "batch: 300/1563 - train loss: 2.1174 - test loss: 17.6693 - train acc: 0.8866 - test acc: 0.4235 - 18m 13s\n",
      "batch: 400/1563 - train loss: 2.2223 - test loss: 17.4902 - train acc: 0.8728 - test acc: 0.4302 - 18m 18s\n",
      "batch: 500/1563 - train loss: 2.3324 - test loss: 17.3831 - train acc: 0.8722 - test acc: 0.4340 - 18m 23s\n",
      "batch: 600/1563 - train loss: 2.5297 - test loss: 19.3949 - train acc: 0.8565 - test acc: 0.3951 - 18m 28s\n",
      "batch: 700/1563 - train loss: 2.5085 - test loss: 17.5990 - train acc: 0.8594 - test acc: 0.4250 - 18m 33s\n",
      "batch: 800/1563 - train loss: 2.3458 - test loss: 17.7843 - train acc: 0.8638 - test acc: 0.4332 - 18m 37s\n",
      "batch: 900/1563 - train loss: 2.9079 - test loss: 17.8116 - train acc: 0.8334 - test acc: 0.4233 - 18m 42s\n",
      "batch: 1000/1563 - train loss: 2.8671 - test loss: 18.0335 - train acc: 0.8344 - test acc: 0.4152 - 18m 47s\n",
      "batch: 1100/1563 - train loss: 3.0024 - test loss: 17.4355 - train acc: 0.8284 - test acc: 0.4297 - 18m 51s\n",
      "batch: 1200/1563 - train loss: 3.1054 - test loss: 18.4090 - train acc: 0.8181 - test acc: 0.4095 - 18m 56s\n",
      "batch: 1300/1563 - train loss: 3.3068 - test loss: 18.0587 - train acc: 0.8172 - test acc: 0.4131 - 19m 1s\n",
      "batch: 1400/1563 - train loss: 3.1515 - test loss: 18.4303 - train acc: 0.8253 - test acc: 0.4006 - 19m 6s\n",
      "batch: 1500/1563 - train loss: 3.3740 - test loss: 18.4602 - train acc: 0.8162 - test acc: 0.4057 - 19m 11s\n",
      "batch: 1563/1563 - train loss: 3.3356 - test loss: 17.7173 - train acc: 0.8155 - test acc: 0.4145 - 19m 15s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.7972 - test loss: 17.5902 - train acc: 0.8982 - test acc: 0.4246 - 19m 20s\n",
      "batch: 200/1563 - train loss: 1.5690 - test loss: 17.9416 - train acc: 0.9189 - test acc: 0.4216 - 19m 24s\n",
      "batch: 300/1563 - train loss: 1.7719 - test loss: 18.2596 - train acc: 0.9041 - test acc: 0.4251 - 19m 29s\n",
      "batch: 400/1563 - train loss: 1.6583 - test loss: 18.5836 - train acc: 0.9082 - test acc: 0.4299 - 19m 34s\n",
      "batch: 500/1563 - train loss: 1.8675 - test loss: 18.0770 - train acc: 0.8966 - test acc: 0.4338 - 19m 39s\n",
      "batch: 600/1563 - train loss: 1.7697 - test loss: 19.0349 - train acc: 0.8988 - test acc: 0.4171 - 19m 44s\n",
      "batch: 700/1563 - train loss: 2.1454 - test loss: 18.5719 - train acc: 0.8791 - test acc: 0.4207 - 19m 48s\n",
      "batch: 800/1563 - train loss: 1.9862 - test loss: 18.5686 - train acc: 0.8857 - test acc: 0.4160 - 19m 53s\n",
      "batch: 900/1563 - train loss: 2.1673 - test loss: 19.1527 - train acc: 0.8719 - test acc: 0.4124 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 901/1563 - train loss: 2.1688 - test loss: 19.2140 - train acc: 0.8719 - test acc: 0.4126 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 26.1393 - test loss: 24.7582 - train acc: 0.0351 - test acc: 0.0468 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.2089 - test loss: 24.0912 - train acc: 0.0599 - test acc: 0.0699 - 0m 6s\n",
      "batch: 300/1563 - train loss: 22.7937 - test loss: 22.4171 - train acc: 0.0855 - test acc: 0.1006 - 0m 10s\n",
      "batch: 400/1563 - train loss: 22.3866 - test loss: 21.9148 - train acc: 0.0946 - test acc: 0.1130 - 0m 15s\n",
      "batch: 500/1563 - train loss: 21.7509 - test loss: 21.4329 - train acc: 0.1105 - test acc: 0.1199 - 0m 20s\n",
      "batch: 600/1563 - train loss: 21.2166 - test loss: 21.4496 - train acc: 0.1369 - test acc: 0.1228 - 0m 25s\n",
      "batch: 700/1563 - train loss: 20.9381 - test loss: 20.6934 - train acc: 0.1306 - test acc: 0.1459 - 0m 29s\n",
      "batch: 800/1563 - train loss: 20.4284 - test loss: 19.9690 - train acc: 0.1444 - test acc: 0.1599 - 0m 34s\n",
      "batch: 900/1563 - train loss: 20.5090 - test loss: 19.8519 - train acc: 0.1469 - test acc: 0.1702 - 0m 39s\n",
      "batch: 1000/1563 - train loss: 19.9519 - test loss: 20.4712 - train acc: 0.1616 - test acc: 0.1591 - 0m 43s\n",
      "batch: 1100/1563 - train loss: 19.4134 - test loss: 19.7836 - train acc: 0.1895 - test acc: 0.1746 - 0m 48s\n",
      "batch: 1200/1563 - train loss: 19.3866 - test loss: 19.7592 - train acc: 0.1813 - test acc: 0.1742 - 0m 53s\n",
      "batch: 1300/1563 - train loss: 19.0364 - test loss: 19.0462 - train acc: 0.1825 - test acc: 0.1950 - 0m 58s\n",
      "batch: 1400/1563 - train loss: 18.9553 - test loss: 19.5247 - train acc: 0.1928 - test acc: 0.1979 - 1m 2s\n",
      "batch: 1500/1563 - train loss: 18.6102 - test loss: 19.3040 - train acc: 0.2063 - test acc: 0.1971 - 1m 7s\n",
      "batch: 1563/1563 - train loss: 18.4668 - test loss: 18.9094 - train acc: 0.2072 - test acc: 0.1985 - 1m 11s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.7479 - test loss: 18.6471 - train acc: 0.2269 - test acc: 0.2105 - 1m 16s\n",
      "batch: 200/1563 - train loss: 17.6016 - test loss: 17.9469 - train acc: 0.2256 - test acc: 0.2250 - 1m 20s\n",
      "batch: 300/1563 - train loss: 17.6436 - test loss: 18.3157 - train acc: 0.2260 - test acc: 0.2101 - 1m 25s\n",
      "batch: 400/1563 - train loss: 17.4756 - test loss: 18.3398 - train acc: 0.2387 - test acc: 0.2091 - 1m 30s\n",
      "batch: 500/1563 - train loss: 17.4738 - test loss: 18.4003 - train acc: 0.2328 - test acc: 0.2043 - 1m 35s\n",
      "batch: 600/1563 - train loss: 17.3349 - test loss: 17.9469 - train acc: 0.2363 - test acc: 0.2327 - 1m 40s\n",
      "batch: 700/1563 - train loss: 16.9236 - test loss: 16.9094 - train acc: 0.2656 - test acc: 0.2583 - 1m 44s\n",
      "batch: 800/1563 - train loss: 16.8025 - test loss: 17.8861 - train acc: 0.2656 - test acc: 0.2306 - 1m 49s\n",
      "batch: 900/1563 - train loss: 16.9842 - test loss: 16.8542 - train acc: 0.2566 - test acc: 0.2533 - 1m 54s\n",
      "batch: 1000/1563 - train loss: 16.8700 - test loss: 16.4011 - train acc: 0.2641 - test acc: 0.2750 - 1m 59s\n",
      "batch: 1100/1563 - train loss: 16.6008 - test loss: 16.9634 - train acc: 0.2591 - test acc: 0.2646 - 2m 3s\n",
      "batch: 1200/1563 - train loss: 16.5056 - test loss: 18.6656 - train acc: 0.2781 - test acc: 0.2229 - 2m 8s\n",
      "batch: 1300/1563 - train loss: 16.3113 - test loss: 18.1811 - train acc: 0.2853 - test acc: 0.2320 - 2m 13s\n",
      "batch: 1400/1563 - train loss: 16.2141 - test loss: 16.9835 - train acc: 0.2906 - test acc: 0.2565 - 2m 17s\n",
      "batch: 1500/1563 - train loss: 16.2648 - test loss: 17.0833 - train acc: 0.2841 - test acc: 0.2558 - 2m 22s\n",
      "batch: 1563/1563 - train loss: 16.0088 - test loss: 16.2846 - train acc: 0.2947 - test acc: 0.2874 - 2m 26s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 14.7826 - test loss: 16.7150 - train acc: 0.3293 - test acc: 0.2753 - 2m 31s\n",
      "batch: 200/1563 - train loss: 14.9314 - test loss: 16.1418 - train acc: 0.3312 - test acc: 0.2974 - 2m 36s\n",
      "batch: 300/1563 - train loss: 14.6893 - test loss: 15.6898 - train acc: 0.3287 - test acc: 0.3054 - 2m 40s\n",
      "batch: 400/1563 - train loss: 15.0651 - test loss: 16.4631 - train acc: 0.3187 - test acc: 0.2826 - 2m 45s\n",
      "batch: 500/1563 - train loss: 15.2319 - test loss: 17.5292 - train acc: 0.3062 - test acc: 0.2599 - 2m 50s\n",
      "batch: 600/1563 - train loss: 15.0939 - test loss: 15.3912 - train acc: 0.3106 - test acc: 0.3179 - 2m 54s\n",
      "batch: 700/1563 - train loss: 14.9545 - test loss: 15.7770 - train acc: 0.3256 - test acc: 0.3023 - 2m 59s\n",
      "batch: 800/1563 - train loss: 14.6684 - test loss: 15.6833 - train acc: 0.3412 - test acc: 0.3014 - 3m 4s\n",
      "batch: 900/1563 - train loss: 14.9848 - test loss: 16.8000 - train acc: 0.3271 - test acc: 0.2720 - 3m 8s\n",
      "batch: 1000/1563 - train loss: 14.4760 - test loss: 15.0200 - train acc: 0.3353 - test acc: 0.3380 - 3m 13s\n",
      "batch: 1100/1563 - train loss: 14.8496 - test loss: 16.2487 - train acc: 0.3231 - test acc: 0.3020 - 3m 18s\n",
      "batch: 1200/1563 - train loss: 14.2951 - test loss: 15.4931 - train acc: 0.3503 - test acc: 0.3112 - 3m 23s\n",
      "batch: 1300/1563 - train loss: 14.4444 - test loss: 16.3602 - train acc: 0.3403 - test acc: 0.2949 - 3m 27s\n",
      "batch: 1400/1563 - train loss: 14.7004 - test loss: 14.8207 - train acc: 0.3377 - test acc: 0.3368 - 3m 32s\n",
      "batch: 1500/1563 - train loss: 14.4668 - test loss: 15.0871 - train acc: 0.3453 - test acc: 0.3293 - 3m 37s\n",
      "batch: 1563/1563 - train loss: 14.4528 - test loss: 15.5473 - train acc: 0.3406 - test acc: 0.3128 - 3m 41s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.0673 - test loss: 15.2048 - train acc: 0.3796 - test acc: 0.3228 - 3m 46s\n",
      "batch: 200/1563 - train loss: 12.8444 - test loss: 15.0634 - train acc: 0.3997 - test acc: 0.3274 - 3m 51s\n",
      "batch: 300/1563 - train loss: 12.9350 - test loss: 14.5730 - train acc: 0.3953 - test acc: 0.3408 - 3m 55s\n",
      "batch: 400/1563 - train loss: 13.0197 - test loss: 14.7331 - train acc: 0.3941 - test acc: 0.3423 - 4m 0s\n",
      "batch: 500/1563 - train loss: 12.9559 - test loss: 14.9490 - train acc: 0.3993 - test acc: 0.3283 - 4m 5s\n",
      "batch: 600/1563 - train loss: 13.6219 - test loss: 15.1932 - train acc: 0.3772 - test acc: 0.3341 - 4m 10s\n",
      "batch: 700/1563 - train loss: 12.9941 - test loss: 14.4479 - train acc: 0.3916 - test acc: 0.3537 - 4m 15s\n",
      "batch: 800/1563 - train loss: 13.1898 - test loss: 15.0047 - train acc: 0.3969 - test acc: 0.3353 - 4m 19s\n",
      "batch: 900/1563 - train loss: 13.0592 - test loss: 14.8426 - train acc: 0.3966 - test acc: 0.3415 - 4m 24s\n",
      "batch: 1000/1563 - train loss: 13.4179 - test loss: 13.8073 - train acc: 0.3775 - test acc: 0.3662 - 4m 29s\n",
      "batch: 1100/1563 - train loss: 13.1992 - test loss: 14.4260 - train acc: 0.3878 - test acc: 0.3533 - 4m 34s\n",
      "batch: 1200/1563 - train loss: 13.2960 - test loss: 14.8406 - train acc: 0.3944 - test acc: 0.3396 - 4m 39s\n",
      "batch: 1300/1563 - train loss: 13.0505 - test loss: 15.0594 - train acc: 0.3922 - test acc: 0.3386 - 4m 44s\n",
      "batch: 1400/1563 - train loss: 13.1449 - test loss: 15.8443 - train acc: 0.3872 - test acc: 0.3076 - 4m 49s\n",
      "batch: 1500/1563 - train loss: 13.2706 - test loss: 14.9315 - train acc: 0.3794 - test acc: 0.3410 - 4m 53s\n",
      "batch: 1563/1563 - train loss: 13.1418 - test loss: 15.4522 - train acc: 0.3972 - test acc: 0.3277 - 4m 58s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.1175 - test loss: 13.9977 - train acc: 0.4612 - test acc: 0.3681 - 5m 2s\n",
      "batch: 200/1563 - train loss: 11.2326 - test loss: 14.4235 - train acc: 0.4744 - test acc: 0.3641 - 5m 8s\n",
      "batch: 300/1563 - train loss: 11.7636 - test loss: 14.3539 - train acc: 0.4328 - test acc: 0.3590 - 5m 12s\n",
      "batch: 400/1563 - train loss: 11.8012 - test loss: 15.8457 - train acc: 0.4453 - test acc: 0.3296 - 5m 17s\n",
      "batch: 500/1563 - train loss: 11.6267 - test loss: 14.3150 - train acc: 0.4481 - test acc: 0.3576 - 5m 22s\n",
      "batch: 600/1563 - train loss: 11.8474 - test loss: 14.0170 - train acc: 0.4235 - test acc: 0.3704 - 5m 26s\n",
      "batch: 700/1563 - train loss: 12.0394 - test loss: 13.9811 - train acc: 0.4269 - test acc: 0.3676 - 5m 31s\n",
      "batch: 800/1563 - train loss: 11.7294 - test loss: 13.9737 - train acc: 0.4469 - test acc: 0.3771 - 5m 36s\n",
      "batch: 900/1563 - train loss: 11.9928 - test loss: 14.1769 - train acc: 0.4303 - test acc: 0.3721 - 5m 41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 12.1650 - test loss: 13.9322 - train acc: 0.4206 - test acc: 0.3677 - 5m 45s\n",
      "batch: 1100/1563 - train loss: 11.7396 - test loss: 15.6333 - train acc: 0.4406 - test acc: 0.3327 - 5m 50s\n",
      "batch: 1200/1563 - train loss: 12.2248 - test loss: 14.4071 - train acc: 0.4234 - test acc: 0.3538 - 5m 55s\n",
      "batch: 1300/1563 - train loss: 11.8564 - test loss: 13.7596 - train acc: 0.4394 - test acc: 0.3785 - 5m 59s\n",
      "batch: 1400/1563 - train loss: 11.9822 - test loss: 13.3075 - train acc: 0.4366 - test acc: 0.3911 - 6m 4s\n",
      "batch: 1500/1563 - train loss: 11.7536 - test loss: 13.4109 - train acc: 0.4497 - test acc: 0.3908 - 6m 9s\n",
      "batch: 1563/1563 - train loss: 11.6431 - test loss: 13.6197 - train acc: 0.4472 - test acc: 0.3847 - 6m 13s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 9.9549 - test loss: 13.7140 - train acc: 0.5231 - test acc: 0.3887 - 6m 18s\n",
      "batch: 200/1563 - train loss: 10.2232 - test loss: 14.4905 - train acc: 0.5050 - test acc: 0.3712 - 6m 22s\n",
      "batch: 300/1563 - train loss: 10.4453 - test loss: 13.8540 - train acc: 0.4859 - test acc: 0.3862 - 6m 27s\n",
      "batch: 400/1563 - train loss: 10.2072 - test loss: 15.1886 - train acc: 0.5007 - test acc: 0.3515 - 6m 32s\n",
      "batch: 500/1563 - train loss: 10.4541 - test loss: 13.7349 - train acc: 0.5029 - test acc: 0.3920 - 6m 36s\n",
      "batch: 600/1563 - train loss: 10.5485 - test loss: 13.5896 - train acc: 0.4812 - test acc: 0.3899 - 6m 41s\n",
      "batch: 700/1563 - train loss: 10.4590 - test loss: 13.5217 - train acc: 0.4981 - test acc: 0.3920 - 6m 46s\n",
      "batch: 800/1563 - train loss: 10.6745 - test loss: 14.5255 - train acc: 0.4828 - test acc: 0.3733 - 6m 50s\n",
      "batch: 900/1563 - train loss: 10.7103 - test loss: 14.0213 - train acc: 0.4879 - test acc: 0.3817 - 6m 55s\n",
      "batch: 1000/1563 - train loss: 10.6342 - test loss: 14.3606 - train acc: 0.4841 - test acc: 0.3707 - 7m 0s\n",
      "batch: 1100/1563 - train loss: 10.4825 - test loss: 14.1709 - train acc: 0.4819 - test acc: 0.3755 - 7m 4s\n",
      "batch: 1200/1563 - train loss: 10.8790 - test loss: 14.2794 - train acc: 0.4725 - test acc: 0.3834 - 7m 9s\n",
      "batch: 1300/1563 - train loss: 10.9248 - test loss: 13.4997 - train acc: 0.4681 - test acc: 0.3878 - 7m 14s\n",
      "batch: 1400/1563 - train loss: 11.0855 - test loss: 13.3226 - train acc: 0.4804 - test acc: 0.4056 - 7m 19s\n",
      "batch: 1500/1563 - train loss: 10.6556 - test loss: 14.5680 - train acc: 0.4931 - test acc: 0.3703 - 7m 24s\n",
      "batch: 1563/1563 - train loss: 10.8578 - test loss: 13.4675 - train acc: 0.4806 - test acc: 0.4002 - 7m 27s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.5978 - test loss: 13.6637 - train acc: 0.5697 - test acc: 0.4059 - 7m 32s\n",
      "batch: 200/1563 - train loss: 8.6199 - test loss: 14.8911 - train acc: 0.5768 - test acc: 0.3795 - 7m 37s\n",
      "batch: 300/1563 - train loss: 9.3304 - test loss: 14.6544 - train acc: 0.5431 - test acc: 0.3806 - 7m 42s\n",
      "batch: 400/1563 - train loss: 9.5222 - test loss: 13.4145 - train acc: 0.5403 - test acc: 0.4049 - 7m 46s\n",
      "batch: 500/1563 - train loss: 9.1708 - test loss: 13.6598 - train acc: 0.5515 - test acc: 0.4032 - 7m 51s\n",
      "batch: 600/1563 - train loss: 9.1467 - test loss: 14.4949 - train acc: 0.5569 - test acc: 0.3851 - 7m 56s\n",
      "batch: 700/1563 - train loss: 8.9964 - test loss: 13.6011 - train acc: 0.5575 - test acc: 0.4079 - 8m 0s\n",
      "batch: 800/1563 - train loss: 9.6237 - test loss: 15.6136 - train acc: 0.5328 - test acc: 0.3523 - 8m 5s\n",
      "batch: 900/1563 - train loss: 9.7343 - test loss: 15.2730 - train acc: 0.5247 - test acc: 0.3555 - 8m 10s\n",
      "batch: 1000/1563 - train loss: 9.7303 - test loss: 14.2555 - train acc: 0.5218 - test acc: 0.3863 - 8m 15s\n",
      "batch: 1100/1563 - train loss: 9.8780 - test loss: 13.4922 - train acc: 0.5098 - test acc: 0.4046 - 8m 19s\n",
      "batch: 1200/1563 - train loss: 9.8307 - test loss: 13.2625 - train acc: 0.5166 - test acc: 0.4098 - 8m 24s\n",
      "batch: 1300/1563 - train loss: 9.3700 - test loss: 16.8444 - train acc: 0.5403 - test acc: 0.3402 - 8m 29s\n",
      "batch: 1400/1563 - train loss: 9.8296 - test loss: 13.2839 - train acc: 0.5228 - test acc: 0.4133 - 8m 33s\n",
      "batch: 1500/1563 - train loss: 10.1933 - test loss: 14.1778 - train acc: 0.5100 - test acc: 0.3925 - 8m 38s\n",
      "batch: 1563/1563 - train loss: 10.0655 - test loss: 13.5230 - train acc: 0.5150 - test acc: 0.4036 - 8m 42s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.6135 - test loss: 13.4632 - train acc: 0.6169 - test acc: 0.4198 - 8m 47s\n",
      "batch: 200/1563 - train loss: 7.5190 - test loss: 14.0424 - train acc: 0.6172 - test acc: 0.4071 - 8m 52s\n",
      "batch: 300/1563 - train loss: 7.8218 - test loss: 13.9604 - train acc: 0.6072 - test acc: 0.4121 - 8m 57s\n",
      "batch: 400/1563 - train loss: 8.0470 - test loss: 13.8620 - train acc: 0.5858 - test acc: 0.4063 - 9m 1s\n",
      "batch: 500/1563 - train loss: 8.3038 - test loss: 14.2272 - train acc: 0.5847 - test acc: 0.4001 - 9m 6s\n",
      "batch: 600/1563 - train loss: 8.4518 - test loss: 13.8498 - train acc: 0.5774 - test acc: 0.4066 - 9m 11s\n",
      "batch: 700/1563 - train loss: 8.4426 - test loss: 13.7088 - train acc: 0.5759 - test acc: 0.4081 - 9m 15s\n",
      "batch: 800/1563 - train loss: 8.5819 - test loss: 14.1077 - train acc: 0.5715 - test acc: 0.4003 - 9m 20s\n",
      "batch: 900/1563 - train loss: 8.6406 - test loss: 14.4505 - train acc: 0.5515 - test acc: 0.3925 - 9m 25s\n",
      "batch: 1000/1563 - train loss: 8.4694 - test loss: 13.4585 - train acc: 0.5697 - test acc: 0.4151 - 9m 30s\n",
      "batch: 1100/1563 - train loss: 8.7187 - test loss: 13.8540 - train acc: 0.5672 - test acc: 0.4042 - 9m 34s\n",
      "batch: 1200/1563 - train loss: 8.6896 - test loss: 14.7903 - train acc: 0.5637 - test acc: 0.3798 - 9m 39s\n",
      "batch: 1300/1563 - train loss: 8.7254 - test loss: 14.4051 - train acc: 0.5694 - test acc: 0.3906 - 9m 44s\n",
      "batch: 1400/1563 - train loss: 9.0688 - test loss: 15.1454 - train acc: 0.5534 - test acc: 0.3704 - 9m 48s\n",
      "batch: 1500/1563 - train loss: 8.9635 - test loss: 13.5189 - train acc: 0.5413 - test acc: 0.4086 - 9m 54s\n",
      "batch: 1563/1563 - train loss: 8.9106 - test loss: 13.4863 - train acc: 0.5641 - test acc: 0.4107 - 9m 58s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.3312 - test loss: 14.5034 - train acc: 0.6725 - test acc: 0.4054 - 10m 2s\n",
      "batch: 200/1563 - train loss: 6.3695 - test loss: 14.7279 - train acc: 0.6684 - test acc: 0.4019 - 10m 7s\n",
      "batch: 300/1563 - train loss: 6.5942 - test loss: 14.6832 - train acc: 0.6625 - test acc: 0.3978 - 10m 12s\n",
      "batch: 400/1563 - train loss: 6.9741 - test loss: 14.3633 - train acc: 0.6306 - test acc: 0.4138 - 10m 17s\n",
      "batch: 500/1563 - train loss: 6.9311 - test loss: 14.0166 - train acc: 0.6491 - test acc: 0.4175 - 10m 22s\n",
      "batch: 600/1563 - train loss: 7.2259 - test loss: 13.9204 - train acc: 0.6288 - test acc: 0.4233 - 10m 27s\n",
      "batch: 700/1563 - train loss: 7.3874 - test loss: 14.1087 - train acc: 0.6166 - test acc: 0.4125 - 10m 32s\n",
      "batch: 800/1563 - train loss: 7.3285 - test loss: 13.8804 - train acc: 0.6168 - test acc: 0.4131 - 10m 36s\n",
      "batch: 900/1563 - train loss: 7.2846 - test loss: 14.6571 - train acc: 0.6303 - test acc: 0.3990 - 10m 41s\n",
      "batch: 1000/1563 - train loss: 7.2882 - test loss: 14.6001 - train acc: 0.6278 - test acc: 0.3971 - 10m 45s\n",
      "batch: 1100/1563 - train loss: 7.7777 - test loss: 14.0837 - train acc: 0.6159 - test acc: 0.4134 - 10m 50s\n",
      "batch: 1200/1563 - train loss: 7.6658 - test loss: 14.4768 - train acc: 0.6031 - test acc: 0.3994 - 10m 55s\n",
      "batch: 1300/1563 - train loss: 7.8563 - test loss: 14.8182 - train acc: 0.5996 - test acc: 0.3983 - 11m 0s\n",
      "batch: 1400/1563 - train loss: 7.6900 - test loss: 14.2551 - train acc: 0.6040 - test acc: 0.4101 - 11m 4s\n",
      "batch: 1500/1563 - train loss: 8.1463 - test loss: 13.7511 - train acc: 0.5937 - test acc: 0.4145 - 11m 9s\n",
      "batch: 1563/1563 - train loss: 7.9732 - test loss: 14.8298 - train acc: 0.5950 - test acc: 0.3952 - 11m 13s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.3591 - test loss: 13.8939 - train acc: 0.7197 - test acc: 0.4299 - 11m 17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 5.5791 - test loss: 14.0673 - train acc: 0.7031 - test acc: 0.4285 - 11m 22s\n",
      "batch: 300/1563 - train loss: 5.9798 - test loss: 14.3361 - train acc: 0.6944 - test acc: 0.4194 - 11m 27s\n",
      "batch: 400/1563 - train loss: 5.7487 - test loss: 14.6350 - train acc: 0.7047 - test acc: 0.4059 - 11m 32s\n",
      "batch: 500/1563 - train loss: 5.9866 - test loss: 16.2805 - train acc: 0.6809 - test acc: 0.3800 - 11m 36s\n",
      "batch: 600/1563 - train loss: 6.2706 - test loss: 15.0903 - train acc: 0.6688 - test acc: 0.4004 - 11m 41s\n",
      "batch: 700/1563 - train loss: 6.4077 - test loss: 14.6164 - train acc: 0.6597 - test acc: 0.4225 - 11m 46s\n",
      "batch: 800/1563 - train loss: 6.3501 - test loss: 14.7338 - train acc: 0.6644 - test acc: 0.4082 - 11m 50s\n",
      "batch: 900/1563 - train loss: 6.6301 - test loss: 14.8899 - train acc: 0.6557 - test acc: 0.4078 - 11m 55s\n",
      "batch: 1000/1563 - train loss: 6.6232 - test loss: 14.6339 - train acc: 0.6451 - test acc: 0.4105 - 12m 0s\n",
      "batch: 1100/1563 - train loss: 6.6578 - test loss: 14.6545 - train acc: 0.6606 - test acc: 0.4160 - 12m 5s\n",
      "batch: 1200/1563 - train loss: 6.6731 - test loss: 15.7577 - train acc: 0.6504 - test acc: 0.3961 - 12m 9s\n",
      "batch: 1300/1563 - train loss: 6.8099 - test loss: 14.4459 - train acc: 0.6416 - test acc: 0.4170 - 12m 14s\n",
      "batch: 1400/1563 - train loss: 6.6079 - test loss: 15.3010 - train acc: 0.6534 - test acc: 0.3959 - 12m 19s\n",
      "batch: 1500/1563 - train loss: 7.0375 - test loss: 14.4731 - train acc: 0.6391 - test acc: 0.4052 - 12m 23s\n",
      "batch: 1563/1563 - train loss: 7.0153 - test loss: 14.2989 - train acc: 0.6319 - test acc: 0.4233 - 12m 27s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.6728 - test loss: 14.5761 - train acc: 0.7597 - test acc: 0.4228 - 12m 32s\n",
      "batch: 200/1563 - train loss: 4.5572 - test loss: 14.6108 - train acc: 0.7525 - test acc: 0.4236 - 12m 37s\n",
      "batch: 300/1563 - train loss: 4.4303 - test loss: 15.4139 - train acc: 0.7562 - test acc: 0.4162 - 12m 41s\n",
      "batch: 400/1563 - train loss: 4.8147 - test loss: 16.1234 - train acc: 0.7435 - test acc: 0.3973 - 12m 46s\n",
      "batch: 500/1563 - train loss: 5.0635 - test loss: 16.2949 - train acc: 0.7281 - test acc: 0.3947 - 12m 50s\n",
      "batch: 600/1563 - train loss: 5.1653 - test loss: 15.3536 - train acc: 0.7306 - test acc: 0.4109 - 12m 55s\n",
      "batch: 700/1563 - train loss: 5.3927 - test loss: 15.8710 - train acc: 0.7074 - test acc: 0.4040 - 13m 0s\n",
      "batch: 800/1563 - train loss: 5.5854 - test loss: 15.3551 - train acc: 0.6969 - test acc: 0.4100 - 13m 4s\n",
      "batch: 900/1563 - train loss: 5.5999 - test loss: 15.5556 - train acc: 0.7006 - test acc: 0.4062 - 13m 9s\n",
      "batch: 1000/1563 - train loss: 5.7465 - test loss: 15.7396 - train acc: 0.6969 - test acc: 0.4017 - 13m 14s\n",
      "batch: 1100/1563 - train loss: 5.7969 - test loss: 15.6242 - train acc: 0.6838 - test acc: 0.4013 - 13m 19s\n",
      "batch: 1200/1563 - train loss: 5.8591 - test loss: 15.1771 - train acc: 0.6891 - test acc: 0.4178 - 13m 24s\n",
      "batch: 1300/1563 - train loss: 5.9813 - test loss: 15.7042 - train acc: 0.6848 - test acc: 0.4070 - 13m 28s\n",
      "batch: 1400/1563 - train loss: 6.1781 - test loss: 15.8093 - train acc: 0.6750 - test acc: 0.3938 - 13m 33s\n",
      "batch: 1500/1563 - train loss: 6.2088 - test loss: 14.6477 - train acc: 0.6829 - test acc: 0.4296 - 13m 38s\n",
      "batch: 1563/1563 - train loss: 5.9873 - test loss: 15.3865 - train acc: 0.6834 - test acc: 0.4164 - 13m 42s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.6943 - test loss: 15.0836 - train acc: 0.7990 - test acc: 0.4322 - 13m 46s\n",
      "batch: 200/1563 - train loss: 3.5250 - test loss: 15.7524 - train acc: 0.8125 - test acc: 0.4244 - 13m 51s\n",
      "batch: 300/1563 - train loss: 3.9338 - test loss: 15.5111 - train acc: 0.7930 - test acc: 0.4250 - 13m 56s\n",
      "batch: 400/1563 - train loss: 4.2041 - test loss: 15.6455 - train acc: 0.7728 - test acc: 0.4238 - 14m 0s\n",
      "batch: 500/1563 - train loss: 3.8403 - test loss: 16.6212 - train acc: 0.7906 - test acc: 0.4037 - 14m 5s\n",
      "batch: 600/1563 - train loss: 4.4946 - test loss: 15.5376 - train acc: 0.7594 - test acc: 0.4224 - 14m 10s\n",
      "batch: 700/1563 - train loss: 4.4726 - test loss: 16.8051 - train acc: 0.7547 - test acc: 0.3993 - 14m 15s\n",
      "batch: 800/1563 - train loss: 4.6866 - test loss: 16.3314 - train acc: 0.7497 - test acc: 0.4060 - 14m 19s\n",
      "batch: 900/1563 - train loss: 4.6880 - test loss: 16.0191 - train acc: 0.7525 - test acc: 0.4154 - 14m 24s\n",
      "batch: 1000/1563 - train loss: 4.7805 - test loss: 15.9649 - train acc: 0.7422 - test acc: 0.4175 - 14m 29s\n",
      "batch: 1100/1563 - train loss: 4.9400 - test loss: 16.1027 - train acc: 0.7303 - test acc: 0.4133 - 14m 33s\n",
      "batch: 1200/1563 - train loss: 5.0739 - test loss: 15.9858 - train acc: 0.7316 - test acc: 0.4116 - 14m 38s\n",
      "batch: 1300/1563 - train loss: 5.2998 - test loss: 15.8472 - train acc: 0.7144 - test acc: 0.4090 - 14m 43s\n",
      "batch: 1400/1563 - train loss: 5.0874 - test loss: 15.7436 - train acc: 0.7251 - test acc: 0.4126 - 14m 48s\n",
      "batch: 1500/1563 - train loss: 4.9869 - test loss: 16.1087 - train acc: 0.7341 - test acc: 0.4076 - 14m 52s\n",
      "batch: 1563/1563 - train loss: 5.1012 - test loss: 15.9392 - train acc: 0.7288 - test acc: 0.4089 - 14m 56s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 2.9827 - test loss: 16.2571 - train acc: 0.8393 - test acc: 0.4180 - 15m 1s\n",
      "batch: 200/1563 - train loss: 3.1727 - test loss: 16.4965 - train acc: 0.8255 - test acc: 0.4185 - 15m 6s\n",
      "batch: 300/1563 - train loss: 3.4268 - test loss: 16.0796 - train acc: 0.8110 - test acc: 0.4218 - 15m 10s\n",
      "batch: 400/1563 - train loss: 3.2594 - test loss: 16.2831 - train acc: 0.8246 - test acc: 0.4283 - 15m 15s\n",
      "batch: 500/1563 - train loss: 3.5172 - test loss: 17.3300 - train acc: 0.7984 - test acc: 0.4023 - 15m 20s\n",
      "batch: 600/1563 - train loss: 3.4558 - test loss: 17.1039 - train acc: 0.8137 - test acc: 0.4120 - 15m 25s\n",
      "batch: 700/1563 - train loss: 4.0405 - test loss: 16.2362 - train acc: 0.7787 - test acc: 0.4258 - 15m 29s\n",
      "batch: 800/1563 - train loss: 3.7249 - test loss: 17.2486 - train acc: 0.7981 - test acc: 0.4077 - 15m 34s\n",
      "batch: 900/1563 - train loss: 3.9607 - test loss: 16.3757 - train acc: 0.7756 - test acc: 0.4149 - 15m 39s\n",
      "batch: 1000/1563 - train loss: 3.8353 - test loss: 16.2415 - train acc: 0.7921 - test acc: 0.4213 - 15m 43s\n",
      "batch: 1100/1563 - train loss: 4.1799 - test loss: 16.5553 - train acc: 0.7616 - test acc: 0.4207 - 15m 48s\n",
      "batch: 1200/1563 - train loss: 4.5097 - test loss: 17.0191 - train acc: 0.7543 - test acc: 0.4076 - 15m 53s\n",
      "batch: 1300/1563 - train loss: 4.4991 - test loss: 15.9870 - train acc: 0.7581 - test acc: 0.4175 - 15m 58s\n",
      "batch: 1400/1563 - train loss: 4.2415 - test loss: 16.6683 - train acc: 0.7740 - test acc: 0.4105 - 16m 2s\n",
      "batch: 1500/1563 - train loss: 4.5478 - test loss: 16.2189 - train acc: 0.7525 - test acc: 0.4252 - 16m 7s\n",
      "batch: 1563/1563 - train loss: 4.5590 - test loss: 16.1282 - train acc: 0.7550 - test acc: 0.4156 - 16m 11s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.5976 - test loss: 16.5365 - train acc: 0.8578 - test acc: 0.4220 - 16m 15s\n",
      "batch: 200/1563 - train loss: 2.5230 - test loss: 16.6949 - train acc: 0.8597 - test acc: 0.4301 - 16m 20s\n",
      "batch: 300/1563 - train loss: 2.7217 - test loss: 17.0839 - train acc: 0.8506 - test acc: 0.4120 - 16m 25s\n",
      "batch: 400/1563 - train loss: 2.5997 - test loss: 17.1585 - train acc: 0.8528 - test acc: 0.4150 - 16m 30s\n",
      "batch: 500/1563 - train loss: 2.9145 - test loss: 16.9687 - train acc: 0.8399 - test acc: 0.4135 - 16m 34s\n",
      "batch: 600/1563 - train loss: 2.9242 - test loss: 16.9888 - train acc: 0.8381 - test acc: 0.4210 - 16m 39s\n",
      "batch: 700/1563 - train loss: 3.1178 - test loss: 16.9801 - train acc: 0.8274 - test acc: 0.4236 - 16m 44s\n",
      "batch: 800/1563 - train loss: 3.1753 - test loss: 16.9218 - train acc: 0.8246 - test acc: 0.4231 - 16m 49s\n",
      "batch: 900/1563 - train loss: 3.2529 - test loss: 17.7719 - train acc: 0.8212 - test acc: 0.4023 - 16m 53s\n",
      "batch: 1000/1563 - train loss: 3.3551 - test loss: 17.3171 - train acc: 0.8257 - test acc: 0.4134 - 16m 58s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 3.2969 - test loss: 18.0590 - train acc: 0.8125 - test acc: 0.4116 - 17m 3s\n",
      "batch: 1200/1563 - train loss: 3.6351 - test loss: 18.5113 - train acc: 0.7931 - test acc: 0.3907 - 17m 7s\n",
      "batch: 1300/1563 - train loss: 3.8558 - test loss: 17.5560 - train acc: 0.7859 - test acc: 0.4143 - 17m 12s\n",
      "batch: 1400/1563 - train loss: 3.7376 - test loss: 17.1262 - train acc: 0.7900 - test acc: 0.4180 - 17m 17s\n",
      "batch: 1500/1563 - train loss: 3.9264 - test loss: 17.1880 - train acc: 0.7846 - test acc: 0.4135 - 17m 22s\n",
      "batch: 1563/1563 - train loss: 3.6700 - test loss: 17.0742 - train acc: 0.7956 - test acc: 0.4149 - 17m 26s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.4095 - test loss: 17.0064 - train acc: 0.8656 - test acc: 0.4279 - 17m 30s\n",
      "batch: 200/1563 - train loss: 2.0825 - test loss: 17.5061 - train acc: 0.8866 - test acc: 0.4219 - 17m 35s\n",
      "batch: 300/1563 - train loss: 2.1826 - test loss: 17.4727 - train acc: 0.8782 - test acc: 0.4241 - 17m 40s\n",
      "batch: 400/1563 - train loss: 2.1199 - test loss: 17.9481 - train acc: 0.8743 - test acc: 0.4221 - 17m 44s\n",
      "batch: 500/1563 - train loss: 2.2325 - test loss: 18.0814 - train acc: 0.8753 - test acc: 0.4224 - 17m 49s\n",
      "batch: 600/1563 - train loss: 2.3714 - test loss: 17.6532 - train acc: 0.8657 - test acc: 0.4227 - 17m 53s\n",
      "batch: 700/1563 - train loss: 2.5315 - test loss: 18.4703 - train acc: 0.8475 - test acc: 0.4156 - 17m 58s\n",
      "batch: 800/1563 - train loss: 2.6391 - test loss: 18.0860 - train acc: 0.8506 - test acc: 0.4151 - 18m 3s\n",
      "batch: 900/1563 - train loss: 2.7752 - test loss: 19.0166 - train acc: 0.8450 - test acc: 0.4016 - 18m 8s\n",
      "batch: 1000/1563 - train loss: 2.6998 - test loss: 18.0092 - train acc: 0.8525 - test acc: 0.4162 - 18m 12s\n",
      "batch: 1100/1563 - train loss: 2.9393 - test loss: 17.8805 - train acc: 0.8369 - test acc: 0.4204 - 18m 17s\n",
      "batch: 1200/1563 - train loss: 3.1522 - test loss: 18.6242 - train acc: 0.8243 - test acc: 0.4130 - 18m 22s\n",
      "batch: 1300/1563 - train loss: 3.2757 - test loss: 17.8459 - train acc: 0.8171 - test acc: 0.4153 - 18m 26s\n",
      "batch: 1400/1563 - train loss: 3.0225 - test loss: 17.9069 - train acc: 0.8313 - test acc: 0.4188 - 18m 31s\n",
      "batch: 1500/1563 - train loss: 3.2923 - test loss: 17.7492 - train acc: 0.8109 - test acc: 0.4246 - 18m 35s\n",
      "batch: 1563/1563 - train loss: 3.1407 - test loss: 17.8489 - train acc: 0.8215 - test acc: 0.4192 - 18m 39s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 2.0283 - test loss: 17.8353 - train acc: 0.8894 - test acc: 0.4266 - 18m 44s\n",
      "batch: 200/1563 - train loss: 1.7279 - test loss: 17.6005 - train acc: 0.9063 - test acc: 0.4313 - 18m 48s\n",
      "batch: 300/1563 - train loss: 1.6149 - test loss: 18.0460 - train acc: 0.9092 - test acc: 0.4301 - 18m 53s\n",
      "batch: 400/1563 - train loss: 1.8998 - test loss: 19.1003 - train acc: 0.8891 - test acc: 0.4185 - 18m 58s\n",
      "batch: 500/1563 - train loss: 2.1743 - test loss: 18.9227 - train acc: 0.8766 - test acc: 0.4164 - 19m 3s\n",
      "batch: 600/1563 - train loss: 2.1775 - test loss: 18.8532 - train acc: 0.8644 - test acc: 0.4178 - 19m 7s\n",
      "batch: 700/1563 - train loss: 2.2257 - test loss: 18.6063 - train acc: 0.8747 - test acc: 0.4257 - 19m 12s\n",
      "batch: 800/1563 - train loss: 2.2175 - test loss: 19.0783 - train acc: 0.8750 - test acc: 0.4200 - 19m 17s\n",
      "batch: 900/1563 - train loss: 2.2793 - test loss: 18.9994 - train acc: 0.8731 - test acc: 0.4111 - 19m 21s\n",
      "batch: 1000/1563 - train loss: 2.3583 - test loss: 18.8242 - train acc: 0.8660 - test acc: 0.4151 - 19m 26s\n",
      "batch: 1100/1563 - train loss: 2.4269 - test loss: 18.5124 - train acc: 0.8650 - test acc: 0.4235 - 19m 31s\n",
      "batch: 1200/1563 - train loss: 2.3665 - test loss: 18.2651 - train acc: 0.8665 - test acc: 0.4226 - 19m 36s\n",
      "batch: 1300/1563 - train loss: 2.6435 - test loss: 19.4743 - train acc: 0.8465 - test acc: 0.4104 - 19m 41s\n",
      "batch: 1400/1563 - train loss: 2.6683 - test loss: 18.4841 - train acc: 0.8478 - test acc: 0.4229 - 19m 46s\n",
      "batch: 1500/1563 - train loss: 2.8017 - test loss: 19.7483 - train acc: 0.8380 - test acc: 0.4043 - 19m 50s\n",
      "batch: 1563/1563 - train loss: 2.7721 - test loss: 18.6862 - train acc: 0.8409 - test acc: 0.4164 - 19m 54s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 17/100\n",
      "batch: 100/1563 - train loss: 1.5056 - test loss: 18.6540 - train acc: 0.9176 - test acc: 0.4246 - 19m 59s\n",
      "time is up! finishing training\n",
      "batch: 101/1563 - train loss: 1.5050 - test loss: 18.5617 - train acc: 0.9180 - test acc: 0.4276 - 20m 2s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 26.1789 - test loss: 25.0280 - train acc: 0.0332 - test acc: 0.0458 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.4747 - test loss: 23.5502 - train acc: 0.0492 - test acc: 0.0636 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.1912 - test loss: 22.8275 - train acc: 0.0793 - test acc: 0.0880 - 0m 10s\n",
      "batch: 400/1563 - train loss: 22.2691 - test loss: 21.9213 - train acc: 0.1068 - test acc: 0.1109 - 0m 15s\n",
      "batch: 500/1563 - train loss: 21.9969 - test loss: 22.3158 - train acc: 0.1115 - test acc: 0.0905 - 0m 19s\n",
      "batch: 600/1563 - train loss: 21.3600 - test loss: 21.3250 - train acc: 0.1297 - test acc: 0.1337 - 0m 24s\n",
      "batch: 700/1563 - train loss: 20.9972 - test loss: 20.8703 - train acc: 0.1303 - test acc: 0.1348 - 0m 29s\n",
      "batch: 800/1563 - train loss: 20.8365 - test loss: 20.8417 - train acc: 0.1353 - test acc: 0.1365 - 0m 33s\n",
      "batch: 900/1563 - train loss: 20.4548 - test loss: 20.2521 - train acc: 0.1525 - test acc: 0.1519 - 0m 38s\n",
      "batch: 1000/1563 - train loss: 19.9435 - test loss: 20.1688 - train acc: 0.1525 - test acc: 0.1603 - 0m 42s\n",
      "batch: 1100/1563 - train loss: 19.9075 - test loss: 19.6063 - train acc: 0.1635 - test acc: 0.1872 - 0m 47s\n",
      "batch: 1200/1563 - train loss: 19.3972 - test loss: 19.6094 - train acc: 0.1800 - test acc: 0.1782 - 0m 51s\n",
      "batch: 1300/1563 - train loss: 19.1599 - test loss: 18.8890 - train acc: 0.1844 - test acc: 0.1949 - 0m 56s\n",
      "batch: 1400/1563 - train loss: 19.1231 - test loss: 20.3619 - train acc: 0.1845 - test acc: 0.1723 - 1m 0s\n",
      "batch: 1500/1563 - train loss: 18.6933 - test loss: 18.7952 - train acc: 0.2013 - test acc: 0.2020 - 1m 5s\n",
      "batch: 1563/1563 - train loss: 18.6406 - test loss: 19.2247 - train acc: 0.2098 - test acc: 0.1933 - 1m 9s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.7698 - test loss: 18.7102 - train acc: 0.2309 - test acc: 0.2109 - 1m 13s\n",
      "batch: 200/1563 - train loss: 17.8621 - test loss: 19.1724 - train acc: 0.2223 - test acc: 0.2059 - 1m 18s\n",
      "batch: 300/1563 - train loss: 17.5673 - test loss: 17.9301 - train acc: 0.2391 - test acc: 0.2279 - 1m 22s\n",
      "batch: 400/1563 - train loss: 17.6826 - test loss: 20.1747 - train acc: 0.2253 - test acc: 0.1860 - 1m 27s\n",
      "batch: 500/1563 - train loss: 17.4793 - test loss: 19.2603 - train acc: 0.2394 - test acc: 0.2007 - 1m 31s\n",
      "batch: 600/1563 - train loss: 17.4297 - test loss: 17.8889 - train acc: 0.2375 - test acc: 0.2355 - 1m 36s\n",
      "batch: 700/1563 - train loss: 17.1398 - test loss: 18.6456 - train acc: 0.2484 - test acc: 0.2162 - 1m 40s\n",
      "batch: 800/1563 - train loss: 17.1508 - test loss: 17.8782 - train acc: 0.2478 - test acc: 0.2283 - 1m 45s\n",
      "batch: 900/1563 - train loss: 16.6643 - test loss: 17.7182 - train acc: 0.2656 - test acc: 0.2424 - 1m 50s\n",
      "batch: 1000/1563 - train loss: 16.8956 - test loss: 17.5996 - train acc: 0.2572 - test acc: 0.2395 - 1m 54s\n",
      "batch: 1100/1563 - train loss: 16.8608 - test loss: 17.3280 - train acc: 0.2553 - test acc: 0.2481 - 1m 59s\n",
      "batch: 1200/1563 - train loss: 17.0335 - test loss: 16.5116 - train acc: 0.2594 - test acc: 0.2787 - 2m 3s\n",
      "batch: 1300/1563 - train loss: 16.8460 - test loss: 17.5009 - train acc: 0.2644 - test acc: 0.2475 - 2m 8s\n",
      "batch: 1400/1563 - train loss: 16.3300 - test loss: 17.2129 - train acc: 0.2790 - test acc: 0.2507 - 2m 13s\n",
      "batch: 1500/1563 - train loss: 16.2900 - test loss: 20.0126 - train acc: 0.2787 - test acc: 0.1854 - 2m 17s\n",
      "batch: 1563/1563 - train loss: 16.2838 - test loss: 17.6390 - train acc: 0.2812 - test acc: 0.2467 - 2m 21s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.1489 - test loss: 16.4639 - train acc: 0.3150 - test acc: 0.2756 - 2m 26s\n",
      "batch: 200/1563 - train loss: 14.9979 - test loss: 16.6420 - train acc: 0.3203 - test acc: 0.2859 - 2m 30s\n",
      "batch: 300/1563 - train loss: 14.8701 - test loss: 16.2716 - train acc: 0.3178 - test acc: 0.2862 - 2m 35s\n",
      "batch: 400/1563 - train loss: 14.8966 - test loss: 16.3138 - train acc: 0.3359 - test acc: 0.2915 - 2m 39s\n",
      "batch: 500/1563 - train loss: 15.1305 - test loss: 16.4009 - train acc: 0.3212 - test acc: 0.2834 - 2m 44s\n",
      "batch: 600/1563 - train loss: 15.1128 - test loss: 15.8309 - train acc: 0.3337 - test acc: 0.2979 - 2m 48s\n",
      "batch: 700/1563 - train loss: 15.2730 - test loss: 15.8483 - train acc: 0.3209 - test acc: 0.2965 - 2m 53s\n",
      "batch: 800/1563 - train loss: 14.9457 - test loss: 16.1130 - train acc: 0.3284 - test acc: 0.2949 - 2m 58s\n",
      "batch: 900/1563 - train loss: 14.9087 - test loss: 15.7627 - train acc: 0.3275 - test acc: 0.3013 - 3m 2s\n",
      "batch: 1000/1563 - train loss: 14.8769 - test loss: 16.6356 - train acc: 0.3334 - test acc: 0.2895 - 3m 7s\n",
      "batch: 1100/1563 - train loss: 14.9218 - test loss: 15.8970 - train acc: 0.3125 - test acc: 0.2952 - 3m 12s\n",
      "batch: 1200/1563 - train loss: 14.9197 - test loss: 15.7142 - train acc: 0.3171 - test acc: 0.3039 - 3m 16s\n",
      "batch: 1300/1563 - train loss: 14.9109 - test loss: 18.0471 - train acc: 0.3246 - test acc: 0.2412 - 3m 21s\n",
      "batch: 1400/1563 - train loss: 14.8743 - test loss: 14.9153 - train acc: 0.3309 - test acc: 0.3339 - 3m 25s\n",
      "batch: 1500/1563 - train loss: 14.5130 - test loss: 16.1785 - train acc: 0.3359 - test acc: 0.2926 - 3m 30s\n",
      "batch: 1563/1563 - train loss: 14.3869 - test loss: 15.5316 - train acc: 0.3350 - test acc: 0.3160 - 3m 34s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.0795 - test loss: 15.4621 - train acc: 0.3870 - test acc: 0.3244 - 3m 38s\n",
      "batch: 200/1563 - train loss: 13.4906 - test loss: 14.7264 - train acc: 0.3844 - test acc: 0.3377 - 3m 43s\n",
      "batch: 300/1563 - train loss: 13.0756 - test loss: 14.7885 - train acc: 0.3953 - test acc: 0.3425 - 3m 47s\n",
      "batch: 400/1563 - train loss: 13.2582 - test loss: 17.7144 - train acc: 0.4044 - test acc: 0.2767 - 3m 52s\n",
      "batch: 500/1563 - train loss: 13.5910 - test loss: 16.1841 - train acc: 0.3784 - test acc: 0.2991 - 3m 57s\n",
      "batch: 600/1563 - train loss: 13.4515 - test loss: 14.3191 - train acc: 0.3819 - test acc: 0.3600 - 4m 1s\n",
      "batch: 700/1563 - train loss: 13.3057 - test loss: 14.6190 - train acc: 0.3853 - test acc: 0.3503 - 4m 6s\n",
      "batch: 800/1563 - train loss: 13.5197 - test loss: 15.0527 - train acc: 0.3797 - test acc: 0.3360 - 4m 11s\n",
      "batch: 900/1563 - train loss: 13.2495 - test loss: 16.0347 - train acc: 0.3947 - test acc: 0.3129 - 4m 15s\n",
      "batch: 1000/1563 - train loss: 13.2573 - test loss: 14.3257 - train acc: 0.3940 - test acc: 0.3577 - 4m 20s\n",
      "batch: 1100/1563 - train loss: 13.3526 - test loss: 14.3268 - train acc: 0.3826 - test acc: 0.3527 - 4m 25s\n",
      "batch: 1200/1563 - train loss: 13.4449 - test loss: 15.7938 - train acc: 0.3819 - test acc: 0.3209 - 4m 29s\n",
      "batch: 1300/1563 - train loss: 12.8125 - test loss: 14.9449 - train acc: 0.4016 - test acc: 0.3410 - 4m 34s\n",
      "batch: 1400/1563 - train loss: 13.0752 - test loss: 16.3828 - train acc: 0.4001 - test acc: 0.3008 - 4m 38s\n",
      "batch: 1500/1563 - train loss: 13.3399 - test loss: 13.9942 - train acc: 0.3934 - test acc: 0.3713 - 4m 43s\n",
      "batch: 1563/1563 - train loss: 13.2227 - test loss: 14.0166 - train acc: 0.3921 - test acc: 0.3610 - 4m 47s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.5743 - test loss: 14.6813 - train acc: 0.4444 - test acc: 0.3497 - 4m 52s\n",
      "batch: 200/1563 - train loss: 11.5691 - test loss: 14.0352 - train acc: 0.4528 - test acc: 0.3771 - 4m 56s\n",
      "batch: 300/1563 - train loss: 11.8653 - test loss: 14.7084 - train acc: 0.4341 - test acc: 0.3486 - 5m 1s\n",
      "batch: 400/1563 - train loss: 11.8382 - test loss: 15.2228 - train acc: 0.4328 - test acc: 0.3389 - 5m 6s\n",
      "batch: 500/1563 - train loss: 11.7910 - test loss: 16.5983 - train acc: 0.4513 - test acc: 0.3007 - 5m 11s\n",
      "batch: 600/1563 - train loss: 12.0580 - test loss: 14.1332 - train acc: 0.4338 - test acc: 0.3642 - 5m 16s\n",
      "batch: 700/1563 - train loss: 11.9790 - test loss: 14.4338 - train acc: 0.4453 - test acc: 0.3540 - 5m 20s\n",
      "batch: 800/1563 - train loss: 11.9930 - test loss: 14.8632 - train acc: 0.4435 - test acc: 0.3506 - 5m 25s\n",
      "batch: 900/1563 - train loss: 11.7738 - test loss: 14.6581 - train acc: 0.4465 - test acc: 0.3506 - 5m 30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 11.9671 - test loss: 13.9221 - train acc: 0.4313 - test acc: 0.3751 - 5m 34s\n",
      "batch: 1100/1563 - train loss: 11.6963 - test loss: 13.9087 - train acc: 0.4640 - test acc: 0.3801 - 5m 39s\n",
      "batch: 1200/1563 - train loss: 12.1582 - test loss: 13.7193 - train acc: 0.4316 - test acc: 0.3824 - 5m 43s\n",
      "batch: 1300/1563 - train loss: 11.9271 - test loss: 13.9258 - train acc: 0.4357 - test acc: 0.3761 - 5m 48s\n",
      "batch: 1400/1563 - train loss: 12.0555 - test loss: 16.8274 - train acc: 0.4338 - test acc: 0.3094 - 5m 53s\n",
      "batch: 1500/1563 - train loss: 12.0011 - test loss: 14.5264 - train acc: 0.4322 - test acc: 0.3606 - 5m 57s\n",
      "batch: 1563/1563 - train loss: 11.8680 - test loss: 14.4720 - train acc: 0.4484 - test acc: 0.3588 - 6m 2s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.2181 - test loss: 14.6865 - train acc: 0.5006 - test acc: 0.3688 - 6m 6s\n",
      "batch: 200/1563 - train loss: 10.0822 - test loss: 14.7048 - train acc: 0.5175 - test acc: 0.3623 - 6m 11s\n",
      "batch: 300/1563 - train loss: 10.3920 - test loss: 13.5237 - train acc: 0.5007 - test acc: 0.3889 - 6m 16s\n",
      "batch: 400/1563 - train loss: 10.3025 - test loss: 14.4825 - train acc: 0.4866 - test acc: 0.3686 - 6m 20s\n",
      "batch: 500/1563 - train loss: 10.4937 - test loss: 14.5813 - train acc: 0.4884 - test acc: 0.3659 - 6m 25s\n",
      "batch: 600/1563 - train loss: 10.8884 - test loss: 13.8509 - train acc: 0.4834 - test acc: 0.3887 - 6m 29s\n",
      "batch: 700/1563 - train loss: 10.8798 - test loss: 14.4616 - train acc: 0.4797 - test acc: 0.3669 - 6m 34s\n",
      "batch: 800/1563 - train loss: 10.5530 - test loss: 14.0576 - train acc: 0.4844 - test acc: 0.3812 - 6m 39s\n",
      "batch: 900/1563 - train loss: 11.0052 - test loss: 13.6696 - train acc: 0.4825 - test acc: 0.3961 - 6m 43s\n",
      "batch: 1000/1563 - train loss: 10.7269 - test loss: 14.0919 - train acc: 0.4850 - test acc: 0.3818 - 6m 48s\n",
      "batch: 1100/1563 - train loss: 10.6440 - test loss: 13.5112 - train acc: 0.4844 - test acc: 0.4003 - 6m 53s\n",
      "batch: 1200/1563 - train loss: 10.7813 - test loss: 13.7797 - train acc: 0.4918 - test acc: 0.3870 - 6m 58s\n",
      "batch: 1300/1563 - train loss: 11.2363 - test loss: 13.8725 - train acc: 0.4694 - test acc: 0.3820 - 7m 3s\n",
      "batch: 1400/1563 - train loss: 11.2829 - test loss: 14.6921 - train acc: 0.4781 - test acc: 0.3590 - 7m 8s\n",
      "batch: 1500/1563 - train loss: 10.8347 - test loss: 13.5519 - train acc: 0.4844 - test acc: 0.3994 - 7m 12s\n",
      "batch: 1563/1563 - train loss: 10.8975 - test loss: 13.2705 - train acc: 0.4794 - test acc: 0.4041 - 7m 16s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 9.0420 - test loss: 13.4038 - train acc: 0.5472 - test acc: 0.4095 - 7m 20s\n",
      "batch: 200/1563 - train loss: 9.1828 - test loss: 14.9467 - train acc: 0.5528 - test acc: 0.3624 - 7m 25s\n",
      "batch: 300/1563 - train loss: 8.9059 - test loss: 15.1580 - train acc: 0.5478 - test acc: 0.3635 - 7m 30s\n",
      "batch: 400/1563 - train loss: 9.1526 - test loss: 13.8658 - train acc: 0.5462 - test acc: 0.4030 - 7m 35s\n",
      "batch: 500/1563 - train loss: 9.4350 - test loss: 14.0821 - train acc: 0.5444 - test acc: 0.3864 - 7m 39s\n",
      "batch: 600/1563 - train loss: 9.6167 - test loss: 14.1638 - train acc: 0.5247 - test acc: 0.3859 - 7m 44s\n",
      "batch: 700/1563 - train loss: 9.6946 - test loss: 14.1250 - train acc: 0.5265 - test acc: 0.3848 - 7m 48s\n",
      "batch: 800/1563 - train loss: 9.3908 - test loss: 14.5931 - train acc: 0.5353 - test acc: 0.3757 - 7m 53s\n",
      "batch: 900/1563 - train loss: 9.5043 - test loss: 13.9369 - train acc: 0.5284 - test acc: 0.3966 - 7m 57s\n",
      "batch: 1000/1563 - train loss: 9.8837 - test loss: 14.1635 - train acc: 0.5112 - test acc: 0.3829 - 8m 2s\n",
      "batch: 1100/1563 - train loss: 10.0761 - test loss: 13.6986 - train acc: 0.5178 - test acc: 0.4001 - 8m 7s\n",
      "batch: 1200/1563 - train loss: 9.6665 - test loss: 15.2281 - train acc: 0.5256 - test acc: 0.3671 - 8m 12s\n",
      "batch: 1300/1563 - train loss: 9.8562 - test loss: 14.1323 - train acc: 0.5222 - test acc: 0.3869 - 8m 16s\n",
      "batch: 1400/1563 - train loss: 10.2437 - test loss: 13.6374 - train acc: 0.5060 - test acc: 0.4016 - 8m 21s\n",
      "batch: 1500/1563 - train loss: 9.7312 - test loss: 13.7453 - train acc: 0.5284 - test acc: 0.3943 - 8m 25s\n",
      "batch: 1563/1563 - train loss: 9.7568 - test loss: 15.0735 - train acc: 0.5206 - test acc: 0.3562 - 8m 29s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.4325 - test loss: 13.6249 - train acc: 0.6172 - test acc: 0.4212 - 8m 34s\n",
      "batch: 200/1563 - train loss: 7.5714 - test loss: 14.0835 - train acc: 0.6165 - test acc: 0.3985 - 8m 39s\n",
      "batch: 300/1563 - train loss: 7.9443 - test loss: 14.4124 - train acc: 0.5937 - test acc: 0.3941 - 8m 43s\n",
      "batch: 400/1563 - train loss: 8.1245 - test loss: 14.4106 - train acc: 0.5803 - test acc: 0.3946 - 8m 48s\n",
      "batch: 500/1563 - train loss: 8.3829 - test loss: 14.4881 - train acc: 0.5819 - test acc: 0.3884 - 8m 52s\n",
      "batch: 600/1563 - train loss: 8.3832 - test loss: 14.2839 - train acc: 0.5859 - test acc: 0.3909 - 8m 57s\n",
      "batch: 700/1563 - train loss: 8.6731 - test loss: 14.3387 - train acc: 0.5663 - test acc: 0.3864 - 9m 2s\n",
      "batch: 800/1563 - train loss: 8.7926 - test loss: 13.6913 - train acc: 0.5637 - test acc: 0.4038 - 9m 6s\n",
      "batch: 900/1563 - train loss: 8.8446 - test loss: 13.8688 - train acc: 0.5581 - test acc: 0.4057 - 9m 11s\n",
      "batch: 1000/1563 - train loss: 8.8780 - test loss: 13.9724 - train acc: 0.5509 - test acc: 0.4071 - 9m 16s\n",
      "batch: 1100/1563 - train loss: 8.8279 - test loss: 14.3507 - train acc: 0.5622 - test acc: 0.3974 - 9m 20s\n",
      "batch: 1200/1563 - train loss: 8.9607 - test loss: 13.8673 - train acc: 0.5513 - test acc: 0.4076 - 9m 25s\n",
      "batch: 1300/1563 - train loss: 8.6692 - test loss: 14.6311 - train acc: 0.5693 - test acc: 0.3899 - 9m 29s\n",
      "batch: 1400/1563 - train loss: 8.7063 - test loss: 14.0517 - train acc: 0.5578 - test acc: 0.4010 - 9m 34s\n",
      "batch: 1500/1563 - train loss: 9.0060 - test loss: 15.0842 - train acc: 0.5463 - test acc: 0.3710 - 9m 39s\n",
      "batch: 1563/1563 - train loss: 9.1029 - test loss: 14.0721 - train acc: 0.5503 - test acc: 0.4025 - 9m 43s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.4895 - test loss: 13.9181 - train acc: 0.6704 - test acc: 0.4170 - 9m 47s\n",
      "batch: 200/1563 - train loss: 7.0896 - test loss: 14.4383 - train acc: 0.6278 - test acc: 0.4015 - 9m 52s\n",
      "batch: 300/1563 - train loss: 6.8918 - test loss: 14.1807 - train acc: 0.6403 - test acc: 0.4085 - 9m 56s\n",
      "batch: 400/1563 - train loss: 7.2234 - test loss: 15.1215 - train acc: 0.6322 - test acc: 0.3923 - 10m 1s\n",
      "batch: 500/1563 - train loss: 7.2930 - test loss: 14.5640 - train acc: 0.6184 - test acc: 0.3971 - 10m 6s\n",
      "batch: 600/1563 - train loss: 7.4779 - test loss: 14.9376 - train acc: 0.6178 - test acc: 0.3897 - 10m 11s\n",
      "batch: 700/1563 - train loss: 7.2266 - test loss: 14.0785 - train acc: 0.6350 - test acc: 0.4149 - 10m 16s\n",
      "batch: 800/1563 - train loss: 7.4912 - test loss: 14.4498 - train acc: 0.6206 - test acc: 0.3995 - 10m 20s\n",
      "batch: 900/1563 - train loss: 7.9546 - test loss: 13.9534 - train acc: 0.6040 - test acc: 0.4123 - 10m 25s\n",
      "batch: 1000/1563 - train loss: 7.6832 - test loss: 14.7035 - train acc: 0.6122 - test acc: 0.3969 - 10m 29s\n",
      "batch: 1100/1563 - train loss: 7.4175 - test loss: 14.3616 - train acc: 0.6178 - test acc: 0.4088 - 10m 34s\n",
      "batch: 1200/1563 - train loss: 8.0306 - test loss: 14.1232 - train acc: 0.5856 - test acc: 0.4141 - 10m 38s\n",
      "batch: 1300/1563 - train loss: 8.1645 - test loss: 13.9526 - train acc: 0.5912 - test acc: 0.4104 - 10m 43s\n",
      "batch: 1400/1563 - train loss: 7.9701 - test loss: 14.0134 - train acc: 0.6012 - test acc: 0.4112 - 10m 48s\n",
      "batch: 1500/1563 - train loss: 8.1599 - test loss: 14.5966 - train acc: 0.5859 - test acc: 0.3981 - 10m 52s\n",
      "batch: 1563/1563 - train loss: 7.8953 - test loss: 13.8312 - train acc: 0.6016 - test acc: 0.4138 - 10m 56s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.3737 - test loss: 14.5470 - train acc: 0.7175 - test acc: 0.4133 - 11m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 5.4702 - test loss: 14.6024 - train acc: 0.7085 - test acc: 0.4184 - 11m 6s\n",
      "batch: 300/1563 - train loss: 5.7803 - test loss: 15.2164 - train acc: 0.6925 - test acc: 0.4066 - 11m 10s\n",
      "batch: 400/1563 - train loss: 6.0400 - test loss: 15.6193 - train acc: 0.6804 - test acc: 0.3911 - 11m 15s\n",
      "batch: 500/1563 - train loss: 6.2485 - test loss: 15.0511 - train acc: 0.6766 - test acc: 0.4033 - 11m 20s\n",
      "batch: 600/1563 - train loss: 6.4745 - test loss: 15.2259 - train acc: 0.6554 - test acc: 0.3901 - 11m 24s\n",
      "batch: 700/1563 - train loss: 6.4664 - test loss: 15.6459 - train acc: 0.6638 - test acc: 0.3972 - 11m 29s\n",
      "batch: 800/1563 - train loss: 6.4201 - test loss: 14.6769 - train acc: 0.6697 - test acc: 0.4108 - 11m 33s\n",
      "batch: 900/1563 - train loss: 6.4424 - test loss: 15.0115 - train acc: 0.6654 - test acc: 0.4029 - 11m 38s\n",
      "batch: 1000/1563 - train loss: 6.9075 - test loss: 15.0894 - train acc: 0.6444 - test acc: 0.4029 - 11m 43s\n",
      "batch: 1100/1563 - train loss: 7.0137 - test loss: 15.5941 - train acc: 0.6334 - test acc: 0.3784 - 11m 47s\n",
      "batch: 1200/1563 - train loss: 7.1212 - test loss: 15.2387 - train acc: 0.6371 - test acc: 0.3916 - 11m 52s\n",
      "batch: 1300/1563 - train loss: 7.0461 - test loss: 15.2448 - train acc: 0.6453 - test acc: 0.3920 - 11m 57s\n",
      "batch: 1400/1563 - train loss: 6.8870 - test loss: 14.3919 - train acc: 0.6444 - test acc: 0.4115 - 12m 2s\n",
      "batch: 1500/1563 - train loss: 6.9605 - test loss: 15.6896 - train acc: 0.6400 - test acc: 0.3836 - 12m 6s\n",
      "batch: 1563/1563 - train loss: 7.2561 - test loss: 14.0596 - train acc: 0.6141 - test acc: 0.4180 - 12m 10s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.4761 - test loss: 15.7090 - train acc: 0.7590 - test acc: 0.4048 - 12m 15s\n",
      "batch: 200/1563 - train loss: 4.6385 - test loss: 15.1280 - train acc: 0.7591 - test acc: 0.4162 - 12m 19s\n",
      "batch: 300/1563 - train loss: 4.9194 - test loss: 14.7746 - train acc: 0.7428 - test acc: 0.4238 - 12m 24s\n",
      "batch: 400/1563 - train loss: 4.8349 - test loss: 15.2118 - train acc: 0.7529 - test acc: 0.4150 - 12m 29s\n",
      "batch: 500/1563 - train loss: 5.1816 - test loss: 15.7473 - train acc: 0.7188 - test acc: 0.4029 - 12m 34s\n",
      "batch: 600/1563 - train loss: 5.5653 - test loss: 15.6867 - train acc: 0.7097 - test acc: 0.3987 - 12m 38s\n",
      "batch: 700/1563 - train loss: 5.6790 - test loss: 15.8646 - train acc: 0.6981 - test acc: 0.3975 - 12m 43s\n",
      "batch: 800/1563 - train loss: 5.4806 - test loss: 15.3664 - train acc: 0.7060 - test acc: 0.4061 - 12m 47s\n",
      "batch: 900/1563 - train loss: 5.4476 - test loss: 15.2086 - train acc: 0.7201 - test acc: 0.4174 - 12m 52s\n",
      "batch: 1000/1563 - train loss: 5.8669 - test loss: 15.1185 - train acc: 0.6886 - test acc: 0.4209 - 12m 57s\n",
      "batch: 1100/1563 - train loss: 5.7229 - test loss: 15.5927 - train acc: 0.6873 - test acc: 0.4036 - 13m 1s\n",
      "batch: 1200/1563 - train loss: 6.2852 - test loss: 16.0987 - train acc: 0.6741 - test acc: 0.3960 - 13m 6s\n",
      "batch: 1300/1563 - train loss: 6.0194 - test loss: 15.2129 - train acc: 0.6813 - test acc: 0.4128 - 13m 10s\n",
      "batch: 1400/1563 - train loss: 6.2005 - test loss: 15.2878 - train acc: 0.6691 - test acc: 0.4203 - 13m 15s\n",
      "batch: 1500/1563 - train loss: 6.1713 - test loss: 15.6912 - train acc: 0.6782 - test acc: 0.3984 - 13m 20s\n",
      "batch: 1563/1563 - train loss: 6.3773 - test loss: 15.4529 - train acc: 0.6619 - test acc: 0.4073 - 13m 24s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.8089 - test loss: 15.3760 - train acc: 0.8046 - test acc: 0.4266 - 13m 29s\n",
      "batch: 200/1563 - train loss: 3.6732 - test loss: 15.9749 - train acc: 0.8037 - test acc: 0.4086 - 13m 33s\n",
      "batch: 300/1563 - train loss: 4.0310 - test loss: 15.7482 - train acc: 0.7768 - test acc: 0.4111 - 13m 38s\n",
      "batch: 400/1563 - train loss: 4.1102 - test loss: 15.9567 - train acc: 0.7724 - test acc: 0.4101 - 13m 43s\n",
      "batch: 500/1563 - train loss: 4.2242 - test loss: 15.9078 - train acc: 0.7690 - test acc: 0.4222 - 13m 47s\n",
      "batch: 600/1563 - train loss: 4.2899 - test loss: 16.1793 - train acc: 0.7702 - test acc: 0.4104 - 13m 52s\n",
      "batch: 700/1563 - train loss: 4.6840 - test loss: 16.1160 - train acc: 0.7538 - test acc: 0.4035 - 13m 57s\n",
      "batch: 800/1563 - train loss: 5.0723 - test loss: 16.4853 - train acc: 0.7278 - test acc: 0.4057 - 14m 2s\n",
      "batch: 900/1563 - train loss: 5.0427 - test loss: 16.1885 - train acc: 0.7241 - test acc: 0.4047 - 14m 6s\n",
      "batch: 1000/1563 - train loss: 5.2846 - test loss: 16.6171 - train acc: 0.7191 - test acc: 0.3997 - 14m 11s\n",
      "batch: 1100/1563 - train loss: 5.1134 - test loss: 16.4940 - train acc: 0.7303 - test acc: 0.3968 - 14m 15s\n",
      "batch: 1200/1563 - train loss: 5.2362 - test loss: 15.8631 - train acc: 0.7144 - test acc: 0.4067 - 14m 20s\n",
      "batch: 1300/1563 - train loss: 5.2026 - test loss: 15.6554 - train acc: 0.7159 - test acc: 0.4176 - 14m 25s\n",
      "batch: 1400/1563 - train loss: 5.5590 - test loss: 15.5805 - train acc: 0.6975 - test acc: 0.4217 - 14m 30s\n",
      "batch: 1500/1563 - train loss: 5.2660 - test loss: 15.5339 - train acc: 0.7178 - test acc: 0.4154 - 14m 34s\n",
      "batch: 1563/1563 - train loss: 5.5072 - test loss: 16.1353 - train acc: 0.7141 - test acc: 0.3987 - 14m 38s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.2512 - test loss: 16.1071 - train acc: 0.8184 - test acc: 0.4241 - 14m 43s\n",
      "batch: 200/1563 - train loss: 2.9498 - test loss: 16.3442 - train acc: 0.8397 - test acc: 0.4157 - 14m 47s\n",
      "batch: 300/1563 - train loss: 3.1798 - test loss: 16.5206 - train acc: 0.8280 - test acc: 0.4211 - 14m 52s\n",
      "batch: 400/1563 - train loss: 3.4216 - test loss: 16.6947 - train acc: 0.8071 - test acc: 0.4139 - 14m 57s\n",
      "batch: 500/1563 - train loss: 3.6155 - test loss: 16.8672 - train acc: 0.7971 - test acc: 0.4086 - 15m 2s\n",
      "batch: 600/1563 - train loss: 3.8404 - test loss: 16.7892 - train acc: 0.7818 - test acc: 0.4172 - 15m 6s\n",
      "batch: 700/1563 - train loss: 3.6312 - test loss: 16.3398 - train acc: 0.7940 - test acc: 0.4265 - 15m 11s\n",
      "batch: 800/1563 - train loss: 3.8557 - test loss: 16.7821 - train acc: 0.7825 - test acc: 0.4193 - 15m 15s\n",
      "batch: 900/1563 - train loss: 4.0985 - test loss: 17.1177 - train acc: 0.7653 - test acc: 0.4026 - 15m 20s\n",
      "batch: 1000/1563 - train loss: 4.5770 - test loss: 16.7249 - train acc: 0.7443 - test acc: 0.4076 - 15m 25s\n",
      "batch: 1100/1563 - train loss: 4.2262 - test loss: 16.9141 - train acc: 0.7718 - test acc: 0.4084 - 15m 29s\n",
      "batch: 1200/1563 - train loss: 4.3474 - test loss: 16.7205 - train acc: 0.7578 - test acc: 0.4158 - 15m 35s\n",
      "batch: 1300/1563 - train loss: 4.5590 - test loss: 16.9115 - train acc: 0.7438 - test acc: 0.3992 - 15m 39s\n",
      "batch: 1400/1563 - train loss: 4.4335 - test loss: 16.6176 - train acc: 0.7572 - test acc: 0.4140 - 15m 44s\n",
      "batch: 1500/1563 - train loss: 4.6953 - test loss: 16.4026 - train acc: 0.7372 - test acc: 0.4159 - 15m 48s\n",
      "batch: 1563/1563 - train loss: 4.9462 - test loss: 16.7087 - train acc: 0.7319 - test acc: 0.4105 - 15m 52s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.7648 - test loss: 17.0913 - train acc: 0.8478 - test acc: 0.4188 - 15m 57s\n",
      "batch: 200/1563 - train loss: 2.5157 - test loss: 16.8745 - train acc: 0.8553 - test acc: 0.4177 - 16m 1s\n",
      "batch: 300/1563 - train loss: 2.6844 - test loss: 17.1403 - train acc: 0.8556 - test acc: 0.4142 - 16m 6s\n",
      "batch: 400/1563 - train loss: 3.0750 - test loss: 17.3689 - train acc: 0.8271 - test acc: 0.4137 - 16m 11s\n",
      "batch: 500/1563 - train loss: 3.0268 - test loss: 17.3588 - train acc: 0.8284 - test acc: 0.4085 - 16m 16s\n",
      "batch: 600/1563 - train loss: 3.0693 - test loss: 17.5331 - train acc: 0.8334 - test acc: 0.4133 - 16m 20s\n",
      "batch: 700/1563 - train loss: 3.1450 - test loss: 17.5276 - train acc: 0.8218 - test acc: 0.4124 - 16m 25s\n",
      "batch: 800/1563 - train loss: 3.2930 - test loss: 17.8083 - train acc: 0.8196 - test acc: 0.4178 - 16m 29s\n",
      "batch: 900/1563 - train loss: 3.4032 - test loss: 17.3856 - train acc: 0.8071 - test acc: 0.4133 - 16m 34s\n",
      "batch: 1000/1563 - train loss: 3.5244 - test loss: 17.7870 - train acc: 0.8063 - test acc: 0.4080 - 16m 39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 3.6603 - test loss: 17.5623 - train acc: 0.8028 - test acc: 0.4140 - 16m 44s\n",
      "batch: 1200/1563 - train loss: 3.6011 - test loss: 17.2507 - train acc: 0.8006 - test acc: 0.4112 - 16m 48s\n",
      "batch: 1300/1563 - train loss: 3.6789 - test loss: 17.3341 - train acc: 0.7994 - test acc: 0.4125 - 16m 53s\n",
      "batch: 1400/1563 - train loss: 3.9839 - test loss: 17.4518 - train acc: 0.7834 - test acc: 0.4110 - 16m 57s\n",
      "batch: 1500/1563 - train loss: 3.8676 - test loss: 17.2555 - train acc: 0.7809 - test acc: 0.4106 - 17m 2s\n",
      "batch: 1563/1563 - train loss: 3.9638 - test loss: 17.3308 - train acc: 0.7790 - test acc: 0.4176 - 17m 6s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.3660 - test loss: 17.9114 - train acc: 0.8669 - test acc: 0.4131 - 17m 11s\n",
      "batch: 200/1563 - train loss: 2.0983 - test loss: 17.5815 - train acc: 0.8797 - test acc: 0.4190 - 17m 16s\n",
      "batch: 300/1563 - train loss: 2.0893 - test loss: 17.4050 - train acc: 0.8863 - test acc: 0.4284 - 17m 20s\n",
      "batch: 400/1563 - train loss: 2.1360 - test loss: 18.0708 - train acc: 0.8838 - test acc: 0.4165 - 17m 25s\n",
      "batch: 500/1563 - train loss: 2.4895 - test loss: 18.4500 - train acc: 0.8515 - test acc: 0.4077 - 17m 30s\n",
      "batch: 600/1563 - train loss: 2.5259 - test loss: 17.7749 - train acc: 0.8568 - test acc: 0.4222 - 17m 34s\n",
      "batch: 700/1563 - train loss: 2.5082 - test loss: 18.4122 - train acc: 0.8559 - test acc: 0.4135 - 17m 39s\n",
      "batch: 800/1563 - train loss: 2.7753 - test loss: 17.9156 - train acc: 0.8431 - test acc: 0.4129 - 17m 44s\n",
      "batch: 900/1563 - train loss: 3.0356 - test loss: 18.3143 - train acc: 0.8281 - test acc: 0.4059 - 17m 48s\n",
      "batch: 1000/1563 - train loss: 3.0003 - test loss: 17.8267 - train acc: 0.8343 - test acc: 0.4200 - 17m 53s\n",
      "batch: 1100/1563 - train loss: 3.1852 - test loss: 20.3147 - train acc: 0.8202 - test acc: 0.3733 - 17m 57s\n",
      "batch: 1200/1563 - train loss: 3.1585 - test loss: 17.7810 - train acc: 0.8196 - test acc: 0.4160 - 18m 2s\n",
      "batch: 1300/1563 - train loss: 3.2254 - test loss: 18.4258 - train acc: 0.8184 - test acc: 0.3999 - 18m 7s\n",
      "batch: 1400/1563 - train loss: 3.2801 - test loss: 17.6598 - train acc: 0.8203 - test acc: 0.4151 - 18m 12s\n",
      "batch: 1500/1563 - train loss: 3.2274 - test loss: 18.1570 - train acc: 0.8147 - test acc: 0.4125 - 18m 16s\n",
      "batch: 1563/1563 - train loss: 3.3522 - test loss: 18.2124 - train acc: 0.8093 - test acc: 0.4117 - 18m 20s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.8764 - test loss: 17.8930 - train acc: 0.8988 - test acc: 0.4241 - 18m 25s\n",
      "batch: 200/1563 - train loss: 1.8128 - test loss: 17.7590 - train acc: 0.9038 - test acc: 0.4245 - 18m 29s\n",
      "batch: 300/1563 - train loss: 1.7703 - test loss: 18.3602 - train acc: 0.9048 - test acc: 0.4195 - 18m 34s\n",
      "batch: 400/1563 - train loss: 1.9601 - test loss: 18.4714 - train acc: 0.8866 - test acc: 0.4198 - 18m 39s\n",
      "batch: 500/1563 - train loss: 1.9098 - test loss: 18.2894 - train acc: 0.8926 - test acc: 0.4237 - 18m 44s\n",
      "batch: 600/1563 - train loss: 2.2171 - test loss: 18.7981 - train acc: 0.8794 - test acc: 0.4174 - 18m 48s\n",
      "batch: 700/1563 - train loss: 2.1954 - test loss: 19.0052 - train acc: 0.8728 - test acc: 0.4115 - 18m 53s\n",
      "batch: 800/1563 - train loss: 2.1576 - test loss: 18.4361 - train acc: 0.8781 - test acc: 0.4239 - 18m 57s\n",
      "batch: 900/1563 - train loss: 2.1706 - test loss: 18.6462 - train acc: 0.8744 - test acc: 0.4226 - 19m 2s\n",
      "batch: 1000/1563 - train loss: 2.4333 - test loss: 18.5210 - train acc: 0.8619 - test acc: 0.4253 - 19m 7s\n",
      "batch: 1100/1563 - train loss: 2.3115 - test loss: 18.7031 - train acc: 0.8760 - test acc: 0.4167 - 19m 11s\n",
      "batch: 1200/1563 - train loss: 2.5881 - test loss: 18.6717 - train acc: 0.8506 - test acc: 0.4251 - 19m 16s\n",
      "batch: 1300/1563 - train loss: 2.7832 - test loss: 19.2817 - train acc: 0.8418 - test acc: 0.4099 - 19m 21s\n",
      "batch: 1400/1563 - train loss: 2.8441 - test loss: 18.6316 - train acc: 0.8400 - test acc: 0.4126 - 19m 26s\n",
      "batch: 1500/1563 - train loss: 2.7279 - test loss: 18.6213 - train acc: 0.8412 - test acc: 0.4098 - 19m 30s\n",
      "batch: 1563/1563 - train loss: 2.9904 - test loss: 18.6009 - train acc: 0.8300 - test acc: 0.4140 - 19m 34s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 17/100\n",
      "batch: 100/1563 - train loss: 1.6646 - test loss: 18.7475 - train acc: 0.9023 - test acc: 0.4213 - 19m 39s\n",
      "batch: 200/1563 - train loss: 1.5519 - test loss: 18.2476 - train acc: 0.9167 - test acc: 0.4230 - 19m 44s\n",
      "batch: 300/1563 - train loss: 1.6995 - test loss: 19.7276 - train acc: 0.9080 - test acc: 0.3982 - 19m 48s\n",
      "batch: 400/1563 - train loss: 1.7395 - test loss: 18.9395 - train acc: 0.9035 - test acc: 0.4242 - 19m 53s\n",
      "batch: 500/1563 - train loss: 1.7914 - test loss: 18.5814 - train acc: 0.8995 - test acc: 0.4300 - 19m 58s\n",
      "time is up! finishing training\n",
      "batch: 501/1563 - train loss: 1.7709 - test loss: 18.5421 - train acc: 0.9004 - test acc: 0.4294 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 26.1043 - test loss: 25.0462 - train acc: 0.0376 - test acc: 0.0501 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.1697 - test loss: 23.4905 - train acc: 0.0548 - test acc: 0.0763 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.1543 - test loss: 23.0531 - train acc: 0.0814 - test acc: 0.0924 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.2736 - test loss: 22.5014 - train acc: 0.1030 - test acc: 0.1031 - 0m 15s\n",
      "batch: 500/1563 - train loss: 22.0056 - test loss: 21.3511 - train acc: 0.1122 - test acc: 0.1213 - 0m 20s\n",
      "batch: 600/1563 - train loss: 21.4246 - test loss: 21.1803 - train acc: 0.1219 - test acc: 0.1375 - 0m 24s\n",
      "batch: 700/1563 - train loss: 21.2382 - test loss: 20.3534 - train acc: 0.1285 - test acc: 0.1530 - 0m 29s\n",
      "batch: 800/1563 - train loss: 20.7872 - test loss: 20.4132 - train acc: 0.1387 - test acc: 0.1454 - 0m 34s\n",
      "batch: 900/1563 - train loss: 20.3607 - test loss: 20.5304 - train acc: 0.1544 - test acc: 0.1458 - 0m 38s\n",
      "batch: 1000/1563 - train loss: 20.1179 - test loss: 20.0371 - train acc: 0.1497 - test acc: 0.1568 - 0m 43s\n",
      "batch: 1100/1563 - train loss: 19.6077 - test loss: 20.2251 - train acc: 0.1700 - test acc: 0.1604 - 0m 48s\n",
      "batch: 1200/1563 - train loss: 19.4660 - test loss: 19.8070 - train acc: 0.1710 - test acc: 0.1677 - 0m 52s\n",
      "batch: 1300/1563 - train loss: 19.4325 - test loss: 21.6344 - train acc: 0.1829 - test acc: 0.1374 - 0m 57s\n",
      "batch: 1400/1563 - train loss: 19.1220 - test loss: 18.9841 - train acc: 0.1913 - test acc: 0.1844 - 1m 1s\n",
      "batch: 1500/1563 - train loss: 19.0521 - test loss: 19.2976 - train acc: 0.1947 - test acc: 0.1886 - 1m 6s\n",
      "batch: 1563/1563 - train loss: 18.9167 - test loss: 19.1662 - train acc: 0.2007 - test acc: 0.1883 - 1m 10s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 18.1235 - test loss: 19.1002 - train acc: 0.2135 - test acc: 0.2106 - 1m 15s\n",
      "batch: 200/1563 - train loss: 17.9514 - test loss: 18.1541 - train acc: 0.2291 - test acc: 0.2285 - 1m 19s\n",
      "batch: 300/1563 - train loss: 17.8356 - test loss: 19.1711 - train acc: 0.2282 - test acc: 0.1875 - 1m 24s\n",
      "batch: 400/1563 - train loss: 17.6838 - test loss: 17.7474 - train acc: 0.2306 - test acc: 0.2281 - 1m 29s\n",
      "batch: 500/1563 - train loss: 17.6878 - test loss: 18.1533 - train acc: 0.2244 - test acc: 0.2121 - 1m 33s\n",
      "batch: 600/1563 - train loss: 17.6422 - test loss: 17.9718 - train acc: 0.2344 - test acc: 0.2289 - 1m 38s\n",
      "batch: 700/1563 - train loss: 17.5066 - test loss: 17.3132 - train acc: 0.2332 - test acc: 0.2511 - 1m 42s\n",
      "batch: 800/1563 - train loss: 17.3326 - test loss: 16.9746 - train acc: 0.2420 - test acc: 0.2606 - 1m 47s\n",
      "batch: 900/1563 - train loss: 17.3290 - test loss: 16.8898 - train acc: 0.2397 - test acc: 0.2600 - 1m 52s\n",
      "batch: 1000/1563 - train loss: 16.9801 - test loss: 18.4412 - train acc: 0.2544 - test acc: 0.2139 - 1m 56s\n",
      "batch: 1100/1563 - train loss: 17.0641 - test loss: 16.9172 - train acc: 0.2606 - test acc: 0.2586 - 2m 1s\n",
      "batch: 1200/1563 - train loss: 16.7611 - test loss: 16.7876 - train acc: 0.2691 - test acc: 0.2616 - 2m 5s\n",
      "batch: 1300/1563 - train loss: 16.6811 - test loss: 16.2985 - train acc: 0.2657 - test acc: 0.2735 - 2m 10s\n",
      "batch: 1400/1563 - train loss: 16.6436 - test loss: 16.6391 - train acc: 0.2734 - test acc: 0.2714 - 2m 14s\n",
      "batch: 1500/1563 - train loss: 16.5775 - test loss: 17.2427 - train acc: 0.2762 - test acc: 0.2527 - 2m 19s\n",
      "batch: 1563/1563 - train loss: 16.0631 - test loss: 16.1478 - train acc: 0.2927 - test acc: 0.2849 - 2m 23s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.1413 - test loss: 16.4714 - train acc: 0.3131 - test acc: 0.2764 - 2m 28s\n",
      "batch: 200/1563 - train loss: 15.5209 - test loss: 15.9844 - train acc: 0.3047 - test acc: 0.2925 - 2m 32s\n",
      "batch: 300/1563 - train loss: 15.3282 - test loss: 16.2243 - train acc: 0.3046 - test acc: 0.2897 - 2m 37s\n",
      "batch: 400/1563 - train loss: 15.2332 - test loss: 18.1231 - train acc: 0.3078 - test acc: 0.2400 - 2m 41s\n",
      "batch: 500/1563 - train loss: 15.2117 - test loss: 16.2426 - train acc: 0.3081 - test acc: 0.2825 - 2m 46s\n",
      "batch: 600/1563 - train loss: 14.8793 - test loss: 15.9857 - train acc: 0.3302 - test acc: 0.2958 - 2m 51s\n",
      "batch: 700/1563 - train loss: 15.3175 - test loss: 16.8580 - train acc: 0.3112 - test acc: 0.2732 - 2m 55s\n",
      "batch: 800/1563 - train loss: 15.0011 - test loss: 15.8411 - train acc: 0.3262 - test acc: 0.3029 - 3m 0s\n",
      "batch: 900/1563 - train loss: 14.9921 - test loss: 16.6249 - train acc: 0.3274 - test acc: 0.2800 - 3m 4s\n",
      "batch: 1000/1563 - train loss: 15.0057 - test loss: 16.0274 - train acc: 0.3190 - test acc: 0.3002 - 3m 9s\n",
      "batch: 1100/1563 - train loss: 14.8937 - test loss: 15.4606 - train acc: 0.3172 - test acc: 0.3093 - 3m 13s\n",
      "batch: 1200/1563 - train loss: 14.6068 - test loss: 15.4680 - train acc: 0.3422 - test acc: 0.3104 - 3m 18s\n",
      "batch: 1300/1563 - train loss: 14.6715 - test loss: 15.3058 - train acc: 0.3306 - test acc: 0.3281 - 3m 22s\n",
      "batch: 1400/1563 - train loss: 14.7345 - test loss: 16.2566 - train acc: 0.3409 - test acc: 0.2859 - 3m 27s\n",
      "batch: 1500/1563 - train loss: 14.7271 - test loss: 14.9823 - train acc: 0.3259 - test acc: 0.3337 - 3m 32s\n",
      "batch: 1563/1563 - train loss: 14.6074 - test loss: 14.6683 - train acc: 0.3328 - test acc: 0.3353 - 3m 35s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.2269 - test loss: 15.0155 - train acc: 0.3772 - test acc: 0.3370 - 3m 40s\n",
      "batch: 200/1563 - train loss: 13.0302 - test loss: 14.8802 - train acc: 0.4053 - test acc: 0.3306 - 3m 45s\n",
      "batch: 300/1563 - train loss: 13.6019 - test loss: 14.8987 - train acc: 0.3709 - test acc: 0.3425 - 3m 49s\n",
      "batch: 400/1563 - train loss: 13.4560 - test loss: 14.8928 - train acc: 0.3781 - test acc: 0.3395 - 3m 54s\n",
      "batch: 500/1563 - train loss: 13.2375 - test loss: 14.8713 - train acc: 0.3853 - test acc: 0.3408 - 3m 58s\n",
      "batch: 600/1563 - train loss: 13.5092 - test loss: 16.7518 - train acc: 0.3702 - test acc: 0.2759 - 4m 3s\n",
      "batch: 700/1563 - train loss: 13.2140 - test loss: 15.8061 - train acc: 0.3749 - test acc: 0.3096 - 4m 7s\n",
      "batch: 800/1563 - train loss: 13.3861 - test loss: 14.3716 - train acc: 0.3844 - test acc: 0.3505 - 4m 12s\n",
      "batch: 900/1563 - train loss: 13.4686 - test loss: 14.9098 - train acc: 0.3768 - test acc: 0.3447 - 4m 17s\n",
      "batch: 1000/1563 - train loss: 13.5180 - test loss: 14.5902 - train acc: 0.3813 - test acc: 0.3543 - 4m 21s\n",
      "batch: 1100/1563 - train loss: 13.2283 - test loss: 15.5822 - train acc: 0.3947 - test acc: 0.3204 - 4m 26s\n",
      "batch: 1200/1563 - train loss: 13.2157 - test loss: 15.2100 - train acc: 0.3941 - test acc: 0.3342 - 4m 31s\n",
      "batch: 1300/1563 - train loss: 12.9798 - test loss: 15.5650 - train acc: 0.3888 - test acc: 0.3212 - 4m 35s\n",
      "batch: 1400/1563 - train loss: 13.1452 - test loss: 13.7263 - train acc: 0.3928 - test acc: 0.3762 - 4m 40s\n",
      "batch: 1500/1563 - train loss: 13.3482 - test loss: 16.3421 - train acc: 0.3765 - test acc: 0.2965 - 4m 45s\n",
      "batch: 1563/1563 - train loss: 13.0839 - test loss: 16.6062 - train acc: 0.3791 - test acc: 0.2883 - 4m 49s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.7923 - test loss: 14.0304 - train acc: 0.4391 - test acc: 0.3631 - 4m 53s\n",
      "batch: 200/1563 - train loss: 11.9492 - test loss: 15.4612 - train acc: 0.4290 - test acc: 0.3244 - 4m 58s\n",
      "batch: 300/1563 - train loss: 12.0471 - test loss: 14.0060 - train acc: 0.4281 - test acc: 0.3701 - 5m 3s\n",
      "batch: 400/1563 - train loss: 11.9193 - test loss: 14.3660 - train acc: 0.4287 - test acc: 0.3647 - 5m 7s\n",
      "batch: 500/1563 - train loss: 11.9679 - test loss: 14.9350 - train acc: 0.4338 - test acc: 0.3494 - 5m 12s\n",
      "batch: 600/1563 - train loss: 12.0941 - test loss: 14.6424 - train acc: 0.4410 - test acc: 0.3528 - 5m 16s\n",
      "batch: 700/1563 - train loss: 11.8335 - test loss: 14.1178 - train acc: 0.4460 - test acc: 0.3692 - 5m 21s\n",
      "batch: 800/1563 - train loss: 12.1016 - test loss: 14.2064 - train acc: 0.4335 - test acc: 0.3670 - 5m 26s\n",
      "batch: 900/1563 - train loss: 11.9549 - test loss: 15.1861 - train acc: 0.4388 - test acc: 0.3397 - 5m 31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 12.2155 - test loss: 14.6880 - train acc: 0.4241 - test acc: 0.3519 - 5m 35s\n",
      "batch: 1100/1563 - train loss: 11.7200 - test loss: 15.8874 - train acc: 0.4441 - test acc: 0.3116 - 5m 39s\n",
      "batch: 1200/1563 - train loss: 12.0340 - test loss: 14.0388 - train acc: 0.4323 - test acc: 0.3726 - 5m 44s\n",
      "batch: 1300/1563 - train loss: 11.9702 - test loss: 13.9820 - train acc: 0.4385 - test acc: 0.3744 - 5m 49s\n",
      "batch: 1400/1563 - train loss: 11.9935 - test loss: 14.0284 - train acc: 0.4375 - test acc: 0.3761 - 5m 54s\n",
      "batch: 1500/1563 - train loss: 11.8194 - test loss: 13.9264 - train acc: 0.4468 - test acc: 0.3796 - 5m 58s\n",
      "batch: 1563/1563 - train loss: 11.8401 - test loss: 13.8538 - train acc: 0.4481 - test acc: 0.3778 - 6m 2s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.0771 - test loss: 13.5474 - train acc: 0.5122 - test acc: 0.3996 - 6m 7s\n",
      "batch: 200/1563 - train loss: 10.4843 - test loss: 15.4654 - train acc: 0.4888 - test acc: 0.3377 - 6m 11s\n",
      "batch: 300/1563 - train loss: 10.1636 - test loss: 14.5617 - train acc: 0.5034 - test acc: 0.3561 - 6m 16s\n",
      "batch: 400/1563 - train loss: 10.5450 - test loss: 13.8402 - train acc: 0.4850 - test acc: 0.3826 - 6m 20s\n",
      "batch: 500/1563 - train loss: 10.8173 - test loss: 14.7291 - train acc: 0.4775 - test acc: 0.3618 - 6m 25s\n",
      "batch: 600/1563 - train loss: 10.8533 - test loss: 13.4576 - train acc: 0.4785 - test acc: 0.3979 - 6m 30s\n",
      "batch: 700/1563 - train loss: 10.5988 - test loss: 14.5909 - train acc: 0.4891 - test acc: 0.3619 - 6m 34s\n",
      "batch: 800/1563 - train loss: 10.8472 - test loss: 15.5949 - train acc: 0.4776 - test acc: 0.3376 - 6m 39s\n",
      "batch: 900/1563 - train loss: 10.8371 - test loss: 14.6792 - train acc: 0.4713 - test acc: 0.3625 - 6m 44s\n",
      "batch: 1000/1563 - train loss: 10.7023 - test loss: 13.5387 - train acc: 0.4916 - test acc: 0.3954 - 6m 48s\n",
      "batch: 1100/1563 - train loss: 10.8505 - test loss: 13.8534 - train acc: 0.4794 - test acc: 0.3877 - 6m 53s\n",
      "batch: 1200/1563 - train loss: 11.0017 - test loss: 13.9321 - train acc: 0.4628 - test acc: 0.3844 - 6m 58s\n",
      "batch: 1300/1563 - train loss: 10.8300 - test loss: 14.1456 - train acc: 0.4732 - test acc: 0.3826 - 7m 2s\n",
      "batch: 1400/1563 - train loss: 11.1624 - test loss: 15.4378 - train acc: 0.4766 - test acc: 0.3483 - 7m 7s\n",
      "batch: 1500/1563 - train loss: 10.8243 - test loss: 14.8490 - train acc: 0.4756 - test acc: 0.3586 - 7m 12s\n",
      "batch: 1563/1563 - train loss: 11.0308 - test loss: 13.3565 - train acc: 0.4812 - test acc: 0.4003 - 7m 16s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.6062 - test loss: 14.1514 - train acc: 0.5662 - test acc: 0.3862 - 7m 20s\n",
      "batch: 200/1563 - train loss: 8.9115 - test loss: 13.7805 - train acc: 0.5600 - test acc: 0.4022 - 7m 25s\n",
      "batch: 300/1563 - train loss: 9.2041 - test loss: 13.7436 - train acc: 0.5509 - test acc: 0.3986 - 7m 30s\n",
      "batch: 400/1563 - train loss: 9.1487 - test loss: 14.5929 - train acc: 0.5600 - test acc: 0.3721 - 7m 34s\n",
      "batch: 500/1563 - train loss: 9.3581 - test loss: 13.6252 - train acc: 0.5353 - test acc: 0.4016 - 7m 39s\n",
      "batch: 600/1563 - train loss: 9.2759 - test loss: 14.1429 - train acc: 0.5385 - test acc: 0.3853 - 7m 43s\n",
      "batch: 700/1563 - train loss: 9.5173 - test loss: 15.4532 - train acc: 0.5360 - test acc: 0.3493 - 7m 48s\n",
      "batch: 800/1563 - train loss: 9.7593 - test loss: 13.7840 - train acc: 0.5209 - test acc: 0.3938 - 7m 53s\n",
      "batch: 900/1563 - train loss: 9.7855 - test loss: 14.6060 - train acc: 0.5194 - test acc: 0.3762 - 7m 57s\n",
      "batch: 1000/1563 - train loss: 9.7594 - test loss: 14.5055 - train acc: 0.5225 - test acc: 0.3702 - 8m 2s\n",
      "batch: 1100/1563 - train loss: 9.8859 - test loss: 13.5063 - train acc: 0.5141 - test acc: 0.4067 - 8m 7s\n",
      "batch: 1200/1563 - train loss: 9.6027 - test loss: 13.8168 - train acc: 0.5206 - test acc: 0.3961 - 8m 11s\n",
      "batch: 1300/1563 - train loss: 9.7427 - test loss: 13.8364 - train acc: 0.5310 - test acc: 0.3952 - 8m 16s\n",
      "batch: 1400/1563 - train loss: 9.9275 - test loss: 12.9633 - train acc: 0.5169 - test acc: 0.4214 - 8m 20s\n",
      "batch: 1500/1563 - train loss: 9.9913 - test loss: 13.3215 - train acc: 0.5241 - test acc: 0.4092 - 8m 25s\n",
      "batch: 1563/1563 - train loss: 9.7803 - test loss: 14.7034 - train acc: 0.5316 - test acc: 0.3769 - 8m 29s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 8/100\n",
      "batch: 100/1563 - train loss: 7.8241 - test loss: 14.0717 - train acc: 0.6156 - test acc: 0.4036 - 8m 34s\n",
      "batch: 200/1563 - train loss: 7.8135 - test loss: 16.2406 - train acc: 0.6062 - test acc: 0.3531 - 8m 38s\n",
      "batch: 300/1563 - train loss: 7.4970 - test loss: 14.3687 - train acc: 0.6178 - test acc: 0.3881 - 8m 43s\n",
      "batch: 400/1563 - train loss: 8.2997 - test loss: 14.3436 - train acc: 0.5831 - test acc: 0.3887 - 8m 48s\n",
      "batch: 500/1563 - train loss: 8.2038 - test loss: 14.0395 - train acc: 0.5915 - test acc: 0.4010 - 8m 52s\n",
      "batch: 600/1563 - train loss: 8.2750 - test loss: 14.1956 - train acc: 0.5890 - test acc: 0.3997 - 8m 57s\n",
      "batch: 700/1563 - train loss: 8.4666 - test loss: 13.6394 - train acc: 0.5762 - test acc: 0.4122 - 9m 2s\n",
      "batch: 800/1563 - train loss: 8.2183 - test loss: 13.7970 - train acc: 0.5859 - test acc: 0.4088 - 9m 6s\n",
      "batch: 900/1563 - train loss: 8.5562 - test loss: 14.5582 - train acc: 0.5715 - test acc: 0.3874 - 9m 11s\n",
      "batch: 1000/1563 - train loss: 8.5796 - test loss: 14.0919 - train acc: 0.5716 - test acc: 0.4024 - 9m 15s\n",
      "batch: 1100/1563 - train loss: 8.7817 - test loss: 14.0859 - train acc: 0.5668 - test acc: 0.4020 - 9m 20s\n",
      "batch: 1200/1563 - train loss: 8.9917 - test loss: 14.8501 - train acc: 0.5475 - test acc: 0.3828 - 9m 25s\n",
      "batch: 1300/1563 - train loss: 8.8613 - test loss: 14.5346 - train acc: 0.5606 - test acc: 0.3885 - 9m 29s\n",
      "batch: 1400/1563 - train loss: 8.8785 - test loss: 13.6151 - train acc: 0.5528 - test acc: 0.4099 - 9m 34s\n",
      "batch: 1500/1563 - train loss: 8.7606 - test loss: 13.8509 - train acc: 0.5521 - test acc: 0.4062 - 9m 39s\n",
      "batch: 1563/1563 - train loss: 8.8756 - test loss: 13.8407 - train acc: 0.5550 - test acc: 0.4021 - 9m 43s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 9/100\n",
      "batch: 100/1563 - train loss: 6.2814 - test loss: 13.7965 - train acc: 0.6740 - test acc: 0.4219 - 9m 47s\n",
      "batch: 200/1563 - train loss: 6.5469 - test loss: 14.2158 - train acc: 0.6656 - test acc: 0.4138 - 9m 52s\n",
      "batch: 300/1563 - train loss: 6.5218 - test loss: 15.6517 - train acc: 0.6638 - test acc: 0.3768 - 9m 57s\n",
      "batch: 400/1563 - train loss: 7.2509 - test loss: 14.6799 - train acc: 0.6147 - test acc: 0.4005 - 10m 2s\n",
      "batch: 500/1563 - train loss: 7.2700 - test loss: 15.0030 - train acc: 0.6316 - test acc: 0.3926 - 10m 7s\n",
      "batch: 600/1563 - train loss: 7.3076 - test loss: 14.4042 - train acc: 0.6288 - test acc: 0.4066 - 10m 12s\n",
      "batch: 700/1563 - train loss: 7.3861 - test loss: 15.2473 - train acc: 0.6200 - test acc: 0.3856 - 10m 17s\n",
      "batch: 800/1563 - train loss: 7.6760 - test loss: 14.4028 - train acc: 0.6157 - test acc: 0.4084 - 10m 21s\n",
      "batch: 900/1563 - train loss: 7.6503 - test loss: 13.7211 - train acc: 0.6100 - test acc: 0.4263 - 10m 26s\n",
      "batch: 1000/1563 - train loss: 7.3270 - test loss: 14.2295 - train acc: 0.6194 - test acc: 0.4109 - 10m 30s\n",
      "batch: 1100/1563 - train loss: 7.5428 - test loss: 14.6111 - train acc: 0.6238 - test acc: 0.4035 - 10m 35s\n",
      "batch: 1200/1563 - train loss: 7.7040 - test loss: 14.1164 - train acc: 0.6110 - test acc: 0.4148 - 10m 40s\n",
      "batch: 1300/1563 - train loss: 7.8250 - test loss: 13.8363 - train acc: 0.6065 - test acc: 0.4144 - 10m 45s\n",
      "batch: 1400/1563 - train loss: 7.6312 - test loss: 13.9382 - train acc: 0.6019 - test acc: 0.4114 - 10m 50s\n",
      "batch: 1500/1563 - train loss: 7.8233 - test loss: 13.8504 - train acc: 0.5956 - test acc: 0.4185 - 10m 54s\n",
      "batch: 1563/1563 - train loss: 7.9093 - test loss: 14.0573 - train acc: 0.5928 - test acc: 0.4142 - 10m 59s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 10/100\n",
      "batch: 100/1563 - train loss: 5.1988 - test loss: 14.6090 - train acc: 0.7359 - test acc: 0.4136 - 11m 3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 200/1563 - train loss: 5.2725 - test loss: 15.3072 - train acc: 0.7203 - test acc: 0.3990 - 11m 8s\n",
      "batch: 300/1563 - train loss: 5.7252 - test loss: 14.6290 - train acc: 0.7035 - test acc: 0.4201 - 11m 13s\n",
      "batch: 400/1563 - train loss: 5.8659 - test loss: 14.5531 - train acc: 0.6885 - test acc: 0.4136 - 11m 18s\n",
      "batch: 500/1563 - train loss: 5.9927 - test loss: 15.1896 - train acc: 0.6735 - test acc: 0.4071 - 11m 22s\n",
      "batch: 600/1563 - train loss: 6.1284 - test loss: 16.0756 - train acc: 0.6779 - test acc: 0.3814 - 11m 27s\n",
      "batch: 700/1563 - train loss: 5.9910 - test loss: 15.3739 - train acc: 0.6797 - test acc: 0.3982 - 11m 32s\n",
      "batch: 800/1563 - train loss: 6.3661 - test loss: 14.9582 - train acc: 0.6591 - test acc: 0.4166 - 11m 36s\n",
      "batch: 900/1563 - train loss: 6.4443 - test loss: 14.6246 - train acc: 0.6669 - test acc: 0.4149 - 11m 41s\n",
      "batch: 1000/1563 - train loss: 6.4627 - test loss: 14.9518 - train acc: 0.6616 - test acc: 0.4064 - 11m 46s\n",
      "batch: 1100/1563 - train loss: 6.9864 - test loss: 14.8915 - train acc: 0.6354 - test acc: 0.4047 - 11m 50s\n",
      "batch: 1200/1563 - train loss: 6.8853 - test loss: 14.7780 - train acc: 0.6444 - test acc: 0.4084 - 11m 55s\n",
      "batch: 1300/1563 - train loss: 6.7731 - test loss: 14.4271 - train acc: 0.6479 - test acc: 0.4184 - 12m 0s\n",
      "batch: 1400/1563 - train loss: 7.0359 - test loss: 14.6274 - train acc: 0.6312 - test acc: 0.4144 - 12m 4s\n",
      "batch: 1500/1563 - train loss: 7.1312 - test loss: 14.5158 - train acc: 0.6388 - test acc: 0.4189 - 12m 9s\n",
      "batch: 1563/1563 - train loss: 7.0488 - test loss: 14.3382 - train acc: 0.6360 - test acc: 0.4137 - 12m 13s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 11/100\n",
      "batch: 100/1563 - train loss: 4.4863 - test loss: 14.5173 - train acc: 0.7597 - test acc: 0.4295 - 12m 18s\n",
      "batch: 200/1563 - train loss: 4.6115 - test loss: 15.1105 - train acc: 0.7522 - test acc: 0.4187 - 12m 22s\n",
      "batch: 300/1563 - train loss: 4.5869 - test loss: 16.0116 - train acc: 0.7549 - test acc: 0.4021 - 12m 27s\n",
      "batch: 400/1563 - train loss: 4.6020 - test loss: 15.0239 - train acc: 0.7568 - test acc: 0.4212 - 12m 32s\n",
      "batch: 500/1563 - train loss: 5.0282 - test loss: 15.3191 - train acc: 0.7394 - test acc: 0.4099 - 12m 36s\n",
      "batch: 600/1563 - train loss: 5.2839 - test loss: 16.3870 - train acc: 0.7173 - test acc: 0.3924 - 12m 41s\n",
      "batch: 700/1563 - train loss: 5.3520 - test loss: 15.2861 - train acc: 0.7212 - test acc: 0.4129 - 12m 46s\n",
      "batch: 800/1563 - train loss: 5.5859 - test loss: 15.7190 - train acc: 0.7050 - test acc: 0.4058 - 12m 51s\n",
      "batch: 900/1563 - train loss: 5.5642 - test loss: 15.4522 - train acc: 0.7035 - test acc: 0.4136 - 12m 55s\n",
      "batch: 1000/1563 - train loss: 5.5113 - test loss: 15.5001 - train acc: 0.7203 - test acc: 0.4069 - 13m 0s\n",
      "batch: 1100/1563 - train loss: 5.4620 - test loss: 15.7155 - train acc: 0.7025 - test acc: 0.4080 - 13m 5s\n",
      "batch: 1200/1563 - train loss: 5.9005 - test loss: 15.4642 - train acc: 0.6879 - test acc: 0.4039 - 13m 9s\n",
      "batch: 1300/1563 - train loss: 5.9532 - test loss: 14.8043 - train acc: 0.6801 - test acc: 0.4189 - 13m 14s\n",
      "batch: 1400/1563 - train loss: 6.3136 - test loss: 15.3035 - train acc: 0.6648 - test acc: 0.4126 - 13m 19s\n",
      "batch: 1500/1563 - train loss: 6.3342 - test loss: 14.7623 - train acc: 0.6663 - test acc: 0.4217 - 13m 24s\n",
      "batch: 1563/1563 - train loss: 6.1909 - test loss: 15.5391 - train acc: 0.6781 - test acc: 0.4113 - 13m 28s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 12/100\n",
      "batch: 100/1563 - train loss: 3.7141 - test loss: 15.5305 - train acc: 0.7965 - test acc: 0.4165 - 13m 32s\n",
      "batch: 200/1563 - train loss: 3.5077 - test loss: 15.1121 - train acc: 0.8243 - test acc: 0.4292 - 13m 37s\n",
      "batch: 300/1563 - train loss: 3.5494 - test loss: 15.5969 - train acc: 0.8061 - test acc: 0.4190 - 13m 41s\n",
      "batch: 400/1563 - train loss: 3.6945 - test loss: 16.0225 - train acc: 0.7999 - test acc: 0.4118 - 13m 46s\n",
      "batch: 500/1563 - train loss: 3.9876 - test loss: 16.2014 - train acc: 0.7878 - test acc: 0.4075 - 13m 51s\n",
      "batch: 600/1563 - train loss: 4.1590 - test loss: 15.8930 - train acc: 0.7774 - test acc: 0.4208 - 13m 56s\n",
      "batch: 700/1563 - train loss: 4.2542 - test loss: 16.6516 - train acc: 0.7621 - test acc: 0.4061 - 14m 0s\n",
      "batch: 800/1563 - train loss: 4.6912 - test loss: 16.4255 - train acc: 0.7431 - test acc: 0.4026 - 14m 5s\n",
      "batch: 900/1563 - train loss: 4.8804 - test loss: 16.5023 - train acc: 0.7331 - test acc: 0.4082 - 14m 10s\n",
      "batch: 1000/1563 - train loss: 4.9986 - test loss: 16.8171 - train acc: 0.7219 - test acc: 0.3967 - 14m 14s\n",
      "batch: 1100/1563 - train loss: 4.7349 - test loss: 15.8539 - train acc: 0.7366 - test acc: 0.4184 - 14m 19s\n",
      "batch: 1200/1563 - train loss: 4.7809 - test loss: 16.1484 - train acc: 0.7416 - test acc: 0.4051 - 14m 24s\n",
      "batch: 1300/1563 - train loss: 5.3365 - test loss: 16.1438 - train acc: 0.7182 - test acc: 0.4062 - 14m 28s\n",
      "batch: 1400/1563 - train loss: 5.2529 - test loss: 16.2220 - train acc: 0.7234 - test acc: 0.4035 - 14m 33s\n",
      "batch: 1500/1563 - train loss: 5.3710 - test loss: 15.8420 - train acc: 0.7110 - test acc: 0.4090 - 14m 38s\n",
      "batch: 1563/1563 - train loss: 5.2789 - test loss: 15.4736 - train acc: 0.7231 - test acc: 0.4200 - 14m 42s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 13/100\n",
      "batch: 100/1563 - train loss: 3.0265 - test loss: 15.7961 - train acc: 0.8359 - test acc: 0.4181 - 14m 46s\n",
      "batch: 200/1563 - train loss: 3.0257 - test loss: 16.3497 - train acc: 0.8341 - test acc: 0.4176 - 14m 52s\n",
      "batch: 300/1563 - train loss: 3.0897 - test loss: 16.2298 - train acc: 0.8319 - test acc: 0.4299 - 14m 56s\n",
      "batch: 400/1563 - train loss: 3.2349 - test loss: 16.8802 - train acc: 0.8180 - test acc: 0.4105 - 15m 1s\n",
      "batch: 500/1563 - train loss: 3.5355 - test loss: 16.4108 - train acc: 0.8097 - test acc: 0.4255 - 15m 6s\n",
      "batch: 600/1563 - train loss: 3.5339 - test loss: 16.2537 - train acc: 0.8074 - test acc: 0.4308 - 15m 10s\n",
      "batch: 700/1563 - train loss: 3.8783 - test loss: 16.5414 - train acc: 0.7862 - test acc: 0.4187 - 15m 15s\n",
      "batch: 800/1563 - train loss: 3.8874 - test loss: 16.6424 - train acc: 0.7915 - test acc: 0.4121 - 15m 20s\n",
      "batch: 900/1563 - train loss: 3.8762 - test loss: 16.4989 - train acc: 0.7894 - test acc: 0.4186 - 15m 25s\n",
      "batch: 1000/1563 - train loss: 4.0262 - test loss: 17.4151 - train acc: 0.7884 - test acc: 0.3923 - 15m 30s\n",
      "batch: 1100/1563 - train loss: 4.0733 - test loss: 16.2472 - train acc: 0.7808 - test acc: 0.4159 - 15m 35s\n",
      "batch: 1200/1563 - train loss: 4.1038 - test loss: 16.5747 - train acc: 0.7756 - test acc: 0.4173 - 15m 39s\n",
      "batch: 1300/1563 - train loss: 4.4172 - test loss: 16.5098 - train acc: 0.7547 - test acc: 0.4189 - 15m 44s\n",
      "batch: 1400/1563 - train loss: 4.2797 - test loss: 16.1755 - train acc: 0.7647 - test acc: 0.4227 - 15m 49s\n",
      "batch: 1500/1563 - train loss: 4.5140 - test loss: 16.5005 - train acc: 0.7543 - test acc: 0.4177 - 15m 53s\n",
      "batch: 1563/1563 - train loss: 4.4709 - test loss: 16.3726 - train acc: 0.7562 - test acc: 0.4140 - 15m 58s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 14/100\n",
      "batch: 100/1563 - train loss: 2.5034 - test loss: 15.9587 - train acc: 0.8603 - test acc: 0.4258 - 16m 3s\n",
      "batch: 200/1563 - train loss: 2.5741 - test loss: 16.8561 - train acc: 0.8519 - test acc: 0.4256 - 16m 7s\n",
      "batch: 300/1563 - train loss: 2.4480 - test loss: 16.8400 - train acc: 0.8641 - test acc: 0.4208 - 16m 12s\n",
      "batch: 400/1563 - train loss: 2.5541 - test loss: 17.7305 - train acc: 0.8601 - test acc: 0.4158 - 16m 16s\n",
      "batch: 500/1563 - train loss: 2.7249 - test loss: 17.1648 - train acc: 0.8512 - test acc: 0.4246 - 16m 21s\n",
      "batch: 600/1563 - train loss: 2.7172 - test loss: 17.9679 - train acc: 0.8491 - test acc: 0.4091 - 16m 26s\n",
      "batch: 700/1563 - train loss: 2.8441 - test loss: 17.1867 - train acc: 0.8409 - test acc: 0.4293 - 16m 31s\n",
      "batch: 800/1563 - train loss: 3.2151 - test loss: 17.3201 - train acc: 0.8246 - test acc: 0.4200 - 16m 35s\n",
      "batch: 900/1563 - train loss: 2.8559 - test loss: 17.6293 - train acc: 0.8353 - test acc: 0.4144 - 16m 40s\n",
      "batch: 1000/1563 - train loss: 3.0665 - test loss: 17.9144 - train acc: 0.8359 - test acc: 0.4069 - 16m 45s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1100/1563 - train loss: 3.3705 - test loss: 17.4778 - train acc: 0.8130 - test acc: 0.4207 - 16m 50s\n",
      "batch: 1200/1563 - train loss: 3.7729 - test loss: 18.1239 - train acc: 0.7875 - test acc: 0.4077 - 16m 54s\n",
      "batch: 1300/1563 - train loss: 3.5985 - test loss: 17.1152 - train acc: 0.7956 - test acc: 0.4163 - 16m 59s\n",
      "batch: 1400/1563 - train loss: 3.6752 - test loss: 17.1755 - train acc: 0.7890 - test acc: 0.4188 - 17m 4s\n",
      "batch: 1500/1563 - train loss: 3.8501 - test loss: 17.6786 - train acc: 0.7890 - test acc: 0.4046 - 17m 9s\n",
      "batch: 1563/1563 - train loss: 3.9987 - test loss: 17.6662 - train acc: 0.7713 - test acc: 0.4165 - 17m 13s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 15/100\n",
      "batch: 100/1563 - train loss: 2.3083 - test loss: 17.4111 - train acc: 0.8731 - test acc: 0.4234 - 17m 17s\n",
      "batch: 200/1563 - train loss: 2.1467 - test loss: 17.3637 - train acc: 0.8788 - test acc: 0.4211 - 17m 22s\n",
      "batch: 300/1563 - train loss: 2.1996 - test loss: 16.9654 - train acc: 0.8750 - test acc: 0.4287 - 17m 27s\n",
      "batch: 400/1563 - train loss: 2.1989 - test loss: 17.5456 - train acc: 0.8747 - test acc: 0.4211 - 17m 32s\n",
      "batch: 500/1563 - train loss: 2.3345 - test loss: 17.4801 - train acc: 0.8637 - test acc: 0.4310 - 17m 37s\n",
      "batch: 600/1563 - train loss: 2.4313 - test loss: 17.7382 - train acc: 0.8643 - test acc: 0.4195 - 17m 41s\n",
      "batch: 700/1563 - train loss: 2.6528 - test loss: 17.4674 - train acc: 0.8565 - test acc: 0.4183 - 17m 46s\n",
      "batch: 800/1563 - train loss: 2.3999 - test loss: 17.7402 - train acc: 0.8612 - test acc: 0.4263 - 17m 50s\n",
      "batch: 900/1563 - train loss: 2.6119 - test loss: 17.9996 - train acc: 0.8559 - test acc: 0.4231 - 17m 55s\n",
      "batch: 1000/1563 - train loss: 2.8648 - test loss: 17.9907 - train acc: 0.8365 - test acc: 0.4100 - 18m 0s\n",
      "batch: 1100/1563 - train loss: 2.9429 - test loss: 17.8573 - train acc: 0.8346 - test acc: 0.4175 - 18m 5s\n",
      "batch: 1200/1563 - train loss: 3.0416 - test loss: 18.2841 - train acc: 0.8268 - test acc: 0.4097 - 18m 10s\n",
      "batch: 1300/1563 - train loss: 2.9091 - test loss: 17.5200 - train acc: 0.8346 - test acc: 0.4215 - 18m 14s\n",
      "batch: 1400/1563 - train loss: 2.9970 - test loss: 17.7339 - train acc: 0.8343 - test acc: 0.4180 - 18m 18s\n",
      "batch: 1500/1563 - train loss: 3.2197 - test loss: 17.7409 - train acc: 0.8209 - test acc: 0.4217 - 18m 23s\n",
      "batch: 1563/1563 - train loss: 3.1916 - test loss: 17.3949 - train acc: 0.8237 - test acc: 0.4196 - 18m 27s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 16/100\n",
      "batch: 100/1563 - train loss: 1.8796 - test loss: 17.7380 - train acc: 0.8994 - test acc: 0.4287 - 18m 32s\n",
      "batch: 200/1563 - train loss: 1.7949 - test loss: 18.2203 - train acc: 0.8991 - test acc: 0.4246 - 18m 37s\n",
      "batch: 300/1563 - train loss: 1.8262 - test loss: 17.8358 - train acc: 0.8982 - test acc: 0.4253 - 18m 42s\n",
      "batch: 400/1563 - train loss: 1.6985 - test loss: 18.6618 - train acc: 0.9054 - test acc: 0.4132 - 18m 46s\n",
      "batch: 500/1563 - train loss: 1.7694 - test loss: 18.1922 - train acc: 0.8967 - test acc: 0.4256 - 18m 51s\n",
      "batch: 600/1563 - train loss: 1.9317 - test loss: 18.3527 - train acc: 0.8906 - test acc: 0.4245 - 18m 56s\n",
      "batch: 700/1563 - train loss: 2.0735 - test loss: 18.8696 - train acc: 0.8866 - test acc: 0.4194 - 19m 0s\n",
      "batch: 800/1563 - train loss: 2.0727 - test loss: 18.6147 - train acc: 0.8803 - test acc: 0.4161 - 19m 5s\n",
      "batch: 900/1563 - train loss: 2.1258 - test loss: 18.5683 - train acc: 0.8759 - test acc: 0.4205 - 19m 10s\n",
      "batch: 1000/1563 - train loss: 2.2996 - test loss: 18.3327 - train acc: 0.8675 - test acc: 0.4268 - 19m 15s\n",
      "batch: 1100/1563 - train loss: 2.3348 - test loss: 19.3528 - train acc: 0.8641 - test acc: 0.4114 - 19m 19s\n",
      "batch: 1200/1563 - train loss: 2.4610 - test loss: 18.6419 - train acc: 0.8622 - test acc: 0.4166 - 19m 24s\n",
      "batch: 1300/1563 - train loss: 2.5330 - test loss: 18.5683 - train acc: 0.8588 - test acc: 0.4294 - 19m 29s\n",
      "batch: 1400/1563 - train loss: 2.5358 - test loss: 18.5919 - train acc: 0.8531 - test acc: 0.4177 - 19m 33s\n",
      "batch: 1500/1563 - train loss: 2.6694 - test loss: 18.5888 - train acc: 0.8494 - test acc: 0.4165 - 19m 39s\n",
      "batch: 1563/1563 - train loss: 2.8305 - test loss: 18.3974 - train acc: 0.8391 - test acc: 0.4147 - 19m 42s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 17/100\n",
      "batch: 100/1563 - train loss: 1.5787 - test loss: 18.7462 - train acc: 0.9126 - test acc: 0.4194 - 19m 47s\n",
      "batch: 200/1563 - train loss: 1.5340 - test loss: 18.2645 - train acc: 0.9116 - test acc: 0.4296 - 19m 52s\n",
      "batch: 300/1563 - train loss: 1.4596 - test loss: 18.1710 - train acc: 0.9142 - test acc: 0.4258 - 19m 57s\n",
      "time is up! finishing training\n",
      "batch: 354/1563 - train loss: 1.6170 - test loss: 18.7426 - train acc: 0.9045 - test acc: 0.4256 - 20m 1s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "\n",
      "generating CIFAR100 data with 100 classes\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n",
      "starting epoch: 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 100/1563 - train loss: 26.1293 - test loss: 25.0812 - train acc: 0.0445 - test acc: 0.0566 - 0m 1s\n",
      "batch: 200/1563 - train loss: 24.1026 - test loss: 23.2340 - train acc: 0.0614 - test acc: 0.0750 - 0m 6s\n",
      "batch: 300/1563 - train loss: 23.1988 - test loss: 23.9935 - train acc: 0.0774 - test acc: 0.0830 - 0m 11s\n",
      "batch: 400/1563 - train loss: 22.2593 - test loss: 21.6409 - train acc: 0.1049 - test acc: 0.1124 - 0m 15s\n",
      "batch: 500/1563 - train loss: 21.8189 - test loss: 21.2069 - train acc: 0.1109 - test acc: 0.1310 - 0m 20s\n",
      "batch: 600/1563 - train loss: 21.3838 - test loss: 21.0962 - train acc: 0.1231 - test acc: 0.1305 - 0m 24s\n",
      "batch: 700/1563 - train loss: 20.9898 - test loss: 20.5852 - train acc: 0.1340 - test acc: 0.1405 - 0m 29s\n",
      "batch: 800/1563 - train loss: 20.7685 - test loss: 20.3266 - train acc: 0.1419 - test acc: 0.1475 - 0m 34s\n",
      "batch: 900/1563 - train loss: 20.2141 - test loss: 20.8720 - train acc: 0.1522 - test acc: 0.1429 - 0m 39s\n",
      "batch: 1000/1563 - train loss: 20.0835 - test loss: 19.2495 - train acc: 0.1663 - test acc: 0.1855 - 0m 43s\n",
      "batch: 1100/1563 - train loss: 19.4163 - test loss: 19.7231 - train acc: 0.1732 - test acc: 0.1756 - 0m 48s\n",
      "batch: 1200/1563 - train loss: 19.8545 - test loss: 20.3267 - train acc: 0.1760 - test acc: 0.1536 - 0m 53s\n",
      "batch: 1300/1563 - train loss: 19.3707 - test loss: 19.7959 - train acc: 0.1750 - test acc: 0.1644 - 0m 57s\n",
      "batch: 1400/1563 - train loss: 18.8811 - test loss: 19.1690 - train acc: 0.2010 - test acc: 0.1901 - 1m 2s\n",
      "batch: 1500/1563 - train loss: 18.7822 - test loss: 18.6805 - train acc: 0.2022 - test acc: 0.1991 - 1m 7s\n",
      "batch: 1563/1563 - train loss: 18.7944 - test loss: 18.3605 - train acc: 0.2035 - test acc: 0.2141 - 1m 11s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 2/100\n",
      "batch: 100/1563 - train loss: 17.8479 - test loss: 18.7393 - train acc: 0.2266 - test acc: 0.2036 - 1m 15s\n",
      "batch: 200/1563 - train loss: 17.7826 - test loss: 18.8635 - train acc: 0.2403 - test acc: 0.2034 - 1m 20s\n",
      "batch: 300/1563 - train loss: 17.8606 - test loss: 18.2096 - train acc: 0.2257 - test acc: 0.2212 - 1m 25s\n",
      "batch: 400/1563 - train loss: 17.6065 - test loss: 19.4686 - train acc: 0.2316 - test acc: 0.1865 - 1m 29s\n",
      "batch: 500/1563 - train loss: 17.5691 - test loss: 17.8496 - train acc: 0.2347 - test acc: 0.2310 - 1m 34s\n",
      "batch: 600/1563 - train loss: 17.2813 - test loss: 17.9530 - train acc: 0.2494 - test acc: 0.2350 - 1m 39s\n",
      "batch: 700/1563 - train loss: 17.3433 - test loss: 17.4721 - train acc: 0.2504 - test acc: 0.2403 - 1m 43s\n",
      "batch: 800/1563 - train loss: 17.3476 - test loss: 16.9139 - train acc: 0.2351 - test acc: 0.2687 - 1m 48s\n",
      "batch: 900/1563 - train loss: 16.8736 - test loss: 18.0073 - train acc: 0.2569 - test acc: 0.2293 - 1m 52s\n",
      "batch: 1000/1563 - train loss: 16.9767 - test loss: 16.9125 - train acc: 0.2616 - test acc: 0.2638 - 1m 57s\n",
      "batch: 1100/1563 - train loss: 16.9473 - test loss: 16.4817 - train acc: 0.2587 - test acc: 0.2711 - 2m 2s\n",
      "batch: 1200/1563 - train loss: 16.7599 - test loss: 17.1398 - train acc: 0.2663 - test acc: 0.2581 - 2m 7s\n",
      "batch: 1300/1563 - train loss: 16.7024 - test loss: 17.0112 - train acc: 0.2691 - test acc: 0.2621 - 2m 11s\n",
      "batch: 1400/1563 - train loss: 16.7652 - test loss: 16.3051 - train acc: 0.2700 - test acc: 0.2823 - 2m 16s\n",
      "batch: 1500/1563 - train loss: 16.5135 - test loss: 16.4408 - train acc: 0.2768 - test acc: 0.2740 - 2m 21s\n",
      "batch: 1563/1563 - train loss: 16.3900 - test loss: 16.0626 - train acc: 0.2860 - test acc: 0.2899 - 2m 25s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 3/100\n",
      "batch: 100/1563 - train loss: 15.2010 - test loss: 15.9236 - train acc: 0.3141 - test acc: 0.2968 - 2m 29s\n",
      "batch: 200/1563 - train loss: 15.3912 - test loss: 16.1237 - train acc: 0.3115 - test acc: 0.2929 - 2m 34s\n",
      "batch: 300/1563 - train loss: 15.3888 - test loss: 16.7982 - train acc: 0.3013 - test acc: 0.2714 - 2m 39s\n",
      "batch: 400/1563 - train loss: 15.1376 - test loss: 16.4518 - train acc: 0.3181 - test acc: 0.2788 - 2m 44s\n",
      "batch: 500/1563 - train loss: 15.0487 - test loss: 16.5786 - train acc: 0.3235 - test acc: 0.2776 - 2m 48s\n",
      "batch: 600/1563 - train loss: 15.3223 - test loss: 16.0519 - train acc: 0.3212 - test acc: 0.2919 - 2m 53s\n",
      "batch: 700/1563 - train loss: 15.2914 - test loss: 15.3568 - train acc: 0.3175 - test acc: 0.3191 - 2m 57s\n",
      "batch: 800/1563 - train loss: 15.1521 - test loss: 16.0801 - train acc: 0.3197 - test acc: 0.2973 - 3m 2s\n",
      "batch: 900/1563 - train loss: 14.9015 - test loss: 15.3372 - train acc: 0.3253 - test acc: 0.3157 - 3m 7s\n",
      "batch: 1000/1563 - train loss: 15.2061 - test loss: 15.3260 - train acc: 0.3225 - test acc: 0.3165 - 3m 11s\n",
      "batch: 1100/1563 - train loss: 15.0929 - test loss: 15.3460 - train acc: 0.3225 - test acc: 0.3121 - 3m 16s\n",
      "batch: 1200/1563 - train loss: 14.9464 - test loss: 15.7101 - train acc: 0.3199 - test acc: 0.3068 - 3m 21s\n",
      "batch: 1300/1563 - train loss: 14.5148 - test loss: 15.0669 - train acc: 0.3430 - test acc: 0.3321 - 3m 25s\n",
      "batch: 1400/1563 - train loss: 14.6092 - test loss: 15.1283 - train acc: 0.3449 - test acc: 0.3245 - 3m 30s\n",
      "batch: 1500/1563 - train loss: 14.3763 - test loss: 14.9709 - train acc: 0.3525 - test acc: 0.3310 - 3m 34s\n",
      "batch: 1563/1563 - train loss: 14.3641 - test loss: 14.9608 - train acc: 0.3487 - test acc: 0.3370 - 3m 39s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 4/100\n",
      "batch: 100/1563 - train loss: 13.1317 - test loss: 15.1687 - train acc: 0.3884 - test acc: 0.3411 - 3m 43s\n",
      "batch: 200/1563 - train loss: 13.1611 - test loss: 15.7330 - train acc: 0.4000 - test acc: 0.3089 - 3m 48s\n",
      "batch: 300/1563 - train loss: 13.5587 - test loss: 14.6631 - train acc: 0.3644 - test acc: 0.3481 - 3m 52s\n",
      "batch: 400/1563 - train loss: 13.7198 - test loss: 15.0706 - train acc: 0.3671 - test acc: 0.3339 - 3m 57s\n",
      "batch: 500/1563 - train loss: 13.6782 - test loss: 15.5658 - train acc: 0.3628 - test acc: 0.3191 - 4m 1s\n",
      "batch: 600/1563 - train loss: 13.3103 - test loss: 14.8573 - train acc: 0.3862 - test acc: 0.3398 - 4m 6s\n",
      "batch: 700/1563 - train loss: 13.7957 - test loss: 15.4070 - train acc: 0.3593 - test acc: 0.3266 - 4m 11s\n",
      "batch: 800/1563 - train loss: 13.2891 - test loss: 14.8421 - train acc: 0.3928 - test acc: 0.3368 - 4m 15s\n",
      "batch: 900/1563 - train loss: 13.5757 - test loss: 14.9037 - train acc: 0.3757 - test acc: 0.3374 - 4m 20s\n",
      "batch: 1000/1563 - train loss: 12.9494 - test loss: 14.9336 - train acc: 0.3956 - test acc: 0.3392 - 4m 25s\n",
      "batch: 1100/1563 - train loss: 13.4077 - test loss: 16.3641 - train acc: 0.3777 - test acc: 0.3120 - 4m 29s\n",
      "batch: 1200/1563 - train loss: 13.3237 - test loss: 14.5142 - train acc: 0.3844 - test acc: 0.3564 - 4m 34s\n",
      "batch: 1300/1563 - train loss: 13.6117 - test loss: 14.5444 - train acc: 0.3756 - test acc: 0.3515 - 4m 38s\n",
      "batch: 1400/1563 - train loss: 13.5260 - test loss: 14.1257 - train acc: 0.3718 - test acc: 0.3706 - 4m 43s\n",
      "batch: 1500/1563 - train loss: 13.3190 - test loss: 14.1988 - train acc: 0.3950 - test acc: 0.3604 - 4m 48s\n",
      "batch: 1563/1563 - train loss: 13.3305 - test loss: 14.0547 - train acc: 0.3890 - test acc: 0.3677 - 4m 52s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 5/100\n",
      "batch: 100/1563 - train loss: 11.4585 - test loss: 14.6565 - train acc: 0.4638 - test acc: 0.3590 - 4m 56s\n",
      "batch: 200/1563 - train loss: 11.6760 - test loss: 15.4553 - train acc: 0.4432 - test acc: 0.3359 - 5m 1s\n",
      "batch: 300/1563 - train loss: 11.8668 - test loss: 15.2979 - train acc: 0.4382 - test acc: 0.3354 - 5m 6s\n",
      "batch: 400/1563 - train loss: 11.8159 - test loss: 14.9299 - train acc: 0.4360 - test acc: 0.3431 - 5m 10s\n",
      "batch: 500/1563 - train loss: 12.0334 - test loss: 14.4923 - train acc: 0.4403 - test acc: 0.3581 - 5m 15s\n",
      "batch: 600/1563 - train loss: 12.1654 - test loss: 14.4431 - train acc: 0.4272 - test acc: 0.3634 - 5m 20s\n",
      "batch: 700/1563 - train loss: 12.1007 - test loss: 14.2290 - train acc: 0.4328 - test acc: 0.3658 - 5m 25s\n",
      "batch: 800/1563 - train loss: 12.2860 - test loss: 14.2341 - train acc: 0.4350 - test acc: 0.3690 - 5m 29s\n",
      "batch: 900/1563 - train loss: 11.7896 - test loss: 15.0770 - train acc: 0.4328 - test acc: 0.3379 - 5m 34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1000/1563 - train loss: 11.8512 - test loss: 14.2027 - train acc: 0.4428 - test acc: 0.3662 - 5m 38s\n",
      "batch: 1100/1563 - train loss: 12.2654 - test loss: 14.5043 - train acc: 0.4288 - test acc: 0.3593 - 5m 43s\n",
      "batch: 1200/1563 - train loss: 11.8573 - test loss: 15.6651 - train acc: 0.4263 - test acc: 0.3259 - 5m 48s\n",
      "batch: 1300/1563 - train loss: 12.1434 - test loss: 15.8447 - train acc: 0.4341 - test acc: 0.3165 - 5m 52s\n",
      "batch: 1400/1563 - train loss: 11.8809 - test loss: 14.2387 - train acc: 0.4463 - test acc: 0.3768 - 5m 57s\n",
      "batch: 1500/1563 - train loss: 12.4886 - test loss: 13.5532 - train acc: 0.4238 - test acc: 0.3895 - 6m 1s\n",
      "batch: 1563/1563 - train loss: 12.1553 - test loss: 13.7930 - train acc: 0.4316 - test acc: 0.3846 - 6m 5s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 6/100\n",
      "batch: 100/1563 - train loss: 10.1557 - test loss: 13.5565 - train acc: 0.5165 - test acc: 0.3932 - 6m 10s\n",
      "batch: 200/1563 - train loss: 10.2892 - test loss: 14.3453 - train acc: 0.5056 - test acc: 0.3698 - 6m 14s\n",
      "batch: 300/1563 - train loss: 10.5262 - test loss: 16.0000 - train acc: 0.4919 - test acc: 0.3336 - 6m 19s\n",
      "batch: 400/1563 - train loss: 10.6161 - test loss: 15.7839 - train acc: 0.4909 - test acc: 0.3402 - 6m 24s\n",
      "batch: 500/1563 - train loss: 10.6139 - test loss: 14.3968 - train acc: 0.4759 - test acc: 0.3754 - 6m 28s\n",
      "batch: 600/1563 - train loss: 10.7266 - test loss: 14.3994 - train acc: 0.4834 - test acc: 0.3722 - 6m 33s\n",
      "batch: 700/1563 - train loss: 10.7255 - test loss: 13.8734 - train acc: 0.4835 - test acc: 0.3900 - 6m 38s\n",
      "batch: 800/1563 - train loss: 10.6529 - test loss: 13.9409 - train acc: 0.4916 - test acc: 0.3854 - 6m 42s\n",
      "batch: 900/1563 - train loss: 10.6745 - test loss: 14.4014 - train acc: 0.4850 - test acc: 0.3710 - 6m 47s\n",
      "batch: 1000/1563 - train loss: 10.9870 - test loss: 13.5540 - train acc: 0.4638 - test acc: 0.4013 - 6m 52s\n",
      "batch: 1100/1563 - train loss: 10.8324 - test loss: 13.7306 - train acc: 0.4822 - test acc: 0.3920 - 6m 56s\n",
      "batch: 1200/1563 - train loss: 10.9110 - test loss: 14.4016 - train acc: 0.4838 - test acc: 0.3744 - 7m 1s\n",
      "batch: 1300/1563 - train loss: 10.9413 - test loss: 14.1405 - train acc: 0.4856 - test acc: 0.3809 - 7m 5s\n",
      "batch: 1400/1563 - train loss: 11.0177 - test loss: 13.8084 - train acc: 0.4707 - test acc: 0.3837 - 7m 10s\n",
      "batch: 1500/1563 - train loss: 11.0558 - test loss: 13.5815 - train acc: 0.4669 - test acc: 0.3950 - 7m 15s\n",
      "batch: 1563/1563 - train loss: 10.8306 - test loss: 13.6044 - train acc: 0.4838 - test acc: 0.3985 - 7m 19s\n",
      "GPU memory used: 0.13 GB - max: 7.56 GB - memory reserved: 0.23 GB - max: 7.62 GB\n",
      "starting epoch: 7/100\n",
      "batch: 100/1563 - train loss: 8.6223 - test loss: 13.7330 - train acc: 0.5697 - test acc: 0.4037 - 7m 23s\n",
      "batch: 200/1563 - train loss: 8.7657 - test loss: 15.0381 - train acc: 0.5637 - test acc: 0.3744 - 7m 28s\n"
     ]
    }
   ],
   "source": [
    "for _ in range(nruns):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    default_metrics, _ = train_network_fisher_optimization(apply_fisher = False,\n",
    "                                                           net_params = {'p': 0.1},\n",
    "                                                           epochs = 100,\n",
    "                                                           time_limit_secs = 1200)\n",
    "\n",
    "    results_list.append( (default_metrics, buffer_size, partition_size, block_updates) )\n",
    "    results_list_to_json(results_list, step=step_i)\n",
    "    step_i += 1\n",
    "    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-29T04:13:13.867971",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
